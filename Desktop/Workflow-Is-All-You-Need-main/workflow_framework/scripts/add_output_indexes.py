#!/usr/bin/env python3
"""
ä¸ºå·¥ä½œæµè¾“å‡ºæ•°æ®å­—æ®µæ·»åŠ æ•°æ®åº“ç´¢å¼•
Add Database Indexes for Workflow Output Data Fields
"""

import asyncio
import sys
import os
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_database


class OutputDataIndexManager:
    """å·¥ä½œæµè¾“å‡ºæ•°æ®ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.db = None
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            self.db = get_database()
            logger.info("âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def add_workflow_output_columns(self):
        """ä¸ºworkflow_instanceè¡¨æ·»åŠ æ–°çš„è¾“å‡ºæ•°æ®åˆ—"""
        try:
            logger.info("ğŸ“‹ å¼€å§‹æ·»åŠ æ–°çš„è¾“å‡ºæ•°æ®å­—æ®µ...")
            
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å·²å­˜åœ¨
            check_columns_query = """
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'workflow_instance' 
            AND column_name IN ('execution_summary', 'quality_metrics', 'data_lineage', 'output_summary')
            """
            
            existing_columns = await self.db.fetch_all(check_columns_query)
            existing_column_names = [row['column_name'] for row in existing_columns]
            
            logger.info(f"ğŸ“Š ç°æœ‰å­—æ®µ: {existing_column_names}")
            
            # éœ€è¦æ·»åŠ çš„å­—æ®µ
            columns_to_add = [
                ('execution_summary', 'JSONB', 'æ‰§è¡Œæ‘˜è¦æ•°æ®'),
                ('quality_metrics', 'JSONB', 'è´¨é‡è¯„ä¼°æŒ‡æ ‡'),
                ('data_lineage', 'JSONB', 'æ•°æ®è¡€ç¼˜ä¿¡æ¯'),
                ('output_summary', 'JSONB', 'ç»“æ„åŒ–è¾“å‡ºæ‘˜è¦')
            ]
            
            # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
            for column_name, column_type, description in columns_to_add:
                if column_name not in existing_column_names:
                    alter_query = f"""
                    ALTER TABLE workflow_instance 
                    ADD COLUMN {column_name} {column_type}
                    """
                    
                    comment_query = f"""
                    COMMENT ON COLUMN workflow_instance.{column_name} IS '{description}'
                    """
                    
                    try:
                        await self.db.execute(alter_query)
                        await self.db.execute(comment_query)
                        logger.info(f"âœ… æ·»åŠ å­—æ®µæˆåŠŸ: {column_name}")
                    except Exception as e:
                        logger.error(f"âŒ æ·»åŠ å­—æ®µå¤±è´¥ {column_name}: {e}")
                        # ç»§ç»­æ·»åŠ å…¶ä»–å­—æ®µ
                        continue
                else:
                    logger.info(f"â­ï¸ å­—æ®µå·²å­˜åœ¨ï¼Œè·³è¿‡: {column_name}")
            
            logger.info("âœ… è¾“å‡ºæ•°æ®å­—æ®µæ·»åŠ å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è¾“å‡ºæ•°æ®å­—æ®µå¼‚å¸¸: {e}")
            return False
    
    async def create_output_data_indexes(self):
        """åˆ›å»ºè¾“å‡ºæ•°æ®ç›¸å…³çš„ç´¢å¼•"""
        try:
            logger.info("ğŸ“Š å¼€å§‹åˆ›å»ºè¾“å‡ºæ•°æ®ç´¢å¼•...")
            
            # å®šä¹‰éœ€è¦åˆ›å»ºçš„ç´¢å¼•
            indexes = [
                # 1. æ‰§è¡Œç»“æœç±»å‹ç´¢å¼• - ç”¨äºæŒ‰ç»“æœç±»å‹ç­›é€‰
                {
                    'name': 'idx_workflow_instance_result_type',
                    'table': 'workflow_instance', 
                    'definition': "USING GIN ((execution_summary->>'execution_result'->>'result_type'))",
                    'description': 'æ‰§è¡Œç»“æœç±»å‹ç´¢å¼•'
                },
                
                # 2. å·¥ä½œæµçŠ¶æ€å’Œç»“æœç±»å‹å¤åˆç´¢å¼•
                {
                    'name': 'idx_workflow_instance_status_result',
                    'table': 'workflow_instance',
                    'definition': "USING GIN (status, (execution_summary->>'execution_result'->>'result_type'))",
                    'description': 'çŠ¶æ€å’Œç»“æœç±»å‹å¤åˆç´¢å¼•'
                },
                
                # 3. è´¨é‡è¯„ä¼°æŒ‡æ ‡ç´¢å¼• - ç”¨äºæŒ‰è´¨é‡åˆ†æ•°ç­›é€‰
                {
                    'name': 'idx_workflow_instance_quality_score',
                    'table': 'workflow_instance',
                    'definition': "USING GIN ((quality_metrics->>'overall_quality_score'))",
                    'description': 'æ•´ä½“è´¨é‡è¯„åˆ†ç´¢å¼•'
                },
                
                # 4. è´¨é‡é—¨ç¦çŠ¶æ€ç´¢å¼•
                {
                    'name': 'idx_workflow_instance_quality_gates',
                    'table': 'workflow_instance',
                    'definition': "USING GIN ((quality_metrics->>'quality_gates_passed'))",
                    'description': 'è´¨é‡é—¨ç¦çŠ¶æ€ç´¢å¼•'
                },
                
                # 5. æ•°æ®æ¥æºç´¢å¼• - ç”¨äºæŒ‰è¾“å…¥æ¥æºç­›é€‰
                {
                    'name': 'idx_workflow_instance_input_sources',
                    'table': 'workflow_instance',
                    'definition': "USING GIN ((data_lineage->>'input_sources'))",
                    'description': 'æ•°æ®è¾“å…¥æ¥æºç´¢å¼•'
                },
                
                # 6. æ‰§è¡Œç»Ÿè®¡ç´¢å¼• - ç”¨äºæŒ‰èŠ‚ç‚¹æ•°é‡ç­›é€‰
                {
                    'name': 'idx_workflow_instance_execution_stats',
                    'table': 'workflow_instance',
                    'definition': "USING GIN (execution_summary->>'execution_stats')",
                    'description': 'æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯ç´¢å¼•'
                },
                
                # 7. åˆ›å»ºæ—¶é—´å’ŒçŠ¶æ€å¤åˆç´¢å¼• - ä¼˜åŒ–æ—¶é—´èŒƒå›´æŸ¥è¯¢
                {
                    'name': 'idx_workflow_instance_created_status',
                    'table': 'workflow_instance',
                    'definition': "USING BTREE (created_at DESC, status)",
                    'description': 'åˆ›å»ºæ—¶é—´å’ŒçŠ¶æ€å¤åˆç´¢å¼•'
                },
                
                # 8. å®Œæˆæ—¶é—´å’Œç»“æœç±»å‹å¤åˆç´¢å¼•
                {
                    'name': 'idx_workflow_instance_completed_result',
                    'table': 'workflow_instance',
                    'definition': "USING BTREE (completed_at DESC) WHERE completed_at IS NOT NULL",
                    'description': 'å®Œæˆæ—¶é—´ç´¢å¼•ï¼ˆéƒ¨åˆ†ç´¢å¼•ï¼‰'
                },
                
                # 9. å·¥ä½œæµåŸºç¡€IDå’ŒçŠ¶æ€å¤åˆç´¢å¼• - ä¼˜åŒ–æŒ‰å·¥ä½œæµç­›é€‰
                {
                    'name': 'idx_workflow_instance_workflow_base_status',
                    'table': 'workflow_instance',
                    'definition': "USING BTREE (workflow_base_id, status, created_at DESC)",
                    'description': 'å·¥ä½œæµåŸºç¡€IDå’ŒçŠ¶æ€å¤åˆç´¢å¼•'
                },
                
                # 10. æ‰§è¡Œè€…å’ŒçŠ¶æ€å¤åˆç´¢å¼• - ä¼˜åŒ–æŒ‰æ‰§è¡Œè€…ç­›é€‰
                {
                    'name': 'idx_workflow_instance_executor_status',
                    'table': 'workflow_instance',
                    'definition': "USING BTREE (executor_id, status, created_at DESC)",
                    'description': 'æ‰§è¡Œè€…å’ŒçŠ¶æ€å¤åˆç´¢å¼•'
                }
            ]
            
            # åˆ›å»ºç´¢å¼•
            for index_info in indexes:
                try:
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
                    check_index_query = """
                    SELECT indexname FROM pg_indexes 
                    WHERE tablename = $1 AND indexname = $2
                    """
                    
                    existing_index = await self.db.fetch_one(
                        check_index_query, 
                        index_info['table'], 
                        index_info['name']
                    )
                    
                    if existing_index:
                        logger.info(f"â­ï¸ ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡: {index_info['name']}")
                        continue
                    
                    # åˆ›å»ºç´¢å¼•
                    create_index_query = f"""
                    CREATE INDEX CONCURRENTLY IF NOT EXISTS {index_info['name']} 
                    ON {index_info['table']} {index_info['definition']}
                    """
                    
                    logger.info(f"ğŸ”¨ åˆ›å»ºç´¢å¼•: {index_info['name']}")
                    logger.info(f"   - è¡¨: {index_info['table']}")
                    logger.info(f"   - å®šä¹‰: {index_info['definition']}")
                    logger.info(f"   - æè¿°: {index_info['description']}")
                    
                    await self.db.execute(create_index_query)
                    logger.info(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ: {index_info['name']}")
                    
                    # æ·»åŠ ç´¢å¼•æ³¨é‡Š
                    try:
                        comment_query = f"""
                        COMMENT ON INDEX {index_info['name']} IS '{index_info['description']}'
                        """
                        await self.db.execute(comment_query)
                    except Exception as comment_error:
                        logger.warning(f"âš ï¸ æ·»åŠ ç´¢å¼•æ³¨é‡Šå¤±è´¥ {index_info['name']}: {comment_error}")
                    
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥ {index_info['name']}: {e}")
                    # ç»§ç»­åˆ›å»ºå…¶ä»–ç´¢å¼•
                    continue
            
            logger.info("âœ… è¾“å‡ºæ•°æ®ç´¢å¼•åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¾“å‡ºæ•°æ®ç´¢å¼•å¼‚å¸¸: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return False
    
    async def analyze_table_statistics(self):
        """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯ï¼Œä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’"""
        try:
            logger.info("ğŸ“ˆ å¼€å§‹åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯...")
            
            tables_to_analyze = [
                'workflow_instance',
                'node_instance', 
                'task_instance'
            ]
            
            for table_name in tables_to_analyze:
                try:
                    analyze_query = f"ANALYZE {table_name}"
                    await self.db.execute(analyze_query)
                    logger.info(f"âœ… è¡¨ç»Ÿè®¡åˆ†æå®Œæˆ: {table_name}")
                except Exception as e:
                    logger.error(f"âŒ è¡¨ç»Ÿè®¡åˆ†æå¤±è´¥ {table_name}: {e}")
            
            logger.info("âœ… è¡¨ç»Ÿè®¡ä¿¡æ¯åˆ†æå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯å¼‚å¸¸: {e}")
            return False
    
    async def show_index_summary(self):
        """æ˜¾ç¤ºç´¢å¼•åˆ›å»ºæ‘˜è¦"""
        try:
            logger.info("ğŸ“‹ è¾“å‡ºæ•°æ®ç´¢å¼•åˆ›å»ºæ‘˜è¦:")
            logger.info("=" * 60)
            
            # æŸ¥è¯¢workflow_instanceè¡¨çš„æ‰€æœ‰ç´¢å¼•
            index_query = """
            SELECT 
                indexname,
                indexdef,
                pg_size_pretty(pg_relation_size(indexname::regclass)) as size
            FROM pg_indexes 
            WHERE tablename = 'workflow_instance'
            AND indexname LIKE 'idx_workflow_instance_%'
            ORDER BY indexname
            """
            
            indexes = await self.db.fetch_all(index_query)
            
            logger.info(f"ğŸ“Š workflow_instanceè¡¨ç›¸å…³ç´¢å¼• ({len(indexes)} ä¸ª):")
            for index in indexes:
                logger.info(f"  - {index['indexname']} ({index['size']})")
            
            # æŸ¥è¯¢è¡¨å¤§å°
            table_size_query = """
            SELECT 
                pg_size_pretty(pg_total_relation_size('workflow_instance')) as total_size,
                pg_size_pretty(pg_relation_size('workflow_instance')) as table_size
            """
            
            size_info = await self.db.fetch_one(table_size_query)
            if size_info:
                logger.info(f"ğŸ“ è¡¨å¤§å°ä¿¡æ¯:")
                logger.info(f"  - è¡¨å¤§å°: {size_info['table_size']}")
                logger.info(f"  - æ€»å¤§å°(å«ç´¢å¼•): {size_info['total_size']}")
            
            logger.info("=" * 60)
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ˜¾ç¤ºç´¢å¼•æ‘˜è¦å¼‚å¸¸: {e}")
            return False
    
    async def run_complete_setup(self):
        """è¿è¡Œå®Œæ•´çš„è¾“å‡ºæ•°æ®ç´¢å¼•è®¾ç½®"""
        try:
            logger.info("ğŸš€ å¼€å§‹å·¥ä½œæµè¾“å‡ºæ•°æ®ç´¢å¼•å®Œæ•´è®¾ç½®")
            logger.info("=" * 60)
            
            # 1. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if not await self.initialize():
                return False
            
            # 2. æ·»åŠ æ–°çš„è¾“å‡ºæ•°æ®å­—æ®µ
            if not await self.add_workflow_output_columns():
                logger.error("âŒ æ·»åŠ è¾“å‡ºæ•°æ®å­—æ®µå¤±è´¥")
                return False
            
            # 3. åˆ›å»ºç´¢å¼•
            if not await self.create_output_data_indexes():
                logger.error("âŒ åˆ›å»ºè¾“å‡ºæ•°æ®ç´¢å¼•å¤±è´¥")
                return False
            
            # 4. åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
            if not await self.analyze_table_statistics():
                logger.warning("âš ï¸ è¡¨ç»Ÿè®¡åˆ†æå¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»è¦åŠŸèƒ½")
            
            # 5. æ˜¾ç¤ºæ‘˜è¦
            await self.show_index_summary()
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ å·¥ä½œæµè¾“å‡ºæ•°æ®ç´¢å¼•è®¾ç½®å®Œæˆ!")
            logger.info("âœ¨ æ–°åŠŸèƒ½:")
            logger.info("  - æ”¯æŒæŒ‰æ‰§è¡Œç»“æœç±»å‹ç­›é€‰å·¥ä½œæµ")
            logger.info("  - æ”¯æŒæŒ‰è´¨é‡è¯„åˆ†ç­›é€‰å·¥ä½œæµ") 
            logger.info("  - æ”¯æŒæŒ‰æ•°æ®æ¥æºç­›é€‰å·¥ä½œæµ")
            logger.info("  - ä¼˜åŒ–äº†æ—¶é—´èŒƒå›´æŸ¥è¯¢æ€§èƒ½")
            logger.info("  - ä¼˜åŒ–äº†å¤åˆæ¡ä»¶æŸ¥è¯¢æ€§èƒ½")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´è®¾ç½®è¿‡ç¨‹å¼‚å¸¸: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return False


async def main():
    """ä¸»å‡½æ•°"""
    manager = OutputDataIndexManager()
    success = await manager.run_complete_setup()
    
    if success:
        logger.info("âœ… è„šæœ¬æ‰§è¡ŒæˆåŠŸ")
        return 0
    else:
        logger.error("âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥")
        return 1


if __name__ == "__main__":
    import sys
    
    logger.info("å¼€å§‹æ‰§è¡Œå·¥ä½œæµè¾“å‡ºæ•°æ®ç´¢å¼•è®¾ç½®è„šæœ¬...")
    exit_code = asyncio.run(main())
    sys.exit(exit_code)