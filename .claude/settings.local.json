{
  "$schema": "https://json.schemastore.org/claude-code-settings.json",
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(ss:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v -s)",
      "Bash(grep:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest test_simple_connection.py -v -s)",
      "Bash(ls:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe check_environment.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_db_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_config_loading.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python tests/check_environment.py)",
      "Bash(python3:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_environment.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/unit/test_user_repository.py -v)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py -v)",
      "Bash(find:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe workflow_framework/scripts/init_database.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/simple_db_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_tables.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/debug_user_query.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/test_user_creation.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/test_exact_sql.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_processor_table.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/fix_processor_table.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v --tb=short)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py -v --tb=short)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v --tb=line)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_node_table.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_node_view.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/add_execution_tables.py)",
      "Bash(cp:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pip install pytest pytest-asyncio pytest-cov)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncpg\nimport asyncio\n\nasync def check_tables():\n    conn = await asyncpg.connect(\n        host=''127.0.0.1'', port=5432, database=''workflow_db'',\n        user=''postgres'', password=''postgresql''\n    )\n    \n    # 检查表是否存在\n    tables = await conn.fetch(''''''\n        SELECT table_name \n        FROM information_schema.tables \n        WHERE table_schema = ''public'' \n        ORDER BY table_name\n    '''''')\n    \n    print(''Existing tables:'')\n    for table in tables:\n        print(f''  - {table[0]}'')\n    \n    await conn.close()\n\nasyncio.run(check_tables())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pip install httpx)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_test.py)",
      "Bash(pkill:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_workflow_api.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe complete_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe enhanced_test.py)",
      "Bash(touch:*)",
      "Bash(python:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe comprehensive_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe /mnt/d/HuaweiMoveData/Users/Dr.Tom_Great/Desktop/final/execution_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe execution_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_execution_test.py)",
      "Bash(\"/mnt/c/Program Files/PostgreSQL/17/bin/psql.exe\" -h localhost -U postgres -d workflow_db -f update_execution_schema.sql)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe update_schema.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe final_schema_fix.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport sys\nimport os\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\nfrom workflow_framework.utils.database import db_manager\n\nasync def add_instance_name():\n    await db_manager.initialize()\n    await db_manager.execute(''ALTER TABLE workflow_instance ADD COLUMN IF NOT EXISTS instance_name VARCHAR(255)'')\n    await db_manager.execute(''UPDATE workflow_instance SET instance_name = workflow_instance_name WHERE instance_name IS NULL'')\n    print(''[OK] Added instance_name field'')\n    await db_manager.close()\n\nasyncio.run(add_instance_name())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe add_instance_name.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe comprehensive_schema_fix.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys, os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\n\nrepo = NodeRepository()\nprint(''Available methods:'', [m for m in dir(repo) if not m.startswith(''_'')])\nprint(''Has get_workflow_connections:'', hasattr(repo, ''get_workflow_connections''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport importlib\nimport sys, os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.repositories.node import node_repository\nimportlib.reload(node_repository)\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\n\nrepo = NodeRepository()\nprint(''Methods after reload:'', [m for m in dir(repo) if ''connection'' in m.lower()])\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys, os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\n\nrepo = NodeRepository()\nprint(''NodeRepository methods with connection:'', [m for m in dir(repo) if ''connection'' in m.lower()])\nprint(''Has get_workflow_connections:'', hasattr(repo, ''get_workflow_connections''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m py_compile /mnt/d/HuaweiMoveData/Users/Dr.Tom_Great/Desktop/final/workflow_framework/repositories/node/node_repository.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m py_compile workflow_framework/repositories/node/node_repository.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nimport importlib\nsys.path.insert(0, ''.'')\n\n# Force reload the module\nif ''workflow_framework.repositories.node.node_repository'' in sys.modules:\n    importlib.reload(sys.modules[''workflow_framework.repositories.node.node_repository''])\n\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\nrepo = NodeRepository()\nprint(''Available methods:'', [m for m in dir(repo) if not m.startswith(''__'') and ''get_'' in m])\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\nrepo = ProcessorRepository()\nprint(''ProcessorRepository methods:'', [m for m in dir(repo) if ''get_processors'' in m])\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\n\")",
      "Bash(true)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m py_compile workflow_framework/repositories/processor/processor_repository.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\nimport inspect\n\nrepo = ProcessorRepository()\nmethods = [name for name, method in inspect.getmembers(repo, predicate=inspect.iscoroutinefunction)]\nprint(''All async methods:'', methods)\nprint(''Has get_processors_by_node:'', ''get_processors_by_node'' in methods)\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nimport importlib\nimport workflow_framework.repositories.processor.processor_repository as mod\nimportlib.reload(mod)\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\nprint(''Method type:'', type(getattr(repo, ''get_processors_by_node'', None)))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_processor_method.py)",
      "Bash(sed:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\nprint(''Has get_node_processors:'', hasattr(repo, ''get_node_processors''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe processor_methods_fix.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\nif hasattr(repo, ''get_processors_by_node''):\n    import inspect\n    sig = inspect.signature(repo.get_processors_by_node)\n    print(''Method signature:'', sig)\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Available methods:'', [m for m in dir(repo) if ''get_'' in m and not m.startswith(''_'')])\nprint(''Has get_node_processors:'', hasattr(repo, ''get_node_processors''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe check_task_schema.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport sys\nimport os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.utils.database import db_manager\n\nasync def add_missing_field():\n    await db_manager.initialize()\n    await db_manager.execute(''ALTER TABLE task_instance ADD COLUMN IF NOT EXISTS workflow_instance_id UUID'')\n    print(''Added workflow_instance_id field to task_instance table'')\n    await db_manager.close()\n\nasyncio.run(add_missing_field())\n\")",
      "Bash(/mnt d/anaconda3/envs/fornew/python.exe simple_execution_test.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" simple_execution_test.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_task_schema.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_workflow_instance_schema.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_foreign_keys.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_node_methods.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -m py_compile /mnt/d/HuaweiMoveData/Users/Dr.Tom_Great/Desktop/final/workflow_framework/repositories/node/node_repository.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sys\nsys.path.insert(0, ''.'')\ntry:\n    from workflow_framework.repositories.node.node_repository import NodeRepository\n    print(''Import successful'')\n    repo = NodeRepository()\n    print(''NodeRepository instantiated'')\n    print(''Available methods:'')\n    for method in dir(repo):\n        if ''connection'' in method.lower():\n            print(f''  {method}'')\nexcept Exception as e:\n    print(f''Error: {e}'')\n    import traceback\n    traceback.print_exc()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" add_assigned_user_field.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" add_output_data_field.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" test_permanent_methods.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_processor_integration.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_processor_test.py)",
      "Bash(cd /Users/chenshuchen/Desktop/final/frontend)",
      "Bash(npm run build)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python --version)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python -c \"import sys; print(''Python路径:'', sys.executable)\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe --version)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"import fastapi, uvicorn, asyncpg, pydantic; print(''✅ 主要依赖已安装'')\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"import fastapi, uvicorn, asyncpg, pydantic; print(''Main dependencies installed'')\")",
      "Bash(curl:*)",
      "Bash(timeout:*)",
      "Bash(node:*)",
      "Bash(npm:*)",
      "Bash(cat:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python -c \"import asyncpg; print(''asyncpg版本:'', asyncpg.__version__)\")",
      "Bash(pip3 install:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"import sys; print(''Python版本:'', sys.version)\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\ntry:\n    import asyncpg\n    print(''✅ asyncpg已安装，版本:'', asyncpg.__version__)\nexcept ImportError:\n    print(''❌ asyncpg未安装'')\n    \ntry:\n    import psycopg2\n    print(''✅ psycopg2已安装，版本:'', psycopg2.__version__)\nexcept ImportError:\n    print(''❌ psycopg2未安装'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\ntry:\n    import asyncpg\n    print(''asyncpg已安装，版本:'', asyncpg.__version__)\nexcept ImportError:\n    print(''asyncpg未安装'')\n    \ntry:\n    import psycopg2\n    print(''psycopg2已安装，版本:'', psycopg2.__version__)\nexcept ImportError:\n    print(''psycopg2未安装'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport asyncpg\nfrom workflow_framework.config import get_settings\n\nasync def test_db():\n    settings = get_settings()\n    print(''数据库配置:'')\n    print(''  主机:'', settings.database.host)\n    print(''  端口:'', settings.database.port)\n    print(''  数据库:'', settings.database.database)\n    print(''  用户:'', settings.database.username)\n    \n    try:\n        conn = await asyncpg.connect(\n            host=settings.database.host,\n            port=settings.database.port,\n            user=settings.database.username,\n            password=settings.database.password,\n            database=settings.database.database,\n            timeout=5\n        )\n        print(''数据库连接成功!'')\n        await conn.close()\n    except Exception as e:\n        print(''数据库连接失败:'', str(e))\n\nasyncio.run(test_db())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nfrom workflow_framework.utils.database import db_manager\n\nasync def test_user_table():\n    try:\n        # 测试用户表查询\n        result = await db_manager.fetch_all(''SELECT * FROM \"\"user\"\" LIMIT 5'')\n        print(''用户表查询成功，记录数:'', len(result))\n        for user in result:\n            print(''用户:'', user.get(''username'', ''N/A''))\n    except Exception as e:\n        print(''用户表查询失败:'', str(e))\n        \n        # 检查表是否存在\n        try:\n            tables = await db_manager.fetch_all(\"\"\"\"\"\"\n                SELECT table_name FROM information_schema.tables \n                WHERE table_schema = ''public''\n            \"\"\"\"\"\")\n            print(''数据库中的表:'')\n            for table in tables:\n                print(''  -'', table[''table_name''])\n        except Exception as e2:\n            print(''无法查询表结构:'', str(e2))\n\nasyncio.run(test_user_table())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\ntry:\n    import aiohttp\n    print(''aiohttp已安装'')\nexcept ImportError:\n    print(''需要安装aiohttp'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_api_simple.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nfrom workflow_framework.config import get_settings\n\nasync def check_db_settings():\n    settings = get_settings()\n    print(''当前数据库配置:'')\n    print(f''  主机: {settings.database.host}'')\n    print(f''  端口: {settings.database.port}'')  \n    print(f''  数据库: {settings.database.database}'')\n    print(f''  用户: {settings.database.username}'')\n    print(f''  密码: {\"\"有\"\" if settings.database.password else \"\"无\"\"}'')\n\nasyncio.run(check_db_settings())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport sys\nsys.path.append(''.'')\nfrom workflow_framework.api.processor import router\nprint(''Processor API路由已导入成功'')\nprint(''可用路由:'')\nfor route in router.routes:\n    print(f''  {route.methods} {route.path}'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python -c \"\nimport sys\nsys.path.append(''.'')\nfrom workflow_framework.api.processor import router\nprint(''Processor API路由已导入成功'')\nprint(''可用路由:'')\nfor route in router.routes:\n    print(f''  {route.methods} {route.path}'')\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sys\nsys.path.append(''.'')\ntry:\n    from workflow_framework.api.processor import router\n    print(''Processor路由导入成功'')\n    print(''路由数量:'', len(router.routes))\n    for route in router.routes:\n        print(f''  {route.methods} {route.path} - {route.name}'')\nexcept Exception as e:\n    print(''导入失败:'', e)\n    import traceback\n    traceback.print_exc()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def test_user_repo():\n    try:\n        from workflow_framework.repositories.user.user_repository import UserRepository\n        from workflow_framework.utils.database import initialize_database\n        \n        print(''初始化数据库连接...'')\n        await initialize_database()\n        \n        print(''创建用户仓库实例...'')\n        user_repo = UserRepository()\n        \n        print(''获取活跃用户...'')\n        users = await user_repo.get_all_active_users()\n        print(f''找到 {len(users)} 个用户'')\n        \n        for user in users[:2]:  # 只显示前2个用户\n            print(f''  用户: {user.get(\"\"username\"\", \"\"Unknown\"\")} - {user.get(\"\"email\"\", \"\"Unknown\"\")}'')\n            \n    except Exception as e:\n        print(f''错误: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(test_user_repo())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sys\nsys.path.append(''.'')\n\n# 检查路由定义\nprint(''=== 检查路由定义 ==='')\nfrom workflow_framework.api.processor import router\nfor route in router.routes:\n    if ''POST'' in route.methods:\n        print(f''POST路由: {route.path} - {route.name}'')\n\n# 检查main.py中的路由注册\nprint(''\\n=== 检查main.py路由注册 ==='')\ntry:\n    from main import app\n    all_routes = []\n    for route in app.routes:\n        if hasattr(route, ''methods'') and hasattr(route, ''path''):\n            if ''POST'' in route.methods and ''processors'' in route.path:\n                all_routes.append(f''{route.methods} {route.path}'')\n    \n    if all_routes:\n        print(''找到POST processors路由:'')\n        for route in all_routes:\n            print(f''  {route}'')\n    else:\n        print(''未找到POST processors路由在main app中'')\n        \n    # 显示所有processors相关路由\n    print(''\\n=== 所有processors相关路由 ==='')\n    processor_routes = []\n    for route in app.routes:\n        if hasattr(route, ''path'') and ''processors'' in route.path:\n            methods = getattr(route, ''methods'', set())\n            processor_routes.append(f''{methods} {route.path}'')\n    \n    for route in processor_routes:\n        print(f''  {route}'')\n        \nexcept Exception as e:\n    print(f''检查main.py失败: {e}'')\n    import traceback\n    traceback.print_exc()\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def test_processor_creation():\n    try:\n        from workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n        from workflow_framework.models.processor import ProcessorCreate, ProcessorType\n        import uuid\n        \n        # 模拟数据库连接\n        from workflow_framework.utils.database import initialize_database\n        await initialize_database()\n        \n        repo = ProcessorRepository()\n        \n        # 测试创建处理器数据\n        test_data = ProcessorCreate(\n            name=''Test Processor'',\n            type=ProcessorType.HUMAN,\n            user_id=uuid.uuid4()  # 模拟用户ID\n        )\n        \n        print(''测试数据:'', test_data.model_dump())\n        \n        # 尝试创建处理器\n        result = await repo.create_processor(test_data)\n        print(''创建结果:'', result)\n        \n    except Exception as e:\n        print(f''测试错误: {e}'')\n        import traceback\n        print(''详细错误:'', traceback.format_exc())\n\n# 运行测试\nasyncio.run(test_processor_creation())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport asyncio\nimport asyncpg\nfrom workflow_framework.models.node import NodeUpdate\n\n# Test node update validation\ntry:\n    # Valid update data - similar to start/end nodes\n    update_data = NodeUpdate(\n        name=''1'',\n        task_description=None,\n        position_x=100,\n        position_y=100\n    )\n    print(''Start/End node update data validates successfully'')\n    print(f''Name: {update_data.name}'')\n    print(f''Description: {update_data.task_description}'')\n    print(f''Position: ({update_data.position_x}, {update_data.position_y})'')\n    \n    # Try processor-style data\n    processor_data = NodeUpdate(\n        name=''hhh'',\n        task_description=''Some description'',\n        position_x=200,\n        position_y=200\n    )\n    print(''\\nProcessor node update data validates successfully'')\n    print(f''Name: {processor_data.name}'')\n    print(f''Description: {processor_data.task_description}'')\n    print(f''Position: ({processor_data.position_x}, {processor_data.position_y})'')\n    \nexcept Exception as e:\n    print(f''Validation error: {e}'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python check_server_routes.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_server_routes.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" live_debug.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_execution_fix.py)",
      "Bash(rg:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_node_api.py)",
      "Bash(pgrep:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_processor_update.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" test_delayed_task_creation.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_node_schema.py)",
      "Bash(git checkout:*)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(''workflow.db'')\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\ncursor.execute(''''''\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != ''{}'' \nORDER BY created_at DESC \nLIMIT 3\n'''''')\n\nresults = cursor.fetchall()\nprint(''=== 数据库中的context_data检查 ==='')\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f''任务ID: {task_id}'')\n    print(f''任务标题: {title}'')\n    print(f''Context Data类型: {type(context_data)}'')\n    print(f''Context Data内容: {context_data[:200]}...'' if len(str(context_data)) > 200 else f''Context Data内容: {context_data}'')\n    print(f''创建时间: {created_at}'')\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            print(f''JSON解析成功，包含字段: {list(parsed.keys()) if isinstance(parsed, dict) else \"\"非字典类型\"\"}'')\n        else:\n            print(f''Context data不是字符串类型: {type(context_data)}'')\n    except Exception as e:\n        print(f''JSON解析失败: {e}'')\n    print(''-'' * 50)\n\nconn.close()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(''workflow.db'')\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\ncursor.execute(''''''\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != ''{}'' \nORDER BY created_at DESC \nLIMIT 3\n'''''')\n\nresults = cursor.fetchall()\nprint(''=== 数据库中的context_data检查 ==='')\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f''任务ID: {task_id}'')\n    print(f''任务标题: {title}'')\n    print(f''Context Data类型: {type(context_data)}'')\n    if context_data:\n        content_preview = str(context_data)[:200] + ''...'' if len(str(context_data)) > 200 else str(context_data)\n        print(f''Context Data内容: {content_preview}'')\n    print(f''创建时间: {created_at}'')\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            if isinstance(parsed, dict):\n                print(f''JSON解析成功，包含字段: {list(parsed.keys())}'')\n            else:\n                print(f''JSON解析成功，但不是字典类型: {type(parsed)}'')\n        else:\n            print(f''Context data不是字符串类型: {type(context_data)}'')\n    except Exception as e:\n        print(f''JSON解析失败: {e}'')\n    print(''-'' * 50)\n\nconn.close()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c '\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(\"\"workflow.db\"\")\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\ncursor.execute(\"\"\"\"\"\"\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != \"\"{}\"\" \nORDER BY created_at DESC \nLIMIT 3\n\"\"\"\"\"\")\n\nresults = cursor.fetchall()\nprint(\"\"=== 数据库中的context_data检查 ===\"\")\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f\"\"任务ID: {task_id}\"\")\n    print(f\"\"任务标题: {title}\"\")\n    print(f\"\"Context Data类型: {type(context_data)}\"\")\n    if context_data:\n        content_preview = str(context_data)[:200] + \"\"...\"\" if len(str(context_data)) > 200 else str(context_data)\n        print(f\"\"Context Data内容: {content_preview}\"\")\n    print(f\"\"创建时间: {created_at}\"\")\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            if isinstance(parsed, dict):\n                print(f\"\"JSON解析成功，包含字段: {list(parsed.keys())}\"\")\n            else:\n                print(f\"\"JSON解析成功，但不是字典类型: {type(parsed)}\"\")\n        else:\n            print(f\"\"Context data不是字符串类型: {type(context_data)}\"\")\n    except Exception as e:\n        print(f\"\"JSON解析失败: {e}\"\")\n    print(\"\"-\"\" * 50)\n\nconn.close()\n')",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c '\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(\"\"workflow.db\"\")\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\nquery = \"\"\"\"\"\"\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != \"\"{}\"\" \nORDER BY created_at DESC \nLIMIT 3\n\"\"\"\"\"\"\n\ncursor.execute(query)\nresults = cursor.fetchall()\nprint(\"\"=== 数据库中的context_data检查 ===\"\")\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f\"\"任务ID: {task_id}\"\")\n    print(f\"\"任务标题: {title}\"\")\n    print(f\"\"Context Data类型: {type(context_data)}\"\")\n    if context_data:\n        content_preview = str(context_data)[:200] + \"\"...\"\" if len(str(context_data)) > 200 else str(context_data)\n        print(f\"\"Context Data内容: {content_preview}\"\")\n    print(f\"\"创建时间: {created_at}\"\")\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            if isinstance(parsed, dict):\n                print(f\"\"JSON解析成功，包含字段: {list(parsed.keys())}\"\")\n            else:\n                print(f\"\"JSON解析成功，但不是字典类型: {type(parsed)}\"\")\n        else:\n            print(f\"\"Context data不是字符串类型: {type(context_data)}\"\")\n    except Exception as e:\n        print(f\"\"JSON解析失败: {e}\"\")\n    print(\"\"-\"\" * 50)\n\nconn.close()\n')",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport asyncio\nimport uuid\nfrom workflow_framework.services.node_service import NodeService\n\nasync def test_create_connection():\n    service = NodeService()\n    \n    # Test data that simulates what the frontend sends\n    connection_data = {\n        ''from_node_base_id'': str(uuid.uuid4()),\n        ''to_node_base_id'': str(uuid.uuid4()),\n        ''workflow_base_id'': str(uuid.uuid4()),\n        ''connection_type'': ''normal''\n    }\n    \n    print(''Testing connection creation with data:'', connection_data)\n    \n    # This will likely fail, but we want to see the exact error\n    try:\n        result = await service.create_node_connection(connection_data, uuid.uuid4())\n        print(''Success:'', result)\n    except Exception as e:\n        print(''Error type:'', type(e).__name__)\n        print(''Error message:'', str(e))\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(test_create_connection())\n\")",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\nfrom pathlib import Path\n\nasync def apply_migration():\n    try:\n        # 读取SQL迁移脚本\n        sql_file = Path(''update_task_fields_to_text.sql'')\n        if not sql_file.exists():\n            print(''迁移脚本不存在'')\n            return\n            \n        migration_sql = sql_file.read_text(encoding=''utf-8'')\n        print(f''读取迁移脚本: {len(migration_sql)} 字符'')\n        \n        # 连接数据库\n        conn = await asyncpg.connect(''postgresql://postgres:123456@localhost:5432/workflow_framework'')\n        print(''数据库连接成功'')\n        \n        # 执行迁移\n        print(''开始执行数据库迁移...'')\n        await conn.execute(migration_sql)\n        print(''✅ 数据库迁移执行成功!'')\n        \n        # 验证字段类型\n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''task_instance'' \n            AND column_name IN (''input_data'', ''output_data'', ''context_data'')\n            ORDER BY column_name\n        '''''')\n        \n        print(''\\n字段类型验证:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        print(''数据库连接已关闭'')\n        \n    except Exception as e:\n        print(f''迁移失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(apply_migration())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\nfrom pathlib import Path\n\nasync def apply_migration():\n    try:\n        # 读取SQL迁移脚本\n        sql_file = Path(''update_task_fields_to_text.sql'')\n        if not sql_file.exists():\n            print(''迁移脚本不存在'')\n            return\n            \n        migration_sql = sql_file.read_text(encoding=''utf-8'')\n        print(f''读取迁移脚本: {len(migration_sql)} 字符'')\n        \n        # 连接到正确的数据库\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        print(''数据库连接成功'')\n        \n        # 执行迁移 - 分步执行以避免语法错误\n        print(''开始执行数据库迁移...'')\n        \n        # 1. 更新input_data字段类型\n        try:\n            await conn.execute(''ALTER TABLE task_instance ALTER COLUMN input_data TYPE TEXT USING input_data::TEXT;'')\n            print(''✅ input_data字段类型更新成功'')\n        except Exception as e:\n            print(f''input_data字段更新失败: {e}'')\n        \n        # 2. 更新output_data字段类型\n        try:\n            await conn.execute(''ALTER TABLE task_instance ALTER COLUMN output_data TYPE TEXT USING output_data::TEXT;'')\n            print(''✅ output_data字段类型更新成功'') \n        except Exception as e:\n            print(f''output_data字段更新失败: {e}'')\n            \n        # 3. 添加context_data字段\n        try:\n            await conn.execute(''ALTER TABLE task_instance ADD COLUMN IF NOT EXISTS context_data TEXT;'')\n            print(''✅ context_data字段添加成功'')\n        except Exception as e:\n            print(f''context_data字段添加失败: {e}'')\n            \n        # 验证字段类型\n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''task_instance'' \n            AND column_name IN (''input_data'', ''output_data'', ''context_data'')\n            ORDER BY column_name\n        '''''')\n        \n        print(''\\n字段类型验证:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        print(''数据库连接已关闭'')\n        print(''\\n✅ 数据库迁移完成!'')\n        \n    except Exception as e:\n        print(f''迁移失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(apply_migration())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_schema():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        result = await conn.fetch(''''''\n            SELECT column_name, data_type, is_nullable\n            FROM information_schema.columns \n            WHERE table_name = ''task_instance'' \n            AND column_name IN (''input_data'', ''output_data'', ''context_data'', ''priority'', ''instructions'')\n            ORDER BY column_name\n        '''''')\n        \n        print(''task_instance table field types:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]} (nullable: {row[\"\"is_nullable\"\"]})'')\n            \n        await conn.close()\n        print(''Schema check completed successfully'')\n        \n    except Exception as e:\n        print(f''Schema check failed: {e}'')\n\nasyncio.run(check_schema())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom workflow_framework.services.agent_task_service import agent_task_service\nfrom workflow_framework.utils.database import initialize_database\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\nfrom workflow_framework.models.instance import TaskInstanceCreate, TaskInstanceType, TaskInstanceUpdate, TaskInstanceStatus\nimport uuid\n\nasync def test_agent_task():\n    await initialize_database()\n    \n    # Create a simple task with text data\n    task_data = TaskInstanceCreate(\n        node_instance_id=uuid.uuid4(),\n        workflow_instance_id=uuid.uuid4(),\n        processor_id=uuid.uuid4(),\n        task_type=TaskInstanceType.AGENT,\n        task_title=''Test Agent Task'',\n        task_description=''Test Agent task processing with text format'',\n        input_data=''这是一个测试输入'',  # Simple text input\n        context_data=''这是上下文数据'',     # Simple text context\n        assigned_agent_id=uuid.uuid4(),\n        estimated_duration=5\n    )\n    \n    repo = TaskInstanceRepository()\n    \n    # Create task\n    print(''Creating task with text data...'')\n    created_task = await repo.create_task(task_data)\n    if created_task:\n        print(f''Task created successfully: {created_task[\"\"task_instance_id\"\"]}'')\n        \n        # Update task with output (this was failing before)\n        print(''Updating task with output data...'')\n        update_data = TaskInstanceUpdate(\n            status=TaskInstanceStatus.COMPLETED,\n            output_data=''这是Agent的输出结果，现在是文本格式而不是JSON'',\n            result_summary=''任务完成摘要''\n        )\n        \n        updated_task = await repo.update_task(created_task[''task_instance_id''], update_data)\n        \n        if updated_task:\n            print(''SUCCESS: Task updated with text output data!'')\n            print(f''Status: {updated_task[\"\"status\"\"]}'')\n            print(f''Output data type: {type(updated_task[\"\"output_data\"\"])}'')\n            print(f''Output preview: {updated_task[\"\"output_data\"\"][:50]}...'')\n        else:\n            print(''FAILED: Task update failed'')\n    else:\n        print(''FAILED: Task creation failed'')\n\nasyncio.run(test_agent_task())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nfrom workflow_framework.services.agent_task_service import agent_task_service\nfrom workflow_framework.utils.database import initialize_database\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\nfrom workflow_framework.models.instance import TaskInstanceCreate, TaskInstanceType, TaskInstanceUpdate, TaskInstanceStatus\nimport uuid\n\nasync def test_agent_task():\n    await initialize_database()\n    \n    # Create a simple task with text data\n    task_data = TaskInstanceCreate(\n        node_instance_id=uuid.uuid4(),\n        workflow_instance_id=uuid.uuid4(),\n        processor_id=uuid.uuid4(),\n        task_type=TaskInstanceType.AGENT,\n        task_title=''Test Agent Task'',\n        task_description=''Test Agent task processing with text format'',\n        input_data=''This is test input data'',  # Simple text input\n        context_data=''This is context data'',     # Simple text context\n        assigned_agent_id=uuid.uuid4(),\n        estimated_duration=5\n    )\n    \n    repo = TaskInstanceRepository()\n    \n    # Create task\n    print(''Creating task with text data...'')\n    created_task = await repo.create_task(task_data)\n    if created_task:\n        print(f''Task created successfully: {created_task[\"\"task_instance_id\"\"]}'')\n        \n        # Update task with output (this was failing before)\n        print(''Updating task with output data...'')\n        update_data = TaskInstanceUpdate(\n            status=TaskInstanceStatus.COMPLETED,\n            output_data=''This is Agent output result, now in text format instead of JSON'',\n            result_summary=''Task completion summary''\n        )\n        \n        updated_task = await repo.update_task(created_task[''task_instance_id''], update_data)\n        \n        if updated_task:\n            print(''SUCCESS: Task updated with text output data!'')\n            print(f''Status: {updated_task[\"\"status\"\"]}'')\n            print(f''Output data type: {type(updated_task[\"\"output_data\"\"])}'')\n            print(f''Output preview: {updated_task[\"\"output_data\"\"][:50]}...'')\n        else:\n            print(''FAILED: Task update failed'')\n    else:\n        print(''FAILED: Task creation failed'')\n\nasyncio.run(test_agent_task())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_table_structure():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        # Check node_connection table structure\n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''node_connection''\n            ORDER BY ordinal_position\n        '''''')\n        \n        print(''node_connection table columns:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''Error: {e}'')\n\nasyncio.run(check_table_structure())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_join_issue():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试JOIN查询问题 ==='')\n        \n        # 检查最近的node_connection记录\n        connections = await conn.fetch(''''''\n            SELECT from_node_id, to_node_id, workflow_id, created_at\n            FROM node_connection \n            WHERE created_at > NOW() - INTERVAL ''2 hours''\n            ORDER BY created_at DESC\n            LIMIT 5\n        '''''')\n        \n        print(''最近的连接记录:'')\n        for i, conn_row in enumerate(connections):\n            from_id = conn_row[''from_node_id'']\n            to_id = conn_row[''to_node_id'']\n            workflow_id = conn_row[''workflow_id'']\n            \n            print(f''\\n连接 {i+1}:'')\n            print(f''  from_node_id: {from_id}'')\n            print(f''  to_node_id: {to_id}'')  \n            print(f''  workflow_id: {workflow_id}'')\n            \n            # 检查这些ID在node表中是否存在\n            from_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', from_id)\n            to_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', to_id)\n            \n            print(f''  from_node: {from_node[\"\"name\"\"] if from_node else \"\"NOT FOUND\"\"} ({from_node[\"\"type\"\"] if from_node else \"\"N/A\"\"})'')\n            print(f''  to_node: {to_node[\"\"name\"\"] if to_node else \"\"NOT FOUND\"\"} ({to_node[\"\"type\"\"] if to_node else \"\"N/A\"\"})'')\n            \n            # 如果没找到，检查是否是workflow_id匹配问题\n            if not from_node:\n                alt_from = await conn.fetchrow(''SELECT name, type, workflow_id FROM node WHERE node_base_id = $1'', from_id)\n                if alt_from:\n                    print(f''    from_node存在但workflow_id不同: {alt_from[\"\"workflow_id\"\"]} vs {workflow_id}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n\nasyncio.run(debug_join_issue())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_join_issue():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试JOIN查询问题 ==='')\n        \n        # 检查最近的node_connection记录\n        connections = await conn.fetch(''''''\n            SELECT from_node_id, to_node_id, workflow_id, created_at\n            FROM node_connection \n            WHERE created_at > NOW() - INTERVAL ''2 hours''\n            ORDER BY created_at DESC\n            LIMIT 3\n        '''''')\n        \n        print(''最近的连接记录:'')\n        for i, conn_row in enumerate(connections):\n            from_id = conn_row[''from_node_id'']\n            to_id = conn_row[''to_node_id'']\n            workflow_id = conn_row[''workflow_id'']\n            \n            print(f''连接 {i+1}:'')\n            print(f''  from_node_id: {from_id}'')\n            print(f''  to_node_id: {to_id}'')  \n            \n            # 检查这些ID在node表中是否存在\n            from_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', from_id)\n            to_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', to_id)\n            \n            print(f''  from_node: {from_node[\"\"name\"\"] if from_node else \"\"NOT FOUND\"\"}'')\n            print(f''  to_node: {to_node[\"\"name\"\"] if to_node else \"\"NOT FOUND\"\"}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n\nasyncio.run(debug_join_issue())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def test_dependency_query():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        # 获取一个最近的node_instance来测试\n        recent_node = await conn.fetchrow(''''''\n            SELECT node_instance_id, workflow_instance_id, node_id, node_instance_name\n            FROM node_instance \n            WHERE created_at > NOW() - INTERVAL ''2 hours''\n            ORDER BY created_at DESC \n            LIMIT 1\n        '''''')\n        \n        if recent_node:\n            node_instance_id = recent_node[''node_instance_id'']\n            workflow_instance_id = recent_node[''workflow_instance_id'']\n            node_id = recent_node[''node_id'']\n            \n            print(f''测试节点: {recent_node[\"\"node_instance_name\"\"]} ({node_instance_id})'')\n            print(f''对应的node_id: {node_id}'')\n            \n            # 检查是否有对应的连接记录\n            connections = await conn.fetch(''''''\n                SELECT nc.from_node_id, nc.to_node_id\n                FROM node_connection nc\n                WHERE nc.to_node_id = $1 OR nc.from_node_id = $1\n            '''''', node_id)\n            \n            print(f''相关的连接记录数: {len(connections)}'')\n            for conn_row in connections:\n                print(f''  {conn_row[\"\"from_node_id\"\"]} -> {conn_row[\"\"to_node_id\"\"]}'')\n                \n            # 如果有连接，测试依赖查询\n            if connections:\n                dependency_query = ''''''\n                SELECT COUNT(*) as total_dependencies,\n                       COUNT(CASE WHEN upstream_ni.status = ''completed'' THEN 1 END) as completed_dependencies\n                FROM node_connection nc\n                JOIN node_instance ni ON nc.to_node_id = ni.node_id\n                JOIN node_instance upstream_ni ON nc.from_node_id = upstream_ni.node_id\n                WHERE ni.node_instance_id = $1\n                AND ni.workflow_instance_id = $2\n                AND upstream_ni.workflow_instance_id = $2\n                AND ni.is_deleted = FALSE\n                AND upstream_ni.is_deleted = FALSE\n                ''''''\n                \n                result = await conn.fetchrow(dependency_query, node_instance_id, workflow_instance_id)\n                \n                if result:\n                    total_deps = result[''total_dependencies'']\n                    completed_deps = result[''completed_dependencies'']\n                    print(f''依赖查询结果: {completed_deps}/{total_deps} 个依赖已完成'')\n                else:\n                    print(''依赖查询无结果'')\n        else:\n            print(''没有找到最近的node_instance'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''测试失败: {e}'')\n\nasyncio.run(test_dependency_query())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def verify_id_relationship():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== ID关系验证 ==='')\n        \n        # 测试node_connection中的ID对应node表中的哪个字段\n        test_id = ''f9708ae6-e622-4829-8939-2a35037d919a''  # 从上面的数据中取一个\n        \n        # 检查作为node_id\n        as_node_id = await conn.fetchrow(''SELECT name, type FROM node WHERE node_id = $1'', test_id)\n        # 检查作为node_base_id  \n        as_node_base_id = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', test_id)\n        \n        print(f''测试ID: {test_id}'')\n        if as_node_id:\n            print(f''  作为node_id匹配: 是 - {as_node_id[\"\"name\"\"]} ({as_node_id[\"\"type\"\"]})'')\n        else:\n            print(f''  作为node_id匹配: 否'')\n            \n        if as_node_base_id:\n            print(f''  作为node_base_id匹配: 是 - {as_node_base_id[\"\"name\"\"]} ({as_node_base_id[\"\"type\"\"]})'')\n        else:\n            print(f''  作为node_base_id匹配: 否'')\n        \n        print()\n        print(''=== 结论分析 ==='')\n        print(''从数据可以看出:'')\n        print(''1. node表有两个ID字段:'')\n        print(''   - node_id: 节点的版本ID (每个版本唯一)'')\n        print(''   - node_base_id: 节点的基础ID (跨版本共享)'')\n        print(''2. node_connection表使用: node_id (版本级别的连接)'')\n        print(''3. node_instance表使用: node_id (实例对应具体版本)'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''验证失败: {e}'')\n\nasyncio.run(verify_id_relationship())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_node_edit_connection.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport uuid\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\n\nasync def check_task_data():\n    repo = TaskInstanceRepository()\n    \n    # 查询这个工作流实例的任务\n    workflow_instance_id = uuid.UUID(''6a0d2b12-7e2e-414b-8a4b-3a4c24df76fc'')\n    \n    query = ''''''\n    SELECT task_instance_id, task_title, input_data, context_data, output_data, status\n    FROM task_instance \n    WHERE workflow_instance_id = $1 \n    AND is_deleted = FALSE\n    ORDER BY created_at\n    ''''''\n    \n    tasks = await repo.db.fetch_all(query, workflow_instance_id)\n    \n    print(f''工作流 {workflow_instance_id} 的任务数据:'')\n    for i, task in enumerate(tasks, 1):\n        print(f''\\n任务 {i}: {task[\"\"task_title\"\"]}'')\n        print(f''  状态: {task[\"\"status\"\"]}'')\n        print(f''  input_data: {task[\"\"input_data\"\"]}'')\n        print(f''  context_data: {task[\"\"context_data\"\"]}'')\n        print(f''  output_data: {task[\"\"output_data\"\"][:200] if task[\"\"output_data\"\"] else \"\"None\"\"}...'')\n\nasyncio.run(check_task_data())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nfrom workflow_framework.services.workflow_context_manager import WorkflowContextManager\n\nasync def test_unified_system():\n    print(''🔄 测试统一的上下文管理系统...'')\n    \n    manager = WorkflowContextManager()\n    \n    # 测试新方法是否可以调用\n    import uuid\n    test_workflow_id = uuid.uuid4()\n    test_node_id = uuid.uuid4()\n    \n    # 初始化上下文\n    await manager.initialize_workflow_context(test_workflow_id)\n    \n    # 测试获取任务上下文数据\n    context_data = await manager.get_task_context_data(test_workflow_id, test_node_id)\n    \n    print(f''✅ get_task_context_data 方法工作正常'')\n    print(f''  返回数据类型: {type(context_data)}'')\n    print(f''  数据键: {list(context_data.keys())}'')\n    \n    return True\n\nasyncio.run(test_unified_system())\nprint(''🎉 统一上下文管理系统测试成功!'')\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nfrom workflow_framework.services.workflow_context_manager import WorkflowContextManager\n\nasync def test_unified_system():\n    print(''Testing unified context management system...'')\n    \n    manager = WorkflowContextManager()\n    \n    # Test the new method\n    import uuid\n    test_workflow_id = uuid.uuid4()\n    test_node_id = uuid.uuid4()\n    \n    # Initialize context\n    await manager.initialize_workflow_context(test_workflow_id)\n    \n    # Test get_task_context_data method\n    context_data = await manager.get_task_context_data(test_workflow_id, test_node_id)\n    \n    print(''get_task_context_data method works normally'')\n    print(f''Return data type: {type(context_data)}'')\n    print(f''Data keys: {list(context_data.keys())}'')\n    \n    return True\n\nasyncio.run(test_unified_system())\nprint(''Unified context management system test successful!'')\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_user_table():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''user''\n            ORDER BY ordinal_position\n        '''''')\n        \n        print(''user table columns:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''Error: {e}'')\n\nasyncio.run(check_user_table())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_workflow_instance_table():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''workflow_instance''\n            ORDER BY ordinal_position\n        '''''')\n        \n        print(''workflow_instance table columns:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''Error: {e}'')\n\nasyncio.run(check_workflow_instance_table())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport asyncio\nimport uuid\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\n\nasync def check_tasks():\n    repo = TaskInstanceRepository()\n    \n    # 查询提到的任务ID\n    task_ids = [\n        ''183eba7b-160a-437e-9dba-ad0d484126f9'',\n        ''c2cd416c-2c38-4803-8066-4e876ebadb28''\n    ]\n    \n    for task_id_str in task_ids:\n        try:\n            task_id = uuid.UUID(task_id_str)\n            task = await repo.get_task_by_id(task_id)\n            if task:\n                print(f''\\n任务 {task_id_str}:'')\n                print(f''  标题: {task.get(\"\"task_title\"\", \"\"无\"\")}'')\n                print(f''  状态: {task.get(\"\"status\"\", \"\"无\"\")}'')\n                print(f''  类型: {task.get(\"\"task_type\"\", \"\"无\"\")}'')\n                print(f''  分配用户ID: {task.get(\"\"assigned_user_id\"\", \"\"无\"\")}'')\n                print(f''  分配代理ID: {task.get(\"\"assigned_agent_id\"\", \"\"无\"\")}'')\n                print(f''  创建时间: {task.get(\"\"created_at\"\", \"\"无\"\")}'')\n                print(f''  分配时间: {task.get(\"\"assigned_at\"\", \"\"无\"\")}'')\n            else:\n                print(f''\\n任务 {task_id_str}: 未找到'')\n        except Exception as e:\n            print(f''\\n查询任务 {task_id_str} 失败: {e}'')\n\nasyncio.run(check_tasks())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/Scripts/python.exe debug_task_assignment.py)",
      "Bash(sqlite3:*)",
      "Bash(chmod:*)",
      "Bash(./deployment/scripts/test-local.sh:*)",
      "Bash(bash:*)",
      "Bash(dos2unix:*)",
      "Bash(docker-compose:*)",
      "Bash(kill:*)",
      "Bash(powershell.exe:*)",
      "Bash(docker inspect:*)",
      "Bash(docker logs:*)",
      "Bash(docker rm:*)",
      "Bash(docker build:*)",
      "Bash(docker run:*)",
      "Bash(pip install:*)",
      "Bash(mysql:*)",
      "Bash(/home/ubuntu/Workflow-Is-All-You-Need/check_cross_machine_access.sh:*)",
      "Bash(brew services:*)",
      "Bash(/usr/local/mysql/bin/mysql:*)",
      "Bash(sudo:*)",
      "Bash(nginx:*)",
      "Bash(/tmp/test_ip_access.sh:*)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_fixed_subdivision.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python debug_duplicate_task_creation.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python debug_workflow_update_error.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_end_node_integration.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nfrom backend.services.workflow_execution_context import WorkflowContextManager\n\nasync def test_context_data():\n    manager = WorkflowContextManager()\n    \n    # Test with a recent workflow instance\n    import uuid\n    from backend.repositories.instance.workflow_instance_repository import WorkflowInstanceRepository\n    \n    repo = WorkflowInstanceRepository()\n    \n    # Get a recent workflow instance\n    query = ''''''\n        SELECT workflow_instance_id, workflow_instance_name \n        FROM workflow_instance \n        WHERE created_at > NOW() - INTERVAL ''2 hours'' \n        ORDER BY created_at DESC \n        LIMIT 1\n    ''''''\n    \n    result = await repo.db.fetch_one(query)\n    if result:\n        workflow_instance_id = result[''workflow_instance_id'']\n        print(f''Testing context for workflow: {result[\"\"workflow_instance_name\"\"]} ({workflow_instance_id})'')\n        \n        # Get node instances for this workflow\n        node_query = ''''''\n            SELECT node_instance_id, node_instance_name \n            FROM node_instance \n            WHERE workflow_instance_id = $1 \n            ORDER BY created_at DESC \n            LIMIT 1\n        ''''''\n        \n        node_result = await repo.db.fetch_one(node_query, workflow_instance_id)\n        if node_result:\n            node_instance_id = node_result[''node_instance_id'']\n            print(f''Testing context for node: {node_result[\"\"node_instance_name\"\"]} ({node_instance_id})'')\n            \n            # Get context data\n            context_data = await manager.get_task_context_data(workflow_instance_id, node_instance_id)\n            \n            print(f''Context data structure:'')\n            print(f''  Keys: {list(context_data.keys())}'')\n            \n            if ''upstream_outputs'' in context_data:\n                upstream_outputs = context_data[''upstream_outputs'']\n                print(f''  Upstream outputs count: {len(upstream_outputs)}'')\n                \n                for i, (node_name, output) in enumerate(upstream_outputs.items()):\n                    print(f''  Node {i+1}: {node_name}'')\n                    print(f''    Keys: {list(output.keys()) if isinstance(output, dict) else \"\"Non-dict\"\"}'')\n                    if isinstance(output, dict) and ''output_data'' in output:\n                        output_data = output[''output_data'']\n                        print(f''    Output data type: {type(output_data)}'')\n                        print(f''    Output preview: {str(output_data)[:100]}...'')\n        else:\n            print(''No recent node instances found'')\n    else:\n        print(''No recent workflow instances found'')\n\nasyncio.run(test_context_data())\n\")"
    ],
    "deny": []
  }
}