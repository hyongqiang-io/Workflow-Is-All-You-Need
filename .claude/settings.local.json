{
  "$schema": "https://json.schemastore.org/claude-code-settings.json",
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(ss:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v -s)",
      "Bash(grep:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest test_simple_connection.py -v -s)",
      "Bash(ls:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe check_environment.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_db_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_config_loading.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python tests/check_environment.py)",
      "Bash(python3:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_environment.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/unit/test_user_repository.py -v)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py -v)",
      "Bash(find:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe workflow_framework/scripts/init_database.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/simple_db_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_tables.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/debug_user_query.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/test_user_creation.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/test_exact_sql.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_processor_table.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/fix_processor_table.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v --tb=short)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py -v --tb=short)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pytest tests/test_comprehensive_suite.py::TestComprehensiveDataAccessLayer::test_complete_workflow_lifecycle -v --tb=line)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_node_table.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/check_node_view.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe tests/add_execution_tables.py)",
      "Bash(cp:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pip install pytest pytest-asyncio pytest-cov)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncpg\nimport asyncio\n\nasync def check_tables():\n    conn = await asyncpg.connect(\n        host=''127.0.0.1'', port=5432, database=''workflow_db'',\n        user=''postgres'', password=''postgresql''\n    )\n    \n    # 检查表是否存在\n    tables = await conn.fetch(''''''\n        SELECT table_name \n        FROM information_schema.tables \n        WHERE table_schema = ''public'' \n        ORDER BY table_name\n    '''''')\n    \n    print(''Existing tables:'')\n    for table in tables:\n        print(f''  - {table[0]}'')\n    \n    await conn.close()\n\nasyncio.run(check_tables())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m pip install httpx)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_test.py)",
      "Bash(pkill:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_workflow_api.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe complete_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe enhanced_test.py)",
      "Bash(touch:*)",
      "Bash(python:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe comprehensive_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe /mnt/d/HuaweiMoveData/Users/Dr.Tom_Great/Desktop/final/execution_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe execution_test.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_execution_test.py)",
      "Bash(\"/mnt/c/Program Files/PostgreSQL/17/bin/psql.exe\" -h localhost -U postgres -d workflow_db -f update_execution_schema.sql)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe update_schema.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe final_schema_fix.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport sys\nimport os\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\nfrom workflow_framework.utils.database import db_manager\n\nasync def add_instance_name():\n    await db_manager.initialize()\n    await db_manager.execute(''ALTER TABLE workflow_instance ADD COLUMN IF NOT EXISTS instance_name VARCHAR(255)'')\n    await db_manager.execute(''UPDATE workflow_instance SET instance_name = workflow_instance_name WHERE instance_name IS NULL'')\n    print(''[OK] Added instance_name field'')\n    await db_manager.close()\n\nasyncio.run(add_instance_name())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe add_instance_name.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe comprehensive_schema_fix.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys, os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\n\nrepo = NodeRepository()\nprint(''Available methods:'', [m for m in dir(repo) if not m.startswith(''_'')])\nprint(''Has get_workflow_connections:'', hasattr(repo, ''get_workflow_connections''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport importlib\nimport sys, os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.repositories.node import node_repository\nimportlib.reload(node_repository)\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\n\nrepo = NodeRepository()\nprint(''Methods after reload:'', [m for m in dir(repo) if ''connection'' in m.lower()])\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys, os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\n\nrepo = NodeRepository()\nprint(''NodeRepository methods with connection:'', [m for m in dir(repo) if ''connection'' in m.lower()])\nprint(''Has get_workflow_connections:'', hasattr(repo, ''get_workflow_connections''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m py_compile /mnt/d/HuaweiMoveData/Users/Dr.Tom_Great/Desktop/final/workflow_framework/repositories/node/node_repository.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m py_compile workflow_framework/repositories/node/node_repository.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nimport importlib\nsys.path.insert(0, ''.'')\n\n# Force reload the module\nif ''workflow_framework.repositories.node.node_repository'' in sys.modules:\n    importlib.reload(sys.modules[''workflow_framework.repositories.node.node_repository''])\n\nfrom workflow_framework.repositories.node.node_repository import NodeRepository\nrepo = NodeRepository()\nprint(''Available methods:'', [m for m in dir(repo) if not m.startswith(''__'') and ''get_'' in m])\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\nrepo = ProcessorRepository()\nprint(''ProcessorRepository methods:'', [m for m in dir(repo) if ''get_processors'' in m])\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\n\")",
      "Bash(true)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -m py_compile workflow_framework/repositories/processor/processor_repository.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\nimport inspect\n\nrepo = ProcessorRepository()\nmethods = [name for name, method in inspect.getmembers(repo, predicate=inspect.iscoroutinefunction)]\nprint(''All async methods:'', methods)\nprint(''Has get_processors_by_node:'', ''get_processors_by_node'' in methods)\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nimport importlib\nimport workflow_framework.repositories.processor.processor_repository as mod\nimportlib.reload(mod)\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\nprint(''Method type:'', type(getattr(repo, ''get_processors_by_node'', None)))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_processor_method.py)",
      "Bash(sed:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\nprint(''Has get_node_processors:'', hasattr(repo, ''get_node_processors''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe processor_methods_fix.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Has get_processors_by_node:'', hasattr(repo, ''get_processors_by_node''))\nif hasattr(repo, ''get_processors_by_node''):\n    import inspect\n    sig = inspect.signature(repo.get_processors_by_node)\n    print(''Method signature:'', sig)\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport sys\nsys.path.insert(0, ''.'')\nfrom workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n\nrepo = ProcessorRepository()\nprint(''Available methods:'', [m for m in dir(repo) if ''get_'' in m and not m.startswith(''_'')])\nprint(''Has get_node_processors:'', hasattr(repo, ''get_node_processors''))\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe check_task_schema.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport sys\nimport os\nsys.path.insert(0, os.getcwd())\nfrom workflow_framework.utils.database import db_manager\n\nasync def add_missing_field():\n    await db_manager.initialize()\n    await db_manager.execute(''ALTER TABLE task_instance ADD COLUMN IF NOT EXISTS workflow_instance_id UUID'')\n    print(''Added workflow_instance_id field to task_instance table'')\n    await db_manager.close()\n\nasyncio.run(add_missing_field())\n\")",
      "Bash(/mnt d/anaconda3/envs/fornew/python.exe simple_execution_test.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" simple_execution_test.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_task_schema.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_workflow_instance_schema.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_foreign_keys.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_node_methods.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -m py_compile /mnt/d/HuaweiMoveData/Users/Dr.Tom_Great/Desktop/final/workflow_framework/repositories/node/node_repository.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sys\nsys.path.insert(0, ''.'')\ntry:\n    from workflow_framework.repositories.node.node_repository import NodeRepository\n    print(''Import successful'')\n    repo = NodeRepository()\n    print(''NodeRepository instantiated'')\n    print(''Available methods:'')\n    for method in dir(repo):\n        if ''connection'' in method.lower():\n            print(f''  {method}'')\nexcept Exception as e:\n    print(f''Error: {e}'')\n    import traceback\n    traceback.print_exc()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" add_assigned_user_field.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" add_output_data_field.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" test_permanent_methods.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_processor_integration.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe simple_processor_test.py)",
      "Bash(cd /Users/chenshuchen/Desktop/final/frontend)",
      "Bash(npm run build)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python --version)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python -c \"import sys; print(''Python路径:'', sys.executable)\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe --version)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"import fastapi, uvicorn, asyncpg, pydantic; print(''✅ 主要依赖已安装'')\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"import fastapi, uvicorn, asyncpg, pydantic; print(''Main dependencies installed'')\")",
      "Bash(curl:*)",
      "Bash(timeout:*)",
      "Bash(node:*)",
      "Bash(npm:*)",
      "Bash(cat:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python -c \"import asyncpg; print(''asyncpg版本:'', asyncpg.__version__)\")",
      "Bash(pip3 install:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"import sys; print(''Python版本:'', sys.version)\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\ntry:\n    import asyncpg\n    print(''✅ asyncpg已安装，版本:'', asyncpg.__version__)\nexcept ImportError:\n    print(''❌ asyncpg未安装'')\n    \ntry:\n    import psycopg2\n    print(''✅ psycopg2已安装，版本:'', psycopg2.__version__)\nexcept ImportError:\n    print(''❌ psycopg2未安装'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\ntry:\n    import asyncpg\n    print(''asyncpg已安装，版本:'', asyncpg.__version__)\nexcept ImportError:\n    print(''asyncpg未安装'')\n    \ntry:\n    import psycopg2\n    print(''psycopg2已安装，版本:'', psycopg2.__version__)\nexcept ImportError:\n    print(''psycopg2未安装'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport asyncpg\nfrom workflow_framework.config import get_settings\n\nasync def test_db():\n    settings = get_settings()\n    print(''数据库配置:'')\n    print(''  主机:'', settings.database.host)\n    print(''  端口:'', settings.database.port)\n    print(''  数据库:'', settings.database.database)\n    print(''  用户:'', settings.database.username)\n    \n    try:\n        conn = await asyncpg.connect(\n            host=settings.database.host,\n            port=settings.database.port,\n            user=settings.database.username,\n            password=settings.database.password,\n            database=settings.database.database,\n            timeout=5\n        )\n        print(''数据库连接成功!'')\n        await conn.close()\n    except Exception as e:\n        print(''数据库连接失败:'', str(e))\n\nasyncio.run(test_db())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nfrom workflow_framework.utils.database import db_manager\n\nasync def test_user_table():\n    try:\n        # 测试用户表查询\n        result = await db_manager.fetch_all(''SELECT * FROM \"\"user\"\" LIMIT 5'')\n        print(''用户表查询成功，记录数:'', len(result))\n        for user in result:\n            print(''用户:'', user.get(''username'', ''N/A''))\n    except Exception as e:\n        print(''用户表查询失败:'', str(e))\n        \n        # 检查表是否存在\n        try:\n            tables = await db_manager.fetch_all(\"\"\"\"\"\"\n                SELECT table_name FROM information_schema.tables \n                WHERE table_schema = ''public''\n            \"\"\"\"\"\")\n            print(''数据库中的表:'')\n            for table in tables:\n                print(''  -'', table[''table_name''])\n        except Exception as e2:\n            print(''无法查询表结构:'', str(e2))\n\nasyncio.run(test_user_table())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\ntry:\n    import aiohttp\n    print(''aiohttp已安装'')\nexcept ImportError:\n    print(''需要安装aiohttp'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe test_api_simple.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nfrom workflow_framework.config import get_settings\n\nasync def check_db_settings():\n    settings = get_settings()\n    print(''当前数据库配置:'')\n    print(f''  主机: {settings.database.host}'')\n    print(f''  端口: {settings.database.port}'')  \n    print(f''  数据库: {settings.database.database}'')\n    print(f''  用户: {settings.database.username}'')\n    print(f''  密码: {\"\"有\"\" if settings.database.password else \"\"无\"\"}'')\n\nasyncio.run(check_db_settings())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport sys\nsys.path.append(''.'')\nfrom workflow_framework.api.processor import router\nprint(''Processor API路由已导入成功'')\nprint(''可用路由:'')\nfor route in router.routes:\n    print(f''  {route.methods} {route.path}'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python -c \"\nimport sys\nsys.path.append(''.'')\nfrom workflow_framework.api.processor import router\nprint(''Processor API路由已导入成功'')\nprint(''可用路由:'')\nfor route in router.routes:\n    print(f''  {route.methods} {route.path}'')\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sys\nsys.path.append(''.'')\ntry:\n    from workflow_framework.api.processor import router\n    print(''Processor路由导入成功'')\n    print(''路由数量:'', len(router.routes))\n    for route in router.routes:\n        print(f''  {route.methods} {route.path} - {route.name}'')\nexcept Exception as e:\n    print(''导入失败:'', e)\n    import traceback\n    traceback.print_exc()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def test_user_repo():\n    try:\n        from workflow_framework.repositories.user.user_repository import UserRepository\n        from workflow_framework.utils.database import initialize_database\n        \n        print(''初始化数据库连接...'')\n        await initialize_database()\n        \n        print(''创建用户仓库实例...'')\n        user_repo = UserRepository()\n        \n        print(''获取活跃用户...'')\n        users = await user_repo.get_all_active_users()\n        print(f''找到 {len(users)} 个用户'')\n        \n        for user in users[:2]:  # 只显示前2个用户\n            print(f''  用户: {user.get(\"\"username\"\", \"\"Unknown\"\")} - {user.get(\"\"email\"\", \"\"Unknown\"\")}'')\n            \n    except Exception as e:\n        print(f''错误: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(test_user_repo())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sys\nsys.path.append(''.'')\n\n# 检查路由定义\nprint(''=== 检查路由定义 ==='')\nfrom workflow_framework.api.processor import router\nfor route in router.routes:\n    if ''POST'' in route.methods:\n        print(f''POST路由: {route.path} - {route.name}'')\n\n# 检查main.py中的路由注册\nprint(''\\n=== 检查main.py路由注册 ==='')\ntry:\n    from main import app\n    all_routes = []\n    for route in app.routes:\n        if hasattr(route, ''methods'') and hasattr(route, ''path''):\n            if ''POST'' in route.methods and ''processors'' in route.path:\n                all_routes.append(f''{route.methods} {route.path}'')\n    \n    if all_routes:\n        print(''找到POST processors路由:'')\n        for route in all_routes:\n            print(f''  {route}'')\n    else:\n        print(''未找到POST processors路由在main app中'')\n        \n    # 显示所有processors相关路由\n    print(''\\n=== 所有processors相关路由 ==='')\n    processor_routes = []\n    for route in app.routes:\n        if hasattr(route, ''path'') and ''processors'' in route.path:\n            methods = getattr(route, ''methods'', set())\n            processor_routes.append(f''{methods} {route.path}'')\n    \n    for route in processor_routes:\n        print(f''  {route}'')\n        \nexcept Exception as e:\n    print(f''检查main.py失败: {e}'')\n    import traceback\n    traceback.print_exc()\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def test_processor_creation():\n    try:\n        from workflow_framework.repositories.processor.processor_repository import ProcessorRepository\n        from workflow_framework.models.processor import ProcessorCreate, ProcessorType\n        import uuid\n        \n        # 模拟数据库连接\n        from workflow_framework.utils.database import initialize_database\n        await initialize_database()\n        \n        repo = ProcessorRepository()\n        \n        # 测试创建处理器数据\n        test_data = ProcessorCreate(\n            name=''Test Processor'',\n            type=ProcessorType.HUMAN,\n            user_id=uuid.uuid4()  # 模拟用户ID\n        )\n        \n        print(''测试数据:'', test_data.model_dump())\n        \n        # 尝试创建处理器\n        result = await repo.create_processor(test_data)\n        print(''创建结果:'', result)\n        \n    except Exception as e:\n        print(f''测试错误: {e}'')\n        import traceback\n        print(''详细错误:'', traceback.format_exc())\n\n# 运行测试\nasyncio.run(test_processor_creation())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/python.exe:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport asyncio\nimport asyncpg\nfrom workflow_framework.models.node import NodeUpdate\n\n# Test node update validation\ntry:\n    # Valid update data - similar to start/end nodes\n    update_data = NodeUpdate(\n        name=''1'',\n        task_description=None,\n        position_x=100,\n        position_y=100\n    )\n    print(''Start/End node update data validates successfully'')\n    print(f''Name: {update_data.name}'')\n    print(f''Description: {update_data.task_description}'')\n    print(f''Position: ({update_data.position_x}, {update_data.position_y})'')\n    \n    # Try processor-style data\n    processor_data = NodeUpdate(\n        name=''hhh'',\n        task_description=''Some description'',\n        position_x=200,\n        position_y=200\n    )\n    print(''\\nProcessor node update data validates successfully'')\n    print(f''Name: {processor_data.name}'')\n    print(f''Description: {processor_data.task_description}'')\n    print(f''Position: ({processor_data.position_x}, {processor_data.position_y})'')\n    \nexcept Exception as e:\n    print(f''Validation error: {e}'')\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python check_server_routes.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_server_routes.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" live_debug.py)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_execution_fix.py)",
      "Bash(rg:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_node_api.py)",
      "Bash(pgrep:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_processor_update.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" test_delayed_task_creation.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" check_node_schema.py)",
      "Bash(git checkout:*)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(''workflow.db'')\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\ncursor.execute(''''''\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != ''{}'' \nORDER BY created_at DESC \nLIMIT 3\n'''''')\n\nresults = cursor.fetchall()\nprint(''=== 数据库中的context_data检查 ==='')\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f''任务ID: {task_id}'')\n    print(f''任务标题: {title}'')\n    print(f''Context Data类型: {type(context_data)}'')\n    print(f''Context Data内容: {context_data[:200]}...'' if len(str(context_data)) > 200 else f''Context Data内容: {context_data}'')\n    print(f''创建时间: {created_at}'')\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            print(f''JSON解析成功，包含字段: {list(parsed.keys()) if isinstance(parsed, dict) else \"\"非字典类型\"\"}'')\n        else:\n            print(f''Context data不是字符串类型: {type(context_data)}'')\n    except Exception as e:\n        print(f''JSON解析失败: {e}'')\n    print(''-'' * 50)\n\nconn.close()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(''workflow.db'')\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\ncursor.execute(''''''\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != ''{}'' \nORDER BY created_at DESC \nLIMIT 3\n'''''')\n\nresults = cursor.fetchall()\nprint(''=== 数据库中的context_data检查 ==='')\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f''任务ID: {task_id}'')\n    print(f''任务标题: {title}'')\n    print(f''Context Data类型: {type(context_data)}'')\n    if context_data:\n        content_preview = str(context_data)[:200] + ''...'' if len(str(context_data)) > 200 else str(context_data)\n        print(f''Context Data内容: {content_preview}'')\n    print(f''创建时间: {created_at}'')\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            if isinstance(parsed, dict):\n                print(f''JSON解析成功，包含字段: {list(parsed.keys())}'')\n            else:\n                print(f''JSON解析成功，但不是字典类型: {type(parsed)}'')\n        else:\n            print(f''Context data不是字符串类型: {type(context_data)}'')\n    except Exception as e:\n        print(f''JSON解析失败: {e}'')\n    print(''-'' * 50)\n\nconn.close()\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c '\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(\"\"workflow.db\"\")\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\ncursor.execute(\"\"\"\"\"\"\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != \"\"{}\"\" \nORDER BY created_at DESC \nLIMIT 3\n\"\"\"\"\"\")\n\nresults = cursor.fetchall()\nprint(\"\"=== 数据库中的context_data检查 ===\"\")\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f\"\"任务ID: {task_id}\"\")\n    print(f\"\"任务标题: {title}\"\")\n    print(f\"\"Context Data类型: {type(context_data)}\"\")\n    if context_data:\n        content_preview = str(context_data)[:200] + \"\"...\"\" if len(str(context_data)) > 200 else str(context_data)\n        print(f\"\"Context Data内容: {content_preview}\"\")\n    print(f\"\"创建时间: {created_at}\"\")\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            if isinstance(parsed, dict):\n                print(f\"\"JSON解析成功，包含字段: {list(parsed.keys())}\"\")\n            else:\n                print(f\"\"JSON解析成功，但不是字典类型: {type(parsed)}\"\")\n        else:\n            print(f\"\"Context data不是字符串类型: {type(context_data)}\"\")\n    except Exception as e:\n        print(f\"\"JSON解析失败: {e}\"\")\n    print(\"\"-\"\" * 50)\n\nconn.close()\n')",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c '\nimport sqlite3\nimport json\n\n# 连接到数据库\nconn = sqlite3.connect(\"\"workflow.db\"\")\ncursor = conn.cursor()\n\n# 查询一些最近的任务实例，检查context_data字段\nquery = \"\"\"\"\"\"\nSELECT task_instance_id, task_title, context_data, created_at \nFROM task_instance \nWHERE context_data IS NOT NULL AND context_data != \"\"{}\"\" \nORDER BY created_at DESC \nLIMIT 3\n\"\"\"\"\"\"\n\ncursor.execute(query)\nresults = cursor.fetchall()\nprint(\"\"=== 数据库中的context_data检查 ===\"\")\nfor row in results:\n    task_id, title, context_data, created_at = row\n    print(f\"\"任务ID: {task_id}\"\")\n    print(f\"\"任务标题: {title}\"\")\n    print(f\"\"Context Data类型: {type(context_data)}\"\")\n    if context_data:\n        content_preview = str(context_data)[:200] + \"\"...\"\" if len(str(context_data)) > 200 else str(context_data)\n        print(f\"\"Context Data内容: {content_preview}\"\")\n    print(f\"\"创建时间: {created_at}\"\")\n    \n    # 尝试解析JSON\n    try:\n        if isinstance(context_data, str):\n            parsed = json.loads(context_data)\n            if isinstance(parsed, dict):\n                print(f\"\"JSON解析成功，包含字段: {list(parsed.keys())}\"\")\n            else:\n                print(f\"\"JSON解析成功，但不是字典类型: {type(parsed)}\"\")\n        else:\n            print(f\"\"Context data不是字符串类型: {type(context_data)}\"\")\n    except Exception as e:\n        print(f\"\"JSON解析失败: {e}\"\")\n    print(\"\"-\"\" * 50)\n\nconn.close()\n')",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport asyncio\nimport uuid\nfrom workflow_framework.services.node_service import NodeService\n\nasync def test_create_connection():\n    service = NodeService()\n    \n    # Test data that simulates what the frontend sends\n    connection_data = {\n        ''from_node_base_id'': str(uuid.uuid4()),\n        ''to_node_base_id'': str(uuid.uuid4()),\n        ''workflow_base_id'': str(uuid.uuid4()),\n        ''connection_type'': ''normal''\n    }\n    \n    print(''Testing connection creation with data:'', connection_data)\n    \n    # This will likely fail, but we want to see the exact error\n    try:\n        result = await service.create_node_connection(connection_data, uuid.uuid4())\n        print(''Success:'', result)\n    except Exception as e:\n        print(''Error type:'', type(e).__name__)\n        print(''Error message:'', str(e))\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(test_create_connection())\n\")",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\nfrom pathlib import Path\n\nasync def apply_migration():\n    try:\n        # 读取SQL迁移脚本\n        sql_file = Path(''update_task_fields_to_text.sql'')\n        if not sql_file.exists():\n            print(''迁移脚本不存在'')\n            return\n            \n        migration_sql = sql_file.read_text(encoding=''utf-8'')\n        print(f''读取迁移脚本: {len(migration_sql)} 字符'')\n        \n        # 连接数据库\n        conn = await asyncpg.connect(''postgresql://postgres:123456@localhost:5432/workflow_framework'')\n        print(''数据库连接成功'')\n        \n        # 执行迁移\n        print(''开始执行数据库迁移...'')\n        await conn.execute(migration_sql)\n        print(''✅ 数据库迁移执行成功!'')\n        \n        # 验证字段类型\n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''task_instance'' \n            AND column_name IN (''input_data'', ''output_data'', ''context_data'')\n            ORDER BY column_name\n        '''''')\n        \n        print(''\\n字段类型验证:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        print(''数据库连接已关闭'')\n        \n    except Exception as e:\n        print(f''迁移失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(apply_migration())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\nfrom pathlib import Path\n\nasync def apply_migration():\n    try:\n        # 读取SQL迁移脚本\n        sql_file = Path(''update_task_fields_to_text.sql'')\n        if not sql_file.exists():\n            print(''迁移脚本不存在'')\n            return\n            \n        migration_sql = sql_file.read_text(encoding=''utf-8'')\n        print(f''读取迁移脚本: {len(migration_sql)} 字符'')\n        \n        # 连接到正确的数据库\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        print(''数据库连接成功'')\n        \n        # 执行迁移 - 分步执行以避免语法错误\n        print(''开始执行数据库迁移...'')\n        \n        # 1. 更新input_data字段类型\n        try:\n            await conn.execute(''ALTER TABLE task_instance ALTER COLUMN input_data TYPE TEXT USING input_data::TEXT;'')\n            print(''✅ input_data字段类型更新成功'')\n        except Exception as e:\n            print(f''input_data字段更新失败: {e}'')\n        \n        # 2. 更新output_data字段类型\n        try:\n            await conn.execute(''ALTER TABLE task_instance ALTER COLUMN output_data TYPE TEXT USING output_data::TEXT;'')\n            print(''✅ output_data字段类型更新成功'') \n        except Exception as e:\n            print(f''output_data字段更新失败: {e}'')\n            \n        # 3. 添加context_data字段\n        try:\n            await conn.execute(''ALTER TABLE task_instance ADD COLUMN IF NOT EXISTS context_data TEXT;'')\n            print(''✅ context_data字段添加成功'')\n        except Exception as e:\n            print(f''context_data字段添加失败: {e}'')\n            \n        # 验证字段类型\n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''task_instance'' \n            AND column_name IN (''input_data'', ''output_data'', ''context_data'')\n            ORDER BY column_name\n        '''''')\n        \n        print(''\\n字段类型验证:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        print(''数据库连接已关闭'')\n        print(''\\n✅ 数据库迁移完成!'')\n        \n    except Exception as e:\n        print(f''迁移失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(apply_migration())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_schema():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        result = await conn.fetch(''''''\n            SELECT column_name, data_type, is_nullable\n            FROM information_schema.columns \n            WHERE table_name = ''task_instance'' \n            AND column_name IN (''input_data'', ''output_data'', ''context_data'', ''priority'', ''instructions'')\n            ORDER BY column_name\n        '''''')\n        \n        print(''task_instance table field types:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]} (nullable: {row[\"\"is_nullable\"\"]})'')\n            \n        await conn.close()\n        print(''Schema check completed successfully'')\n        \n    except Exception as e:\n        print(f''Schema check failed: {e}'')\n\nasyncio.run(check_schema())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport sys\nimport os\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom workflow_framework.services.agent_task_service import agent_task_service\nfrom workflow_framework.utils.database import initialize_database\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\nfrom workflow_framework.models.instance import TaskInstanceCreate, TaskInstanceType, TaskInstanceUpdate, TaskInstanceStatus\nimport uuid\n\nasync def test_agent_task():\n    await initialize_database()\n    \n    # Create a simple task with text data\n    task_data = TaskInstanceCreate(\n        node_instance_id=uuid.uuid4(),\n        workflow_instance_id=uuid.uuid4(),\n        processor_id=uuid.uuid4(),\n        task_type=TaskInstanceType.AGENT,\n        task_title=''Test Agent Task'',\n        task_description=''Test Agent task processing with text format'',\n        input_data=''这是一个测试输入'',  # Simple text input\n        context_data=''这是上下文数据'',     # Simple text context\n        assigned_agent_id=uuid.uuid4(),\n        estimated_duration=5\n    )\n    \n    repo = TaskInstanceRepository()\n    \n    # Create task\n    print(''Creating task with text data...'')\n    created_task = await repo.create_task(task_data)\n    if created_task:\n        print(f''Task created successfully: {created_task[\"\"task_instance_id\"\"]}'')\n        \n        # Update task with output (this was failing before)\n        print(''Updating task with output data...'')\n        update_data = TaskInstanceUpdate(\n            status=TaskInstanceStatus.COMPLETED,\n            output_data=''这是Agent的输出结果，现在是文本格式而不是JSON'',\n            result_summary=''任务完成摘要''\n        )\n        \n        updated_task = await repo.update_task(created_task[''task_instance_id''], update_data)\n        \n        if updated_task:\n            print(''SUCCESS: Task updated with text output data!'')\n            print(f''Status: {updated_task[\"\"status\"\"]}'')\n            print(f''Output data type: {type(updated_task[\"\"output_data\"\"])}'')\n            print(f''Output preview: {updated_task[\"\"output_data\"\"][:50]}...'')\n        else:\n            print(''FAILED: Task update failed'')\n    else:\n        print(''FAILED: Task creation failed'')\n\nasyncio.run(test_agent_task())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nfrom workflow_framework.services.agent_task_service import agent_task_service\nfrom workflow_framework.utils.database import initialize_database\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\nfrom workflow_framework.models.instance import TaskInstanceCreate, TaskInstanceType, TaskInstanceUpdate, TaskInstanceStatus\nimport uuid\n\nasync def test_agent_task():\n    await initialize_database()\n    \n    # Create a simple task with text data\n    task_data = TaskInstanceCreate(\n        node_instance_id=uuid.uuid4(),\n        workflow_instance_id=uuid.uuid4(),\n        processor_id=uuid.uuid4(),\n        task_type=TaskInstanceType.AGENT,\n        task_title=''Test Agent Task'',\n        task_description=''Test Agent task processing with text format'',\n        input_data=''This is test input data'',  # Simple text input\n        context_data=''This is context data'',     # Simple text context\n        assigned_agent_id=uuid.uuid4(),\n        estimated_duration=5\n    )\n    \n    repo = TaskInstanceRepository()\n    \n    # Create task\n    print(''Creating task with text data...'')\n    created_task = await repo.create_task(task_data)\n    if created_task:\n        print(f''Task created successfully: {created_task[\"\"task_instance_id\"\"]}'')\n        \n        # Update task with output (this was failing before)\n        print(''Updating task with output data...'')\n        update_data = TaskInstanceUpdate(\n            status=TaskInstanceStatus.COMPLETED,\n            output_data=''This is Agent output result, now in text format instead of JSON'',\n            result_summary=''Task completion summary''\n        )\n        \n        updated_task = await repo.update_task(created_task[''task_instance_id''], update_data)\n        \n        if updated_task:\n            print(''SUCCESS: Task updated with text output data!'')\n            print(f''Status: {updated_task[\"\"status\"\"]}'')\n            print(f''Output data type: {type(updated_task[\"\"output_data\"\"])}'')\n            print(f''Output preview: {updated_task[\"\"output_data\"\"][:50]}...'')\n        else:\n            print(''FAILED: Task update failed'')\n    else:\n        print(''FAILED: Task creation failed'')\n\nasyncio.run(test_agent_task())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_table_structure():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        # Check node_connection table structure\n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''node_connection''\n            ORDER BY ordinal_position\n        '''''')\n        \n        print(''node_connection table columns:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''Error: {e}'')\n\nasyncio.run(check_table_structure())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_join_issue():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试JOIN查询问题 ==='')\n        \n        # 检查最近的node_connection记录\n        connections = await conn.fetch(''''''\n            SELECT from_node_id, to_node_id, workflow_id, created_at\n            FROM node_connection \n            WHERE created_at > NOW() - INTERVAL ''2 hours''\n            ORDER BY created_at DESC\n            LIMIT 5\n        '''''')\n        \n        print(''最近的连接记录:'')\n        for i, conn_row in enumerate(connections):\n            from_id = conn_row[''from_node_id'']\n            to_id = conn_row[''to_node_id'']\n            workflow_id = conn_row[''workflow_id'']\n            \n            print(f''\\n连接 {i+1}:'')\n            print(f''  from_node_id: {from_id}'')\n            print(f''  to_node_id: {to_id}'')  \n            print(f''  workflow_id: {workflow_id}'')\n            \n            # 检查这些ID在node表中是否存在\n            from_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', from_id)\n            to_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', to_id)\n            \n            print(f''  from_node: {from_node[\"\"name\"\"] if from_node else \"\"NOT FOUND\"\"} ({from_node[\"\"type\"\"] if from_node else \"\"N/A\"\"})'')\n            print(f''  to_node: {to_node[\"\"name\"\"] if to_node else \"\"NOT FOUND\"\"} ({to_node[\"\"type\"\"] if to_node else \"\"N/A\"\"})'')\n            \n            # 如果没找到，检查是否是workflow_id匹配问题\n            if not from_node:\n                alt_from = await conn.fetchrow(''SELECT name, type, workflow_id FROM node WHERE node_base_id = $1'', from_id)\n                if alt_from:\n                    print(f''    from_node存在但workflow_id不同: {alt_from[\"\"workflow_id\"\"]} vs {workflow_id}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n\nasyncio.run(debug_join_issue())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_join_issue():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试JOIN查询问题 ==='')\n        \n        # 检查最近的node_connection记录\n        connections = await conn.fetch(''''''\n            SELECT from_node_id, to_node_id, workflow_id, created_at\n            FROM node_connection \n            WHERE created_at > NOW() - INTERVAL ''2 hours''\n            ORDER BY created_at DESC\n            LIMIT 3\n        '''''')\n        \n        print(''最近的连接记录:'')\n        for i, conn_row in enumerate(connections):\n            from_id = conn_row[''from_node_id'']\n            to_id = conn_row[''to_node_id'']\n            workflow_id = conn_row[''workflow_id'']\n            \n            print(f''连接 {i+1}:'')\n            print(f''  from_node_id: {from_id}'')\n            print(f''  to_node_id: {to_id}'')  \n            \n            # 检查这些ID在node表中是否存在\n            from_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', from_id)\n            to_node = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', to_id)\n            \n            print(f''  from_node: {from_node[\"\"name\"\"] if from_node else \"\"NOT FOUND\"\"}'')\n            print(f''  to_node: {to_node[\"\"name\"\"] if to_node else \"\"NOT FOUND\"\"}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n\nasyncio.run(debug_join_issue())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def test_dependency_query():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        # 获取一个最近的node_instance来测试\n        recent_node = await conn.fetchrow(''''''\n            SELECT node_instance_id, workflow_instance_id, node_id, node_instance_name\n            FROM node_instance \n            WHERE created_at > NOW() - INTERVAL ''2 hours''\n            ORDER BY created_at DESC \n            LIMIT 1\n        '''''')\n        \n        if recent_node:\n            node_instance_id = recent_node[''node_instance_id'']\n            workflow_instance_id = recent_node[''workflow_instance_id'']\n            node_id = recent_node[''node_id'']\n            \n            print(f''测试节点: {recent_node[\"\"node_instance_name\"\"]} ({node_instance_id})'')\n            print(f''对应的node_id: {node_id}'')\n            \n            # 检查是否有对应的连接记录\n            connections = await conn.fetch(''''''\n                SELECT nc.from_node_id, nc.to_node_id\n                FROM node_connection nc\n                WHERE nc.to_node_id = $1 OR nc.from_node_id = $1\n            '''''', node_id)\n            \n            print(f''相关的连接记录数: {len(connections)}'')\n            for conn_row in connections:\n                print(f''  {conn_row[\"\"from_node_id\"\"]} -> {conn_row[\"\"to_node_id\"\"]}'')\n                \n            # 如果有连接，测试依赖查询\n            if connections:\n                dependency_query = ''''''\n                SELECT COUNT(*) as total_dependencies,\n                       COUNT(CASE WHEN upstream_ni.status = ''completed'' THEN 1 END) as completed_dependencies\n                FROM node_connection nc\n                JOIN node_instance ni ON nc.to_node_id = ni.node_id\n                JOIN node_instance upstream_ni ON nc.from_node_id = upstream_ni.node_id\n                WHERE ni.node_instance_id = $1\n                AND ni.workflow_instance_id = $2\n                AND upstream_ni.workflow_instance_id = $2\n                AND ni.is_deleted = FALSE\n                AND upstream_ni.is_deleted = FALSE\n                ''''''\n                \n                result = await conn.fetchrow(dependency_query, node_instance_id, workflow_instance_id)\n                \n                if result:\n                    total_deps = result[''total_dependencies'']\n                    completed_deps = result[''completed_dependencies'']\n                    print(f''依赖查询结果: {completed_deps}/{total_deps} 个依赖已完成'')\n                else:\n                    print(''依赖查询无结果'')\n        else:\n            print(''没有找到最近的node_instance'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''测试失败: {e}'')\n\nasyncio.run(test_dependency_query())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def verify_id_relationship():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== ID关系验证 ==='')\n        \n        # 测试node_connection中的ID对应node表中的哪个字段\n        test_id = ''f9708ae6-e622-4829-8939-2a35037d919a''  # 从上面的数据中取一个\n        \n        # 检查作为node_id\n        as_node_id = await conn.fetchrow(''SELECT name, type FROM node WHERE node_id = $1'', test_id)\n        # 检查作为node_base_id  \n        as_node_base_id = await conn.fetchrow(''SELECT name, type FROM node WHERE node_base_id = $1'', test_id)\n        \n        print(f''测试ID: {test_id}'')\n        if as_node_id:\n            print(f''  作为node_id匹配: 是 - {as_node_id[\"\"name\"\"]} ({as_node_id[\"\"type\"\"]})'')\n        else:\n            print(f''  作为node_id匹配: 否'')\n            \n        if as_node_base_id:\n            print(f''  作为node_base_id匹配: 是 - {as_node_base_id[\"\"name\"\"]} ({as_node_base_id[\"\"type\"\"]})'')\n        else:\n            print(f''  作为node_base_id匹配: 否'')\n        \n        print()\n        print(''=== 结论分析 ==='')\n        print(''从数据可以看出:'')\n        print(''1. node表有两个ID字段:'')\n        print(''   - node_id: 节点的版本ID (每个版本唯一)'')\n        print(''   - node_base_id: 节点的基础ID (跨版本共享)'')\n        print(''2. node_connection表使用: node_id (版本级别的连接)'')\n        print(''3. node_instance表使用: node_id (实例对应具体版本)'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''验证失败: {e}'')\n\nasyncio.run(verify_id_relationship())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python test_node_edit_connection.py)",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport uuid\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\n\nasync def check_task_data():\n    repo = TaskInstanceRepository()\n    \n    # 查询这个工作流实例的任务\n    workflow_instance_id = uuid.UUID(''6a0d2b12-7e2e-414b-8a4b-3a4c24df76fc'')\n    \n    query = ''''''\n    SELECT task_instance_id, task_title, input_data, context_data, output_data, status\n    FROM task_instance \n    WHERE workflow_instance_id = $1 \n    AND is_deleted = FALSE\n    ORDER BY created_at\n    ''''''\n    \n    tasks = await repo.db.fetch_all(query, workflow_instance_id)\n    \n    print(f''工作流 {workflow_instance_id} 的任务数据:'')\n    for i, task in enumerate(tasks, 1):\n        print(f''\\n任务 {i}: {task[\"\"task_title\"\"]}'')\n        print(f''  状态: {task[\"\"status\"\"]}'')\n        print(f''  input_data: {task[\"\"input_data\"\"]}'')\n        print(f''  context_data: {task[\"\"context_data\"\"]}'')\n        print(f''  output_data: {task[\"\"output_data\"\"][:200] if task[\"\"output_data\"\"] else \"\"None\"\"}...'')\n\nasyncio.run(check_task_data())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nfrom workflow_framework.services.workflow_context_manager import WorkflowContextManager\n\nasync def test_unified_system():\n    print(''🔄 测试统一的上下文管理系统...'')\n    \n    manager = WorkflowContextManager()\n    \n    # 测试新方法是否可以调用\n    import uuid\n    test_workflow_id = uuid.uuid4()\n    test_node_id = uuid.uuid4()\n    \n    # 初始化上下文\n    await manager.initialize_workflow_context(test_workflow_id)\n    \n    # 测试获取任务上下文数据\n    context_data = await manager.get_task_context_data(test_workflow_id, test_node_id)\n    \n    print(f''✅ get_task_context_data 方法工作正常'')\n    print(f''  返回数据类型: {type(context_data)}'')\n    print(f''  数据键: {list(context_data.keys())}'')\n    \n    return True\n\nasyncio.run(test_unified_system())\nprint(''🎉 统一上下文管理系统测试成功!'')\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nfrom workflow_framework.services.workflow_context_manager import WorkflowContextManager\n\nasync def test_unified_system():\n    print(''Testing unified context management system...'')\n    \n    manager = WorkflowContextManager()\n    \n    # Test the new method\n    import uuid\n    test_workflow_id = uuid.uuid4()\n    test_node_id = uuid.uuid4()\n    \n    # Initialize context\n    await manager.initialize_workflow_context(test_workflow_id)\n    \n    # Test get_task_context_data method\n    context_data = await manager.get_task_context_data(test_workflow_id, test_node_id)\n    \n    print(''get_task_context_data method works normally'')\n    print(f''Return data type: {type(context_data)}'')\n    print(f''Data keys: {list(context_data.keys())}'')\n    \n    return True\n\nasyncio.run(test_unified_system())\nprint(''Unified context management system test successful!'')\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_user_table():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''user''\n            ORDER BY ordinal_position\n        '''''')\n        \n        print(''user table columns:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''Error: {e}'')\n\nasyncio.run(check_user_table())\n\")",
      "Bash(\"/mnt/d/anaconda3/envs/fornew/python.exe\" -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_workflow_instance_table():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        result = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''workflow_instance''\n            ORDER BY ordinal_position\n        '''''')\n        \n        print(''workflow_instance table columns:'')\n        for row in result:\n            print(f''  {row[\"\"column_name\"\"]}: {row[\"\"data_type\"\"]}'')\n            \n        await conn.close()\n        \n    except Exception as e:\n        print(f''Error: {e}'')\n\nasyncio.run(check_workflow_instance_table())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python -c \"\nimport asyncio\nimport uuid\nfrom workflow_framework.repositories.instance.task_instance_repository import TaskInstanceRepository\n\nasync def check_tasks():\n    repo = TaskInstanceRepository()\n    \n    # 查询提到的任务ID\n    task_ids = [\n        ''183eba7b-160a-437e-9dba-ad0d484126f9'',\n        ''c2cd416c-2c38-4803-8066-4e876ebadb28''\n    ]\n    \n    for task_id_str in task_ids:\n        try:\n            task_id = uuid.UUID(task_id_str)\n            task = await repo.get_task_by_id(task_id)\n            if task:\n                print(f''\\n任务 {task_id_str}:'')\n                print(f''  标题: {task.get(\"\"task_title\"\", \"\"无\"\")}'')\n                print(f''  状态: {task.get(\"\"status\"\", \"\"无\"\")}'')\n                print(f''  类型: {task.get(\"\"task_type\"\", \"\"无\"\")}'')\n                print(f''  分配用户ID: {task.get(\"\"assigned_user_id\"\", \"\"无\"\")}'')\n                print(f''  分配代理ID: {task.get(\"\"assigned_agent_id\"\", \"\"无\"\")}'')\n                print(f''  创建时间: {task.get(\"\"created_at\"\", \"\"无\"\")}'')\n                print(f''  分配时间: {task.get(\"\"assigned_at\"\", \"\"无\"\")}'')\n            else:\n                print(f''\\n任务 {task_id_str}: 未找到'')\n        except Exception as e:\n            print(f''\\n查询任务 {task_id_str} 失败: {e}'')\n\nasyncio.run(check_tasks())\n\")",
      "Bash(/mnt/d/anaconda3/envs/fornew/bin/python:*)",
      "Bash(/mnt/d/anaconda3/envs/fornew/Scripts/python.exe debug_task_assignment.py)",
      "Bash(sqlite3:*)",
      "Bash(chmod:*)",
      "Bash(./deployment/scripts/test-local.sh:*)",
      "Bash(bash:*)",
      "Bash(dos2unix:*)",
      "Bash(docker-compose:*)",
      "Bash(kill:*)",
      "Bash(powershell.exe:*)",
      "Bash(docker inspect:*)",
      "Bash(docker logs:*)",
      "Bash(docker rm:*)",
      "Bash(docker build:*)",
      "Bash(docker run:*)",
      "Bash(pip install:*)",
      "Bash(mysql:*)",
      "Bash(/home/ubuntu/Workflow-Is-All-You-Need/check_cross_machine_access.sh:*)",
      "Bash(brew services:*)",
      "Bash(/usr/local/mysql/bin/mysql:*)",
      "Bash(sudo:*)",
      "Bash(nginx:*)",
      "Bash(/tmp/test_ip_access.sh:*)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_fixed_subdivision.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python debug_duplicate_task_creation.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python debug_workflow_update_error.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_end_node_integration.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nfrom backend.services.workflow_execution_context import WorkflowContextManager\n\nasync def test_context_data():\n    manager = WorkflowContextManager()\n    \n    # Test with a recent workflow instance\n    import uuid\n    from backend.repositories.instance.workflow_instance_repository import WorkflowInstanceRepository\n    \n    repo = WorkflowInstanceRepository()\n    \n    # Get a recent workflow instance\n    query = ''''''\n        SELECT workflow_instance_id, workflow_instance_name \n        FROM workflow_instance \n        WHERE created_at > NOW() - INTERVAL ''2 hours'' \n        ORDER BY created_at DESC \n        LIMIT 1\n    ''''''\n    \n    result = await repo.db.fetch_one(query)\n    if result:\n        workflow_instance_id = result[''workflow_instance_id'']\n        print(f''Testing context for workflow: {result[\"\"workflow_instance_name\"\"]} ({workflow_instance_id})'')\n        \n        # Get node instances for this workflow\n        node_query = ''''''\n            SELECT node_instance_id, node_instance_name \n            FROM node_instance \n            WHERE workflow_instance_id = $1 \n            ORDER BY created_at DESC \n            LIMIT 1\n        ''''''\n        \n        node_result = await repo.db.fetch_one(node_query, workflow_instance_id)\n        if node_result:\n            node_instance_id = node_result[''node_instance_id'']\n            print(f''Testing context for node: {node_result[\"\"node_instance_name\"\"]} ({node_instance_id})'')\n            \n            # Get context data\n            context_data = await manager.get_task_context_data(workflow_instance_id, node_instance_id)\n            \n            print(f''Context data structure:'')\n            print(f''  Keys: {list(context_data.keys())}'')\n            \n            if ''upstream_outputs'' in context_data:\n                upstream_outputs = context_data[''upstream_outputs'']\n                print(f''  Upstream outputs count: {len(upstream_outputs)}'')\n                \n                for i, (node_name, output) in enumerate(upstream_outputs.items()):\n                    print(f''  Node {i+1}: {node_name}'')\n                    print(f''    Keys: {list(output.keys()) if isinstance(output, dict) else \"\"Non-dict\"\"}'')\n                    if isinstance(output, dict) and ''output_data'' in output:\n                        output_data = output[''output_data'']\n                        print(f''    Output data type: {type(output_data)}'')\n                        print(f''    Output preview: {str(output_data)[:100]}...'')\n        else:\n            print(''No recent node instances found'')\n    else:\n        print(''No recent workflow instances found'')\n\nasyncio.run(test_context_data())\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport uuid\nfrom backend.repositories.instance.task_instance_repository import TaskInstanceRepository\n\nasync def test_task_deletion_fix():\n    repo = TaskInstanceRepository()\n    \n    # Test the data type matching that was causing issues\n    # Simulate database UUID (as returned from database) vs CurrentUser UUID\n    db_user_id = ''123e4567-e89b-12d3-a456-426614174000''  # String from DB\n    current_user_id = uuid.UUID(''123e4567-e89b-12d3-a456-426614174000'')  # UUID object\n    \n    print(''=== Testing UUID type matching fix ==='')\n    print(f''Database user_id: {db_user_id} (type: {type(db_user_id)})'')\n    print(f''Current user_id: {current_user_id} (type: {type(current_user_id)})'')\n    \n    # Test the fixed comparison logic\n    if db_user_id and str(db_user_id) == str(current_user_id):\n        print(''✅ Permission check would PASS with string conversion'')\n        has_permission = True\n        permission_reason = ''任务分配者''\n    else:\n        print(''❌ Permission check would FAIL'')\n        has_permission = False\n        permission_reason = ''''\n    \n    print(f''Permission result: {has_permission} ({permission_reason})'')\n    \n    # Test edge cases\n    print(''\\n=== Testing edge cases ==='')\n    \n    # Case 1: None vs UUID\n    none_user_id = None\n    result1 = none_user_id and str(none_user_id) == str(current_user_id)\n    print(f''None vs UUID: {result1} (should be False)'')\n    \n    # Case 2: Different UUIDs\n    different_uuid = uuid.uuid4()\n    result2 = str(db_user_id) == str(different_uuid)\n    print(f''Different UUIDs: {result2} (should be False)'')\n    \n    # Case 3: Same UUID different formats\n    uuid_obj = uuid.UUID(db_user_id)\n    result3 = str(db_user_id) == str(uuid_obj)\n    print(f''Same UUID different formats: {result3} (should be True)'')\n    \n    print(''\\n✅ All UUID type matching tests completed successfully!'')\n\nasyncio.run(test_task_deletion_fix())\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_subdivision_no_auto_submit.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python debug_manual_submission.py)",
      "WebSearch",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python debug_subdivision_integration.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport httpx\nimport json\n\nasync def test_subdivision_api():\n    print(''=== Testing subdivision API endpoint ==='')\n    \n    # Test with the workflow instance from the previous conversation\n    workflow_instance_id = ''0c375fe3-806b-4dd7-b616-27c4f9e5e2b3''\n    api_url = f''http://localhost:8000/api/execution/workflows/{workflow_instance_id}/subdivision-info''\n    \n    print(f''Testing URL: {api_url}'')\n    \n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(api_url, timeout=10.0)\n            \n            print(f''Status Code: {response.status_code}'')\n            print(f''Headers: {dict(response.headers)}'')\n            \n            if response.status_code == 200:\n                try:\n                    data = response.json()\n                    print(f''Response Data: {json.dumps(data, indent=2, default=str)}'')\n                    \n                    if data.get(''success'') and data.get(''data''):\n                        node_subdivisions = data[''data''].get(''node_subdivisions'', {})\n                        print(f''\\nFound {len(node_subdivisions)} nodes with subdivision info'')\n                        \n                        for node_id, info in node_subdivisions.items():\n                            print(f''  Node {node_id}: has_subdivision={info.get(\"\"has_subdivision\"\")}, count={info.get(\"\"subdivision_count\"\")}'')\n                    else:\n                        print(''API returned success=false or no data'')\n                        \n                except json.JSONDecodeError as e:\n                    print(f''JSON decode error: {e}'')\n                    print(f''Raw response: {response.text[:500]}...'')\n            else:\n                print(f''Error response: {response.text}'')\n                \n    except httpx.RequestError as e:\n        print(f''Request error: {e}'')\n    except Exception as e:\n        print(f''Unexpected error: {e}'')\n\nasyncio.run(test_subdivision_api())\n\")",
      "Bash(PYTHONPATH=. python3:*)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_recursive_subdivision.py)",
      "Bash(/home/ubuntu/Workflow-Is-All-You-Need/final_fix_summary.sh:*)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def debug_nested_subdivision():\n    print(''🔍 调试嵌套子工作流subdivision信息问题'')\n    \n    # 从数据库中查找一个有子工作流的工作流实例\n    from backend.repositories.instance.workflow_instance_repository import WorkflowInstanceRepository\n    from backend.repositories.task_subdivision.task_subdivision_repository import TaskSubdivisionRepository\n    \n    workflow_repo = WorkflowInstanceRepository()\n    subdivision_repo = TaskSubdivisionRepository()\n    \n    # 查找最近创建的工作流实例\n    recent_workflow_query = ''''''\n        SELECT workflow_instance_id, workflow_instance_name \n        FROM workflow_instance \n        WHERE created_at > NOW() - INTERVAL ''12 hours'' \n        ORDER BY created_at DESC \n        LIMIT 3\n    ''''''\n    \n    recent_workflows = await workflow_repo.db.fetch_all(recent_workflow_query)\n    \n    print(f''最近的工作流实例:'')\n    for workflow in recent_workflows:\n        workflow_id = workflow[''workflow_instance_id'']\n        print(f''  - {workflow[\"\"workflow_instance_name\"\"]} ({workflow_id})'')\n        \n        # 查找这个工作流的subdivision情况\n        subdivision_query = ''''''\n            SELECT ts.subdivision_id, ts.sub_workflow_instance_id, \n                   ts.subdivision_name, ts.status,\n                   ti.node_instance_id, ti.task_title\n            FROM task_subdivision ts\n            JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id\n            WHERE ti.workflow_instance_id = %s\n            AND ts.is_deleted = FALSE\n            AND ts.sub_workflow_instance_id IS NOT NULL\n            LIMIT 5\n        ''''''\n        \n        subdivisions = await subdivision_repo.db.fetch_all(subdivision_query, workflow_id)\n        \n        if subdivisions:\n            print(f''    发现 {len(subdivisions)} 个子工作流:'')\n            for sub in subdivisions:\n                sub_workflow_id = sub[''sub_workflow_instance_id'']\n                print(f''      子工作流: {sub[\"\"subdivision_name\"\"]} ({sub_workflow_id})'')\n                \n                # 检查子工作流是否还有进一步的subdivision\n                nested_subdivisions = await subdivision_repo.db.fetch_all(subdivision_query, sub_workflow_id)\n                if nested_subdivisions:\n                    print(f''        🎯 发现嵌套层级! 有 {len(nested_subdivisions)} 个二级子工作流'')\n                    for nested in nested_subdivisions:\n                        print(f''          - {nested[\"\"subdivision_name\"\"]} ({nested[\"\"sub_workflow_instance_id\"\"]})'')\n                else:\n                    print(f''        📍 没有更深层级的subdivision'')\n        else:\n            print(f''    没有子工作流'')\n        print()\n\nasyncio.run(debug_nested_subdivision())\n\")",
      "Bash(npx tsc:*)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nprint(''🔍 检查多层嵌套subdivision展开系统状态'')\n\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def test_nested_subdivision_system():\n    try:\n        # 检查相关API端点\n        from backend.repositories.instance.workflow_instance_repository import WorkflowInstanceRepository\n        from backend.repositories.task_subdivision.task_subdivision_repository import TaskSubdivisionRepository\n        \n        workflow_repo = WorkflowInstanceRepository()\n        subdivision_repo = TaskSubdivisionRepository()\n        \n        print(''✅ 数据库连接和仓库初始化成功'')\n        \n        # 查找最近的工作流实例\n        recent_workflow_query = ''''''\n            SELECT workflow_instance_id, workflow_instance_name, status\n            FROM workflow_instance \n            WHERE created_at > NOW() - INTERVAL ''24 hours'' \n            ORDER BY created_at DESC \n            LIMIT 5\n        ''''''\n        \n        recent_workflows = await workflow_repo.db.fetch_all(recent_workflow_query)\n        print(f''找到 {len(recent_workflows)} 个最近的工作流实例'')\n        \n        nested_subdivision_found = False\n        \n        for workflow in recent_workflows:\n            workflow_id = workflow[''workflow_instance_id'']\n            print(f''\\n🔍 检查工作流: {workflow[\"\"workflow_instance_name\"\"]} ({workflow_id})'')\n            \n            # 查找子工作流\n            subdivision_query = ''''''\n                SELECT ts.subdivision_id, ts.sub_workflow_instance_id, \n                       ts.subdivision_name, ts.status,\n                       ti.node_instance_id, ti.task_title\n                FROM task_subdivision ts\n                JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id\n                WHERE ti.workflow_instance_id = %s\n                AND ts.is_deleted = FALSE\n                AND ts.sub_workflow_instance_id IS NOT NULL\n            ''''''\n            \n            subdivisions = await subdivision_repo.db.fetch_all(subdivision_query, workflow_id)\n            \n            if subdivisions:\n                print(f''  发现 {len(subdivisions)} 个子工作流'')\n                \n                for sub in subdivisions:\n                    sub_workflow_id = sub[''sub_workflow_instance_id'']\n                    print(f''    子工作流: {sub[\"\"subdivision_name\"\"]} ({sub_workflow_id})'')\n                    \n                    # 检查二级嵌套\n                    nested_subdivisions = await subdivision_repo.db.fetch_all(subdivision_query, sub_workflow_id)\n                    if nested_subdivisions:\n                        nested_subdivision_found = True\n                        print(f''      🎯 发现二级嵌套! {len(nested_subdivisions)} 个三级子工作流'')\n                        for nested in nested_subdivisions:\n                            print(f''        - {nested[\"\"subdivision_name\"\"]} ({nested[\"\"sub_workflow_instance_id\"\"]})'')\n                            \n                            # 检查三级嵌套\n                            third_level = await subdivision_repo.db.fetch_all(subdivision_query, nested[''sub_workflow_instance_id''])\n                            if third_level:\n                                print(f''          🚀 发现三级嵌套! {len(third_level)} 个四级子工作流'')\n                    else:\n                        print(f''      📍 没有更深层级的subdivision'')\n            else:\n                print(f''  没有子工作流'')\n        \n        if nested_subdivision_found:\n            print(''\\n✅ 系统中存在多层嵌套subdivision，可以进行深度测试'')\n        else:\n            print(''\\n⚠️ 系统中暂无多层嵌套subdivision，建议创建测试数据'')\n        \n        return nested_subdivision_found\n        \n    except Exception as e:\n        print(f''❌ 检查失败: {e}'')\n        import traceback\n        traceback.print_exc()\n        return False\n\n# 运行检查\nresult = asyncio.run(test_nested_subdivision_system())\nprint(f''\\n🎯 多层嵌套检查结果: {\"\"有\"\" if result else \"\"无\"\"}多层嵌套数据'')\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_detailed_graph_data.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def debug_complete_data_flow():\n    print(''🔍 完整数据流调试 - 从API到前端显示'')\n    print(''='' * 80)\n    \n    try:\n        from backend.services.workflow_template_connection_service import WorkflowTemplateConnectionService\n        from backend.repositories.base import BaseRepository\n        \n        service = WorkflowTemplateConnectionService()\n        db = BaseRepository(''debug'').db\n        \n        # 1. 查找测试用的工作流实例\n        print(''📊 步骤1: 查找有合并数据的工作流实例'')\n        query = ''''''\n        SELECT DISTINCT wi.workflow_instance_id, wi.workflow_instance_name,\n               COUNT(ts.subdivision_id) as subdivision_count\n        FROM workflow_instance wi\n        JOIN node_instance ni ON wi.workflow_instance_id = ni.workflow_instance_id\n        JOIN task_instance ti ON ni.node_instance_id = ti.node_instance_id\n        JOIN task_subdivision ts ON ti.task_instance_id = ts.original_task_id\n        WHERE wi.is_deleted = FALSE \n        AND ni.is_deleted = FALSE\n        AND ts.is_deleted = FALSE\n        AND ts.sub_workflow_instance_id IS NOT NULL\n        GROUP BY wi.workflow_instance_id, wi.workflow_instance_name\n        ORDER BY subdivision_count DESC\n        LIMIT 2\n        ''''''\n        \n        instances = await db.fetch_all(query)\n        \n        if not instances:\n            print(''❌ 没有找到有subdivision数据的工作流实例'')\n            return\n        \n        test_instance = instances[0]\n        instance_id = test_instance[''workflow_instance_id'']\n        print(f''   选择测试实例: {test_instance[\"workflow_instance_name\"]} ({instance_id})'')\n        print(f''   细分数量: {test_instance[\"subdivision_count\"]}'')\n        \n        # 2. 调用详细连接图API（模拟前端调用）\n        print(f''\\n📡 步骤2: 调用详细连接图API'')\n        try:\n            detailed_data = await service.get_detailed_workflow_connections(instance_id, 5)\n            print(f''   ✅ API调用成功'')\n            print(f''   返回数据类型: {type(detailed_data)}'')\n            print(f''   主要键: {list(detailed_data.keys()) if isinstance(detailed_data, dict) else \"非字典\"}'')\n            \n            # 检查数据结构\n            if isinstance(detailed_data, dict):\n                template_connections = detailed_data.get(''template_connections'', [])\n                detailed_workflows = detailed_data.get(''detailed_workflows'', {})\n                merge_candidates = detailed_data.get(''merge_candidates'', [])\n                detailed_connection_graph = detailed_data.get(''detailed_connection_graph'', {})\n                \n                print(f''   template_connections数量: {len(template_connections)}'')\n                print(f''   detailed_workflows数量: {len(detailed_workflows)}'')\n                print(f''   merge_candidates数量: {len(merge_candidates)}'')\n                print(f''   detailed_connection_graph结构: {list(detailed_connection_graph.keys()) if isinstance(detailed_connection_graph, dict) else \"无\"}'')\n                \n                # 分析merge_candidates数据格式\n                if merge_candidates:\n                    print(''\\n   📋 merge_candidates示例数据:'')\n                    for i, candidate in enumerate(merge_candidates[:2]):\n                        print(f''     候选{i+1}: {type(candidate)}'')\n                        if isinstance(candidate, dict):\n                            print(f''       键: {list(candidate.keys())}'')\n                            print(f''       subdivision_id: {candidate.get(\"subdivision_id\", \"缺少\")}'')\n                            print(f''       replaceable_node: {candidate.get(\"replaceable_node\", \"缺少\")}'')\n                            print(f''       compatibility: {candidate.get(\"compatibility\", \"缺少\")}'')\n                \n                # 分析detailed_connection_graph数据\n                if detailed_connection_graph:\n                    nodes = detailed_connection_graph.get(''nodes'', [])\n                    edges = detailed_connection_graph.get(''edges'', [])\n                    print(f''\\n   🔗 详细连接图数据:'')\n                    print(f''     nodes数量: {len(nodes)}'')\n                    print(f''     edges数量: {len(edges)}'')\n                    \n                    # 检查节点数据格式\n                    if nodes:\n                        print(''     节点示例:'')\n                        for i, node in enumerate(nodes[:3]):\n                            print(f''       节点{i+1}: id={node.get(\"id\", \"无\")}, type={node.get(\"type\", \"无\")}, label={node.get(\"label\", \"无\")}'')\n                            print(f''                position={node.get(\"position\", \"无\")}'')\n                    \n                    # 检查边数据格式\n                    if edges:\n                        print(''     边示例:'')\n                        for i, edge in enumerate(edges[:3]):\n                            print(f''       边{i+1}: id={edge.get(\"id\", \"无\")}, source={edge.get(\"source\", \"无\")}, target={edge.get(\"target\", \"无\")}'')\n                            print(f''              sourceHandle={edge.get(\"sourceHandle\", \"无\")}, targetHandle={edge.get(\"targetHandle\", \"无\")}'')\n                \n                # 3. 检查前端期望的数据格式\n                print(f''\\n🎨 步骤3: 检查前端期望的数据格式'')\n                \n                # 模拟前端处理逻辑\n                if detailed_connection_graph.get(''nodes''):\n                    flow_nodes = []\n                    for node in detailed_connection_graph[''nodes'']:\n                        flow_node = {\n                            ''id'': node.get(''id''),\n                            ''type'': ''workflowTemplate'',\n                            ''position'': node.get(''position'', {''x'': 0, ''y'': 0}),\n                            ''data'': {\n                                **node.get(''data'', {}),\n                                ''label'': node.get(''label'') or node.get(''name'', ''Unknown Node''),\n                                ''isInternalNode'': node.get(''type'') == ''internal_node'',\n                                ''parentWorkflowId'': node.get(''data'', {}).get(''parent_workflow_id''),\n                                ''originalType'': node.get(''type'')\n                            }\n                        }\n                        flow_nodes.append(flow_node)\n                    \n                    print(f''     处理后的flow_nodes数量: {len(flow_nodes)}'')\n                    \n                    # 验证边的有效性\n                    valid_edges = []\n                    invalid_edges = []\n                    \n                    for edge in detailed_connection_graph.get(''edges'', []):\n                        if not edge.get(''id'') or not edge.get(''source'') or not edge.get(''target''):\n                            invalid_edges.append(f''边缺少基本字段: {edge}'')\n                            continue\n                        \n                        has_valid_source = any(node[''id''] == edge[''source''] for node in flow_nodes)\n                        has_valid_target = any(node[''id''] == edge[''target''] for node in flow_nodes)\n                        \n                        if has_valid_source and has_valid_target:\n                            valid_edges.append(edge)\n                        else:\n                            invalid_edges.append(f''边{edge.get(\"id\", \"无ID\")}: source={edge.get(\"source\")}存在={has_valid_source}, target={edge.get(\"target\")}存在={has_valid_target}'')\n                    \n                    print(f''     有效边数量: {len(valid_edges)}'')\n                    print(f''     无效边数量: {len(invalid_edges)}'')\n                    \n                    if invalid_edges:\n                        print(''     ⚠️ 无效边详情:'')\n                        for invalid in invalid_edges[:5]:\n                            print(f''       {invalid}'')\n                \n                # 4. 检查合并候选是否满足前端要求\n                print(f''\\n🔄 步骤4: 检查合并功能数据完整性'')\n                \n                # 模拟前端合并逻辑检查\n                enable_merge_mode = True  # 假设启用了合并模式\n                selected_merge_candidates = set()  # 假设没有选择候选\n                is_loading_merge_preview = False  # 假设没有在加载\n                \n                merge_button_disabled = selected_merge_candidates == set() or is_loading_merge_preview\n                \n                print(f''     enableMergeMode: {enable_merge_mode}'')\n                print(f''     merge_candidates数量: {len(merge_candidates)}'')\n                print(f''     selectedMergeCandidates数量: {len(selected_merge_candidates)}'')\n                print(f''     isLoadingMergePreview: {is_loading_merge_preview}'')\n                print(f''     合并按钮disabled状态: {merge_button_disabled}'')\n                print(f''     合并按钮灰色原因: {\"未选择候选\" if len(selected_merge_candidates) == 0 else \"正在加载\" if is_loading_merge_preview else \"正常\"}'')\n                \n                # 5. 总结问题\n                print(f''\\n📝 步骤5: 问题总结'')\n                \n                if len(merge_candidates) == 0:\n                    print(''   ❌ 问题1: 没有merge_candidates数据 - 这是合并按钮灰色的主要原因'')\n                else:\n                    print(''   ✅ 有merge_candidates数据'')\n                \n                if len(invalid_edges) > 0:\n                    print(''   ❌ 问题2: 存在无效边数据 - 这会导致React Flow错误'')\n                else:\n                    print(''   ✅ 边数据验证通过'')\n                \n                if not detailed_connection_graph.get(''nodes''):\n                    print(''   ❌ 问题3: 缺少nodes数据'')\n                else:\n                    print(''   ✅ 有nodes数据'')\n                \n                return {\n                    ''has_merge_candidates'': len(merge_candidates) > 0,\n                    ''merge_candidates_count'': len(merge_candidates),\n                    ''has_nodes'': len(detailed_connection_graph.get(''nodes'', [])) > 0,\n                    ''nodes_count'': len(detailed_connection_graph.get(''nodes'', [])),\n                    ''valid_edges_count'': len(valid_edges),\n                    ''invalid_edges_count'': len(invalid_edges),\n                    ''test_instance_id'': instance_id\n                }\n            \n        except Exception as e:\n            print(f''   ❌ API调用失败: {e}'')\n            import traceback\n            traceback.print_exc()\n            return None\n        \n    except Exception as e:\n        print(f''❌ 调试过程失败: {e}'')\n        import traceback\n        traceback.print_exc()\n        return None\n\n# 运行调试\nresult = asyncio.run(debug_complete_data_flow())\nif result:\n    print(f''\\n🎯 调试结果:'')\n    print(f''   有合并候选: {result[\"has_merge_candidates\"]}'')\n    print(f''   合并候选数: {result[\"merge_candidates_count\"]}'')\n    print(f''   有节点数据: {result[\"has_nodes\"]}'')\n    print(f''   节点数量: {result[\"nodes_count\"]}'')\n    print(f''   有效边数: {result[\"valid_edges_count\"]}'')\n    print(f''   无效边数: {result[\"invalid_edges_count\"]}'')\n    print(f''   测试实例ID: {result[\"test_instance_id\"]}'')\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python test_enhanced_logging.py)",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_edges_issue():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试edges查询问题 ==='')\n        \n        # 1. 检查node_connection表结构\n        print(''1. 检查node_connection表字段:'')\n        columns = await conn.fetch(''''''\n            SELECT column_name, data_type \n            FROM information_schema.columns \n            WHERE table_name = ''node_connection''\n            ORDER BY ordinal_position\n        '''''')\n        \n        for col in columns:\n            print(f''  - {col[\"\"column_name\"\"]}: {col[\"\"data_type\"\"]}'')\n            \n        # 2. 检查是否有workflow_base_id字段\n        has_workflow_base_id = any(col[''column_name''] == ''workflow_base_id'' for col in columns)\n        has_workflow_id = any(col[''column_name''] == ''workflow_id'' for col in columns)\n        \n        print(f''  node_connection表有workflow_base_id字段: {has_workflow_base_id}'')\n        print(f''  node_connection表有workflow_id字段: {has_workflow_id}'')\n        \n        # 3. 获取一个最近的工作流实例来测试\n        print(''\\n2. 获取最近的工作流实例:'')\n        workflow_instance = await conn.fetchrow(''''''\n            SELECT workflow_instance_id, workflow_base_id\n            FROM workflow_instance \n            WHERE created_at > NOW() - INTERVAL ''24 hours''\n            ORDER BY created_at DESC \n            LIMIT 1\n        '''''')\n        \n        if workflow_instance:\n            instance_id = workflow_instance[''workflow_instance_id'']\n            base_id = workflow_instance[''workflow_base_id'']\n            print(f''  实例ID: {instance_id}'')\n            print(f''  基础ID: {base_id}'')\n            \n            # 4. 查找对应的workflow_id\n            print(''\\n3. 查找对应的workflow_id:'')\n            workflow = await conn.fetchrow(''''''\n                SELECT workflow_id FROM workflow \n                WHERE workflow_base_id = $1 AND is_current_version = TRUE\n            '''''', base_id)\n            \n            if workflow:\n                workflow_id = workflow[''workflow_id'']\n                print(f''  找到workflow_id: {workflow_id}'')\n                \n                # 5. 测试两种edges查询方式\n                print(''\\n4. 测试edges查询:'')\n                \n                # 方式1: 使用workflow_id\n                edges1 = await conn.fetch(''''''\n                    SELECT \n                        nc.from_node_id,\n                        nc.to_node_id,\n                        nc.condition_config\n                    FROM node_connection nc\n                    WHERE nc.workflow_id = $1\n                    LIMIT 10\n                '''''', workflow_id)\n                print(f''  使用workflow_id查询结果: {len(edges1)} 条边'')\n                \n                # 方式2: 使用workflow_base_id (如果字段存在)\n                if has_workflow_base_id:\n                    edges2 = await conn.fetch(''''''\n                        SELECT \n                            nc.from_node_id,\n                            nc.to_node_id,\n                            nc.condition_config\n                        FROM node_connection nc\n                        WHERE nc.workflow_base_id = $1\n                        LIMIT 10\n                    '''''', base_id)\n                    print(f''  使用workflow_base_id查询结果: {len(edges2)} 条边'')\n                \n                # 6. 检查node_connection表中的数据\n                print(''\\n5. 检查node_connection表数据:'')\n                all_connections = await conn.fetch(''''''\n                    SELECT workflow_id, workflow_base_id, from_node_id, to_node_id\n                    FROM node_connection \n                    LIMIT 5\n                '''''')\n                print(f''  node_connection表总记录数(前5条):'')\n                for conn_row in all_connections:\n                    print(f''    workflow_id: {conn_row.get(\"\"workflow_id\"\", \"\"无此字段\"\")}, workflow_base_id: {conn_row.get(\"\"workflow_base_id\"\", \"\"无此字段\"\")}'')\n                    \n            else:\n                print(''  未找到对应的workflow记录'')\n        else:\n            print(''  未找到最近的工作流实例'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(debug_edges_issue())\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_workflow_merge_error():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试工作流合并父工作流不存在问题 ==='')\n        \n        # 1. 检查最近的工作流实例数据\n        print(''1. 检查最近的工作流实例:'')\n        recent_instances = await conn.fetch(''''''\n            SELECT workflow_instance_id, workflow_base_id, workflow_instance_name, status\n            FROM workflow_instance \n            WHERE created_at > NOW() - INTERVAL ''24 hours''\n            ORDER BY created_at DESC \n            LIMIT 5\n        '''''')\n        \n        for instance in recent_instances:\n            instance_id = instance[''workflow_instance_id'']\n            base_id = instance[''workflow_base_id''] \n            name = instance[''workflow_instance_name'']\n            status = instance[''status'']\n            print(f''  实例: {name} ({instance_id})'')\n            print(f''    workflow_base_id: {base_id}'')\n            print(f''    状态: {status}'')\n            \n            # 2. 检查这个base_id对应的workflow记录\n            workflow = await conn.fetchrow(''''''\n                SELECT workflow_id, workflow_base_id, name, is_current_version, is_deleted\n                FROM workflow \n                WHERE workflow_base_id = $1\n            '''''', base_id)\n            \n            if workflow:\n                print(f''    对应workflow存在: {workflow[\"name\"]}'')\n                print(f''      workflow_id: {workflow[\"workflow_id\"]}'')\n                print(f''      is_current_version: {workflow[\"is_current_version\"]}'')\n                print(f''      is_deleted: {workflow[\"is_deleted\"]}'')\n                \n                # 检查是否满足查询条件\n                current_version = await conn.fetchrow(''''''\n                    SELECT workflow_id, name\n                    FROM workflow \n                    WHERE workflow_base_id = $1\n                    AND is_current_version = TRUE\n                    AND is_deleted = FALSE\n                '''''', base_id)\n                \n                if current_version:\n                    print(f''    ✅ 满足合并条件，可以找到当前版本'')\n                else:\n                    print(f''    ❌ 不满足合并条件 (is_current_version=TRUE且is_deleted=FALSE)'')\n            else:\n                print(f''    ❌ 对应的workflow记录不存在'')\n            print()\n        \n        # 3. 检查最近的合并候选数据\n        print(''2. 检查最近的合并候选数据:'')\n        merge_candidates = await conn.fetch(''''''\n            SELECT ts.subdivision_id, ts.sub_workflow_instance_id,\n                   ti.workflow_instance_id as parent_workflow_instance_id,\n                   wi.workflow_base_id as parent_workflow_base_id\n            FROM task_subdivision ts\n            JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id  \n            JOIN workflow_instance wi ON ti.workflow_instance_id = wi.workflow_instance_id\n            WHERE ts.sub_workflow_instance_id IS NOT NULL\n            AND ts.created_at > NOW() - INTERVAL ''24 hours''\n            ORDER BY ts.created_at DESC\n            LIMIT 3\n        '''''')\n        \n        for candidate in merge_candidates:\n            subdivision_id = candidate[''subdivision_id'']\n            parent_base_id = candidate[''parent_workflow_base_id'']\n            print(f''  合并候选 {subdivision_id}:'')\n            print(f''    parent_workflow_base_id: {parent_base_id}'')\n            \n            # 检查这个ID是否能在workflow表中找到\n            workflow_check = await conn.fetchrow(''''''\n                SELECT workflow_id, name, is_current_version, is_deleted\n                FROM workflow \n                WHERE workflow_base_id = $1\n                AND is_current_version = TRUE\n                AND is_deleted = FALSE\n            '''''', parent_base_id)\n            \n            if workflow_check:\n                print(f''    ✅ 父工作流可以找到: {workflow_check[\"name\"]}'')\n            else:\n                print(f''    ❌ 父工作流找不到或不满足条件'')\n        \n        await conn.close()\n        print(''\\n调试完成'')\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(debug_workflow_merge_error())\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport sys\nsys.path.append(''.'')\n\nasync def test_real_merge_execution():\n    print(''🔍 测试真实的工作流合并执行'')\n    print(''='' * 60)\n    \n    try:\n        from backend.services.workflow_merge_service import WorkflowMergeService\n        from backend.repositories.base import BaseRepository\n        \n        merge_service = WorkflowMergeService()\n        db = BaseRepository(''test'').db\n        \n        # 1. 获取真实的合并候选数据\n        print(''📋 步骤1: 获取真实的合并候选数据'')\n        query = ''''''\n        SELECT \n            ts.subdivision_id,\n            ts.sub_workflow_instance_id,\n            ts.sub_workflow_base_id,\n            ni.node_id as replaceable_node_id,\n            n.node_base_id as replaceable_node_base_id,\n            n.name as replaceable_node_name,\n            n.type as replaceable_node_type,\n            wi.workflow_base_id as parent_workflow_base_id,\n            wi.workflow_instance_id as parent_workflow_instance_id\n        FROM task_subdivision ts\n        JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id\n        JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id\n        JOIN node n ON ni.node_id = n.node_id\n        JOIN workflow_instance wi ON ti.workflow_instance_id = wi.workflow_instance_id\n        WHERE ts.sub_workflow_instance_id IS NOT NULL\n        AND ts.is_deleted = FALSE\n        AND ts.created_at > NOW() - INTERVAL ''24 hours''\n        ORDER BY ts.created_at DESC\n        LIMIT 1\n        ''''''\n        \n        result = await db.fetch_one(query)\n        \n        if not result:\n            print(''❌ 没有找到合适的合并候选数据'')\n            return\n        \n        print(''✅ 找到合并候选:'')\n        for key, value in result.items():\n            print(f''   - {key}: {value}'')\n        \n        # 2. 构造合并请求数据\n        print(''\\n🔧 步骤2: 构造合并请求数据'')\n        selected_merges = [{\n            ''subdivision_id'': str(result[''subdivision_id'']),\n            ''target_node_id'': str(result[''replaceable_node_base_id'']),\n            ''sub_workflow_id'': str(result[''sub_workflow_base_id'']),\n            ''nodes_to_add'': 5,\n            ''connections_to_add'': 3\n        }]\n        \n        merge_config = {\n            ''new_workflow_name'': f''Test_Merged_Workflow_{result[\"\"subdivision_id\"\"]}'',\n            ''new_workflow_description'': ''Test merged workflow created by debug script'',\n            ''preserve_original'': True,\n            ''execute_immediately'': False,\n            ''notify_on_completion'': True\n        }\n        \n        parent_workflow_base_id = result[''parent_workflow_base_id'']\n        \n        print(f''   - parent_workflow_base_id: {parent_workflow_base_id}'')\n        print(f''   - selected_merges: {selected_merges}'')\n        print(f''   - merge_config: {merge_config}'')\n        \n        # 3. 执行合并操作\n        print(''\\n🚀 步骤3: 执行合并操作'')\n        \n        # 模拟用户ID\n        import uuid\n        user_id = uuid.uuid4()\n        \n        merge_result = await merge_service.execute_workflow_merge(\n            parent_workflow_base_id, selected_merges, merge_config, user_id\n        )\n        \n        print(''✅ 合并执行完成'')\n        print(''📊 合并结果:'')\n        for key, value in merge_result.items():\n            print(f''   - {key}: {value}'')\n            \n        if merge_result.get(''success''):\n            print(''🎉 工作流合并成功!'')\n        else:\n            print(''❌ 工作流合并失败:'', merge_result.get(''message''))\n            if merge_result.get(''errors''):\n                print(''错误详情:'', merge_result[''errors''])\n        \n    except Exception as e:\n        print(f''❌ 测试失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(test_real_merge_execution())\n\")",
      "Bash(/home/ubuntu/anaconda3/envs/workflow/bin/python -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_subdivision_data():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试子工作流查询问题 ==='')\n        \n        # 1. 检查任务ID是否存在\n        task_id = ''1dc9b041-eb4c-4cbc-bd3f-2e6c5eff0ffd''\n        print(f''1. 检查任务 {task_id} 是否存在:'')\n        \n        task_exists = await conn.fetchrow(''SELECT task_instance_id, task_title, status FROM task_instance WHERE task_instance_id = $1'', task_id)\n        if task_exists:\n            print(f''   ✅ 任务存在: {task_exists[\"\"task_title\"\"]} (状态: {task_exists[\"\"status\"\"]})'')\n        else:\n            print(f''   ❌ 任务不存在'')\n            await conn.close()\n            return\n        \n        # 2. 检查该任务是否有subdivision记录\n        print(f''2. 检查任务的subdivision记录:'')\n        subdivisions = await conn.fetch(''SELECT subdivision_id, subdivision_name, status, sub_workflow_instance_id, sub_workflow_base_id FROM task_subdivision WHERE original_task_id = $1 AND is_deleted = FALSE'', task_id)\n        \n        if subdivisions:\n            print(f''   ✅ 找到 {len(subdivisions)} 个subdivision记录:'')\n            for sub in subdivisions:\n                print(f''     - {sub[\"\"subdivision_name\"\"]} (ID: {sub[\"\"subdivision_id\"\"]})'')\n                print(f''       状态: {sub[\"\"status\"\"]}'')\n                print(f''       子工作流实例ID: {sub[\"\"sub_workflow_instance_id\"\"]}'')\n                print(f''       子工作流基础ID: {sub[\"\"sub_workflow_base_id\"\"]}'')\n        else:\n            print(f''   ❌ 没有找到subdivision记录'')\n            \n            # 检查所有subdivision记录\n            print(f''3. 检查数据库中所有subdivision记录（最近20条）:'')\n            all_subdivisions = await conn.fetch(''''''\n                SELECT ts.subdivision_id, ts.subdivision_name, ts.original_task_id, \n                       ti.task_title, ts.sub_workflow_instance_id\n                FROM task_subdivision ts\n                LEFT JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id\n                WHERE ts.is_deleted = FALSE\n                ORDER BY ts.created_at DESC\n                LIMIT 20\n            '''''')\n            \n            for sub in all_subdivisions:\n                print(f''   - {sub[\"\"subdivision_name\"\"]} (任务: {sub[\"\"task_title\"\"]})'')\n                print(f''     原始任务ID: {sub[\"\"original_task_id\"\"]}'')\n                print(f''     子工作流实例ID: {sub[\"\"sub_workflow_instance_id\"\"]}'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(debug_subdivision_data())\n\")",
      "Bash(PYTHONPATH=. /usr/bin/python3 -c \"\nimport asyncio\nimport asyncpg\n\nasync def debug_subdivision_data():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 调试子工作流查询问题 ==='')\n        \n        # 1. 检查任务ID是否存在\n        task_id = ''1dc9b041-eb4c-4cbc-bd3f-2e6c5eff0ffd''\n        print(f''1. 检查任务 {task_id} 是否存在:'')\n        \n        task_exists = await conn.fetchrow(''SELECT task_instance_id, task_title, status FROM task_instance WHERE task_instance_id = $1'', task_id)\n        if task_exists:\n            print(f''   ✅ 任务存在: {task_exists[\"\"task_title\"\"]} (状态: {task_exists[\"\"status\"\"]})'')\n        else:\n            print(f''   ❌ 任务不存在'')\n            await conn.close()\n            return\n        \n        # 2. 检查该任务是否有subdivision记录\n        print(f''2. 检查任务的subdivision记录:'')\n        subdivisions = await conn.fetch(''SELECT subdivision_id, subdivision_name, status, sub_workflow_instance_id, sub_workflow_base_id FROM task_subdivision WHERE original_task_id = $1 AND is_deleted = FALSE'', task_id)\n        \n        if subdivisions:\n            print(f''   ✅ 找到 {len(subdivisions)} 个subdivision记录:'')\n            for sub in subdivisions:\n                print(f''     - {sub[\"\"subdivision_name\"\"]} (ID: {sub[\"\"subdivision_id\"\"]})'')\n                print(f''       状态: {sub[\"\"status\"\"]}'')\n                print(f''       子工作流实例ID: {sub[\"\"sub_workflow_instance_id\"\"]}'')\n                print(f''       子工作流基础ID: {sub[\"\"sub_workflow_base_id\"\"]}'')\n        else:\n            print(f''   ❌ 没有找到subdivision记录'')\n            \n            # 检查所有subdivision记录\n            print(f''3. 检查数据库中所有subdivision记录（最近20条）:'')\n            all_subdivisions = await conn.fetch(''''''\n                SELECT ts.subdivision_id, ts.subdivision_name, ts.original_task_id, \n                       ti.task_title, ts.sub_workflow_instance_id\n                FROM task_subdivision ts\n                LEFT JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id\n                WHERE ts.is_deleted = FALSE\n                ORDER BY ts.created_at DESC\n                LIMIT 20\n            '''''')\n            \n            for sub in all_subdivisions:\n                print(f''   - {sub[\"\"subdivision_name\"\"]} (任务: {sub[\"\"task_title\"\"]})'')\n                print(f''     原始任务ID: {sub[\"\"original_task_id\"\"]}'')\n                print(f''     子工作流实例ID: {sub[\"\"sub_workflow_instance_id\"\"]}'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''调试失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(debug_subdivision_data())\n\")",
      "Bash(PYTHONPATH=. /usr/bin/python3 -c \"\nimport asyncio\nimport asyncpg\n\nasync def verify_existing_tasks():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 验证已知有子工作流的任务ID ==='')\n        \n        # 验证前端日志中提到的任务ID\n        task_ids = [\n            ''c97166a9-4099-48bf-9832-eb486e9a685f'',\n            ''0e69c924-fbe7-4be4-9514-5bbf7dc9c8d1'', \n            ''e4f58eae-60de-4ebb-b42f-4d5f5de76642''\n        ]\n        \n        for task_id in task_ids:\n            print(f''\\n检查任务 {task_id}:'')\n            \n            # 检查任务是否存在\n            task = await conn.fetchrow(''SELECT task_instance_id, task_title, status FROM task_instance WHERE task_instance_id = $1'', task_id)\n            if task:\n                print(f''  ✅ 任务存在: {task[\"\"task_title\"\"]} (状态: {task[\"\"status\"\"]})'')\n                \n                # 检查subdivisions\n                subdivisions = await conn.fetch(''''''\n                    SELECT subdivision_id, subdivision_name, sub_workflow_instance_id, status\n                    FROM task_subdivision \n                    WHERE original_task_id = $1 AND is_deleted = FALSE\n                '''''', task_id)\n                \n                print(f''  📋 Subdivisions: {len(subdivisions)} 个'')\n                \n                # 检查有实例的subdivisions\n                with_instances = [s for s in subdivisions if s[''sub_workflow_instance_id'']]\n                print(f''  🎯 有工作流实例的: {len(with_instances)} 个'')\n                \n                for sub in with_instances[:3]:  # 只显示前3个\n                    print(f''    - {sub[\"\"subdivision_name\"\"]} (实例ID: {sub[\"\"sub_workflow_instance_id\"\"]})'')\n                    print(f''      状态: {sub[\"\"status\"\"]}'')\n            else:\n                print(f''  ❌ 任务不存在'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''验证失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(verify_existing_tasks())\n\")",
      "Bash(PYTHONPATH=. /usr/bin/python3 -c \"\nimport asyncio\nimport asyncpg\n\nasync def find_actual_subdivisions():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 查找实际存在的subdivision数据 ==='')\n        \n        # 查找最近的有子工作流实例的任务\n        recent_subdivisions = await conn.fetch(''''''\n            SELECT \n                ts.subdivision_id,\n                ts.subdivision_name,\n                ts.original_task_id,\n                ts.sub_workflow_instance_id,\n                ts.status,\n                ti.task_title,\n                ti.status as task_status,\n                ts.created_at\n            FROM task_subdivision ts\n            JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id\n            WHERE ts.is_deleted = FALSE \n            AND ts.sub_workflow_instance_id IS NOT NULL\n            ORDER BY ts.created_at DESC\n            LIMIT 10\n        '''''')\n        \n        if recent_subdivisions:\n            print(f''✅ 找到 {len(recent_subdivisions)} 个有子工作流实例的subdivision:'')\n            for sub in recent_subdivisions:\n                print(f''\\n任务: {sub[\"\"task_title\"\"]}'')\n                print(f''  - 任务ID: {sub[\"\"original_task_id\"\"]}'')\n                print(f''  - 任务状态: {sub[\"\"task_status\"\"]}'')\n                print(f''  - Subdivision: {sub[\"\"subdivision_name\"\"]}'')\n                print(f''  - Subdivision状态: {sub[\"\"status\"\"]}'')\n                print(f''  - 子工作流实例ID: {sub[\"\"sub_workflow_instance_id\"\"]}'')\n                print(f''  - 创建时间: {sub[\"\"created_at\"\"]}'')\n        else:\n            print(''❌ 没有找到任何有子工作流实例的subdivision'')\n            \n            # 检查是否有任何subdivision记录\n            all_subdivisions = await conn.fetch(''''''\n                SELECT COUNT(*) as total_count,\n                       COUNT(CASE WHEN sub_workflow_instance_id IS NOT NULL THEN 1 END) as with_instances\n                FROM task_subdivision \n                WHERE is_deleted = FALSE\n            '''''')\n            \n            if all_subdivisions:\n                total = all_subdivisions[0][''total_count'']\n                with_instances = all_subdivisions[0][''with_instances'']\n                print(f''数据统计:'')\n                print(f''  - 总subdivision数: {total}'')\n                print(f''  - 有实例的subdivision数: {with_instances}'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''查找失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(find_actual_subdivisions())\n\")",
      "Bash(PYTHONPATH=. /usr/bin/python3 -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_table_schemas():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 子工作流相关表结构检查 ==='')\n        \n        tables_to_check = [\n            ''task_instance'',\n            ''task_subdivision'', \n            ''workflow_instance'',\n            ''node_instance''\n        ]\n        \n        for table_name in tables_to_check:\n            print(f''\\n📋 {table_name} 表结构:'')\n            columns = await conn.fetch(''''''\n                SELECT column_name, data_type, is_nullable\n                FROM information_schema.columns \n                WHERE table_name = $1 \n                ORDER BY ordinal_position\n            '''''', table_name)\n            \n            for col in columns:\n                nullable = ''(nullable)'' if col[''is_nullable''] == ''YES'' else ''(not null)''\n                print(f''  - {col[\"\"column_name\"\"]}: {col[\"\"data_type\"\"]} {nullable}'')\n            \n            # 检查表中的记录数量\n            count_result = await conn.fetchrow(f''SELECT COUNT(*) as count FROM {table_name}'')\n            total_count = count_result[''count'']\n            print(f''  📊 总记录数: {total_count}'')\n            \n            # 如果是task_subdivision表，检查有sub_workflow_instance_id的记录\n            if table_name == ''task_subdivision'':\n                with_instance_count = await conn.fetchrow(''''''\n                    SELECT COUNT(*) as count \n                    FROM task_subdivision \n                    WHERE sub_workflow_instance_id IS NOT NULL \n                    AND is_deleted = FALSE\n                '''''')\n                print(f''  🎯 有子工作流实例的记录: {with_instance_count[\"\"count\"\"]}'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''检查失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(check_table_schemas())\n\")",
      "Bash(PYTHONPATH=. /usr/bin/python3 -c \"\nimport asyncio\nimport asyncpg\n\nasync def check_recent_workflow_data():\n    try:\n        conn = await asyncpg.connect(''postgresql://postgres:postgresql@localhost:5432/workflow_db'')\n        \n        print(''=== 最近的工作流执行数据检查 ==='')\n        \n        # 1. 检查最近的工作流实例\n        print(''\\n🔍 最近的工作流实例:'')\n        recent_workflows = await conn.fetch(''''''\n            SELECT workflow_instance_id, workflow_instance_name, status, \n                   created_at, started_at, completed_at\n            FROM workflow_instance \n            ORDER BY created_at DESC \n            LIMIT 5\n        '''''')\n        \n        for wf in recent_workflows:\n            status_icon = ''✅'' if wf[''status''] == ''completed'' else ''🔄'' if wf[''status''] == ''running'' else ''❌''\n            print(f''  {status_icon} {wf[\"\"workflow_instance_name\"\"]} ({wf[\"\"workflow_instance_id\"\"]})'')\n            print(f''      状态: {wf[\"\"status\"\"]} | 创建: {wf[\"\"created_at\"\"]} | 完成: {wf[\"\"completed_at\"\"]}'')\n        \n        # 2. 检查最近的任务实例\n        print(''\\n📋 最近的任务实例:'')\n        recent_tasks = await conn.fetch(''''''\n            SELECT ti.task_instance_id, ti.task_title, ti.status,\n                   ti.workflow_instance_id, wi.workflow_instance_name,\n                   ti.created_at, ti.completed_at\n            FROM task_instance ti\n            JOIN workflow_instance wi ON ti.workflow_instance_id = wi.workflow_instance_id\n            ORDER BY ti.created_at DESC \n            LIMIT 5\n        '''''')\n        \n        for task in recent_tasks:\n            status_icon = ''✅'' if task[''status''] == ''completed'' else ''🔄'' if task[''status''] in [''assigned'', ''running''] else ''❌''\n            print(f''  {status_icon} {task[\"\"task_title\"\"]} ({task[\"\"task_instance_id\"\"]})'')\n            print(f''      所属工作流: {task[\"\"workflow_instance_name\"\"]}'')\n            print(f''      状态: {task[\"\"status\"\"]} | 创建: {task[\"\"created_at\"\"]} | 完成: {task[\"\"completed_at\"\"]}'')\n        \n        # 3. 检查是否有任务被标记为subdivision相关\n        print(''\\n🔍 检查任务是否有subdivision痕迹:'')\n        \n        # 检查任务描述或名称中是否包含subdivision相关关键词\n        subdivision_hints = await conn.fetch(''''''\n            SELECT task_instance_id, task_title, task_description, \n                   context_data, workflow_instance_id\n            FROM task_instance \n            WHERE (\n                LOWER(task_title) LIKE ''%subdivision%'' OR \n                LOWER(task_title) LIKE ''%sub%'' OR\n                LOWER(task_description) LIKE ''%subdivision%'' OR\n                LOWER(context_data) LIKE ''%subdivision%''\n            )\n            ORDER BY created_at DESC\n        '''''')\n        \n        if subdivision_hints:\n            print(f''  找到 {len(subdivision_hints)} 个可能的subdivision相关任务:'')\n            for hint in subdivision_hints:\n                print(f''    - {hint[\"\"task_title\"\"]} ({hint[\"\"task_instance_id\"\"]})'')\n                print(f''      工作流: {hint[\"\"workflow_instance_id\"\"]}'')\n        else:\n            print(''  没有找到subdivision相关的任务'')\n        \n        # 4. 检查工作流名称中是否有subdivision相关\n        print(''\\n🔍 检查工作流名称中的subdivision痕迹:'')\n        subdivision_workflows = await conn.fetch(''''''\n            SELECT workflow_instance_id, workflow_instance_name, status, created_at\n            FROM workflow_instance \n            WHERE (\n                LOWER(workflow_instance_name) LIKE ''%subdivision%'' OR\n                LOWER(workflow_instance_name) LIKE ''%sub%''\n            )\n            ORDER BY created_at DESC\n        '''''')\n        \n        if subdivision_workflows:\n            print(f''  找到 {len(subdivision_workflows)} 个可能的subdivision相关工作流:'')\n            for wf in subdivision_workflows:\n                print(f''    - {wf[\"\"workflow_instance_name\"\"]} ({wf[\"\"workflow_instance_id\"\"]})'')\n                print(f''      状态: {wf[\"\"status\"\"]} | 创建: {wf[\"\"created_at\"\"]}'')\n        else:\n            print(''  没有找到subdivision相关的工作流'')\n        \n        await conn.close()\n        \n    except Exception as e:\n        print(f''检查失败: {e}'')\n        import traceback\n        traceback.print_exc()\n\nasyncio.run(check_recent_workflow_data())\n\")",
      "Bash(PYTHONPATH:*)",
      "Bash(/usr/bin/python3:*)",
      "Bash(mv:*)",
      "Bash(nslookup:*)"
    ],
    "deny": [],
    "defaultMode": "acceptEdits"
  }
}