"""
å·¥ä½œæµè¾“å‡ºæ•°æ®å¤„ç†å™¨
Output Data Processor for Workflow Results
"""

import uuid
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from loguru import logger

from ..models.instance import (
    WorkflowOutputSummary, ExecutionResult, ExecutionStatistics,
    QualityMetrics, DataLineage, DataLineageStep, ExecutionIssues,
    WorkflowInstanceStatus, NodeInstanceStatus, TaskInstanceStatus
)
from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
from ..repositories.instance.node_instance_repository import NodeInstanceRepository
from ..repositories.instance.task_instance_repository import TaskInstanceRepository
from ..utils.helpers import now_utc


class OutputDataProcessor:
    """å·¥ä½œæµè¾“å‡ºæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.workflow_repo = WorkflowInstanceRepository()
        self.node_repo = NodeInstanceRepository()
        self.task_repo = TaskInstanceRepository()
    
    async def generate_workflow_output_summary(self, workflow_instance_id: uuid.UUID) -> Optional[WorkflowOutputSummary]:
        """ç”Ÿæˆå·¥ä½œæµè¾“å‡ºæ‘˜è¦"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆå·¥ä½œæµè¾“å‡ºæ‘˜è¦: {workflow_instance_id}")
            
            # è·å–å·¥ä½œæµå®ä¾‹ä¿¡æ¯
            workflow_instance = await self.workflow_repo.get_instance_by_id(workflow_instance_id)
            if not workflow_instance:
                logger.error(f"âŒ å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {workflow_instance_id}")
                return None
            
            # è·å–èŠ‚ç‚¹å®ä¾‹åˆ—è¡¨
            node_instances = await self.node_repo.get_instances_by_workflow(workflow_instance_id)
            
            # è·å–ä»»åŠ¡å®ä¾‹åˆ—è¡¨
            task_instances = await self.task_repo.get_instances_by_workflow(workflow_instance_id)
            
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: èŠ‚ç‚¹æ•°={len(node_instances)}, ä»»åŠ¡æ•°={len(task_instances)}")
            
            # ç”Ÿæˆå„ä¸ªç»„ä»¶
            execution_result = await self._generate_execution_result(workflow_instance, node_instances, task_instances)
            execution_stats = await self._generate_execution_statistics(workflow_instance_id, workflow_instance, node_instances, task_instances)
            quality_metrics = await self._generate_quality_metrics(workflow_instance, node_instances, task_instances)
            data_lineage = await self._generate_data_lineage(workflow_instance, node_instances)
            issues = await self._generate_execution_issues(workflow_instance, node_instances, task_instances)
            
            # æ„å»ºè¾“å‡ºæ‘˜è¦
            output_summary = WorkflowOutputSummary(
                execution_result=execution_result,
                execution_stats=execution_stats,
                quality_metrics=quality_metrics,
                data_lineage=data_lineage,
                issues=issues,
                generated_at=datetime.utcnow()
            )
            
            logger.info(f"âœ… å·¥ä½œæµè¾“å‡ºæ‘˜è¦ç”Ÿæˆå®Œæˆ: {workflow_instance_id}")
            return output_summary
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå·¥ä½œæµè¾“å‡ºæ‘˜è¦å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None
    
    async def _generate_execution_result(self, workflow_instance: Dict[str, Any], 
                                       node_instances: List[Dict[str, Any]], 
                                       task_instances: List[Dict[str, Any]]) -> ExecutionResult:
        """ç”Ÿæˆæ‰§è¡Œç»“æœ"""
        try:
            # åˆ†æå·¥ä½œæµçŠ¶æ€
            status = workflow_instance.get('status')
            if status == WorkflowInstanceStatus.COMPLETED.value:
                result_type = "success"
            elif status == WorkflowInstanceStatus.FAILED.value:
                result_type = "failure"
            elif status in [WorkflowInstanceStatus.CANCELLED.value]:
                result_type = "failure"
            else:
                # éƒ¨åˆ†å®Œæˆçš„æƒ…å†µ
                completed_nodes = len([n for n in node_instances if n.get('status') == NodeInstanceStatus.COMPLETED.value])
                total_nodes = len(node_instances)
                if completed_nodes > 0 and completed_nodes < total_nodes:
                    result_type = "partial_success"
                else:
                    result_type = "failure"
            
            # ç»Ÿè®¡å¤„ç†æ•°é‡
            processed_count = len([t for t in task_instances if t.get('status') in [
                TaskInstanceStatus.COMPLETED.value, TaskInstanceStatus.FAILED.value
            ]])
            success_count = len([t for t in task_instances if t.get('status') == TaskInstanceStatus.COMPLETED.value])
            error_count = len([t for t in task_instances if t.get('status') == TaskInstanceStatus.FAILED.value])
            
            # èšåˆä¸šåŠ¡è¾“å‡ºæ•°æ®
            data_output = {}
            if workflow_instance.get('output_data'):
                data_output = workflow_instance['output_data']
            
            # å¦‚æœå·¥ä½œæµæ²¡æœ‰è¾“å‡ºæ•°æ®ï¼Œå°è¯•èšåˆèŠ‚ç‚¹è¾“å‡º
            if not data_output:
                node_outputs = {}
                for node in node_instances:
                    if node.get('output_data'):
                        node_name = node.get('node_instance_name', f"node_{node.get('node_instance_id')}")
                        node_output_data = node['output_data']
                        
                        # ä¼˜åŒ–ï¼šæå–ä¸»è¦è¾“å‡ºæ•°æ®
                        if isinstance(node_output_data, dict):
                            # å¦‚æœæœ‰primary_outputï¼Œä¼˜å…ˆä½¿ç”¨
                            if 'primary_output' in node_output_data:
                                node_outputs[node_name] = node_output_data['primary_output']
                            # å¦‚æœæœ‰tasks_outputä¸”åªæœ‰ä¸€ä¸ªä»»åŠ¡ï¼Œç›´æ¥æå‡
                            elif 'tasks_output' in node_output_data and len(node_output_data.get('tasks_output', {})) == 1:
                                task_outputs = list(node_output_data['tasks_output'].values())
                                node_outputs[node_name] = task_outputs[0] if task_outputs else node_output_data
                            else:
                                node_outputs[node_name] = node_output_data
                        else:
                            node_outputs[node_name] = node_output_data
                            
                if node_outputs:
                    data_output = {"aggregated_node_outputs": node_outputs}
                    # å¦‚æœåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹è¾“å‡ºï¼Œç›´æ¥æå‡åˆ°é¡¶å±‚
                    if len(node_outputs) == 1:
                        single_output = list(node_outputs.values())[0]
                        if isinstance(single_output, dict):
                            data_output.update(single_output)
                            data_output["primary_node_output"] = single_output
            
            return ExecutionResult(
                result_type=result_type,
                processed_count=processed_count,
                success_count=success_count,
                error_count=error_count,
                data_output=data_output
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‰§è¡Œç»“æœå¤±è´¥: {e}")
            return ExecutionResult(
                result_type="failure",
                processed_count=0,
                success_count=0,
                error_count=1,
                data_output={"error": str(e)}
            )
    
    async def _generate_execution_statistics(self, workflow_instance_id: uuid.UUID,
                                           workflow_instance: Dict[str, Any],
                                           node_instances: List[Dict[str, Any]], 
                                           task_instances: List[Dict[str, Any]]) -> ExecutionStatistics:
        """ç”Ÿæˆæ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        try:
            # èŠ‚ç‚¹ç»Ÿè®¡
            total_nodes = len(node_instances)
            completed_nodes = len([n for n in node_instances if n.get('status') == NodeInstanceStatus.COMPLETED.value])
            failed_nodes = len([n for n in node_instances if n.get('status') == NodeInstanceStatus.FAILED.value])
            pending_nodes = len([n for n in node_instances if n.get('status') in [
                NodeInstanceStatus.PENDING.value, NodeInstanceStatus.WAITING.value
            ]])
            
            # ä»»åŠ¡ç»Ÿè®¡
            total_tasks = len(task_instances)
            completed_tasks = len([t for t in task_instances if t.get('status') == TaskInstanceStatus.COMPLETED.value])
            failed_tasks = len([t for t in task_instances if t.get('status') == TaskInstanceStatus.FAILED.value])
            pending_tasks = len([t for t in task_instances if t.get('status') in [
                TaskInstanceStatus.PENDING.value, TaskInstanceStatus.ASSIGNED.value, TaskInstanceStatus.WAITING.value
            ]])
            
            # ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            human_tasks = len([t for t in task_instances if t.get('task_type') == 'human'])
            agent_tasks = len([t for t in task_instances if t.get('task_type') == 'agent'])
            mixed_tasks = len([t for t in task_instances if t.get('task_type') == 'mixed'])
            
            # è®¡ç®—å¹³å‡ä»»åŠ¡æ—¶é•¿
            completed_task_durations = []
            for task in task_instances:
                if task.get('actual_duration'):
                    completed_task_durations.append(task['actual_duration'])
            
            average_task_duration = None
            if completed_task_durations:
                average_task_duration = sum(completed_task_durations) / len(completed_task_durations)
            
            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            total_execution_time = None
            total_duration_minutes = None
            if workflow_instance.get('started_at') and workflow_instance.get('completed_at'):
                start_time = datetime.fromisoformat(workflow_instance['started_at'].replace('Z', '+00:00'))
                end_time = datetime.fromisoformat(workflow_instance['completed_at'].replace('Z', '+00:00'))
                total_execution_time = int((end_time - start_time).total_seconds())
                total_duration_minutes = int(total_execution_time / 60)
            
            # è®¡ç®—å¹³å‡èŠ‚ç‚¹æ‰§è¡Œæ—¶é•¿
            average_node_duration = None
            if total_duration_minutes and completed_nodes > 0:
                average_node_duration = total_duration_minutes / completed_nodes
            
            return ExecutionStatistics(
                workflow_instance_id=workflow_instance_id,
                total_nodes=total_nodes,
                completed_nodes=completed_nodes,
                failed_nodes=failed_nodes,
                pending_nodes=pending_nodes,
                total_tasks=total_tasks,
                completed_tasks=completed_tasks,
                failed_tasks=failed_tasks,
                pending_tasks=pending_tasks,
                human_tasks=human_tasks,
                agent_tasks=agent_tasks,
                mixed_tasks=mixed_tasks,
                average_task_duration=average_task_duration,
                total_execution_time=total_execution_time,
                total_duration_minutes=total_duration_minutes,
                average_node_duration=average_node_duration
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‰§è¡Œç»Ÿè®¡å¤±è´¥: {e}")
            return ExecutionStatistics(
                workflow_instance_id=workflow_instance_id,
                total_nodes=0, completed_nodes=0, failed_nodes=0, pending_nodes=0,
                total_tasks=0, completed_tasks=0, failed_tasks=0, pending_tasks=0,
                human_tasks=0, agent_tasks=0, mixed_tasks=0
            )
    
    async def _generate_quality_metrics(self, workflow_instance: Dict[str, Any],
                                      node_instances: List[Dict[str, Any]], 
                                      task_instances: List[Dict[str, Any]]) -> QualityMetrics:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°æŒ‡æ ‡"""
        try:
            validation_errors = []
            
            # æ•°æ®å®Œæ•´æ€§è¯„ä¼°
            total_outputs = 0
            valid_outputs = 0
            
            for node in node_instances:
                total_outputs += 1
                if node.get('output_data') and node.get('status') == NodeInstanceStatus.COMPLETED.value:
                    valid_outputs += 1
                elif node.get('status') == NodeInstanceStatus.FAILED.value:
                    validation_errors.append(f"èŠ‚ç‚¹ {node.get('node_instance_name', 'unknown')} æ‰§è¡Œå¤±è´¥")
            
            data_completeness = valid_outputs / total_outputs if total_outputs > 0 else 0.0
            
            # å‡†ç¡®æ€§è¯„åˆ†ï¼ˆåŸºäºæˆåŠŸç‡ï¼‰
            total_tasks = len(task_instances)
            successful_tasks = len([t for t in task_instances if t.get('status') == TaskInstanceStatus.COMPLETED.value])
            accuracy_score = successful_tasks / total_tasks if total_tasks > 0 else 0.0
            
            # è´¨é‡é—¨ç¦æ£€æŸ¥
            quality_gates_passed = (
                data_completeness >= 0.8 and  # æ•°æ®å®Œæ•´æ€§è‡³å°‘80%
                accuracy_score >= 0.8 and     # å‡†ç¡®æ€§è‡³å°‘80%
                len(validation_errors) == 0   # æ— éªŒè¯é”™è¯¯
            )
            
            # æ•´ä½“è´¨é‡è¯„åˆ†
            overall_quality_score = (data_completeness + accuracy_score) / 2
            
            return QualityMetrics(
                data_completeness=data_completeness,
                accuracy_score=accuracy_score,
                validation_errors=validation_errors,
                quality_gates_passed=quality_gates_passed,
                overall_quality_score=overall_quality_score
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè´¨é‡è¯„ä¼°æŒ‡æ ‡å¤±è´¥: {e}")
            return QualityMetrics(
                data_completeness=0.0,
                accuracy_score=0.0,
                validation_errors=[f"è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}"],
                quality_gates_passed=False,
                overall_quality_score=0.0
            )
    
    async def _generate_data_lineage(self, workflow_instance: Dict[str, Any],
                                   node_instances: List[Dict[str, Any]]) -> DataLineage:
        """ç”Ÿæˆæ•°æ®è¡€ç¼˜è¿½æº¯ä¿¡æ¯"""
        try:
            # è¾“å…¥æ¥æºåˆ†æ
            input_sources = []
            if workflow_instance.get('input_data'):
                input_data = workflow_instance['input_data']
                if isinstance(input_data, dict):
                    for key in input_data.keys():
                        if 'file' in key.lower() or 'upload' in key.lower():
                            input_sources.append("file_upload")
                        elif 'api' in key.lower():
                            input_sources.append("api_input")
                        else:
                            input_sources.append("manual_input")
            
            if not input_sources:
                input_sources = ["workflow_input"]
            
            # è½¬æ¢æ­¥éª¤åˆ†æ
            transformation_steps = []
            for node in sorted(node_instances, key=lambda x: x.get('created_at', '')):
                if node.get('status') == NodeInstanceStatus.COMPLETED.value:
                    operations = []
                    
                    # åŸºäºèŠ‚ç‚¹åç§°å’Œè¾“å‡ºæ•°æ®æ¨æ–­æ“ä½œ
                    node_name = node.get('node_instance_name', 'unknown_node')
                    output_data = node.get('output_data', {})
                    
                    # åˆ†æèŠ‚ç‚¹åç§°
                    if 'clean' in node_name.lower():
                        operations.append("data_cleaning")
                    if 'validate' in node_name.lower():
                        operations.append("data_validation")
                    if 'transform' in node_name.lower():
                        operations.append("data_transformation")
                    if 'analysis' in node_name.lower() or 'analyze' in node_name.lower():
                        operations.append("data_analysis")
                    if 'process' in node_name.lower():
                        operations.append("data_processing")
                    
                    # åˆ†æè¾“å‡ºæ•°æ®ç»“æ„ä»¥æ¨æ–­æ›´å¤šæ“ä½œ
                    if isinstance(output_data, dict):
                        if 'tasks_output' in output_data:
                            operations.append("task_aggregation")
                        if 'primary_output' in output_data:
                            operations.append("output_processing")
                        if 'all_tasks_completed' in output_data:
                            operations.append("completion_verification")
                    
                    if not operations:
                        operations = ["node_execution"]
                    
                    # æ·»åŠ æ•°æ®é‡ä¿¡æ¯
                    data_size_info = ""
                    if isinstance(output_data, dict):
                        task_count = output_data.get('task_count', 0)
                        if task_count > 0:
                            data_size_info = f" ({task_count} tasks processed)"
                    
                    transformation_steps.append(DataLineageStep(
                        node=node_name + data_size_info,
                        operations=operations,
                        timestamp=node.get('completed_at')
                    ))
            
            # è¾“å‡ºç›®æ ‡åˆ†æ
            output_destinations = []
            if workflow_instance.get('output_data'):
                output_data = workflow_instance['output_data']
                if isinstance(output_data, dict):
                    for key in output_data.keys():
                        if 'database' in key.lower() or 'db' in key.lower():
                            output_destinations.append("database")
                        elif 'file' in key.lower() or 'report' in key.lower():
                            output_destinations.append("report_file")
                        elif 'api' in key.lower():
                            output_destinations.append("api_response")
                        else:
                            output_destinations.append("workflow_output")
            
            if not output_destinations:
                output_destinations = ["workflow_result"]
            
            return DataLineage(
                input_sources=list(set(input_sources)),  # å»é‡
                transformation_steps=transformation_steps,
                output_destinations=list(set(output_destinations))  # å»é‡
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ•°æ®è¡€ç¼˜ä¿¡æ¯å¤±è´¥: {e}")
            return DataLineage(
                input_sources=["unknown"],
                transformation_steps=[],
                output_destinations=["unknown"]
            )
    
    async def _generate_execution_issues(self, workflow_instance: Dict[str, Any],
                                       node_instances: List[Dict[str, Any]], 
                                       task_instances: List[Dict[str, Any]]) -> ExecutionIssues:
        """ç”Ÿæˆæ‰§è¡Œé—®é¢˜å’Œè­¦å‘Šä¿¡æ¯"""
        try:
            errors = []
            warnings = []
            recoverable_failures = []
            
            # å·¥ä½œæµçº§åˆ«é”™è¯¯
            if workflow_instance.get('error_message'):
                errors.append(f"å·¥ä½œæµé”™è¯¯: {workflow_instance['error_message']}")
            
            # èŠ‚ç‚¹çº§åˆ«é—®é¢˜
            for node in node_instances:
                if node.get('status') == NodeInstanceStatus.FAILED.value:
                    node_name = node.get('node_instance_name', 'unknown_node')
                    if node.get('error_message'):
                        errors.append(f"èŠ‚ç‚¹ {node_name} å¤±è´¥: {node.get('error_message')}")
                    else:
                        errors.append(f"èŠ‚ç‚¹ {node_name} æ‰§è¡Œå¤±è´¥")
                
                # é‡è¯•æ¬¡æ•°æ£€æŸ¥
                retry_count = node.get('retry_count', 0)
                if retry_count > 0:
                    node_name = node.get('node_instance_name', 'unknown_node')
                    if retry_count >= 3:
                        warnings.append(f"èŠ‚ç‚¹ {node_name} é‡è¯•æ¬¡æ•°è¿‡å¤š ({retry_count} æ¬¡)")
                    else:
                        recoverable_failures.append(f"èŠ‚ç‚¹ {node_name} ç»è¿‡ {retry_count} æ¬¡é‡è¯•åæˆåŠŸ")
            
            # ä»»åŠ¡çº§åˆ«é—®é¢˜
            failed_tasks = [t for t in task_instances if t.get('status') == TaskInstanceStatus.FAILED.value]
            for task in failed_tasks:
                task_title = task.get('task_title', 'unknown_task')
                if task.get('error_message'):
                    errors.append(f"ä»»åŠ¡ {task_title} å¤±è´¥: {task.get('error_message')}")
                else:
                    errors.append(f"ä»»åŠ¡ {task_title} æ‰§è¡Œå¤±è´¥")
            
            # æ€§èƒ½è­¦å‘Š
            long_running_tasks = [t for t in task_instances if t.get('actual_duration') and t['actual_duration'] > 60]
            for task in long_running_tasks:
                task_title = task.get('task_title', 'unknown_task')
                duration = task['actual_duration']
                warnings.append(f"ä»»åŠ¡ {task_title} æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({duration} åˆ†é’Ÿ)")
            
            # æ•°æ®é‡è­¦å‘Š
            total_tasks = len(task_instances)
            if total_tasks > 100:
                warnings.append(f"ä»»åŠ¡æ•°é‡è¾ƒå¤š ({total_tasks} ä¸ª)ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
            
            return ExecutionIssues(
                errors=errors,
                warnings=warnings,
                recoverable_failures=recoverable_failures
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‰§è¡Œé—®é¢˜ä¿¡æ¯å¤±è´¥: {e}")
            return ExecutionIssues(
                errors=[f"é—®é¢˜åˆ†æå¤±è´¥: {str(e)}"],
                warnings=[],
                recoverable_failures=[]
            )
    
    async def update_workflow_output_summary(self, workflow_instance_id: uuid.UUID) -> bool:
        """æ›´æ–°å·¥ä½œæµè¾“å‡ºæ‘˜è¦åˆ°æ•°æ®åº“"""
        try:
            logger.info(f"ğŸ’¾ å¼€å§‹æ›´æ–°å·¥ä½œæµè¾“å‡ºæ‘˜è¦åˆ°æ•°æ®åº“: {workflow_instance_id}")
            
            # ç”Ÿæˆè¾“å‡ºæ‘˜è¦
            output_summary = await self.generate_workflow_output_summary(workflow_instance_id)
            if not output_summary:
                logger.error(f"âŒ ç”Ÿæˆè¾“å‡ºæ‘˜è¦å¤±è´¥: {workflow_instance_id}")
                return False
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼å­˜å‚¨
            summary_dict = output_summary.dict()
            
            # åˆ†ç¦»å„ä¸ªç»„ä»¶åˆ°ä¸åŒå­—æ®µ
            execution_summary = {
                "execution_result": summary_dict.get("execution_result"),
                "execution_stats": summary_dict.get("execution_stats")
            }
            
            quality_metrics = summary_dict.get("quality_metrics")
            data_lineage = summary_dict.get("data_lineage")
            
            # æ›´æ–°æ•°æ®åº“
            from ..models.instance import WorkflowInstanceUpdate
            update_data = WorkflowInstanceUpdate(
                execution_summary=execution_summary,
                quality_metrics=quality_metrics,
                data_lineage=data_lineage,
                output_summary=output_summary
            )
            
            result = await self.workflow_repo.update_instance(workflow_instance_id, update_data)
            if result:
                logger.info(f"âœ… å·¥ä½œæµè¾“å‡ºæ‘˜è¦æ›´æ–°æˆåŠŸ: {workflow_instance_id}")
                return True
            else:
                logger.error(f"âŒ å·¥ä½œæµè¾“å‡ºæ‘˜è¦æ›´æ–°å¤±è´¥: {workflow_instance_id}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å·¥ä½œæµè¾“å‡ºæ‘˜è¦å¼‚å¸¸: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False