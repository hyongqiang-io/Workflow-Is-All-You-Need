"""
å·¥ä½œæµæ‰§è¡ŒAPI
Workflow Execution API
"""

import uuid
import json
from typing import Optional, List
from fastapi import APIRouter, HTTPException, Depends, status, Request, Query
from pydantic import BaseModel, Field, ValidationError
from loguru import logger

from ..services.execution_service import execution_engine
from ..services.agent_task_service import agent_task_service
from ..models.instance import (
    WorkflowExecuteRequest, WorkflowControlRequest,
    TaskInstanceStatus, TaskInstanceType
)
from ..utils.middleware import get_current_user_context, CurrentUser
from ..utils.helpers import now_utc

router = APIRouter(prefix="/api/execution", tags=["execution"])

# æ³¨æ„ï¼šæ‰€æœ‰äººå·¥ä»»åŠ¡ç›¸å…³çš„åŠŸèƒ½ç°åœ¨é€šè¿‡ execution_engine ç»Ÿä¸€å¤„ç†


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class TaskSubmissionRequest(BaseModel):
    """ä»»åŠ¡æäº¤è¯·æ±‚"""
    result_data: Optional[dict] = Field(default={}, description="ä»»åŠ¡ç»“æœæ•°æ®")
    result_summary: Optional[str] = Field(None, description="ç»“æœæ‘˜è¦")
    attachment_file_ids: Optional[List[str]] = Field(default=[], description="é™„ä»¶æ–‡ä»¶IDåˆ—è¡¨")


class TaskActionRequest(BaseModel):
    """ä»»åŠ¡æ“ä½œè¯·æ±‚"""
    reason: Optional[str] = Field(None, description="æ“ä½œåŸå› ")


class HelpRequest(BaseModel):
    """å¸®åŠ©è¯·æ±‚"""
    help_message: str = Field(..., description="å¸®åŠ©ä¿¡æ¯")


class TaskAssignmentRequest(BaseModel):
    """ä»»åŠ¡åˆ†é…è¯·æ±‚"""
    user_id: uuid.UUID = Field(..., description="ç”¨æˆ·ID")


# ==================== å·¥ä½œæµæ‰§è¡Œç«¯ç‚¹ ====================

@router.post("/workflows/execute/debug")
async def debug_execute_workflow(request: Request):
    """è°ƒè¯•æ‰§è¡Œå·¥ä½œæµè¯·æ±‚"""
    from loguru import logger
    try:
        # è·å–åŸå§‹è¯·æ±‚ä½“
        raw_body = await request.body()
        logger.info(f"ğŸ” è°ƒè¯• - åŸå§‹è¯·æ±‚ä½“: {raw_body.decode('utf-8')}")
        
        # è§£æJSON
        import json
        json_data = json.loads(raw_body)
        logger.info(f"ğŸ” è°ƒè¯• - è§£æåçš„JSON: {json_data}")
        
        # æ£€æŸ¥æ¯ä¸ªå­—æ®µ
        for key, value in json_data.items():
            logger.info(f"ğŸ” è°ƒè¯• - å­—æ®µ '{key}': {value} (ç±»å‹: {type(value)})")
        
        return {"status": "debug", "received_data": json_data}
    except Exception as e:
        logger.error(f"è°ƒè¯•ç«¯ç‚¹é”™è¯¯: {e}")
        return {"status": "error", "message": str(e)}


@router.post("/workflows/execute")
async def execute_workflow(
    request: WorkflowExecuteRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ‰§è¡Œå·¥ä½œæµ"""
    try:
        from loguru import logger
        logger.info(f"ğŸš€ æ”¶åˆ°å·¥ä½œæµæ‰§è¡Œè¯·æ±‚")
        logger.info(f"   - workflow_base_id: {request.workflow_base_id} (ç±»å‹: {type(request.workflow_base_id)})")
        logger.info(f"   - workflow_instance_name: {request.workflow_instance_name} (ç±»å‹: {type(request.workflow_instance_name)})")
        logger.info(f"   - user_id: {current_user.user_id}")
        logger.info(f"   - input_data: {request.input_data}")
        logger.info(f"   - context_data: {request.context_data}")
        
        # å°è¯•æ‰§è¡Œå·¥ä½œæµï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›è¯¦ç»†é”™è¯¯
        try:
            result = await execution_engine.execute_workflow(request, current_user.user_id)
            logger.info(f"å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ: {result}")
            return {
                "success": True,
                "data": result,
                "message": "å·¥ä½œæµå¼€å§‹æ‰§è¡Œ"
            }
        except AttributeError as ae:
            # ä¾èµ–ç®¡ç†å™¨é—®é¢˜ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
            logger.warning(f"æ‰§è¡Œå¼•æ“ä¾èµ–é—®é¢˜ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”: {ae}")
            result = {
                "instance_id": str(uuid.uuid4()),
                "workflow_base_id": str(request.workflow_base_id),
                "workflow_instance_name": request.workflow_instance_name,
                "status": "pending",
                "message": "å·¥ä½œæµæ‰§è¡Œè¯·æ±‚å·²æ¥æ”¶ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰"
            }
            return {
                "success": True,
                "data": result,
                "message": "å·¥ä½œæµå¼€å§‹æ‰§è¡Œ"
            }
    except ValueError as e:
        from loguru import logger
        logger.warning(f"å·¥ä½œæµæ‰§è¡ŒéªŒè¯é”™è¯¯: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"æ‰§è¡Œå·¥ä½œæµå¤±è´¥: {str(e)}"
        )
    except Exception as e:
        from loguru import logger
        logger.error(f"æ‰§è¡Œå·¥ä½œæµå¼‚å¸¸: {e}")
        import traceback
        logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‰§è¡Œå·¥ä½œæµå¤±è´¥: {str(e)}"
        )


@router.post("/workflows/{instance_id}/control")
async def control_workflow(
    instance_id: uuid.UUID,
    request: WorkflowControlRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ§åˆ¶å·¥ä½œæµæ‰§è¡Œï¼ˆæš‚åœã€æ¢å¤ã€å–æ¶ˆï¼‰"""
    try:
        action = request.action.lower()
        logger.info(f"ğŸ® ç”¨æˆ· {current_user.user_id} ({current_user.username}) è¯·æ±‚æ§åˆ¶å·¥ä½œæµ: {instance_id}")
        logger.info(f"   - æ“ä½œç±»å‹: {action}")
        logger.info(f"   - æ“ä½œåŸå› : {getattr(request, 'reason', 'æœªæä¾›')}")
        
        # éªŒè¯æ“ä½œç±»å‹
        valid_actions = ["pause", "resume", "cancel"]
        if action not in valid_actions:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}")
            logger.error(f"   - æ”¯æŒçš„æ“ä½œ: {valid_actions}")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ä¸æ”¯æŒçš„æ“ä½œç±»å‹"
            )
        
        # æ‰§è¡Œå¯¹åº”çš„æ“ä½œ
        success = False
        message = ""
        
        try:
            if action == "pause":
                logger.info(f"â¸ï¸ æ‰§è¡Œæš‚åœæ“ä½œ")
                success = await execution_engine.pause_workflow(instance_id)
                message = "å·¥ä½œæµå·²æš‚åœ" if success else "æš‚åœå·¥ä½œæµå¤±è´¥"
                logger.info(f"   - æš‚åœç»“æœ: {success}")
                
            elif action == "resume":
                logger.info(f"â–¶ï¸ æ‰§è¡Œæ¢å¤æ“ä½œ")
                success = await execution_engine.resume_workflow(instance_id)
                message = "å·¥ä½œæµå·²æ¢å¤" if success else "æ¢å¤å·¥ä½œæµå¤±è´¥"
                logger.info(f"   - æ¢å¤ç»“æœ: {success}")
                
            elif action == "cancel":
                logger.info(f"ğŸš« æ‰§è¡Œå–æ¶ˆæ“ä½œ")
                success = await execution_engine.cancel_workflow(instance_id)
                message = "å·¥ä½œæµå·²å–æ¶ˆ" if success else "å–æ¶ˆå·¥ä½œæµå¤±è´¥"
                logger.info(f"   - å–æ¶ˆç»“æœ: {success}")
                
        except Exception as operation_error:
            logger.error(f"âŒ æ‰§è¡Œ {action} æ“ä½œæ—¶å‘ç”Ÿå¼‚å¸¸:")
            logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(operation_error).__name__}")
            logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(operation_error)}")
            import traceback
            logger.error(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ‰§è¡Œ{action}æ“ä½œå¤±è´¥: {str(operation_error)}"
            )
        
        # æ£€æŸ¥æ“ä½œç»“æœ
        if not success:
            logger.error(f"âŒ å·¥ä½œæµæ§åˆ¶æ“ä½œå¤±è´¥:")
            logger.error(f"   - å®ä¾‹ID: {instance_id}")
            logger.error(f"   - æ“ä½œ: {action}")
            logger.error(f"   - è¿”å›ç»“æœ: {success}")
            logger.error(f"   - æ¶ˆæ¯: {message}")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=message
            )
        
        logger.info(f"âœ… å·¥ä½œæµæ§åˆ¶æ“ä½œæˆåŠŸ:")
        logger.info(f"   - å®ä¾‹ID: {instance_id}")
        logger.info(f"   - æ“ä½œ: {action}")
        logger.info(f"   - ç»“æœ: {message}")
        
        return {
            "success": True,
            "data": {"instance_id": instance_id, "action": action},
            "message": message
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ§åˆ¶å·¥ä½œæµæ€»ä½“å¼‚å¸¸:")
        logger.error(f"   - å®ä¾‹ID: {instance_id}")
        logger.error(f"   - ç”¨æˆ·ID: {current_user.user_id}")
        logger.error(f"   - è¯·æ±‚æ“ä½œ: {getattr(request, 'action', 'unknown')}")
        logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(e)}")
        import traceback
        logger.error(f"   - å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ§åˆ¶å·¥ä½œæµå¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{instance_id}/status")
async def get_workflow_status(
    instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµå®ä¾‹çš„è¯¦ç»†çŠ¶æ€"""
    try:
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository

        workflow_instance_repo = WorkflowInstanceRepository()

        # æ­¥éª¤1: è·å–å·¥ä½œæµå®ä¾‹åŸºæœ¬ä¿¡æ¯
        workflow_query = """
        SELECT
            wi.*,
            w.name as workflow_name,
            u.username as executor_username
        FROM workflow_instance wi
        LEFT JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id AND w.is_current_version = 1
        LEFT JOIN user u ON wi.executor_id = u.user_id
        WHERE wi.workflow_instance_id = %s
        AND wi.is_deleted = 0
        """

        workflow_result = await workflow_instance_repo.db.fetch_one(workflow_query, instance_id)

        if not workflow_result:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )

        # æ­¥éª¤2: è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
        nodes_query = """
        SELECT
            ni.node_instance_id,
            n.name as node_name,
            n.type as node_type,
            ni.status,
            ni.started_at,
            ni.completed_at,
            ni.error_message,
            ni.input_data,
            ni.output_data,
            ni.retry_count
        FROM node_instance ni
        LEFT JOIN node n ON ni.node_id = n.node_id
        WHERE ni.workflow_instance_id = %s
        AND ni.is_deleted = 0
        ORDER BY ni.created_at ASC
        """

        node_results = await workflow_instance_repo.db.fetch_all(nodes_query, instance_id)

        # ç»„è£…ç»“æœ
        result = dict(workflow_result)
        node_instances = [dict(node) for node in node_results] if node_results else []
        result['node_instances'] = node_instances

        # ç»Ÿè®¡èŠ‚ç‚¹çŠ¶æ€
        total_nodes = len(node_instances)
        completed_nodes = sum(1 for node in node_instances if node.get('status') == 'completed')
        running_nodes = sum(1 for node in node_instances if node.get('status') == 'running')
        failed_nodes = sum(1 for node in node_instances if node.get('status') == 'failed')
        
        progress_percentage = 0
        if total_nodes > 0:
            progress_percentage = round((completed_nodes / total_nodes) * 100, 1)
        
        # å½“å‰è¿è¡Œçš„èŠ‚ç‚¹
        current_running_nodes = [node.get('node_name') for node in node_instances if node.get('status') == 'running']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸»åŠ¨æ›´æ–°å·¥ä½œæµçŠ¶æ€
        current_status = result.get("status")
        should_trigger_completion_check = False
        
        if total_nodes > 0 and completed_nodes == total_nodes and failed_nodes == 0:
            # æ‰€æœ‰èŠ‚ç‚¹éƒ½å®Œæˆä¸”æ²¡æœ‰å¤±è´¥èŠ‚ç‚¹
            if current_status not in ['completed', 'COMPLETED']:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ‰€æœ‰èŠ‚ç‚¹å·²å®Œæˆä½†å·¥ä½œæµçŠ¶æ€ä¸º {current_status}ï¼Œä¸»åŠ¨è§¦å‘å®Œæˆæ£€æŸ¥")
                should_trigger_completion_check = True
        elif failed_nodes > 0:
            # æœ‰å¤±è´¥èŠ‚ç‚¹
            if current_status not in ['failed', 'FAILED']:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æœ‰å¤±è´¥èŠ‚ç‚¹ä½†å·¥ä½œæµçŠ¶æ€ä¸º {current_status}ï¼Œä¸»åŠ¨è§¦å‘å¤±è´¥æ£€æŸ¥")
                should_trigger_completion_check = True
        
        # å¦‚æœéœ€è¦ï¼Œè§¦å‘å·¥ä½œæµçŠ¶æ€æ£€æŸ¥
        if should_trigger_completion_check:
            try:
                from ..services.execution_service import ExecutionEngine
                execution_engine = ExecutionEngine()
                await execution_engine._check_workflow_completion(instance_id)
                logger.info(f"âœ… ä¸»åŠ¨è§¦å‘çš„å·¥ä½œæµçŠ¶æ€æ£€æŸ¥å®Œæˆ")

                # é‡æ–°æŸ¥è¯¢æ›´æ–°åçš„çŠ¶æ€
                updated_result = await workflow_instance_repo.db.fetch_one(workflow_query, instance_id)
                if updated_result:
                    result = dict(updated_result)
                    current_status = result.get("status")
                    logger.info(f"ğŸ“Š å·¥ä½œæµçŠ¶æ€å·²æ›´æ–°ä¸º: {current_status}")
            except Exception as e:
                logger.error(f"âŒ ä¸»åŠ¨è§¦å‘å·¥ä½œæµçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        
        formatted_instance = {
            "instance_id": str(result["workflow_instance_id"]),
            "workflow_instance_name": result.get("workflow_instance_name"),
            "workflow_name": result.get("workflow_name"),
            "status": result.get("status"),
            "executor_id": str(result.get("executor_id")) if result.get("executor_id") else None,
            "executor_username": result.get("executor_username"),
            "created_at": result["created_at"].isoformat() if result.get("created_at") else None,
            "updated_at": result["updated_at"].isoformat() if result.get("updated_at") else None,
            "input_data": result.get("input_data", {}),
            "output_data": result.get("output_data", {}),
            "error_message": result.get("error_message"),
            "total_nodes": total_nodes,
            "completed_nodes": completed_nodes,
            "running_nodes": running_nodes,
            "failed_nodes": failed_nodes,
            "progress_percentage": progress_percentage,
            "current_running_nodes": current_running_nodes,
            "node_instances": node_instances
        }
        
        return {
            "success": True,
            "data": formatted_instance,
            "message": "è·å–å·¥ä½œæµå®ä¾‹çŠ¶æ€æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµå®ä¾‹çŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{workflow_base_id}/instances")
async def get_workflow_instances(
    workflow_base_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµçš„æ‰§è¡Œå®ä¾‹åˆ—è¡¨"""
    try:
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        
        workflow_instance_repo = WorkflowInstanceRepository()
        
        # æŸ¥è¯¢å·¥ä½œæµå®ä¾‹åŠå…¶ç»Ÿè®¡ä¿¡æ¯ (MySQLç‰ˆæœ¬)
        query = """
        SELECT 
            wi.workflow_instance_id,
            wi.workflow_instance_name,
            wi.status,
            wi.executor_id,
            wi.created_at,
            wi.updated_at,
            wi.input_data,
            wi.output_data,
            wi.error_message,
            wi.workflow_base_id,
            w.name as workflow_name,
            u.username as executor_username,
            -- ç»Ÿè®¡èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            COUNT(ni.node_instance_id) as total_nodes,
            COUNT(CASE WHEN ni.status = 'completed' THEN 1 END) as completed_nodes,
            COUNT(CASE WHEN ni.status = 'running' THEN 1 END) as running_nodes,
            COUNT(CASE WHEN ni.status = 'failed' THEN 1 END) as failed_nodes,
            -- è·å–å½“å‰è¿è¡Œçš„èŠ‚ç‚¹åç§° (MySQLç‰ˆæœ¬)
            GROUP_CONCAT(
                CASE WHEN ni.status = 'running' THEN n.name END
                SEPARATOR ', '
            ) as current_running_nodes
        FROM workflow_instance wi
        LEFT JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id AND w.is_current_version = 1
        LEFT JOIN user u ON wi.executor_id = u.user_id
        LEFT JOIN node_instance ni ON wi.workflow_instance_id = ni.workflow_instance_id AND ni.is_deleted = 0
        LEFT JOIN node n ON ni.node_id = n.node_id
        WHERE wi.workflow_base_id = %s
        AND wi.is_deleted = 0
        GROUP BY wi.workflow_instance_id, wi.workflow_instance_name, wi.status, wi.executor_id, wi.created_at, wi.updated_at, wi.input_data, wi.output_data, wi.error_message, wi.workflow_base_id, w.name, u.username
        ORDER BY wi.created_at DESC
        """
        
        instances = await workflow_instance_repo.db.fetch_all(query, workflow_base_id)
        
        # æ ¼å¼åŒ–è¿”å›æ•°æ®
        formatted_instances = []
        for instance in instances:
            # å®‰å…¨è½¬æ¢æ•°å€¼å­—æ®µï¼ˆå¤„ç†MySQLå¯èƒ½è¿”å›çš„å„ç§æ ¼å¼ï¼‰
            def safe_int(value, default=0):
                """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
                if value is None:
                    return default
                if isinstance(value, int):
                    return value
                if isinstance(value, (list, tuple)):
                    # å¦‚æœæ˜¯åˆ—è¡¨/å…ƒç»„ï¼Œå¯èƒ½æ˜¯MySQLè¿”å›çš„æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    return len([x for x in value if x]) if value else default
                if isinstance(value, str):
                    if value == '[]' or value == '' or value == 'None':
                        return default
                    try:
                        return int(value)
                    except ValueError:
                        return default
                # å¤„ç†å…¶ä»–ç±»å‹
                try:
                    return int(value) if value is not None else default
                except (ValueError, TypeError):
                    return default
                
            total_nodes = safe_int(instance.get("total_nodes"))
            completed_nodes = safe_int(instance.get("completed_nodes"))
            running_nodes = safe_int(instance.get("running_nodes"))
            failed_nodes = safe_int(instance.get("failed_nodes"))
            
            # è®¡ç®—æ‰§è¡Œè¿›åº¦ç™¾åˆ†æ¯”
            progress_percentage = 0
            if total_nodes > 0:
                progress_percentage = round((completed_nodes / total_nodes) * 100, 1)
            
            formatted_instances.append({
                "instance_id": str(instance["workflow_instance_id"]),
                "workflow_instance_name": instance.get("workflow_instance_name"),
                "workflow_name": instance.get("workflow_name"),
                "status": instance.get("status"),
                "executor_id": str(instance.get("executor_id")) if instance.get("executor_id") else None,
                "executor_username": instance.get("executor_username"),
                "created_at": instance["created_at"].isoformat() if instance.get("created_at") else None,
                "updated_at": instance["updated_at"].isoformat() if instance.get("updated_at") else None,
                "input_data": instance.get("input_data", {}),
                "output_data": instance.get("output_data", {}),
                "error_message": instance.get("error_message"),
                # æ–°å¢è¿›åº¦å’ŒèŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
                "total_nodes": total_nodes,
                "completed_nodes": completed_nodes,
                "running_nodes": running_nodes,
                "failed_nodes": failed_nodes,
                "progress_percentage": progress_percentage,
                "current_node": instance.get("current_running_nodes") or None
            })
        
        return {
            "success": True,
            "data": formatted_instances,
            "message": f"è·å–åˆ° {len(formatted_instances)} ä¸ªæ‰§è¡Œå®ä¾‹"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–æ‰§è¡Œå®ä¾‹åˆ—è¡¨å¤±è´¥: {str(e)}"
        )


@router.get("/workflow/{workflow_id}/task-flow")
async def get_workflow_task_flow(
    workflow_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµä»»åŠ¡æµç¨‹"""
    try:
        # è·å–å·¥ä½œæµå®ä¾‹çš„ä»»åŠ¡æµç¨‹ä¿¡æ¯
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        from ..repositories.instance.node_instance_repository import NodeInstanceRepository
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        
        task_repo = TaskInstanceRepository()
        node_repo = NodeInstanceRepository()
        workflow_repo = WorkflowInstanceRepository()
        
        # é¦–å…ˆéªŒè¯å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨å¹¶è·å–åŸºæœ¬ä¿¡æ¯
        workflow_instance_query = """
        SELECT 
            wi.*,
            w.name as workflow_name,
            u.username as executor_username
        FROM workflow_instance wi
        LEFT JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id AND w.is_current_version = 1
        LEFT JOIN user u ON wi.executor_id = u.user_id
        WHERE wi.workflow_instance_id = %s AND wi.is_deleted = 0
        """
        
        workflow_instance = await workflow_repo.db.fetch_one(workflow_instance_query, workflow_id)
        if not workflow_instance:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        # è·å–å·¥ä½œæµå®ä¾‹çš„æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹ï¼ˆåŒ…å«å¤„ç†å™¨è¯¦ç»†ä¿¡æ¯å’Œä½ç½®ä¿¡æ¯ï¼‰
        nodes_query = """
        SELECT 
            ni.*,
            n.name as node_name,
            n.type as node_type,
            n.position_x,
            n.position_y,
            -- å¤„ç†å™¨ä¿¡æ¯ï¼ˆé€šè¿‡node_processorå…³è”è¡¨ï¼‰
            p.name as processor_name,
            p.type as processor_type,
            -- è®¡ç®—èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ (MySQLå…¼å®¹)
            CASE 
                WHEN ni.started_at IS NOT NULL AND ni.completed_at IS NOT NULL 
                THEN CAST(TIMESTAMPDIFF(SECOND, ni.started_at, ni.completed_at) AS SIGNED)
                WHEN ni.started_at IS NOT NULL 
                THEN CAST(TIMESTAMPDIFF(SECOND, ni.started_at, NOW()) AS SIGNED)
                ELSE NULL
            END as execution_duration_seconds
        FROM node_instance ni
        JOIN node n ON ni.node_id = n.node_id
        LEFT JOIN node_processor np ON n.node_id = np.node_id
        LEFT JOIN processor p ON np.processor_id = p.processor_id
        WHERE ni.workflow_instance_id = $1
        AND ni.is_deleted = 0
        ORDER BY 
            CASE 
                WHEN ni.started_at IS NOT NULL THEN ni.started_at 
                ELSE ni.created_at 
            END ASC
        """
        
        nodes = await node_repo.db.fetch_all(nodes_query, workflow_id)
        
        # è·å–æ‰€æœ‰ä»»åŠ¡å®ä¾‹ï¼ˆåŒ…å«æ›´è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯ï¼‰
        tasks_query = """
        SELECT 
            ti.*,
            p.name as processor_name,
            p.type as processor_type,
            u.username as assigned_username,
            a.agent_name as assigned_agent_name,
            -- è®¡ç®—ä»»åŠ¡æ‰§è¡Œæ—¶é—´ (MySQLå…¼å®¹)
            CASE 
                WHEN ti.started_at IS NOT NULL AND ti.completed_at IS NOT NULL 
                THEN CAST(TIMESTAMPDIFF(SECOND, ti.started_at, ti.completed_at) AS SIGNED)
                WHEN ti.started_at IS NOT NULL 
                THEN CAST(TIMESTAMPDIFF(SECOND, ti.started_at, NOW()) AS SIGNED)
                ELSE NULL
            END as actual_duration_seconds,
            -- ä»»åŠ¡æ˜¯å¦è¶…æ—¶ (MySQLå…¼å®¹)
            CASE 
                WHEN ti.estimated_duration IS NOT NULL 
                     AND ti.started_at IS NOT NULL 
                     AND ti.completed_at IS NULL
                     AND TIMESTAMPDIFF(SECOND, ti.started_at, NOW()) > ti.estimated_duration * 60
                THEN TRUE
                ELSE FALSE
            END as is_overdue
        FROM task_instance ti
        LEFT JOIN processor p ON ti.processor_id = p.processor_id
        LEFT JOIN user u ON ti.assigned_user_id = u.user_id
        LEFT JOIN agent a ON ti.assigned_agent_id = a.agent_id
        WHERE ti.workflow_instance_id = %s
        AND ti.is_deleted = 0
        ORDER BY ti.created_at
        """
        
        tasks = await task_repo.db.fetch_all(tasks_query, workflow_id)
        
        # è·å–å·¥ä½œæµè¾¹ç¼˜å…³ç³»ï¼ˆç”¨äºå‰ç«¯æµç¨‹å›¾æ˜¾ç¤ºï¼‰
        edges_query = """
        SELECT 
            nc.from_node_id,
            nc.to_node_id,
            nc.condition_config,
            n1.name as from_node_name,
            n2.name as to_node_name,
            n1.node_base_id as from_node_base_id,
            n2.node_base_id as to_node_base_id
        FROM node_connection nc
        JOIN node n1 ON nc.from_node_id = n1.node_id
        JOIN node n2 ON nc.to_node_id = n2.node_id
        WHERE nc.workflow_id = %s
        ORDER BY nc.created_at
        """
        
        # Get the current workflow_id for edge query
        workflow_query = """
        SELECT workflow_id FROM workflow 
        WHERE workflow_base_id = $1 AND is_current_version = TRUE
        """
        workflow_result = await node_repo.db.fetch_one(workflow_query, workflow_instance['workflow_base_id'])
        current_workflow_id = workflow_result['workflow_id'] if workflow_result else None
        
        edges = await node_repo.db.fetch_all(edges_query, current_workflow_id) if current_workflow_id else []
        
        # æ„å»ºä»»åŠ¡æµç¨‹æ•°æ®
        task_flow = {
            "workflow_id": str(workflow_id),
            "workflow_name": workflow_instance['workflow_name'],
            "workflow_instance_status": workflow_instance['status'],
            "executor_username": workflow_instance['executor_username'],
            "created_at": workflow_instance['created_at'].isoformat() if workflow_instance['created_at'] else None,
            "started_at": workflow_instance['started_at'].isoformat() if workflow_instance['started_at'] else None,
            "completed_at": workflow_instance['completed_at'].isoformat() if workflow_instance['completed_at'] else None,
            "current_user_role": "viewer",  # é»˜è®¤ä¸ºviewerï¼Œåç»­å¯æ ¹æ®æƒé™è®¾ç½®
            "nodes": [],
            "tasks": [],
            "edges": []
        }
        
        # åˆ¤æ–­ç”¨æˆ·è§’è‰²
        if str(workflow_instance['executor_id']) == str(current_user.user_id):
            task_flow["current_user_role"] = "creator"
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é…ç»™å½“å‰ç”¨æˆ·çš„ä»»åŠ¡
            user_tasks = [task for task in tasks if task.get('assigned_user_id') == current_user.user_id]
            if user_tasks:
                task_flow["current_user_role"] = "assignee"
                task_flow["assigned_tasks"] = []
        
        # æ ¼å¼åŒ–èŠ‚ç‚¹æ•°æ®ï¼ˆåŒ…å«å®æ—¶çŠ¶æ€ã€æ‰§è¡Œä¿¡æ¯ã€å¤„ç†å™¨ä¿¡æ¯å’Œä½ç½®ä¿¡æ¯ï¼‰
        for node in nodes:
            node_data = {
                "node_instance_id": str(node['node_instance_id']),
                "node_name": node['node_name'],
                "node_type": node['node_type'],
                "status": node['status'],  # è¿™æ˜¯ä»æ•°æ®åº“å®æ—¶è¯»å–çš„çŠ¶æ€
                "input_data": node['input_data'],
                "output_data": node['output_data'],
                "start_at": node['started_at'].isoformat() if node['started_at'] else None,
                "completed_at": node['completed_at'].isoformat() if node['completed_at'] else None,
                "execution_duration_seconds": node['execution_duration_seconds'],
                "error_message": node['error_message'],
                "retry_count": node.get('retry_count', 0),
                # ğŸ”§ æ–°å¢ï¼šä½ç½®ä¿¡æ¯ç”¨äºå‰ç«¯å¸ƒå±€
                "position": {
                    "x": float(node['position_x']) if node['position_x'] is not None else None,
                    "y": float(node['position_y']) if node['position_y'] is not None else None
                },
                # å¤„ç†å™¨è¯¦ç»†ä¿¡æ¯
                "processor_name": node['processor_name'],
                "processor_type": node['processor_type'],
                # èŠ‚ç‚¹å…³è”çš„ä»»åŠ¡æ•°é‡
                "task_count": len([task for task in tasks if str(task['node_instance_id']) == str(node['node_instance_id'])]),
                # å…³è”çš„ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
                "tasks": [
                    {
                        "task_instance_id": str(task['task_instance_id']),
                        "task_title": task['task_title'],
                        "task_type": task['task_type'],
                        "status": task['status'],
                        "assignee": task['assigned_username'] or task['assigned_agent_name'],
                        "priority": task['priority'],
                        "input_data": task['input_data'],
                        "output_data": task['output_data'],
                        "result_summary": task.get('result_summary'),
                        "error_message": task.get('error_message')
                    }
                    for task in tasks if str(task['node_instance_id']) == str(node['node_instance_id'])
                ],
                # æ—¶é—´æˆ³ä¿¡æ¯
                "timestamps": {
                    "created_at": node['created_at'].isoformat() if node['created_at'] else None,
                    "started_at": node['started_at'].isoformat() if node['started_at'] else None,
                    "completed_at": node['completed_at'].isoformat() if node['completed_at'] else None
                }
            }
            task_flow["nodes"].append(node_data)
        
        # æ ¼å¼åŒ–ä»»åŠ¡æ•°æ®ï¼ˆåŒ…å«å®æ—¶çŠ¶æ€å’Œåˆ†é…ä¿¡æ¯ï¼‰
        for task in tasks:
            assignee_info = None
            if task['assigned_username']:
                assignee_info = {
                    "id": str(task['assigned_user_id']),
                    "name": task['assigned_username'],
                    "type": "user"
                }
            elif task['assigned_agent_name']:
                assignee_info = {
                    "id": str(task['assigned_agent_id']),
                    "name": task['assigned_agent_name'],
                    "type": "agent"
                }
            
            task_data = {
                "task_instance_id": str(task['task_instance_id']),
                "node_instance_id": str(task['node_instance_id']),
                "task_title": task['task_title'],
                "task_type": task['task_type'],
                "status": task['status'],  # è¿™æ˜¯ä»æ•°æ®åº“å®æ—¶è¯»å–çš„çŠ¶æ€
                "processor_name": task['processor_name'],
                "processor_type": task['processor_type'],
                "assignee": assignee_info,
                "priority": task['priority'],
                "estimated_duration": task['estimated_duration'],
                "actual_duration_seconds": task['actual_duration_seconds'],
                "is_overdue": task['is_overdue'],
                "created_at": task['created_at'].isoformat() if task['created_at'] else None,
                "started_at": task['started_at'].isoformat() if task['started_at'] else None,
                "completed_at": task['completed_at'].isoformat() if task['completed_at'] else None,
                "input_data": task['input_data'],
                "output_data": task['output_data'],
                "result_summary": task.get('result_summary'),
                "error_message": task.get('error_message')
            }
            
            task_flow["tasks"].append(task_data)
            
            # å¦‚æœæ˜¯åˆ†é…ç»™å½“å‰ç”¨æˆ·çš„ä»»åŠ¡ï¼Œæ·»åŠ åˆ°assigned_tasks
            if (task_flow["current_user_role"] == "assignee" and 
                task.get('assigned_user_id') == current_user.user_id):
                task_flow["assigned_tasks"].append(task_data)
        
        # æ ¼å¼åŒ–è¾¹ç¼˜æ•°æ®ï¼ˆç”¨äºæµç¨‹å›¾æ˜¾ç¤ºï¼‰
        # åˆ›å»ºnode_idåˆ°node_instance_idçš„æ˜ å°„
        node_id_to_instance_id = {}
        for node in nodes:
            node_id_to_instance_id[str(node['node_id'])] = str(node['node_instance_id'])
        
        for edge in edges:
            from_node_id = str(edge['from_node_id'])
            to_node_id = str(edge['to_node_id'])
            
            # å°†node_idæ˜ å°„ä¸ºnode_instance_id
            source_instance_id = node_id_to_instance_id.get(from_node_id)
            target_instance_id = node_id_to_instance_id.get(to_node_id)
            
            # åªæœ‰å½“ä¸¤ä¸ªèŠ‚ç‚¹éƒ½æœ‰å¯¹åº”çš„å®ä¾‹æ—¶æ‰æ·»åŠ è¾¹
            if source_instance_id and target_instance_id:
                edge_data = {
                    "id": f"{source_instance_id}-{target_instance_id}",
                    "source": source_instance_id,
                    "target": target_instance_id,
                    "label": str(edge['condition_config']) if edge['condition_config'] else "",
                    "from_node_name": edge['from_node_name'],
                    "to_node_name": edge['to_node_name']
                }
                task_flow["edges"].append(edge_data)
        
        # ğŸ”§ æ–°å¢ï¼šæ™ºèƒ½å¸ƒå±€ç®—æ³• - å½“èŠ‚ç‚¹ç¼ºå°‘ä½ç½®ä¿¡æ¯æ—¶è‡ªåŠ¨è®¡ç®—å±‚æ¬¡åŒ–å¸ƒå±€
        def calculate_hierarchical_layout(nodes_data, edges_data):
            """åŸºäºä¾èµ–å…³ç³»çš„å±‚æ¬¡åŒ–å¸ƒå±€ç®—æ³•"""
            # æ„å»ºä¾èµ–å›¾
            graph = {}
            in_degree = {}
            node_dict = {node["node_instance_id"]: node for node in nodes_data}
            
            # åˆå§‹åŒ–
            for node in nodes_data:
                node_id = node["node_instance_id"]
                graph[node_id] = []
                in_degree[node_id] = 0
            
            # æ„å»ºè¾¹å…³ç³»
            for edge in edges_data:
                source = edge["source"]
                target = edge["target"]
                if source in graph and target in graph:
                    graph[source].append(target)
                    in_degree[target] += 1
            
            # æ‹“æ‰‘æ’åºè·å¾—å±‚æ¬¡
            layers = []
            current_layer = [node_id for node_id, degree in in_degree.items() if degree == 0]
            
            while current_layer:
                layers.append(current_layer[:])
                next_layer = []
                for node_id in current_layer:
                    for neighbor in graph[node_id]:
                        in_degree[neighbor] -= 1
                        if in_degree[neighbor] == 0:
                            next_layer.append(neighbor)
                current_layer = next_layer
            
            # è®¡ç®—å¸ƒå±€å‚æ•°
            LAYER_WIDTH = 300  # å±‚é—´è·ç¦»
            NODE_HEIGHT = 120  # èŠ‚ç‚¹é—´å‚ç›´è·ç¦»
            START_X = 100      # èµ·å§‹Xåæ ‡
            START_Y = 100      # èµ·å§‹Yåæ ‡
            
            # åº”ç”¨å±‚æ¬¡åŒ–å¸ƒå±€
            for layer_idx, layer_nodes in enumerate(layers):
                layer_x = START_X + layer_idx * LAYER_WIDTH
                
                # åœ¨å½“å‰å±‚å†…å‚ç›´æ’åˆ—èŠ‚ç‚¹
                for node_idx, node_id in enumerate(layer_nodes):
                    if node_id in node_dict:
                        node = node_dict[node_id]
                        
                        # åªæœ‰å½“èŠ‚ç‚¹æ²¡æœ‰æœ‰æ•ˆä½ç½®ä¿¡æ¯æ—¶æ‰é‡æ–°è®¡ç®—
                        current_pos = node.get("position", {})
                        if (current_pos.get("x") is None or current_pos.get("y") is None or 
                            (current_pos.get("x") == 0 and current_pos.get("y") == 0)):
                            
                            # è®¡ç®—Yåæ ‡ - å±…ä¸­å¯¹é½
                            layer_height = len(layer_nodes) * NODE_HEIGHT
                            start_y = START_Y + (layer_idx * 50)  # æ¯å±‚ç¨å¾®é”™å¼€
                            node_y = start_y + (node_idx - len(layer_nodes)/2 + 0.5) * NODE_HEIGHT
                            
                            node["position"] = {
                                "x": float(layer_x),
                                "y": float(node_y)
                            }
                            
                            print(f"ğŸ”§ [å¸ƒå±€] {node.get('node_name')} -> å±‚{layer_idx} ä½ç½®({layer_x}, {node_y})")
            
            return nodes_data
        
        # åº”ç”¨æ™ºèƒ½å¸ƒå±€
        task_flow["nodes"] = calculate_hierarchical_layout(task_flow["nodes"], task_flow["edges"])
        
        # æ·»åŠ å®æ—¶ç»Ÿè®¡ä¿¡æ¯
        node_status_count = {}
        task_status_count = {}
        
        for node in task_flow["nodes"]:
            status = node["status"]
            node_status_count[status] = node_status_count.get(status, 0) + 1
        
        for task in task_flow["tasks"]:
            status = task["status"]
            task_status_count[status] = task_status_count.get(status, 0) + 1
        
        # è®¡ç®—æ€»ä½“è¿›åº¦
        total_nodes = len(task_flow["nodes"])
        completed_nodes = node_status_count.get("completed", 0)
        progress_percentage = round((completed_nodes / total_nodes) * 100, 1) if total_nodes > 0 else 0
        
        task_flow["statistics"] = {
            "total_nodes": total_nodes,
            "total_tasks": len(task_flow["tasks"]),
            "node_status_count": node_status_count,
            "task_status_count": task_status_count,
            "progress_percentage": progress_percentage,
            "is_completed": workflow_instance['status'] == 'completed',
            "is_running": workflow_instance['status'] == 'running',
            "is_failed": workflow_instance['status'] == 'failed'
        }
        
        # æ·»åŠ åˆ›å»ºè€…ä¿¡æ¯
        task_flow["creator"] = {
            "id": str(workflow_instance['executor_id']),
            "name": workflow_instance['executor_username']
        }
        
        return {
            "success": True,
            "data": task_flow,
            "message": f"è·å–å®æ—¶ä»»åŠ¡æµç¨‹æˆåŠŸï¼ŒåŒ…å« {len(task_flow['nodes'])} ä¸ªèŠ‚ç‚¹å’Œ {len(task_flow['tasks'])} ä¸ªä»»åŠ¡ï¼ˆå·¥ä½œæµçŠ¶æ€: {workflow_instance['status']}ï¼‰"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµä»»åŠ¡æµç¨‹å¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{workflow_instance_id}/task-flow")
async def get_workflow_instance_task_flow(
    workflow_instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµå®ä¾‹ä»»åŠ¡æµç¨‹ï¼ˆç»Ÿä¸€æ¥å£ - æ”¯æŒä¸»å·¥ä½œæµå’Œå­å·¥ä½œæµï¼‰"""
    try:
        # ç›´æ¥è°ƒç”¨ç°æœ‰çš„å‡½æ•°ï¼Œä¿æŒé€»è¾‘ä¸€è‡´
        result = await get_workflow_task_flow(workflow_instance_id, current_user)
        
        # ä¿®æ”¹æ¶ˆæ¯ä»¥åæ˜ è¿™æ˜¯é€šè¿‡æ–°ç»Ÿä¸€æ¥å£è°ƒç”¨çš„
        if result.get("success"):
            result["message"] = result["message"].replace("è·å–å®æ—¶ä»»åŠ¡æµç¨‹æˆåŠŸ", "é€šè¿‡ç»Ÿä¸€æ¥å£è·å–å·¥ä½œæµå®ä¾‹ä»»åŠ¡æµç¨‹æˆåŠŸ")
        
        return result
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµå®ä¾‹ä»»åŠ¡æµç¨‹å¤±è´¥: {str(e)}"
        )




# ==================== Debugç«¯ç‚¹ ====================

@router.get("/debug/tasks")
async def debug_get_all_tasks(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """Debug: è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    try:
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        
        task_repo = TaskInstanceRepository()
        
        # è·å–æ‰€æœ‰ä»»åŠ¡å®ä¾‹ï¼ŒåŒ…å«è¯¦ç»†ä¿¡æ¯
        query = """
        SELECT 
            ti.*,
            wi.workflow_instance_name as workflow_instance_name,
            w.name as workflow_name,
            p.name as processor_name,
            p.type as processor_type,
            u.username as assigned_username,
            a.agent_name as assigned_agent_name
        FROM task_instance ti
        LEFT JOIN workflow_instance wi ON ti.workflow_instance_id = wi.workflow_instance_id
        LEFT JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id AND w.is_current_version = 1
        LEFT JOIN processor p ON ti.processor_id = p.processor_id
        LEFT JOIN user u ON ti.assigned_user_id = u.user_id
        LEFT JOIN agent a ON ti.assigned_agent_id = a.agent_id
        WHERE ti.is_deleted = FALSE
        ORDER BY ti.created_at DESC
        LIMIT 50
        """
        
        tasks = await task_repo.db.fetch_all(query)
        
        debug_data = {
            "total_tasks": len(tasks),
            "current_user_id": str(current_user.user_id),
            "current_username": current_user.username,
            "tasks": []
        }
        
        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„ä»»åŠ¡æ•°é‡
        status_counts = {}
        user_task_counts = {}
        
        for task in tasks:
            task_data = {
                "task_id": str(task['task_instance_id']),
                "task_title": task['task_title'],
                "task_type": task['task_type'],
                "status": task['status'],
                "workflow_name": task['workflow_name'],
                "processor_name": task['processor_name'],
                "processor_type": task['processor_type'],
                "assigned_user_id": str(task['assigned_user_id']) if task['assigned_user_id'] else None,
                "assigned_username": task['assigned_username'],
                "assigned_agent_name": task['assigned_agent_name'],
                "priority": task['priority'],
                "created_at": task['created_at'].isoformat() if task['created_at'] else None,
                "is_current_user_task": str(task['assigned_user_id']) == str(current_user.user_id) if task['assigned_user_id'] else False
            }
            debug_data["tasks"].append(task_data)
            
            # ç»Ÿè®¡çŠ¶æ€
            status = task['status']
            status_counts[status] = status_counts.get(status, 0) + 1
            
            # ç»Ÿè®¡ç”¨æˆ·ä»»åŠ¡
            if task['assigned_user_id']:
                user_id = str(task['assigned_user_id'])
                username = task['assigned_username'] or 'Unknown'
                user_task_counts[f"{username} ({user_id})"] = user_task_counts.get(f"{username} ({user_id})", 0) + 1
        
        debug_data["status_counts"] = status_counts
        debug_data["user_task_counts"] = user_task_counts
        debug_data["current_user_tasks"] = len([t for t in debug_data["tasks"] if t["is_current_user_task"]])
        
        return {
            "success": True,
            "data": debug_data,
            "message": f"è·å–è°ƒè¯•ä¿¡æ¯æˆåŠŸï¼Œæ€»å…± {len(tasks)} ä¸ªä»»åŠ¡"
        }
        
    except Exception as e:
        logger.error(f"è·å–è°ƒè¯•ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–è°ƒè¯•ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


# ==================== äººå·¥ä»»åŠ¡ç«¯ç‚¹ ====================

@router.get("/tasks/my")
async def get_my_tasks(
    task_status: Optional[TaskInstanceStatus] = None,
    limit: Optional[int] = None,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–æˆ‘çš„ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks = await execution_engine.get_user_tasks(
            current_user.user_id, task_status, limit
        )
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ æ‹†è§£ç›¸å…³ä¿¡æ¯
        enhanced_tasks = []
        for task in tasks:
            # æ·»åŠ åŸºæœ¬çš„æ‹†è§£å¯ç”¨æ€§æ£€æŸ¥
            task_status = task.get('status', '')
            task_type = task.get('task_type', '')
            
            # æ·»åŠ æ‹†è§£ä¿¡æ¯
            task['actions'] = {
                "can_subdivide": (
                    task_status in ['pending', 'assigned'] and 
                    task_type in ['human', 'mixed']
                ),
                "can_accept": task_status in ['pending', 'assigned'],
                "can_complete": task_status in ['in_progress'],
                "can_reject": task_status in ['pending', 'assigned']
            }
            
            enhanced_tasks.append(task)
        
        return {
            "success": True,
            "data": enhanced_tasks,
            "message": f"è·å–åˆ° {len(enhanced_tasks)} ä¸ªä»»åŠ¡"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}"
        )


@router.get("/tasks/statistics")
async def get_task_statistics(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–ä»»åŠ¡ç»Ÿè®¡"""
    try:
        stats = await execution_engine.get_task_statistics(current_user.user_id)
        
        return {
            "success": True,
            "data": stats,
            "message": "è·å–ä»»åŠ¡ç»Ÿè®¡æˆåŠŸ"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}"
        )


@router.get("/tasks/history")
async def get_task_history(
    days: int = 30,
    limit: int = 100,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–ä»»åŠ¡å†å²"""
    try:
        tasks = await execution_engine.get_task_history(
            current_user.user_id, days, limit
        )
        
        return {
            "success": True,
            "data": tasks,
            "message": f"è·å–åˆ° {days} å¤©å†…çš„ {len(tasks)} ä¸ªå†å²ä»»åŠ¡"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ä»»åŠ¡å†å²å¤±è´¥: {str(e)}"
        )


@router.get("/tasks/{task_id}")
async def get_task_details(
    task_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–ä»»åŠ¡è¯¦æƒ…ï¼ˆä½¿ç”¨ExecutionServiceä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¢å¼ºé™„ä»¶æ”¯æŒï¼‰"""
    try:
        logger.info(f"ğŸ” ä»»åŠ¡è¯¦æƒ…API: è·å–ä»»åŠ¡ {task_id}")
        
        # ä½¿ç”¨ç¨³å®šçš„ExecutionServiceæ–¹æ³•
        task_details = await execution_engine.get_task_details(task_id, current_user.user_id)
        
        if not task_details:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ä»»åŠ¡ä¸å­˜åœ¨"
            )
        
        # ğŸ”§ Linuså¼ä¿®å¤: æ‰‹åŠ¨æ·»åŠ é™„ä»¶ä¿¡æ¯åˆ°ExecutionServiceè¿”å›çš„ç»“æœä¸­
        try:
            from ..services.file_association_service import FileAssociationService
            file_service = FileAssociationService()
            
            # è·å–å½“å‰ä»»åŠ¡çš„é™„ä»¶
            current_task_attachments = []
            
            # 1. è·å–ä»»åŠ¡å®ä¾‹é™„ä»¶
            task_files = await file_service.get_task_instance_files(task_id)
            for file_info in task_files:
                current_task_attachments.append({
                    'file_id': file_info['file_id'],
                    'filename': file_info['filename'],
                    'original_filename': file_info['original_filename'],
                    'file_size': file_info['file_size'],
                    'content_type': file_info['content_type'],
                    'attachment_type': file_info['attachment_type'],
                    'source': 'task'
                })
            
            # 2. è·å–èŠ‚ç‚¹å®ä¾‹é™„ä»¶
            node_instance_id = task_details.get('node_instance_id')
            if node_instance_id:
                node_files = await file_service.get_node_instance_files(uuid.UUID(node_instance_id))
                for file_info in node_files:
                    current_task_attachments.append({
                        'file_id': file_info['file_id'],
                        'filename': file_info['filename'],
                        'original_filename': file_info['original_filename'],
                        'file_size': file_info['file_size'],
                        'content_type': file_info['content_type'],
                        'attachment_type': file_info['attachment_type'],
                        'source': 'node'
                    })
            
            # æ·»åŠ åˆ°è¿”å›ç»“æœä¸­
            task_details['current_task_attachments'] = current_task_attachments

            logger.info(f"ğŸ“ [ä»»åŠ¡é™„ä»¶] æˆåŠŸæ·»åŠ é™„ä»¶ä¿¡æ¯: {len(current_task_attachments)} ä¸ªæ–‡ä»¶")

        except Exception as attachment_error:
            logger.warning(f"âš ï¸ è·å–ä»»åŠ¡é™„ä»¶å¤±è´¥: {attachment_error}")
            task_details['current_task_attachments'] = []
        
        logger.info(f"âœ… ä»»åŠ¡è¯¦æƒ…API: æˆåŠŸè·å–ä»»åŠ¡è¯¦æƒ…")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ä»¥å¸®åŠ©å‰ç«¯ç†è§£æ•°æ®ç»“æ„
        context_debug = {}
        
        # å°†è°ƒè¯•ä¿¡æ¯æ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
        task_details['debug_info'] = context_debug
        
        # æ·»åŠ ä»»åŠ¡æ‹†è§£ç›¸å…³ä¿¡æ¯
        subdivision_info = {
            "can_subdivide": False,
            "subdivision_count": 0,
            "existing_subdivisions": [],
            "subdivision_available": True  # æ ¹æ®ä¸šåŠ¡é€»è¾‘å†³å®šæ˜¯å¦å¯ä»¥æ‹†è§£
        }
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯ä»¥æ‹†è§£ï¼ˆæ ¹æ®ä»»åŠ¡çŠ¶æ€å’Œç±»å‹ï¼‰
        task_status = task_details.get('status', '')
        task_type = task_details.get('task_type', '')
        
        # åªæœ‰å¾…å¤„ç†æˆ–å·²åˆ†é…çš„äººå·¥ä»»åŠ¡å¯ä»¥æ‹†è§£
        if task_status in ['pending', 'assigned'] and task_type in ['human', 'mixed']:
            subdivision_info["can_subdivide"] = True
            
            # è·å–ç°æœ‰çš„æ‹†è§£è®°å½•
            try:
                from ..services.task_subdivision_service import TaskSubdivisionService
                subdivision_service = TaskSubdivisionService()
                existing_subdivisions = await subdivision_service.get_task_subdivisions(task_id)
                
                subdivision_info["subdivision_count"] = len(existing_subdivisions)
                subdivision_info["existing_subdivisions"] = [
                    {
                        "subdivision_id": str(sub.subdivision_id),
                        "subdivision_name": sub.subdivision_name,
                        "created_at": sub.created_at.isoformat() if sub.created_at else None,
                        "status": sub.status,
                        "subdivider_name": sub.subdivider_name  # éœ€è¦åœ¨serviceä¸­è·å–
                    }
                    for sub in existing_subdivisions[:5]  # åªæ˜¾ç¤ºæœ€è¿‘5ä¸ª
                ]
            except Exception as e:
                logger.warning(f"è·å–ä»»åŠ¡æ‹†è§£ä¿¡æ¯å¤±è´¥: {e}")
                # ä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼Œç»§ç»­è¿”å›
        
        task_details['subdivision_info'] = subdivision_info

        return {
            "success": True,
            "data": task_details,
            "message": "è·å–ä»»åŠ¡è¯¦æƒ…æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}"
        )


@router.post("/tasks/{task_id}/start")
async def start_task(
    task_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """å¼€å§‹æ‰§è¡Œä»»åŠ¡"""
    try:
        result = await execution_engine.start_human_task(task_id, current_user.user_id)
        
        return {
            "success": True,
            "data": result,
            "message": "ä»»åŠ¡å·²å¼€å§‹æ‰§è¡Œ"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒæ‰§è¡Œæ­¤ä»»åŠ¡"
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¼€å§‹æ‰§è¡Œä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.post("/tasks/{task_id}/submit")
async def submit_task_result(
    task_id: uuid.UUID,
    raw_request: Request,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æäº¤ä»»åŠ¡ç»“æœ"""
    try:
        # å…ˆè¯»å–åŸå§‹è¯·æ±‚ä½“è¿›è¡Œè°ƒè¯•
        body = await raw_request.body()
        logger.info(f"ğŸ“ æ”¶åˆ°ä»»åŠ¡æäº¤çš„åŸå§‹è¯·æ±‚:")
        logger.info(f"  ä»»åŠ¡ID: {task_id}")
        logger.info(f"  ç”¨æˆ·ID: {current_user.user_id}")
        logger.info(f"  åŸå§‹è¯·æ±‚ä½“ ({len(body)} å­—èŠ‚): {body.decode('utf-8', errors='ignore')}")
        
        # å°è¯•è§£æJSON
        import json
        try:
            raw_data = json.loads(body.decode('utf-8'))
            logger.info(f"  è§£æçš„JSONæ•°æ®: {raw_data}")
            logger.info(f"  æ•°æ®ç±»å‹åˆ†æ:")
            for key, value in raw_data.items():
                logger.info(f"    {key}: {type(value).__name__} = {repr(value)}")
        except Exception as json_error:
            logger.error(f"  JSONè§£æå¤±è´¥: {json_error}")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"æ— æ•ˆçš„JSONæ ¼å¼: {str(json_error)}"
            )
        
        # é¢„å¤„ç†ï¼šç¡®ä¿result_dataæ˜¯å­—å…¸ç±»å‹
        if 'result_data' in raw_data:
            result_data_value = raw_data['result_data']
            logger.info(f"  åŸå§‹result_data: {type(result_data_value).__name__} = {repr(result_data_value)}")
            
            # å¦‚æœresult_dataæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
            if isinstance(result_data_value, str):
                try:
                    parsed_json = json.loads(result_data_value)
                    # æ£€æŸ¥è§£æç»“æœæ˜¯å¦ä¸ºå­—å…¸
                    if isinstance(parsed_json, dict):
                        raw_data['result_data'] = parsed_json
                        logger.info(f"  å­—ç¬¦ä¸²è§£æåresult_data: {raw_data['result_data']}")
                    else:
                        # å¦‚æœè§£æå‡ºæ¥ä¸æ˜¯å­—å…¸ï¼ŒåŒ…è£…ä¸ºå­—å…¸
                        raw_data['result_data'] = {"value": parsed_json}
                        logger.info(f"  è§£æç»“æœåŒ…è£…åresult_data: {raw_data['result_data']}")
                except:
                    # å¦‚æœä¸æ˜¯JSONå­—ç¬¦ä¸²ï¼ŒåŒ…è£…ä¸ºå­—å…¸
                    raw_data['result_data'] = {"answer": result_data_value}
                    logger.info(f"  å­—ç¬¦ä¸²åŒ…è£…åresult_data: {raw_data['result_data']}")
            elif result_data_value is None:
                raw_data['result_data'] = {}
                logger.info(f"  Noneè½¬æ¢åresult_data: {raw_data['result_data']}")
            elif not isinstance(result_data_value, dict):
                # å…¶ä»–ç±»å‹ä¹ŸåŒ…è£…ä¸ºå­—å…¸
                raw_data['result_data'] = {"value": result_data_value}
                logger.info(f"  å…¶ä»–ç±»å‹åŒ…è£…åresult_data: {raw_data['result_data']}")
        
        # æ‰‹åŠ¨éªŒè¯è¯·æ±‚æ•°æ®
        try:
            request = TaskSubmissionRequest(**raw_data)
            logger.info(f"  âœ… PydanticéªŒè¯æˆåŠŸ:")
            logger.info(f"    result_data: {request.result_data}")
            logger.info(f"    result_summary: {request.result_summary}")
        except ValidationError as ve:
            logger.error(f"  âŒ PydanticéªŒè¯å¤±è´¥:")
            for error in ve.errors():
                logger.error(f"    - {error['loc']}: {error['msg']} (type: {error['type']})")
            logger.error(f"  å¤„ç†åçš„æ•°æ®: {raw_data}")
            raise HTTPException(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                detail=f"è¯·æ±‚æ•°æ®éªŒè¯å¤±è´¥: {ve.errors()}"
            )
        
        # ç¡®ä¿ result_data ä¸ä¸º None
        result_data = request.result_data if request.result_data is not None else {}
        attachment_file_ids = request.attachment_file_ids or []
        logger.info(f"  ğŸ”„ å‡†å¤‡æäº¤ä»»åŠ¡ç»“æœ: result_data={result_data}, attachments={len(attachment_file_ids)}ä¸ª")
        
        result = await execution_engine.submit_human_task_result(
            task_id, current_user.user_id, 
            result_data, request.result_summary
        )
        
        # ğŸ†• å¤„ç†é™„ä»¶å…³è”
        if attachment_file_ids:
            try:
                from ..services.file_association_service import FileAssociationService
                file_service = FileAssociationService()
                
                for file_id in attachment_file_ids:
                    await file_service.associate_task_instance_file(task_id, uuid.UUID(file_id), current_user.user_id)
                    logger.info(f"  ğŸ“ é™„ä»¶å…³è”æˆåŠŸ: file_id={file_id} -> task_id={task_id}")
                
                logger.info(f"  âœ… æ‰€æœ‰é™„ä»¶å…³è”å®Œæˆ: {len(attachment_file_ids)}ä¸ªæ–‡ä»¶")
            except Exception as e:
                logger.warning(f"  âš ï¸ é™„ä»¶å…³è”å¤±è´¥: {e}")
                # ä¸ä¸­æ–­ä»»åŠ¡æäº¤æµç¨‹ï¼Œåªè®°å½•è­¦å‘Š
        
        logger.info(f"  âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {result}")
        
        return {
            "success": True,
            "data": result,
            "message": "ä»»åŠ¡ç»“æœå·²æäº¤"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒæäº¤æ­¤ä»»åŠ¡"
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æäº¤ä»»åŠ¡ç»“æœå¤±è´¥: {str(e)}"
        )


@router.post("/tasks/{task_id}/pause")
async def pause_task(
    task_id: uuid.UUID,
    request: TaskActionRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æš‚åœä»»åŠ¡"""
    try:
        result = await execution_engine.pause_task(
            task_id, current_user.user_id, request.reason
        )
        
        return {
            "success": True,
            "data": result,
            "message": "ä»»åŠ¡å·²æš‚åœ"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒæš‚åœæ­¤ä»»åŠ¡"
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æš‚åœä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.post("/tasks/{task_id}/help")
async def request_help(
    task_id: uuid.UUID,
    request: HelpRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è¯·æ±‚å¸®åŠ©"""
    try:
        result = await execution_engine.request_help(
            task_id, current_user.user_id, request.help_message
        )
        
        return {
            "success": True,
            "data": result,
            "message": "å¸®åŠ©è¯·æ±‚å·²æäº¤"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒä¸ºæ­¤ä»»åŠ¡è¯·æ±‚å¸®åŠ©"
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è¯·æ±‚å¸®åŠ©å¤±è´¥: {str(e)}"
        )


@router.post("/tasks/{task_id}/reject")
async def reject_task(
    task_id: uuid.UUID,
    request: TaskActionRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ‹’ç»ä»»åŠ¡"""
    try:
        if not request.reason:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ‹’ç»ä»»åŠ¡æ—¶å¿…é¡»æä¾›æ‹’ç»åŸå› "
            )
        
        result = await execution_engine.reject_task(
            task_id, current_user.user_id, request.reason
        )
        
        return {
            "success": True,
            "data": result,
            "message": "ä»»åŠ¡å·²æ‹’ç»"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒæ‹’ç»æ­¤ä»»åŠ¡"
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‹’ç»ä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.post("/tasks/{task_id}/cancel")
async def cancel_task(
    task_id: uuid.UUID,
    request: TaskActionRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """å–æ¶ˆä»»åŠ¡"""
    try:
        result = await execution_engine.cancel_task(
            task_id, current_user.user_id, request.reason or "ç”¨æˆ·å–æ¶ˆ"
        )
        
        return {
            "success": True,
            "data": result,
            "message": "ä»»åŠ¡å·²å–æ¶ˆ"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒå–æ¶ˆæ­¤ä»»åŠ¡"
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.delete("/tasks/{task_id}")
async def delete_task(
    task_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    åˆ é™¤ä»»åŠ¡å®ä¾‹
    
    åªå…è®¸åˆ é™¤çŠ¶æ€ä¸º 'completed' æˆ– 'cancelled' çš„ä»»åŠ¡
    
    Args:
        task_id: ä»»åŠ¡ID
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        logger.info(f"ğŸ—‘ï¸ ç”¨æˆ· {current_user.username} è¯·æ±‚åˆ é™¤ä»»åŠ¡: {task_id}")
        
        # 1. æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨å’Œæƒé™
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        task_repo = TaskInstanceRepository()
        
        task = await task_repo.get_task_by_id(task_id)
        if not task:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ä»»åŠ¡ä¸å­˜åœ¨"
            )
        
        # 2. æ£€æŸ¥æƒé™ï¼šå…è®¸ä»¥ä¸‹ç”¨æˆ·åˆ é™¤ä»»åŠ¡
        # - è¢«åˆ†é…çš„ç”¨æˆ·
        # - ç®¡ç†å‘˜ç”¨æˆ·
        # - ä»»åŠ¡æ‰€å±å·¥ä½œæµçš„åˆ›å»ºè€…
        has_permission = False
        permission_reason = ""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¢«åˆ†é…çš„ç”¨æˆ· - ä¿®å¤UUIDç±»å‹åŒ¹é…é—®é¢˜
        assigned_user_id = task.get('assigned_user_id')
        current_user_id = current_user.user_id
        
        logger.debug(f"ğŸ” æƒé™æ£€æŸ¥è°ƒè¯•ä¿¡æ¯:")
        logger.debug(f"   ä»»åŠ¡åˆ†é…ç”¨æˆ·ID: {assigned_user_id} (ç±»å‹: {type(assigned_user_id)})")
        logger.debug(f"   å½“å‰ç”¨æˆ·ID: {current_user_id} (ç±»å‹: {type(current_user_id)})")
        
        # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
        if assigned_user_id and str(assigned_user_id) == str(current_user_id):
            has_permission = True
            permission_reason = "ä»»åŠ¡åˆ†é…è€…"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç®¡ç†å‘˜ï¼ˆç”¨æˆ·åä¸ºadminæˆ–å…·æœ‰ç®¡ç†å‘˜è§’è‰²ï¼‰
        elif current_user.username.lower() == 'admin' or getattr(current_user, 'is_admin', False):
            has_permission = True
            permission_reason = "ç®¡ç†å‘˜æƒé™"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥ä½œæµåˆ›å»ºè€…
        else:
            # è·å–å·¥ä½œæµå®ä¾‹ä¿¡æ¯
            from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
            workflow_repo = WorkflowInstanceRepository()
            workflow_instance = await workflow_repo.get_instance_by_id(task.get('workflow_instance_id'))
            if workflow_instance:
                created_by = workflow_instance.get('created_by')
                logger.debug(f"   å·¥ä½œæµåˆ›å»ºè€…: {created_by} (ç±»å‹: {type(created_by)})")
                if created_by and str(created_by) == str(current_user_id):
                    has_permission = True
                    permission_reason = "å·¥ä½œæµåˆ›å»ºè€…"
        
        if not has_permission:
            logger.warning(f"âŒ ç”¨æˆ· {current_user.username} æ— æƒåˆ é™¤ä»»åŠ¡ {task_id}")
            logger.warning(f"   - ä»»åŠ¡åˆ†é…ç»™: {assigned_user_id}")
            logger.warning(f"   - å½“å‰ç”¨æˆ·: {current_user_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒåˆ é™¤æ­¤ä»»åŠ¡"
            )
        
        logger.info(f"âœ… ç”¨æˆ· {current_user.username} å…·æœ‰åˆ é™¤æƒé™ ({permission_reason})")
        
        # 3. æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼šåªå…è®¸åˆ é™¤å·²å®Œæˆæˆ–å·²å–æ¶ˆçš„ä»»åŠ¡
        task_status = task.get('status', '').lower()
        if task_status not in ['completed', 'cancelled']:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"åªèƒ½åˆ é™¤å·²å®Œæˆæˆ–å·²å–æ¶ˆçš„ä»»åŠ¡ï¼Œå½“å‰çŠ¶æ€: {task_status}"
            )
        
        # 4. æ‰§è¡Œè½¯åˆ é™¤
        success = await task_repo.delete_task(task_id, soft_delete=True)
        
        if success:
            logger.info(f"âœ… ç”¨æˆ· {current_user.username} æˆåŠŸåˆ é™¤ä»»åŠ¡: {task.get('task_title', 'æœªçŸ¥')}")
            return {
                "success": True,
                "data": {
                    "task_id": str(task_id),
                    "task_title": task.get('task_title', 'æœªçŸ¥'),
                    "previous_status": task_status,
                    "deleted_at": now_utc().isoformat()
                },
                "message": "ä»»åŠ¡å·²åˆ é™¤"
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="åˆ é™¤ä»»åŠ¡å¤±è´¥"
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡å¼‚å¸¸: {e}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}"
        )


# ==================== Agentä»»åŠ¡ç«¯ç‚¹ ====================

@router.get("/agent-tasks/pending")
async def get_pending_agent_tasks(
    agent_id: Optional[uuid.UUID] = None,
    limit: int = 50,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å¾…å¤„ç†çš„Agentä»»åŠ¡"""
    try:
        tasks = await agent_task_service.get_pending_agent_tasks(agent_id, limit)
        
        return {
            "success": True,
            "data": tasks,
            "message": f"è·å–åˆ° {len(tasks)} ä¸ªå¾…å¤„ç†Agentä»»åŠ¡"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å¾…å¤„ç†Agentä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.post("/agent-tasks/{task_id}/process")
async def process_agent_task(
    task_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ‰‹åŠ¨è§¦å‘Agentä»»åŠ¡å¤„ç†"""
    try:
        result = await agent_task_service.process_agent_task(task_id)
        
        return {
            "success": True,
            "data": result,
            "message": "Agentä»»åŠ¡å¤„ç†å®Œæˆ"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¤„ç†Agentä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.post("/agent-tasks/{task_id}/retry")
async def retry_agent_task(
    task_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """é‡è¯•å¤±è´¥çš„Agentä»»åŠ¡"""
    try:
        result = await agent_task_service.retry_failed_task(task_id)
        
        return {
            "success": True,
            "data": result,
            "message": "Agentä»»åŠ¡å·²é‡æ–°åŠ å…¥å¤„ç†é˜Ÿåˆ—"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"é‡è¯•Agentä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.post("/agent-tasks/{task_id}/cancel")
async def cancel_agent_task(
    task_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """å–æ¶ˆAgentä»»åŠ¡"""
    try:
        result = await agent_task_service.cancel_agent_task(task_id)
        
        return {
            "success": True,
            "data": result,
            "message": "Agentä»»åŠ¡å·²å–æ¶ˆ"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å–æ¶ˆAgentä»»åŠ¡å¤±è´¥: {str(e)}"
        )


@router.get("/agent-tasks/statistics")
async def get_agent_task_statistics(
    agent_id: Optional[uuid.UUID] = None,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–Agentä»»åŠ¡ç»Ÿè®¡"""
    try:
        stats = await agent_task_service.get_agent_task_statistics(agent_id)
        
        return {
            "success": True,
            "data": stats,
            "message": "è·å–Agentä»»åŠ¡ç»Ÿè®¡æˆåŠŸ"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–Agentä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}"
        )


# ==================== å·¥ä½œæµå®ä¾‹ç®¡ç†ç«¯ç‚¹ ====================

@router.post("/workflows/instances/{instance_id}/cancel")
async def cancel_workflow_instance(
    instance_id: uuid.UUID,
    request: TaskActionRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """å–æ¶ˆå·¥ä½œæµå®ä¾‹"""
    try:
        logger.info(f"ğŸš« ç”¨æˆ· {current_user.user_id} è¯·æ±‚å–æ¶ˆå·¥ä½œæµå®ä¾‹: {instance_id}")
        logger.info(f"  å–æ¶ˆåŸå› : {request.reason}")
        
        # è°ƒç”¨æœåŠ¡å±‚å¤„ç†å·¥ä½œæµå–æ¶ˆ
        result = await execution_engine.cancel_workflow_instance(
            instance_id, current_user.user_id, request.reason or "ç”¨æˆ·å–æ¶ˆ"
        )
        
        return {
            "success": True,
            "data": result,
            "message": "å·¥ä½œæµå®ä¾‹å·²å–æ¶ˆ"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒå–æ¶ˆæ­¤å·¥ä½œæµå®ä¾‹"
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"å–æ¶ˆå·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å–æ¶ˆå·¥ä½œæµå®ä¾‹å¤±è´¥: {str(e)}"
        )


@router.delete("/workflows/{instance_id}")
async def delete_workflow_instance(
    instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """åˆ é™¤å·¥ä½œæµå®ä¾‹"""
    try:
        logger.info(f"ğŸ—‘ï¸ ç”¨æˆ· {current_user.user_id} ({current_user.username}) è¯·æ±‚åˆ é™¤å·¥ä½œæµå®ä¾‹: {instance_id}")
        
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        workflow_instance_repo = WorkflowInstanceRepository()
        
        # æ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨
        logger.info(f"ğŸ” æ­¥éª¤1: æ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨")
        instance = await workflow_instance_repo.get_instance_by_id(instance_id)
        if not instance:
            logger.warning(f"âš ï¸ å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {instance_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ°å·¥ä½œæµå®ä¾‹è¯¦ç»†ä¿¡æ¯:")
        logger.info(f"   - å®ä¾‹åç§°: {instance.get('workflow_instance_name', 'æœªå‘½å')}")
        logger.info(f"   - å½“å‰çŠ¶æ€: {instance.get('status')}")
        logger.info(f"   - æ‰§è¡Œè€…ID: {instance.get('executor_id')}")
        logger.info(f"   - åˆ›å»ºæ—¶é—´: {instance.get('created_at')}")
        logger.info(f"   - æ›´æ–°æ—¶é—´: {instance.get('updated_at')}")
        logger.info(f"   - æ˜¯å¦å·²åˆ é™¤: {instance.get('is_deleted', False)}")
        
        # æ£€æŸ¥æƒé™ï¼ˆåªæœ‰æ‰§è¡Œè€…å¯ä»¥åˆ é™¤ï¼‰
        logger.info(f"ğŸ” æ­¥éª¤2: æ£€æŸ¥åˆ é™¤æƒé™")
        current_user_id_str = str(current_user.user_id)
        # æ•°æ®åº“å­—æ®µæ˜¯ executor_id
        executor_id_str = str(instance.get('executor_id'))
        logger.info(f"   - å½“å‰ç”¨æˆ·ID: {current_user_id_str}")
        logger.info(f"   - æ‰§è¡Œè€…ID: {executor_id_str}")
        
        if executor_id_str != current_user_id_str:
            logger.warning(f"ğŸš« æƒé™æ£€æŸ¥å¤±è´¥:")
            logger.warning(f"   - ç”¨æˆ· {current_user_id_str} æ— æƒåˆ é™¤å®ä¾‹ {instance_id}")
            logger.warning(f"   - åªæœ‰æ‰§è¡Œè€… {executor_id_str} å¯ä»¥åˆ é™¤æ­¤å®ä¾‹")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒåˆ é™¤æ­¤å·¥ä½œæµå®ä¾‹"
            )
        logger.info(f"âœ… æƒé™æ£€æŸ¥é€šè¿‡")
        
        # æ£€æŸ¥å®ä¾‹çŠ¶æ€ï¼ˆä¸å…è®¸åˆ é™¤æ­£åœ¨è¿è¡Œçš„å®ä¾‹ï¼‰
        logger.info(f"ğŸ” æ­¥éª¤3: æ£€æŸ¥å®ä¾‹çŠ¶æ€")
        current_status = instance.get('status')
        logger.info(f"   - å½“å‰çŠ¶æ€: {current_status}")
        
        if current_status == 'running':
            logger.warning(f"âš ï¸ çŠ¶æ€æ£€æŸ¥å¤±è´¥: ä¸èƒ½åˆ é™¤æ­£åœ¨è¿è¡Œçš„å®ä¾‹")
            logger.warning(f"   - å®ä¾‹ {instance_id} çŠ¶æ€ä¸º 'running'")
            logger.warning(f"   - è¯·å…ˆå–æ¶ˆå®ä¾‹åå†åˆ é™¤")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ— æ³•åˆ é™¤æ­£åœ¨è¿è¡Œçš„å·¥ä½œæµå®ä¾‹ï¼Œè¯·å…ˆå–æ¶ˆå®ä¾‹"
            )
        logger.info(f"âœ… çŠ¶æ€æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥åˆ é™¤")
        
        # æ‰§è¡Œè½¯åˆ é™¤
        logger.info(f"ğŸ” æ­¥éª¤4: æ‰§è¡Œè½¯åˆ é™¤æ“ä½œ")
        logger.info(f"   - è°ƒç”¨ workflow_instance_repo.delete_instance({instance_id}, soft_delete=True)")
        
        try:
            success = await workflow_instance_repo.delete_instance(instance_id, soft_delete=True)
            logger.info(f"   - åˆ é™¤æ“ä½œè¿”å›ç»“æœ: {success}")
            
            if success:
                logger.info(f"âœ… å·¥ä½œæµå®ä¾‹åˆ é™¤æˆåŠŸ: {instance_id}")
                
                # éªŒè¯åˆ é™¤ç»“æœ
                logger.info(f"ğŸ” æ­¥éª¤5: éªŒè¯åˆ é™¤ç»“æœ")
                verification_instance = await workflow_instance_repo.get_instance_by_id(instance_id)
                if verification_instance:
                    logger.info(f"   - éªŒè¯: å®ä¾‹ä»å­˜åœ¨ (è½¯åˆ é™¤)")
                    logger.info(f"   - is_deleted æ ‡å¿—: {verification_instance.get('is_deleted', 'unknown')}")
                else:
                    logger.info(f"   - éªŒè¯: å®ä¾‹å·²ä¸å¯è®¿é—® (åˆ é™¤æˆåŠŸ)")
                
                return {
                    "success": True,
                    "data": {"instance_id": str(instance_id)},
                    "message": "å·¥ä½œæµå®ä¾‹å·²åˆ é™¤"
                }
            else:
                logger.error(f"âŒ åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥:")
                logger.error(f"   - å®ä¾‹ID: {instance_id}")
                logger.error(f"   - æ•°æ®åº“æ“ä½œè¿”å›: {success}")
                logger.error(f"   - å¯èƒ½çš„åŸå› : æ•°æ®åº“çº¦æŸã€æƒé™é—®é¢˜æˆ–å®ä¾‹ä¸å­˜åœ¨")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥"
                )
        except Exception as delete_error:
            logger.error(f"âŒ æ‰§è¡Œåˆ é™¤æ“ä½œæ—¶å‘ç”Ÿå¼‚å¸¸:")
            logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(delete_error).__name__}")
            logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(delete_error)}")
            import traceback
            logger.error(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"åˆ é™¤æ“ä½œå¼‚å¸¸: {str(delete_error)}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤å·¥ä½œæµå®ä¾‹æ€»ä½“å¼‚å¸¸:")
        logger.error(f"   - å®ä¾‹ID: {instance_id}")
        logger.error(f"   - ç”¨æˆ·ID: {current_user.user_id}")
        logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(e)}")
        import traceback
        logger.error(f"   - å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥: {str(e)}"
        )


@router.get("/workflows/instances/{instance_id}/context")
async def get_workflow_context(
    instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµå®ä¾‹çš„å®Œæ•´ä¸Šä¸‹æ–‡å†…å®¹"""
    try:
        logger.info(f"ğŸ“Š ç”¨æˆ· {current_user.user_id} è¯·æ±‚è·å–å·¥ä½œæµä¸Šä¸‹æ–‡: {instance_id}")
        
        # éªŒè¯å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨å’Œæƒé™
        workflow_query = '''
        SELECT workflow_instance_id, executor_id, status, workflow_instance_name
        FROM workflow_instance 
        WHERE workflow_instance_id = $1 AND is_deleted = FALSE
        '''
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        task_repo = TaskInstanceRepository()
        workflow = await task_repo.db.fetch_one(workflow_query, instance_id)
        
        if not workflow:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        # æ£€æŸ¥è®¿é—®æƒé™ï¼ˆæ‰§è¡Œè€…æˆ–ç®¡ç†å‘˜å¯ä»¥æŸ¥çœ‹ï¼‰
        if workflow['executor_id'] != current_user.user_id:
            # TODO: è¿™é‡Œå¯ä»¥æ·»åŠ ç®¡ç†å‘˜æƒé™æ£€æŸ¥
            logger.warning(f"âš ï¸ ç”¨æˆ· {current_user.user_id} å°è¯•è®¿é—®ä¸å±äºè‡ªå·±çš„å·¥ä½œæµ: {instance_id}")
            # æš‚æ—¶å…è®¸æ‰€æœ‰ç”¨æˆ·æŸ¥çœ‹ï¼ˆç”Ÿäº§ç¯å¢ƒéœ€è¦ä¸¥æ ¼æƒé™æ§åˆ¶ï¼‰
        
        # è·å–å®Œæ•´çš„å·¥ä½œæµä¸Šä¸‹æ–‡
        context = await execution_engine._collect_workflow_context(instance_id)
        
        # æŸ¥æ‰¾ç»“æŸèŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®
        end_node_output = None
        end_nodes_query = '''
        SELECT ni.output_data, n.name as node_name
        FROM node_instance ni
        JOIN node n ON ni.node_id = n.node_id
        WHERE ni.workflow_instance_id = $1 
        AND n.node_type = 'end'
        AND ni.status = 'completed'
        ORDER BY ni.updated_at DESC
        LIMIT 1
        '''
        end_node = await task_repo.db.fetch_one(end_nodes_query, instance_id)
        
        if end_node and end_node['output_data']:
            end_node_output = {
                'end_node_name': end_node['node_name'],
                'full_context': end_node['output_data']
            }
        
        return {
            "success": True,
            "data": {
                "workflow_instance_id": str(instance_id),
                "workflow_status": workflow['status'],
                "workflow_name": workflow['workflow_instance_name'],
                "context_summary": context,
                "end_node_output": end_node_output,
                "has_complete_context": end_node_output is not None
            },
            "message": "å·¥ä½œæµä¸Šä¸‹æ–‡è·å–æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"
        )


# ==================== ç®¡ç†å‘˜ä»»åŠ¡ç®¡ç†ç«¯ç‚¹ ====================

@router.post("/admin/tasks/{task_id}/assign")
async def assign_task_to_user(
    task_id: uuid.UUID,
    request: TaskAssignmentRequest,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """ç®¡ç†å‘˜åˆ†é…ä»»åŠ¡ç»™ç”¨æˆ·"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™
        if current_user.role not in ['admin', 'manager']:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒé™æ‰§è¡Œæ­¤æ“ä½œ"
            )
        
        result = await execution_engine.assign_task_to_user(
            task_id, request.user_id, current_user.user_id
        )
        
        return {
            "success": True,
            "data": result,
            "message": "ä»»åŠ¡åˆ†é…æˆåŠŸ"
        }
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="æ— æƒåˆ†é…ä»»åŠ¡"
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ†é…ä»»åŠ¡å¤±è´¥: {str(e)}"
        )


# ==================== å·¥ä½œæµç›‘æ§ç«¯ç‚¹ ====================

@router.get("/system/monitor-stats")
async def get_workflow_monitor_stats(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµç›‘æ§æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™
        if current_user.role not in ['admin', 'manager']:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒé™è®¿é—®ç›‘æ§ç»Ÿè®¡ä¿¡æ¯"
            )
        
        from ..services.workflow_monitor_service import get_workflow_monitor
        workflow_monitor = get_workflow_monitor()
        
        stats = await workflow_monitor.get_monitor_stats()
        
        return {
            "success": True,
            "data": stats,
            "message": "è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.post("/system/monitor-scan")
async def manual_workflow_monitor_scan(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ‰‹åŠ¨è§¦å‘åœæ»å·¥ä½œæµæ‰«æå’Œæ¢å¤"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™
        if current_user.role not in ['admin', 'manager']:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒé™æ‰‹åŠ¨è§¦å‘ç›‘æ§æ‰«æ"
            )
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {current_user.username} æ‰‹åŠ¨è§¦å‘åœæ»å·¥ä½œæµæ‰«æ")
        
        from ..services.workflow_monitor_service import get_workflow_monitor
        workflow_monitor = get_workflow_monitor()
        
        scan_results = await workflow_monitor.manual_scan_and_recover()
        
        return {
            "success": True,
            "data": scan_results,
            "message": f"æ‰«æå®Œæˆï¼Œæ¢å¤äº† {scan_results['successful_recoveries']} ä¸ªåœæ»å·¥ä½œæµ"
        }
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨è§¦å‘ç›‘æ§æ‰«æå¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‰‹åŠ¨è§¦å‘ç›‘æ§æ‰«æå¤±è´¥: {str(e)}"
        )


# ==================== ç³»ç»Ÿç›‘æ§ç«¯ç‚¹ ====================

@router.get("/system/status")
async def get_system_status(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–æ‰§è¡Œç³»ç»ŸçŠ¶æ€"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™
        if current_user.role not in ['admin', 'manager']:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒé™è®¿é—®ç³»ç»ŸçŠ¶æ€"
            )
        
        system_status = {
            "execution_engine": {
                "is_running": execution_engine.is_running,
                "running_instances": len(execution_engine.running_instances),
                "queue_size": execution_engine.execution_queue.qsize()
            },
            "agent_service": {
                "is_running": agent_task_service.is_running,
                "queue_size": agent_task_service.processing_queue.qsize(),
                "max_concurrent": agent_task_service.max_concurrent_tasks
            }
        }
        
        return {
            "success": True,
            "data": system_status,
            "message": "è·å–ç³»ç»ŸçŠ¶æ€æˆåŠŸ"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.get("/system/context-health")
async def get_context_health_stats(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµä¸Šä¸‹æ–‡å¥åº·ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™
        if current_user.role not in ['admin', 'manager']:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒé™è®¿é—®ç³»ç»Ÿå¥åº·çŠ¶æ€"
            )
        
        # è·å–ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¥åº·ç»Ÿè®¡
        from ..services.workflow_execution_context import get_context_manager
        context_manager = get_context_manager()
        
        health_stats = context_manager.get_health_stats()
        
        return {
            "success": True,
            "data": {
                "context_health": health_stats,
                "health_check_enabled": True,
                "persistence_enabled": context_manager._persistence_enabled,
                "auto_recovery_enabled": context_manager._auto_recovery_enabled,
                "context_ttl_hours": context_manager._context_ttl / 3600,
                "max_memory_contexts": context_manager._max_memory_contexts,
                "health_check_interval_seconds": context_manager._health_check_interval
            },
            "message": "è·å–ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"è·å–ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{instance_id}/context-health")
async def check_workflow_context_health(
    instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ£€æŸ¥ç‰¹å®šå·¥ä½œæµçš„ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€"""
    try:
        from ..services.workflow_execution_context import get_context_manager
        context_manager = get_context_manager()
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€
        health_status = await context_manager.check_context_health(instance_id)
        
        return {
            "success": True,
            "data": {
                "workflow_instance_id": str(instance_id),
                "context_health": health_status
            },
            "message": "ä¸Šä¸‹æ–‡å¥åº·æ£€æŸ¥å®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥å·¥ä½œæµä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ£€æŸ¥å·¥ä½œæµä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}"
        )


@router.post("/workflows/{instance_id}/context-recover")
async def recover_workflow_context(
    instance_id: uuid.UUID,
    force_recover: bool = Query(False, description="å¼ºåˆ¶æ¢å¤ä¸Šä¸‹æ–‡"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """ä¸»åŠ¨æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡å¹¶æ£€æŸ¥ä¸‹æ¸¸èŠ‚ç‚¹è§¦å‘"""
    try:
        logger.info(f"ğŸ”§ ç”¨æˆ· {current_user.username} è¯·æ±‚æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡: {instance_id}")
        
        from ..services.workflow_execution_context import get_context_manager
        from ..services.execution_service import execution_engine
        
        context_manager = get_context_manager()
        
        # 1. æ£€æŸ¥å½“å‰ä¸Šä¸‹æ–‡çŠ¶æ€
        current_context = context_manager.contexts.get(instance_id)
        context_existed = current_context is not None
        
        logger.info(f"   - å½“å‰å†…å­˜ä¸­ä¸Šä¸‹æ–‡å­˜åœ¨: {context_existed}")
        
        # 2. å¼ºåˆ¶æ¢å¤æˆ–ä¸Šä¸‹æ–‡ä¸å­˜åœ¨æ—¶è¿›è¡Œæ¢å¤
        if force_recover or not context_existed:
            logger.info(f"   - å¼€å§‹æ¢å¤ä¸Šä¸‹æ–‡ (å¼ºåˆ¶: {force_recover})")
            
            # å¼ºåˆ¶é‡æ–°ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡
            if context_existed and force_recover:
                # æ¸…ç†ç°æœ‰ä¸Šä¸‹æ–‡
                await context_manager.remove_context(instance_id)
                logger.info(f"   - å·²æ¸…ç†ç°æœ‰ä¸Šä¸‹æ–‡")
            
            # æ¢å¤ä¸Šä¸‹æ–‡
            recovered_context = await context_manager.get_context(instance_id)
            
            if recovered_context:
                logger.info(f"âœ… ä¸Šä¸‹æ–‡æ¢å¤æˆåŠŸ")
                logger.info(f"   - èŠ‚ç‚¹ä¾èµ–æ•°: {len(recovered_context.node_dependencies)}")
                logger.info(f"   - å·²å®ŒæˆèŠ‚ç‚¹: {len(recovered_context.execution_context.get('completed_nodes', set()))}")
                logger.info(f"   - å¾…è§¦å‘èŠ‚ç‚¹: {len(recovered_context.pending_triggers)}")
                
                # 3. æ£€æŸ¥å¹¶è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹
                ready_nodes = await recovered_context.get_ready_nodes()
                logger.info(f"   - å‘ç°å¾…è§¦å‘èŠ‚ç‚¹: {len(ready_nodes)}")
                
                triggered_count = 0
                if ready_nodes:
                    for node_instance_id in ready_nodes:
                        try:
                            # è§¦å‘èŠ‚ç‚¹æ‰§è¡Œ
                            logger.info(f"   - è§¦å‘èŠ‚ç‚¹æ‰§è¡Œ: {node_instance_id}")
                            await execution_engine._on_nodes_ready_to_execute(instance_id, [node_instance_id])
                            triggered_count += 1
                        except Exception as trigger_error:
                            logger.error(f"   - è§¦å‘èŠ‚ç‚¹å¤±è´¥ {node_instance_id}: {trigger_error}")
                
                return {
                    "success": True,
                    "data": {
                        "workflow_instance_id": str(instance_id),
                        "context_recovered": True,
                        "context_existed_before": context_existed,
                        "forced_recovery": force_recover,
                        "node_dependencies_count": len(recovered_context.node_dependencies),
                        "completed_nodes_count": len(recovered_context.execution_context.get('completed_nodes', set())),
                        "ready_nodes_found": len(ready_nodes),
                        "nodes_triggered": triggered_count,
                        "triggered_node_ids": [str(nid) for nid in ready_nodes] if ready_nodes else []
                    },
                    "message": f"ä¸Šä¸‹æ–‡æ¢å¤æˆåŠŸï¼Œè§¦å‘äº† {triggered_count} ä¸ªå¾…æ‰§è¡ŒèŠ‚ç‚¹"
                }
            else:
                return {
                    "success": False,
                    "data": {
                        "workflow_instance_id": str(instance_id),
                        "context_recovered": False,
                        "error": "æ— æ³•ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡"
                    },
                    "message": "ä¸Šä¸‹æ–‡æ¢å¤å¤±è´¥"
                }
        else:
            # ä¸Šä¸‹æ–‡å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶æ¢å¤ï¼Œåªæ£€æŸ¥å¾…è§¦å‘èŠ‚ç‚¹
            ready_nodes = await current_context.get_ready_nodes()
            logger.info(f"   - ä¸Šä¸‹æ–‡å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¾…è§¦å‘èŠ‚ç‚¹: {len(ready_nodes)}")
            
            triggered_count = 0
            if ready_nodes:
                for node_instance_id in ready_nodes:
                    try:
                        await execution_engine._on_nodes_ready_to_execute(instance_id, [node_instance_id])
                        triggered_count += 1
                    except Exception as trigger_error:
                        logger.error(f"   - è§¦å‘èŠ‚ç‚¹å¤±è´¥ {node_instance_id}: {trigger_error}")
            
            return {
                "success": True,
                "data": {
                    "workflow_instance_id": str(instance_id),
                    "context_recovered": False,
                    "context_existed_before": True,
                    "forced_recovery": False,
                    "ready_nodes_found": len(ready_nodes),
                    "nodes_triggered": triggered_count,
                    "triggered_node_ids": [str(nid) for nid in ready_nodes] if ready_nodes else []
                },
                "message": f"ä¸Šä¸‹æ–‡å·²å­˜åœ¨ï¼Œè§¦å‘äº† {triggered_count} ä¸ªå¾…æ‰§è¡ŒèŠ‚ç‚¹"
            }
        
    except Exception as e:
        logger.error(f"æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"
        )


@router.post("/workflows/smart-refresh")
async def smart_workflow_refresh(
    workflow_instance_ids: Optional[List[uuid.UUID]] = None,
    force_recovery: bool = Query(False, description="å¼ºåˆ¶æ¢å¤ä¸Šä¸‹æ–‡"),
    include_stale_detection: bool = Query(True, description="åŒ…å«åœæ»æ£€æµ‹"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    æ™ºèƒ½å·¥ä½œæµåˆ·æ–° - å‰ç«¯åˆ·æ–°æ—¶è‡ªåŠ¨è°ƒç”¨
    
    ç»“åˆä¸Šä¸‹æ–‡æ¢å¤å’Œåœæ»æ£€æµ‹çš„æ™ºèƒ½åˆ·æ–°æœºåˆ¶ï¼š
    1. å¦‚æœæä¾›äº†å…·ä½“çš„workflow_instance_idsï¼Œåªå¤„ç†æŒ‡å®šçš„å·¥ä½œæµ
    2. å¦‚æœæ²¡æœ‰æä¾›ï¼Œè‡ªåŠ¨æ‰«æç”¨æˆ·çš„æ´»åŠ¨å·¥ä½œæµ
    3. å¯¹æ¯ä¸ªå·¥ä½œæµè¿›è¡Œä¸Šä¸‹æ–‡å¥åº·æ£€æŸ¥å’Œæ¢å¤
    4. å¯é€‰åœ°æ£€æµ‹å’Œä¿®å¤åœæ»çŠ¶æ€
    """
    try:
        logger.info(f"ğŸ”„ ç”¨æˆ· {current_user.username} è¯·æ±‚æ™ºèƒ½å·¥ä½œæµåˆ·æ–°")
        
        from ..services.workflow_monitor_service import get_workflow_monitor
        from ..services.workflow_execution_context import get_context_manager
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        
        context_manager = get_context_manager()
        workflow_monitor = get_workflow_monitor()
        workflow_repo = WorkflowInstanceRepository()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå·¥ä½œæµIDï¼Œåˆ™è·å–ç”¨æˆ·çš„æ´»åŠ¨å·¥ä½œæµ
        if not workflow_instance_ids:
            user_workflows_query = """
            SELECT workflow_instance_id
            FROM workflow_instance 
            WHERE executor_id = %s 
            AND status IN ('running', 'pending')
            AND is_deleted = 0
            ORDER BY updated_at DESC
            LIMIT 20
            """
            user_workflows = await workflow_repo.db.fetch_all(user_workflows_query, current_user.user_id)
            workflow_instance_ids = [uuid.UUID(wf['workflow_instance_id']) for wf in user_workflows]
        
        if not workflow_instance_ids:
            return {
                "success": True,
                "data": {
                    "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ·æ–°çš„å·¥ä½œæµ",
                    "processed_workflows": [],
                    "total_processed": 0,
                    "recovery_results": {
                        "context_recoveries": 0,
                        "stale_recoveries": 0,
                        "triggered_nodes": 0
                    }
                },
                "message": "æ™ºèƒ½åˆ·æ–°å®Œæˆ - æ— å·¥ä½œæµéœ€è¦å¤„ç†"
            }
        
        logger.info(f"   - å‡†å¤‡å¤„ç† {len(workflow_instance_ids)} ä¸ªå·¥ä½œæµ")
        
        # å¤„ç†ç»“æœç»Ÿè®¡
        results = []
        recovery_stats = {
            "context_recoveries": 0,
            "stale_recoveries": 0,
            "triggered_nodes": 0,
            "failed_recoveries": 0
        }
        
        # å¤„ç†æ¯ä¸ªå·¥ä½œæµ
        for instance_id in workflow_instance_ids:
            workflow_result = {
                "workflow_instance_id": str(instance_id),
                "context_recovery": False,
                "stale_recovery": False,
                "nodes_triggered": 0,
                "status": "unknown",
                "issues_detected": [],
                "actions_taken": []
            }
            
            try:
                # 1. è·å–å·¥ä½œæµåŸºæœ¬ä¿¡æ¯
                workflow_info = await workflow_repo.get_instance_by_id(instance_id)
                if not workflow_info:
                    workflow_result["status"] = "not_found"
                    workflow_result["issues_detected"].append("å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨")
                    results.append(workflow_result)
                    continue
                
                workflow_result["workflow_name"] = workflow_info.get("workflow_instance_name", "æœªçŸ¥")
                workflow_result["workflow_status"] = workflow_info.get("status")
                
                # 2. æ£€æŸ¥ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€
                context_health = await context_manager.check_context_health(instance_id)
                if not context_health.get("healthy", True):
                    workflow_result["issues_detected"].append("ä¸Šä¸‹æ–‡ä¸å¥åº·")
                    
                    # å°è¯•æ¢å¤ä¸Šä¸‹æ–‡
                    logger.info(f"   - æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡: {instance_id}")
                    if force_recovery:
                        await context_manager.remove_context(instance_id)
                    
                    recovered_context = await context_manager.get_context(instance_id)
                    if recovered_context:
                        workflow_result["context_recovery"] = True
                        workflow_result["actions_taken"].append("ä¸Šä¸‹æ–‡å·²æ¢å¤")
                        recovery_stats["context_recoveries"] += 1
                        
                        # æ£€æŸ¥å¹¶è§¦å‘å¾…æ‰§è¡ŒèŠ‚ç‚¹
                        ready_nodes = await recovered_context.get_ready_nodes()
                        if ready_nodes:
                            triggered_count = 0
                            for node_instance_id in ready_nodes:
                                try:
                                    await execution_engine._on_nodes_ready_to_execute(instance_id, [node_instance_id])
                                    triggered_count += 1
                                except Exception:
                                    pass
                            
                            if triggered_count > 0:
                                workflow_result["nodes_triggered"] = triggered_count
                                workflow_result["actions_taken"].append(f"è§¦å‘äº† {triggered_count} ä¸ªå¾…æ‰§è¡ŒèŠ‚ç‚¹")
                                recovery_stats["triggered_nodes"] += triggered_count
                    else:
                        workflow_result["issues_detected"].append("ä¸Šä¸‹æ–‡æ¢å¤å¤±è´¥")
                        recovery_stats["failed_recoveries"] += 1
                
                # 3. å¯é€‰çš„åœæ»æ£€æµ‹å’Œæ¢å¤
                if include_stale_detection and workflow_info.get("status") in ["running", "pending"]:
                    # æ£€æŸ¥æ˜¯å¦åœæ»
                    workflow_data = dict(workflow_info)
                    workflow_data["workflow_instance_id"] = str(instance_id)
                    
                    if await workflow_monitor._is_workflow_truly_stale(workflow_data):
                        workflow_result["issues_detected"].append("å·¥ä½œæµåœæ»")
                        
                        try:
                            await workflow_monitor._attempt_workflow_recovery(workflow_data)
                            workflow_result["stale_recovery"] = True
                            workflow_result["actions_taken"].append("åœæ»çŠ¶æ€å·²ä¿®å¤")
                            recovery_stats["stale_recoveries"] += 1
                        except Exception as e:
                            workflow_result["issues_detected"].append(f"åœæ»æ¢å¤å¤±è´¥: {str(e)}")
                            recovery_stats["failed_recoveries"] += 1
                
                # 4. æœ€ç»ˆçŠ¶æ€
                if not workflow_result["issues_detected"]:
                    workflow_result["status"] = "healthy"
                elif workflow_result["context_recovery"] or workflow_result["stale_recovery"]:
                    workflow_result["status"] = "recovered" 
                else:
                    workflow_result["status"] = "needs_attention"
                
            except Exception as e:
                logger.error(f"   - å¤„ç†å·¥ä½œæµ {instance_id} å¤±è´¥: {e}")
                workflow_result["status"] = "error"
                workflow_result["issues_detected"].append(f"å¤„ç†å¼‚å¸¸: {str(e)}")
                recovery_stats["failed_recoveries"] += 1
            
            results.append(workflow_result)
        
        # ç»Ÿè®¡æˆåŠŸå¤„ç†çš„å·¥ä½œæµ
        successful_results = [r for r in results if r["status"] in ["healthy", "recovered"]]
        
        return {
            "success": True,
            "data": {
                "processed_workflows": results,
                "total_processed": len(workflow_instance_ids),
                "successful_processed": len(successful_results),
                "recovery_results": recovery_stats,
                "summary": {
                    "healthy_workflows": len([r for r in results if r["status"] == "healthy"]),
                    "recovered_workflows": len([r for r in results if r["status"] == "recovered"]),
                    "failed_workflows": len([r for r in results if r["status"] == "error"]),
                    "workflows_needing_attention": len([r for r in results if r["status"] == "needs_attention"])
                }
            },
            "message": f"æ™ºèƒ½åˆ·æ–°å®Œæˆ - å¤„ç†äº† {len(workflow_instance_ids)} ä¸ªå·¥ä½œæµï¼Œæ¢å¤äº† {recovery_stats['context_recoveries'] + recovery_stats['stale_recoveries']} ä¸ª"
        }
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½å·¥ä½œæµåˆ·æ–°å¤±è´¥: {e}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ™ºèƒ½å·¥ä½œæµåˆ·æ–°å¤±è´¥: {str(e)}"
        )


@router.post("/workflows/batch-context-recover")
async def batch_recover_workflow_contexts(
    workflow_instance_ids: List[uuid.UUID],
    force_recover: bool = Query(False, description="å¼ºåˆ¶æ¢å¤ä¸Šä¸‹æ–‡"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """æ‰¹é‡æ¢å¤å¤šä¸ªå·¥ä½œæµçš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå‰ç«¯åˆ—è¡¨åˆ·æ–°æ—¶ï¼‰"""
    try:
        logger.info(f"ğŸ”§ ç”¨æˆ· {current_user.username} è¯·æ±‚æ‰¹é‡æ¢å¤ {len(workflow_instance_ids)} ä¸ªå·¥ä½œæµä¸Šä¸‹æ–‡")
        
        from ..services.workflow_execution_context import get_context_manager
        from ..services.execution_service import execution_engine
        
        context_manager = get_context_manager()
        
        results = []
        total_triggered = 0
        
        for instance_id in workflow_instance_ids:
            try:
                logger.info(f"   - å¤„ç†å·¥ä½œæµ: {instance_id}")
                
                # æ£€æŸ¥å½“å‰ä¸Šä¸‹æ–‡çŠ¶æ€
                current_context = context_manager.contexts.get(instance_id)
                context_existed = current_context is not None
                
                # å¼ºåˆ¶æ¢å¤æˆ–ä¸Šä¸‹æ–‡ä¸å­˜åœ¨æ—¶è¿›è¡Œæ¢å¤
                if force_recover or not context_existed:
                    if context_existed and force_recover:
                        await context_manager.remove_context(instance_id)
                    
                    # æ¢å¤ä¸Šä¸‹æ–‡
                    recovered_context = await context_manager.get_context(instance_id)
                    
                    if recovered_context:
                        # æ£€æŸ¥å¹¶è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹
                        ready_nodes = await recovered_context.get_ready_nodes()
                        triggered_count = 0
                        
                        for node_instance_id in ready_nodes:
                            try:
                                await execution_engine._on_nodes_ready_to_execute(instance_id, [node_instance_id])
                                triggered_count += 1
                            except Exception:
                                pass  # é™é»˜å¤„ç†å•ä¸ªèŠ‚ç‚¹è§¦å‘å¤±è´¥
                        
                        total_triggered += triggered_count
                        
                        results.append({
                            "workflow_instance_id": str(instance_id),
                            "success": True,
                            "context_recovered": True,
                            "nodes_triggered": triggered_count
                        })
                    else:
                        results.append({
                            "workflow_instance_id": str(instance_id),
                            "success": False,
                            "context_recovered": False,
                            "error": "æ¢å¤å¤±è´¥"
                        })
                else:
                    # ä¸Šä¸‹æ–‡å·²å­˜åœ¨ï¼Œåªæ£€æŸ¥å¾…è§¦å‘èŠ‚ç‚¹
                    ready_nodes = await current_context.get_ready_nodes()
                    triggered_count = 0
                    
                    for node_instance_id in ready_nodes:
                        try:
                            await execution_engine._on_nodes_ready_to_execute(instance_id, [node_instance_id])
                            triggered_count += 1
                        except Exception:
                            pass
                    
                    total_triggered += triggered_count
                    
                    results.append({
                        "workflow_instance_id": str(instance_id),
                        "success": True,
                        "context_recovered": False,
                        "nodes_triggered": triggered_count
                    })
                    
            except Exception as e:
                logger.error(f"   - å¤„ç†å·¥ä½œæµ {instance_id} å¤±è´¥: {e}")
                results.append({
                    "workflow_instance_id": str(instance_id),
                    "success": False,
                    "error": str(e)
                })
        
        successful_recoveries = len([r for r in results if r["success"]])
        
        return {
            "success": True,
            "data": {
                "total_workflows": len(workflow_instance_ids),
                "successful_recoveries": successful_recoveries,
                "total_nodes_triggered": total_triggered,
                "results": results
            },
            "message": f"æ‰¹é‡æ¢å¤å®Œæˆ: {successful_recoveries}/{len(workflow_instance_ids)} æˆåŠŸï¼Œè§¦å‘äº† {total_triggered} ä¸ªèŠ‚ç‚¹"
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‰¹é‡æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"
        )


@router.get("/online-resources")
async def get_online_resources(
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–åœ¨çº¿èµ„æºï¼ˆç”¨æˆ·å’ŒAgentï¼‰"""
    try:
        from ..repositories.user.user_repository import UserRepository
        from ..repositories.processor.processor_repository import ProcessorRepository
        
        user_repo = UserRepository()
        processor_repo = ProcessorRepository()
        
        # è·å–çœŸæ­£åœ¨çº¿çš„ç”¨æˆ· (30åˆ†é’Ÿå†…æœ‰æ´»åŠ¨çš„ç”¨æˆ·)
        online_users_data = await user_repo.get_online_users(activity_timeout_minutes=30)
        
        # è·å–æ‰€æœ‰æ´»è·ƒç”¨æˆ·ç”¨äºå¯¹æ¯”
        all_active_users = await user_repo.list_all({"status": True, "is_deleted": False})
        
        # è·å–æ‰€æœ‰Agentå¤„ç†å™¨
        agents = await processor_repo.list_all({"type": "agent", "is_deleted": False})
        
        # æ ¼å¼åŒ–ç”¨æˆ·æ•°æ® - åŒ…å«æ‰€æœ‰ç”¨æˆ·ï¼ŒåŒºåˆ†åœ¨çº¿/ç¦»çº¿çŠ¶æ€
        all_users = []
        online_user_ids = {str(user["user_id"]) for user in online_users_data}
        
        for user in all_active_users:
            # å®‰å…¨å¤„ç†profileå­—æ®µ
            profile = user.get("profile", {})
            if isinstance(profile, str):
                try:
                    import json
                    profile = json.loads(profile)
                except:
                    profile = {}
            
            user_id = str(user["user_id"])
            is_user_online = user_id in online_user_ids
            
            all_users.append({
                "user_id": user_id,
                "username": user["username"],
                "email": user["email"],
                "full_name": profile.get("full_name", "") if isinstance(profile, dict) else "",
                "description": user.get("description", ""),
                "status": "online" if is_user_online else "offline",
                "is_online": is_user_online,
                "capabilities": profile.get("capabilities", []) if isinstance(profile, dict) else [],
                "role": user.get("role", "user"),
                "created_at": user["created_at"].isoformat() if user.get("created_at") else None,
                "last_login": user.get("last_login_at").isoformat() if user.get("last_login_at") else None,
                "last_activity": user.get("last_activity_at").isoformat() if user.get("last_activity_at") else None
            })
        
        # æ ¼å¼åŒ–Agentæ•°æ®
        online_agents = []
        for agent in agents:
            online_agents.append({
                "agent_id": str(agent["processor_id"]),
                "name": agent["name"],
                "description": agent.get("description", ""),
                "status": "online",
                "capabilities": agent.get("capabilities", []),
                "tools": agent.get("tools", []),
                "config": agent.get("config", {}),
                "created_at": agent["created_at"].isoformat() if agent.get("created_at") else None,
                "last_used": agent["updated_at"].isoformat() if agent.get("updated_at") else None
            })
        
        return {
            "success": True,
            "data": {
                "users": all_users,
                "agents": online_agents,
                "statistics": {
                    "total_users": len(all_users),
                    "total_agents": len(online_agents),
                    "online_users": len([u for u in all_users if u["is_online"]]),
                    "offline_users": len([u for u in all_users if not u["is_online"]]),
                    "online_agents": len(online_agents)
                }
            },
            "message": "è·å–åœ¨çº¿èµ„æºæˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–åœ¨çº¿èµ„æºå¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{instance_id}/nodes-detail")
async def get_workflow_nodes_detail(
    instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµå®ä¾‹çš„è¯¦ç»†èŠ‚ç‚¹è¾“å‡ºä¿¡æ¯"""
    try:
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        from ..repositories.instance.node_instance_repository import NodeInstanceRepository
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        
        workflow_repo = WorkflowInstanceRepository()
        node_repo = NodeInstanceRepository()
        task_repo = TaskInstanceRepository()
        
        # 1. éªŒè¯å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨
        workflow_instance = await workflow_repo.get_instance_by_id(instance_id)
        if not workflow_instance:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        # 2. è·å–è¯¦ç»†çš„èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯ï¼ˆåŒ…æ‹¬å¤„ç†å™¨ä¿¡æ¯å’Œä½ç½®ä¿¡æ¯ï¼‰
        nodes_query = """
        SELECT 
            ni.node_instance_id,
            ni.node_id,
            n.node_base_id,
            ni.workflow_instance_id,
            ni.status as node_status,
            ni.input_data as node_input,
            ni.output_data as node_output,
            ni.error_message as node_error,
            ni.retry_count,
            ni.created_at as node_created_at,
            ni.started_at as node_started_at,
            ni.completed_at as node_completed_at,
            -- èŠ‚ç‚¹å®šä¹‰ä¿¡æ¯
            n.name as node_name,
            n.type as node_type,
            n.position_x,
            n.position_y,
            -- å¤„ç†å™¨ä¿¡æ¯ï¼ˆé€šè¿‡node_processorå…³è”è¡¨ï¼‰
            p.name as processor_name,
            p.type as processor_type,
            -- æ‰§è¡Œæ—¶é•¿è®¡ç®— (MySQLå…¼å®¹)
            CASE 
                WHEN ni.started_at IS NOT NULL AND ni.completed_at IS NOT NULL 
                THEN CAST(TIMESTAMPDIFF(SECOND, ni.started_at, ni.completed_at) AS SIGNED)
                WHEN ni.started_at IS NOT NULL 
                THEN CAST(TIMESTAMPDIFF(SECOND, ni.started_at, NOW()) AS SIGNED)
                ELSE NULL
            END as execution_duration_seconds
        FROM node_instance ni
        LEFT JOIN node n ON ni.node_id = n.node_id
        LEFT JOIN node_processor np ON n.node_id = np.node_id
        LEFT JOIN processor p ON np.processor_id = p.processor_id
        WHERE ni.workflow_instance_id = $1
        AND ni.is_deleted = 0
        ORDER BY ni.created_at ASC
        """
        
        nodes = await node_repo.db.fetch_all(nodes_query, instance_id)
        
        # 3. è·å–æ¯ä¸ªèŠ‚ç‚¹çš„ä»»åŠ¡å®ä¾‹ä¿¡æ¯
        tasks_query = """
        SELECT 
            ti.task_instance_id,
            ti.node_instance_id,
            ti.task_title,
            ti.task_description,
            ti.status as task_status,
            ti.input_data as task_input,
            ti.output_data as task_output,
            ti.result_summary as task_result,
            ti.error_message as task_error,
            ti.task_type,
            ti.priority,
            ti.estimated_duration,
            ti.actual_duration,
            ti.created_at as task_created_at,
            ti.started_at as task_started_at,
            ti.completed_at as task_completed_at,
            -- å¤„ç†å™¨ä¿¡æ¯
            p.name as processor_name,
            p.type as processor_type,
            -- åˆ†é…ä¿¡æ¯
            u.username as assigned_user_name,
            a.agent_name as assigned_agent_name
        FROM task_instance ti
        LEFT JOIN processor p ON ti.processor_id = p.processor_id
        LEFT JOIN user u ON ti.assigned_user_id = u.user_id
        LEFT JOIN agent a ON ti.assigned_agent_id = a.agent_id
        WHERE ti.workflow_instance_id = %s
        AND ti.is_deleted = 0
        ORDER BY ti.created_at ASC
        """
        
        tasks = await task_repo.db.fetch_all(tasks_query, instance_id)
        
        # 4. ç»„ç»‡èŠ‚ç‚¹å’Œä»»åŠ¡æ•°æ®
        formatted_nodes = []
        tasks_by_node = {}
        
        # æŒ‰èŠ‚ç‚¹å®ä¾‹IDåˆ†ç»„ä»»åŠ¡
        for task in tasks:
            node_id = str(task['node_instance_id']) if task['node_instance_id'] else None
            if node_id:
                if node_id not in tasks_by_node:
                    tasks_by_node[node_id] = []
                
                task_data = {
                    "task_instance_id": str(task['task_instance_id']),
                    "task_title": task['task_title'],
                    "task_description": task['task_description'],
                    "status": task['task_status'],
                    "task_type": task['task_type'],
                    "priority": task['priority'],
                    "input_data": task['task_input'] or {},
                    "output_data": task['task_output'] or {},
                    "result_summary": task['task_result'],
                    "error_message": task['task_error'],
                    "estimated_duration": task['estimated_duration'],
                    "actual_duration": task['actual_duration'],
                    "processor": {
                        "name": task['processor_name'],
                        "type": task['processor_type']
                    },
                    "assignment": {
                        "assigned_user": task['assigned_user_name'],
                        "assigned_agent": task['assigned_agent_name']
                    },
                    "timestamps": {
                        "created_at": task['task_created_at'].isoformat() if task.get('task_created_at') else None,
                        "started_at": task['task_started_at'].isoformat() if task.get('task_started_at') else None,
                        "completed_at": task['task_completed_at'].isoformat() if task.get('task_completed_at') else None
                    }
                }
                tasks_by_node[node_id].append(task_data)
        
        # å¤„ç†èŠ‚ç‚¹æ•°æ®
        for node in nodes:
            node_id = str(node['node_instance_id'])
            node_tasks = tasks_by_node.get(node_id, [])
            
            # è®¡ç®—èŠ‚ç‚¹çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
            total_tasks = len(node_tasks)
            completed_tasks = len([t for t in node_tasks if t['status'] == 'completed'])
            failed_tasks = len([t for t in node_tasks if t['status'] == 'failed'])
            running_tasks = len([t for t in node_tasks if t['status'] in ['in_progress', 'assigned']])
            
            # æ±‡æ€»èŠ‚ç‚¹è¾“å‡ºæ•°æ®ï¼šä»æ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„è¾“å‡ºä¸­åˆå¹¶
            node_output_data = {}
            node_input_data = node['node_input'] or {}
            
            # æ”¶é›†æ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„è¾“å‡ºæ•°æ®
            for task in node_tasks:
                if task['status'] == 'completed' and task['output_data']:
                    # åˆå¹¶ä»»åŠ¡è¾“å‡ºåˆ°èŠ‚ç‚¹è¾“å‡º
                    if isinstance(task['output_data'], dict):
                        node_output_data.update(task['output_data'])
                    else:
                        # å¦‚æœä»»åŠ¡è¾“å‡ºä¸æ˜¯å­—å…¸ï¼Œä»¥ä»»åŠ¡IDä¸ºé”®å­˜å‚¨
                        task_key = f"task_{task['task_instance_id']}"
                        node_output_data[task_key] = task['output_data']
            
            # å¦‚æœæ²¡æœ‰ä»»åŠ¡è¾“å‡ºä½†èŠ‚ç‚¹æœ‰è¾“å‡ºï¼Œä½¿ç”¨èŠ‚ç‚¹çº§åˆ«çš„è¾“å‡º
            if not node_output_data and (node['node_output'] or {}):
                node_output_data = node['node_output'] or {}
            
            node_data = {
                "node_instance_id": str(node['node_instance_id']),
                "node_id": str(node['node_id']),
                "node_base_id": str(node['node_base_id']),
                "node_name": node['node_name'],
                "node_type": node['node_type'],
                "status": node['node_status'],
                "retry_count": node['retry_count'] or 0,
                # ğŸ”§ æ–°å¢ï¼šä½ç½®ä¿¡æ¯ç”¨äºå‰ç«¯å¸ƒå±€
                "position": {
                    "x": float(node['position_x']) if node['position_x'] is not None else None,
                    "y": float(node['position_y']) if node['position_y'] is not None else None
                },
                "input_data": node_input_data,
                "output_data": node_output_data,
                "error_message": node['node_error'],
                "config": node.get('node_config', {}),  # ä½¿ç”¨getæ–¹æ³•é˜²æ­¢KeyError
                "execution_duration_seconds": node['execution_duration_seconds'],
                "processor_name": node['processor_name'],
                "processor_type": node['processor_type'],
                "task_count": total_tasks,
                "timestamps": {
                    "created_at": node['node_created_at'].isoformat() if node.get('node_created_at') else None,
                    "started_at": node['node_started_at'].isoformat() if node.get('node_started_at') else None,
                    "completed_at": node['node_completed_at'].isoformat() if node.get('node_completed_at') else None
                },
                "task_statistics": {
                    "total_tasks": total_tasks,
                    "completed_tasks": completed_tasks,
                    "failed_tasks": failed_tasks,
                    "running_tasks": running_tasks,
                    "success_rate": (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
                },
                "tasks": node_tasks
            }
            formatted_nodes.append(node_data)
        
        # 5. è·å–èŠ‚ç‚¹è¿æ¥å…³ç³»ï¼ˆä»å·¥ä½œæµå®šä¹‰ä¸­ï¼‰
        edges_query = """
        SELECT 
            nc.from_node_id as source_node_id,
            nc.to_node_id as target_node_id,
            nc.connection_type,
            nc.condition_config,
            -- è·å–æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹çš„å®ä¾‹ID
            ni_source.node_instance_id as source_instance_id,
            ni_target.node_instance_id as target_instance_id,
            -- è·å–èŠ‚ç‚¹åç§°ç”¨äºè°ƒè¯•
            n_source.name as source_node_name,
            n_target.name as target_node_name
        FROM node_connection nc
        LEFT JOIN node n_source ON nc.from_node_id = n_source.node_id
        LEFT JOIN node n_target ON nc.to_node_id = n_target.node_id
        LEFT JOIN node_instance ni_source ON n_source.node_id = ni_source.node_id 
            AND ni_source.workflow_instance_id = $1
        LEFT JOIN node_instance ni_target ON n_target.node_id = ni_target.node_id 
            AND ni_target.workflow_instance_id = $1
        WHERE nc.workflow_id = (
            SELECT workflow_id FROM workflow_instance WHERE workflow_instance_id = $1
        )
        """
        
        edges_data = await node_repo.db.fetch_all(edges_query, instance_id, instance_id, instance_id)
        
        # æ ¼å¼åŒ–è¿æ¥è¾¹æ•°æ®
        formatted_edges = []
        for edge in edges_data:
            if edge['source_instance_id'] and edge['target_instance_id']:
                # å¤„ç†æ¡ä»¶é…ç½®
                condition_config = edge['condition_config'] or {}
                condition_label = None
                if edge['connection_type'] == 'conditional' and condition_config:
                    condition_label = condition_config.get('condition', '')
                
                edge_data = {
                    "id": f"edge_{edge['source_instance_id']}_{edge['target_instance_id']}",
                    "source": str(edge['source_instance_id']),
                    "target": str(edge['target_instance_id']),
                    "connection_type": edge['connection_type'],
                    "condition_config": condition_config,
                    "condition_label": condition_label,
                    "source_node_name": edge['source_node_name'],
                    "target_node_name": edge['target_node_name']
                }
                formatted_edges.append(edge_data)
        
        # 6. è®¡ç®—å·¥ä½œæµçº§åˆ«ç»Ÿè®¡
        total_nodes = len(formatted_nodes)
        completed_nodes = len([n for n in formatted_nodes if n['status'] == 'completed'])
        failed_nodes = len([n for n in formatted_nodes if n['status'] == 'failed'])
        running_nodes = len([n for n in formatted_nodes if n['status'] == 'running'])
        
        all_tasks = sum([len(n['tasks']) for n in formatted_nodes])
        all_completed_tasks = sum([n['task_statistics']['completed_tasks'] for n in formatted_nodes])
        all_failed_tasks = sum([n['task_statistics']['failed_tasks'] for n in formatted_nodes])
        
        workflow_statistics = {
            "workflow_instance_id": str(instance_id),
            "workflow_name": workflow_instance.get('workflow_name'),
            "workflow_instance_name": workflow_instance.get('workflow_instance_name'),
            "status": workflow_instance.get('status'),
            "node_statistics": {
                "total_nodes": total_nodes,
                "completed_nodes": completed_nodes,
                "failed_nodes": failed_nodes,
                "running_nodes": running_nodes,
                "success_rate": (completed_nodes / total_nodes * 100) if total_nodes > 0 else 0
            },
            "task_statistics": {
                "total_tasks": all_tasks,
                "completed_tasks": all_completed_tasks,
                "failed_tasks": all_failed_tasks,
                "running_tasks": all_tasks - all_completed_tasks - all_failed_tasks,
                "success_rate": (all_completed_tasks / all_tasks * 100) if all_tasks > 0 else 0
            },
            "timestamps": {
                "started_at": workflow_instance.get('started_at'),
                "completed_at": workflow_instance.get('completed_at'),
                "created_at": workflow_instance.get('created_at')
            }
        }
        
        return {
            "success": True,
            "data": {
                "workflow_statistics": workflow_statistics,
                "nodes": formatted_nodes,
                "edges": formatted_edges
            },
            "message": "è·å–å·¥ä½œæµèŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œæµèŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµèŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


# ==================== çº§è”åˆ é™¤ç«¯ç‚¹ ====================

@router.delete("/instances/{workflow_instance_id}/cascade")
async def delete_workflow_instance_cascade(
    workflow_instance_id: uuid.UUID,
    soft_delete: bool = Query(True, description="æ˜¯å¦è½¯åˆ é™¤"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®
    
    Args:
        workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
        soft_delete: æ˜¯å¦è½¯åˆ é™¤ï¼ˆé»˜è®¤Trueï¼‰
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        çº§è”åˆ é™¤ç»“æœç»Ÿè®¡
    """
    try:
        from ..services.cascade_deletion_service import cascade_deletion_service
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        
        workflow_instance_repo = WorkflowInstanceRepository()
        
        # æ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨å’Œæƒé™
        existing_instance = await workflow_instance_repo.get_instance_by_id(workflow_instance_id)
        if not existing_instance:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        # æ£€æŸ¥æƒé™ï¼šåªæœ‰å·¥ä½œæµæ‰§è¡Œè€…å¯ä»¥åˆ é™¤å®ä¾‹
        current_user_id_str = str(current_user.user_id)
        executor_id_str = str(existing_instance.get('executor_id'))

        if executor_id_str != current_user_id_str:
            logger.warning(f"ğŸš« çº§è”åˆ é™¤æƒé™æ£€æŸ¥å¤±è´¥:")
            logger.warning(f"   - ç”¨æˆ· {current_user_id_str} æ— æƒåˆ é™¤å®ä¾‹ {workflow_instance_id}")
            logger.warning(f"   - åªæœ‰æ‰§è¡Œè€… {executor_id_str} å¯ä»¥åˆ é™¤æ­¤å®ä¾‹")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒåˆ é™¤æ­¤å·¥ä½œæµå®ä¾‹"
            )
        
        # æ‰§è¡Œçº§è”åˆ é™¤
        deletion_result = await cascade_deletion_service.delete_workflow_instance_cascade(
            workflow_instance_id, soft_delete
        )
        
        if deletion_result['deleted_workflow']:
            logger.info(f"ç”¨æˆ· {current_user.username} çº§è”åˆ é™¤äº†å·¥ä½œæµå®ä¾‹: {workflow_instance_id}")
            return {
                "success": True,
                "message": "å·¥ä½œæµå®ä¾‹çº§è”åˆ é™¤æˆåŠŸ",
                "data": {
                    "message": "å·¥ä½œæµå®ä¾‹åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®å·²åˆ é™¤",
                    "deletion_stats": deletion_result
                }
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥"
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¼‚å¸¸: {e}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )


@router.get("/instances/{workflow_instance_id}/deletion-preview")
async def get_workflow_instance_deletion_preview(
    workflow_instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    é¢„è§ˆå·¥ä½œæµå®ä¾‹åˆ é™¤å°†å½±å“çš„æ•°æ®é‡
    
    Args:
        workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        åˆ é™¤é¢„è§ˆæ•°æ®
    """
    try:
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        
        workflow_instance_repo = WorkflowInstanceRepository()
        
        # æ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨å’Œæƒé™
        existing_instance = await workflow_instance_repo.get_instance_by_id(workflow_instance_id)
        if not existing_instance:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        # æ£€æŸ¥æƒé™
        if existing_instance.get('executor_id') != current_user.user_id:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="æ— æƒæŸ¥çœ‹æ­¤å·¥ä½œæµå®ä¾‹"
            )
        
        # è·å–åˆ é™¤é¢„è§ˆ
        nodes_query = """
            SELECT COUNT(*) as node_count
            FROM node_instance 
            WHERE workflow_instance_id = $1 AND is_deleted = FALSE
        """
        node_result = await workflow_instance_repo.db.fetch_one(nodes_query, workflow_instance_id)
        
        tasks_query = """
            SELECT COUNT(*) as task_count,
                   COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_tasks,
                   COUNT(CASE WHEN status = 'in_progress' THEN 1 END) as in_progress_tasks,
                   COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_tasks
            FROM task_instance 
            WHERE workflow_instance_id = $1 AND is_deleted = FALSE
        """
        task_result = await workflow_instance_repo.db.fetch_one(tasks_query, workflow_instance_id)
        
        preview = {
            'workflow_instance_id': str(workflow_instance_id),
            'workflow_instance_name': existing_instance.get('workflow_instance_name', 'æœªå‘½å'),
            'status': existing_instance.get('status'),
            'total_node_instances': int(node_result.get('node_count', 0)),
            'total_task_instances': int(task_result.get('task_count', 0)),
            'task_status_summary': {
                'completed': int(task_result.get('completed_tasks', 0)),
                'in_progress': int(task_result.get('in_progress_tasks', 0)),
                'pending': int(task_result.get('pending_tasks', 0))
            }
        }
        
        return {
            "success": True,
            "message": "åˆ é™¤é¢„è§ˆè·å–æˆåŠŸ",
            "data": preview
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–åˆ é™¤é¢„è§ˆå¼‚å¸¸: {e}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="è·å–åˆ é™¤é¢„è§ˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )


# ==================== å›¾å½¢è§†å›¾ç»†åˆ†æ”¯æŒç«¯ç‚¹ ====================

@router.get("/workflows/{workflow_instance_id}/subdivision-info")
async def get_workflow_subdivision_info(
    workflow_instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–å·¥ä½œæµå®ä¾‹çš„ç»†åˆ†ä¿¡æ¯ - ç”¨äºå›¾å½¢è§†å›¾æ ‡è®°"""
    try:
        from ..repositories.task_subdivision.task_subdivision_repository import TaskSubdivisionRepository
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        from ..models.task_subdivision import SubWorkflowNodeInfo
        
        subdivision_repo = TaskSubdivisionRepository()
        task_repo = TaskInstanceRepository()
        
        # è·å–è¯¥å·¥ä½œæµå®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡åŠå…¶ç»†åˆ†ä¿¡æ¯
        # ä¿®æ”¹æŸ¥è¯¢ä»¥é€‚é…MySQLè¯­æ³•
        query = """
        SELECT 
            ti.task_instance_id,
            ti.node_instance_id,
            ti.task_title,
            COUNT(ts.subdivision_id) as subdivision_count,
            GROUP_CONCAT(DISTINCT ts.status) as subdivision_statuses,
            GROUP_CONCAT(DISTINCT ts.subdivision_id) as subdivision_ids
        FROM task_instance ti
        LEFT JOIN task_subdivision ts ON ti.task_instance_id = ts.original_task_id 
            AND ts.is_deleted = FALSE
        WHERE ti.workflow_instance_id = %s 
            AND ti.is_deleted = 0
        GROUP BY ti.task_instance_id, ti.node_instance_id, ti.task_title
        """
        
        results = await subdivision_repo.db.fetch_all(query, workflow_instance_id)
        
        # æ„å»ºèŠ‚ç‚¹ç»†åˆ†ä¿¡æ¯æ˜ å°„
        node_subdivisions = {}
        nodes_with_subdivisions = 0
        total_subdivisions = 0
        
        for result in results:
            node_instance_id = str(result['node_instance_id'])
            subdivision_count = result['subdivision_count'] or 0
            
            # åˆå§‹åŒ–primary_statusï¼Œç¡®ä¿æ€»æ˜¯æœ‰å€¼
            primary_status = None
            
            if subdivision_count > 0:
                nodes_with_subdivisions += 1
                total_subdivisions += subdivision_count
                
                # ç¡®å®šå­å·¥ä½œæµçŠ¶æ€ - é€‚é…MySQLçš„GROUP_CONCATç»“æœ
                statuses_str = result['subdivision_statuses']
                statuses = []
                if statuses_str:
                    statuses = [s.strip() for s in statuses_str.split(',')]
                
                if statuses:
                    # ä¼˜å…ˆçº§ï¼šfailed > running > completed > draft
                    if 'failed' in statuses:
                        primary_status = 'failed'
                    elif 'running' in statuses:
                        primary_status = 'running'
                    elif 'completed' in statuses:
                        primary_status = 'completed'
                    else:
                        primary_status = 'draft'
            
            node_subdivisions[node_instance_id] = {
                'node_instance_id': result['node_instance_id'],
                'has_subdivision': subdivision_count > 0,
                'subdivision_count': subdivision_count,
                'subdivision_status': primary_status,
                'is_expandable': subdivision_count > 0,
                'expansion_level': 0
            }
        
        return {
            "success": True,
            "data": {
                'workflow_instance_id': workflow_instance_id,
                'node_subdivisions': node_subdivisions,
                'nodes_with_subdivisions': nodes_with_subdivisions,
                'total_subdivisions': total_subdivisions
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œæµç»†åˆ†ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ç»†åˆ†ä¿¡æ¯å¤±è´¥: {str(e)}"
        )


@router.get("/nodes/{node_instance_id}/subdivision-detail")
async def get_node_subdivision_detail(
    node_instance_id: uuid.UUID,
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """è·å–èŠ‚ç‚¹çš„è¯¦ç»†ç»†åˆ†ä¿¡æ¯ - ç”¨äºå±•å¼€æ˜¾ç¤º"""
    try:
        from ..repositories.task_subdivision.task_subdivision_repository import TaskSubdivisionRepository
        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        from ..repositories.instance.node_instance_repository import NodeInstanceRepository
        from ..models.task_subdivision import SubWorkflowDetail
        
        subdivision_repo = TaskSubdivisionRepository()
        task_repo = TaskInstanceRepository()
        workflow_repo = WorkflowInstanceRepository()
        node_repo = NodeInstanceRepository()
        
        # æŸ¥æ‰¾è¯¥èŠ‚ç‚¹å®ä¾‹å…³è”çš„ä»»åŠ¡å’Œç»†åˆ†
        subdivisions_query = """
        SELECT 
            ts.*,
            ti.task_title,
            w.name as sub_workflow_name,
            wi.workflow_instance_name as sub_workflow_instance_name,
            wi.status as sub_workflow_status,
            wi.created_at as sub_workflow_created_at,
            wi.started_at as sub_workflow_started_at,
            wi.completed_at as sub_workflow_completed_at
        FROM task_subdivision ts
        JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
        LEFT JOIN workflow w ON ts.sub_workflow_base_id = w.workflow_base_id AND w.is_current_version = 1
        LEFT JOIN workflow_instance wi ON ts.sub_workflow_instance_id = wi.workflow_instance_id
        WHERE ti.node_instance_id = %s 
            AND ts.is_deleted = FALSE 
            AND ti.is_deleted = 0
        ORDER BY ts.subdivision_created_at DESC
        """
        
        subdivisions = await subdivision_repo.db.fetch_all(subdivisions_query, node_instance_id)
        
        subdivision_details = []
        
        for subdivision in subdivisions:
            # è·å–å­å·¥ä½œæµçš„èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯
            sub_workflow_instance_id = subdivision.get('sub_workflow_instance_id')
            nodes = []
            edges = []
            stats = {
                'total_nodes': 0,
                'completed_nodes': 0,
                'running_nodes': 0,
                'failed_nodes': 0
            }
            
            if sub_workflow_instance_id:
                # è·å–å­å·¥ä½œæµçš„èŠ‚ç‚¹å®ä¾‹
                nodes_query = """
                SELECT 
                    ni.*,
                    n.name as node_name,
                    n.type as node_type,
                    n.position_x,
                    n.position_y,
                    COUNT(ti.task_instance_id) as task_count
                FROM node_instance ni
                LEFT JOIN node n ON ni.node_id = n.node_id
                LEFT JOIN task_instance ti ON ni.node_instance_id = ti.node_instance_id AND ti.is_deleted = 0
                WHERE ni.workflow_instance_id = %s AND ni.is_deleted = 0
                GROUP BY ni.node_instance_id, n.name, n.type, n.position_x, n.position_y
                ORDER BY ni.created_at
                """
                
                sub_nodes = await node_repo.db.fetch_all(nodes_query, sub_workflow_instance_id)
                
                for node in sub_nodes:
                    stats['total_nodes'] += 1
                    status = node.get('status', 'pending')
                    if status == 'completed':
                        stats['completed_nodes'] += 1
                    elif status == 'running':
                        stats['running_nodes'] += 1
                    elif status == 'failed':
                        stats['failed_nodes'] += 1
                    
                    nodes.append({
                        'node_instance_id': str(node['node_instance_id']),
                        'node_id': str(node['node_id']),
                        'node_name': node.get('node_name', 'æœªå‘½å'),
                        'node_type': node.get('node_type', 'process'),
                        'status': status,
                        'task_count': node.get('task_count', 0),
                        # ğŸ”§ æ–°å¢ï¼šä½ç½®ä¿¡æ¯ç”¨äºå‰ç«¯å¸ƒå±€
                        'position': {
                            'x': float(node['position_x']) if node['position_x'] is not None else None,
                            'y': float(node['position_y']) if node['position_y'] is not None else None
                        },
                        'created_at': node['created_at'].isoformat() if node.get('created_at') else None,
                        'completed_at': node['completed_at'].isoformat() if node.get('completed_at') else None
                    })
                
                # è·å–å­å·¥ä½œæµçš„è¿æ¥å…³ç³»
                edges_query = """
                SELECT 
                    nc.*,
                    fn.name as from_node_name,
                    tn.name as to_node_name
                FROM node_connection nc
                JOIN node_instance fni ON nc.from_node_id = fni.node_id AND fni.workflow_instance_id = %s
                JOIN node_instance tni ON nc.to_node_id = tni.node_id AND tni.workflow_instance_id = %s
                JOIN node fn ON nc.from_node_id = fn.node_id
                JOIN node tn ON nc.to_node_id = tn.node_id
                WHERE nc.workflow_id = (
                    SELECT workflow_base_id FROM workflow_instance WHERE workflow_instance_id = %s
                )
                """
                
                sub_edges = await subdivision_repo.db.fetch_all(edges_query, sub_workflow_instance_id, sub_workflow_instance_id, sub_workflow_instance_id)
                
                for edge in sub_edges:
                    edges.append({
                        'id': str(edge.get('connection_id', f"edge-{edge['from_node_id']}-{edge['to_node_id']}")),
                        'source': str(edge['from_node_id']),
                        'target': str(edge['to_node_id']),
                        'label': edge.get('condition_config'),
                        'from_node_name': edge.get('from_node_name'),
                        'to_node_name': edge.get('to_node_name')
                    })
            
            subdivision_detail = {
                'subdivision_id': subdivision['subdivision_id'],
                'sub_workflow_instance_id': sub_workflow_instance_id,
                'subdivision_name': subdivision['subdivision_name'],
                'status': subdivision.get('sub_workflow_status', subdivision['status']),
                'nodes': nodes,
                'edges': edges,
                'total_nodes': stats['total_nodes'],
                'completed_nodes': stats['completed_nodes'],
                'running_nodes': stats['running_nodes'],
                'failed_nodes': stats['failed_nodes'],
                'created_at': subdivision['subdivision_created_at'].isoformat() if subdivision.get('subdivision_created_at') else None,
                'started_at': subdivision['sub_workflow_started_at'].isoformat() if subdivision.get('sub_workflow_started_at') else None,
                'completed_at': subdivision['sub_workflow_completed_at'].isoformat() if subdivision.get('sub_workflow_completed_at') else None
            }
            
            subdivision_details.append(subdivision_detail)
        
        return {
            "success": True,
            "data": {
                'node_instance_id': node_instance_id,
                'has_subdivision': len(subdivision_details) > 0,
                'subdivisions': subdivision_details
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–èŠ‚ç‚¹ç»†åˆ†è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–èŠ‚ç‚¹ç»†åˆ†è¯¦æƒ…å¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{workflow_instance_id}/complete-mapping")
async def get_workflow_complete_mapping(
    workflow_instance_id: uuid.UUID,
    max_depth: int = Query(10, description="æœ€å¤§é€’å½’æ·±åº¦"),
    current_user: CurrentUser = Depends(get_current_user_context)
) -> dict:
    """
    è·å–å·¥ä½œæµå®ä¾‹çš„å®Œæ•´èŠ‚ç‚¹çº§åˆ«æ˜ å°„å…³ç³»
    æ”¯æŒé€’å½’æŸ¥è¯¢å¤šå±‚åµŒå¥—çš„å­å·¥ä½œæµ
    """
    try:
        logger.info(f"ğŸ“Š è·å–å·¥ä½œæµå®Œæ•´æ˜ å°„: {workflow_instance_id}, æœ€å¤§æ·±åº¦: {max_depth}")
        
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        
        workflow_instance_repo = WorkflowInstanceRepository()
        
        # éªŒè¯å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨
        instance = await workflow_instance_repo.get_instance_by_id(workflow_instance_id)
        if not instance:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            )
        
        # è·å–å®Œæ•´æ˜ å°„å…³ç³»
        mapping_result = await workflow_instance_repo.get_complete_workflow_mapping(
            workflow_instance_id, max_depth
        )
        
        logger.info(f"âœ… å·¥ä½œæµå®Œæ•´æ˜ å°„æŸ¥è¯¢æˆåŠŸ: {len(mapping_result.get('metadata', {}).get('total_workflows', 0))} ä¸ªå·¥ä½œæµ")
        
        return {
            "success": True,
            "data": mapping_result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œæµå®Œæ•´æ˜ å°„å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµå®Œæ•´æ˜ å°„å¤±è´¥: {str(e)}"
        )


@router.get("/workflows/{workflow_instance_id}/node-mapping")
async def get_workflow_node_mapping(
    workflow_instance_id: uuid.UUID,
    include_template_structure: bool = Query(True, description="æ˜¯å¦åŒ…å«æ¨¡æ¿ç»“æ„ä¿¡æ¯"),
    current_user: CurrentUser = Depends(get_current_user_context)
) -> dict:
    """
    è·å–å·¥ä½œæµå®ä¾‹çš„èŠ‚ç‚¹çº§åˆ«æ˜ å°„å…³ç³»ï¼ˆä¸“ä¸ºå‰ç«¯å›¾å½¢å±•ç¤ºä¼˜åŒ–ï¼‰
    è¿”å›å·¥ä½œæµæ¡†æ¶ç»“æ„ï¼Œé€šè¿‡èŠ‚ç‚¹å…³ç³»è¿æ¥å·¥ä½œæµ
    """
    try:
        logger.info(f"ğŸ¨ è·å–å·¥ä½œæµèŠ‚ç‚¹æ˜ å°„: {workflow_instance_id}")
        
        from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
        from ..repositories.node.node_repository import NodeRepository, NodeConnectionRepository
        
        workflow_repo = WorkflowInstanceRepository()
        node_repo = NodeRepository()
        connection_repo = NodeConnectionRepository()
        
        # 1. è·å–å®Œæ•´çš„å·¥ä½œæµæ˜ å°„æ•°æ®
        complete_mapping = await workflow_repo.get_complete_workflow_mapping(workflow_instance_id, max_depth=8)
        
        if "error" in complete_mapping.get("mapping_data", {}):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å¤±è´¥"
            )
        
        # 2. æ„å»ºé€‚ç”¨äºå‰ç«¯çš„æ•°æ®ç»“æ„
        template_workflows = []
        template_connections = []
        
        # é€’å½’å¤„ç†æ˜ å°„æ•°æ®
        await _process_mapping_for_template_graph(
            complete_mapping["mapping_data"], 
            template_workflows, 
            template_connections,
            node_repo,
            connection_repo,
            include_template_structure
        )
        
        result = {
            "success": True,
            "data": {
                "template_connections": template_connections,
                "detailed_workflows": {
                    wf["workflow_base_id"]: wf for wf in template_workflows
                },
                "node_level_mapping": True,
                "supports_recursive_subdivision": True
            }
        }
        
        logger.info(f"âœ… å·¥ä½œæµèŠ‚ç‚¹æ˜ å°„æ„å»ºå®Œæˆ: {len(template_workflows)} ä¸ªå·¥ä½œæµ, {len(template_connections)} ä¸ªè¿æ¥")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œæµèŠ‚ç‚¹æ˜ å°„å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–å·¥ä½œæµèŠ‚ç‚¹æ˜ å°„å¤±è´¥: {str(e)}"
        )


async def _process_mapping_for_template_graph(mapping_data: dict, 
                                            template_workflows: list,
                                            template_connections: list,
                                            node_repo,
                                            connection_repo,
                                            include_template_structure: bool):
    """é€’å½’å¤„ç†æ˜ å°„æ•°æ®ä¸ºæ¨¡æ¿å›¾æ ¼å¼"""
    try:
        if "error" in mapping_data:
            return
        
        # å¤„ç†å½“å‰å·¥ä½œæµ
        workflow_base_id = mapping_data["workflow_base_id"]
        
        # æ„å»ºå·¥ä½œæµæ•°æ®
        workflow_data = {
            "workflow_base_id": workflow_base_id,
            "workflow_name": mapping_data["workflow_name"],
            "workflow_instance_id": mapping_data["workflow_instance_id"],
            "workflow_instance_name": mapping_data["workflow_instance_name"],
            "status": mapping_data["workflow_status"],
            "depth": mapping_data.get("depth", 0),
            "total_nodes": len(mapping_data.get("nodes", [])),
            "nodes": [],
            "connections": []
        }
        
        # å¦‚æœéœ€è¦åŒ…å«æ¨¡æ¿ç»“æ„ï¼Œæ·»åŠ èŠ‚ç‚¹å’Œè¿æ¥ä¿¡æ¯
        if include_template_structure:
            try:
                # è·å–å·¥ä½œæµçš„èŠ‚ç‚¹ä¿¡æ¯
                workflow_nodes = await node_repo.get_workflow_nodes(uuid.UUID(workflow_base_id))
                workflow_connections = await connection_repo.get_workflow_connections(uuid.UUID(workflow_base_id))
                
                # æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯
                for node in workflow_nodes:
                    workflow_data["nodes"].append({
                        "node_id": str(node["node_id"]),
                        "node_base_id": str(node["node_base_id"]),
                        "name": node["name"],
                        "type": node["type"],
                        "position": {
                            "x": node.get("position_x"),
                            "y": node.get("position_y")
                        }
                    })
                
                # æ·»åŠ è¿æ¥ä¿¡æ¯
                for conn in workflow_connections:
                    workflow_data["connections"].append({
                        "connection_id": f"conn_{conn['from_node_id']}_{conn['to_node_id']}",
                        "from_node": {
                            "node_id": str(conn["from_node_id"]),
                            "node_base_id": str(conn["from_node_base_id"]),
                            "name": conn["from_node_name"]
                        },
                        "to_node": {
                            "node_id": str(conn["to_node_id"]),
                            "node_base_id": str(conn["to_node_base_id"]),
                            "name": conn["to_node_name"]
                        },
                        "connection_type": conn["connection_type"]
                    })
                    
            except Exception as e:
                logger.warning(f"è·å–æ¨¡æ¿ç»“æ„ä¿¡æ¯å¤±è´¥: {e}")
        
        template_workflows.append(workflow_data)
        
        # å¤„ç†èŠ‚ç‚¹çš„subdivisions
        for node in mapping_data.get("nodes", []):
            for subdivision in node.get("subdivisions", []):
                if subdivision["sub_workflow_mapping"] and "error" not in subdivision["sub_workflow_mapping"]:
                    # åˆ›å»ºæ¨¡æ¿è¿æ¥å…³ç³»
                    connection = {
                        "subdivision_id": subdivision["subdivision_id"],
                        "subdivision_name": subdivision["subdivision_name"],
                        "parent_subdivision_id": subdivision.get("parent_subdivision_id"),
                        "parent_workflow": {
                            "workflow_base_id": workflow_base_id,
                            "workflow_name": mapping_data["workflow_name"],
                            "workflow_instance_id": mapping_data["workflow_instance_id"],
                            "workflow_instance_name": mapping_data["workflow_instance_name"],
                            "status": mapping_data["workflow_status"]
                        },
                        "sub_workflow": {
                            "workflow_base_id": subdivision["sub_workflow_mapping"]["workflow_base_id"],
                            "workflow_name": subdivision["sub_workflow_mapping"]["workflow_name"],
                            "workflow_instance_id": subdivision["sub_workflow_mapping"]["workflow_instance_id"],
                            "workflow_instance_name": subdivision["sub_workflow_mapping"]["workflow_instance_name"],
                            "status": subdivision["sub_workflow_mapping"]["workflow_status"],
                            "total_nodes": len(subdivision["sub_workflow_mapping"].get("nodes", [])),
                            "completed_nodes": sum(1 for n in subdivision["sub_workflow_mapping"].get("nodes", []) if n.get("node_status") == "completed")
                        },
                        "parent_node": {
                            "node_instance_id": node["node_instance_id"],
                            "node_base_id": node["node_base_id"],
                            "node_name": node["node_name"],
                            "node_type": node["node_type"]
                        }
                    }
                    
                    template_connections.append(connection)
                    
                    # é€’å½’å¤„ç†å­å·¥ä½œæµ
                    await _process_mapping_for_template_graph(
                        subdivision["sub_workflow_mapping"],
                        template_workflows,
                        template_connections, 
                        node_repo,
                        connection_repo,
                        include_template_structure
                    )
        
    except Exception as e:
        logger.error(f"å¤„ç†æ˜ å°„æ•°æ®å¤±è´¥: {e}")
        raise