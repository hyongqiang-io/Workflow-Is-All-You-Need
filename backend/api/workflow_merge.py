"""
å·¥ä½œæµåˆå¹¶APIè·¯ç”±
Workflow Merge API Routes

å¤„ç†å·¥ä½œæµæ¨¡æ¿é—´çš„åˆå¹¶ç›¸å…³APIè¯·æ±‚
"""

import uuid
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, status, Path, Body, Query
from loguru import logger

from ..models.base import BaseResponse
from ..services.workflow_merge_service import WorkflowMergeService
from ..services.workflow_template_connection_service import WorkflowTemplateConnectionService
from ..utils.middleware import get_current_active_user, CurrentUser, get_current_user_context
from ..utils.exceptions import ValidationError, handle_validation_error

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/workflow-merge", tags=["å·¥ä½œæµåˆå¹¶"])

# æœåŠ¡å®ä¾‹
merge_service = WorkflowMergeService()
template_connection_service = WorkflowTemplateConnectionService()


@router.get("/{workflow_instance_id}/detailed-connections", response_model=BaseResponse)
async def get_detailed_workflow_connections(
    workflow_instance_id: uuid.UUID = Path(..., description="å·¥ä½œæµå®ä¾‹ID"),
    max_depth: int = Query(10, ge=1, le=20, description="æœ€å¤§é€’å½’æ·±åº¦"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    è·å–åŒ…å«å†…éƒ¨èŠ‚ç‚¹è¯¦æƒ…çš„å·¥ä½œæµæ¨¡æ¿è¿æ¥å›¾æ•°æ®
    
    Args:
        workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
        max_depth: æœ€å¤§é€’å½’æ·±åº¦
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        è¯¦ç»†è¿æ¥å›¾æ•°æ®ï¼ŒåŒ…æ‹¬å¯åˆå¹¶å€™é€‰
    """
    try:
        logger.info(f"ğŸ” è·å–è¯¦ç»†å·¥ä½œæµè¿æ¥å›¾: å®ä¾‹={workflow_instance_id}, æ·±åº¦={max_depth}")
        
        # è·å–è¯¦ç»†è¿æ¥æ•°æ®
        detailed_connections = await template_connection_service.get_detailed_workflow_connections(
            workflow_instance_id, max_depth
        )
        
        if not detailed_connections.get("template_connections"):
            return BaseResponse(
                success=True,
                message="è¯¥å·¥ä½œæµå®ä¾‹æš‚æ— æ¨¡æ¿è¿æ¥å…³ç³»",
                data={
                    "detailed_connections": detailed_connections,
                    "has_merge_candidates": False
                }
            )
        
        has_merge_candidates = len(detailed_connections.get("merge_candidates", [])) > 0
        
        logger.info(f"âœ… è¯¦ç»†è¿æ¥å›¾è·å–æˆåŠŸ: è¿æ¥æ•°={len(detailed_connections['template_connections'])}, åˆå¹¶å€™é€‰={len(detailed_connections.get('merge_candidates', []))}")
        
        return BaseResponse(
            success=True,
            message="è·å–è¯¦ç»†è¿æ¥å›¾æˆåŠŸ",
            data={
                "detailed_connections": detailed_connections,
                "has_merge_candidates": has_merge_candidates,
                "merge_candidates_count": len(detailed_connections.get("merge_candidates", [])),
                "statistics": {
                    "total_workflows": len(detailed_connections.get("detailed_workflows", {})),
                    "mergeable_connections": len([c for c in detailed_connections.get("merge_candidates", []) if c.get("compatibility", {}).get("is_compatible", False)])
                }
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ è·å–è¯¦ç»†å·¥ä½œæµè¿æ¥å›¾å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="è·å–è¯¦ç»†è¿æ¥å›¾å¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )


@router.post("/{workflow_base_id}/merge-preview", response_model=BaseResponse)
async def preview_workflow_merge(
    workflow_base_id: uuid.UUID = Path(..., description="å·¥ä½œæµåŸºç¡€ID"),
    merge_candidates: List[Dict[str, Any]] = Body(..., description="åˆå¹¶å€™é€‰åˆ—è¡¨"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    é¢„è§ˆå·¥ä½œæµåˆå¹¶ç»“æœ
    
    Args:
        workflow_base_id: å·¥ä½œæµåŸºç¡€ID  
        merge_candidates: åˆå¹¶å€™é€‰åˆ—è¡¨
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        åˆå¹¶é¢„è§ˆæ•°æ®
    """
    try:
        logger.info(f"ğŸ” é¢„è§ˆå·¥ä½œæµåˆå¹¶: å·¥ä½œæµ={workflow_base_id}, å€™é€‰æ•°={len(merge_candidates)}")
        
        if not merge_candidates:
            raise ValidationError("è¯·æä¾›è‡³å°‘ä¸€ä¸ªåˆå¹¶å€™é€‰")
        
        # é¢„è§ˆåˆå¹¶ç»“æœ
        merge_preview = await merge_service.preview_workflow_merge(
            workflow_base_id, merge_candidates, current_user.user_id
        )
        
        can_proceed = merge_preview.get("merge_feasibility", {}).get("can_proceed", False)
        valid_merges_count = merge_preview.get("merge_summary", {}).get("valid_merges", 0)
        
        logger.info(f"âœ… åˆå¹¶é¢„è§ˆå®Œæˆ: å¯è¡Œ={can_proceed}, æœ‰æ•ˆåˆå¹¶æ•°={valid_merges_count}")
        
        return BaseResponse(
            success=True,
            message="åˆå¹¶é¢„è§ˆå®Œæˆ",
            data={
                "merge_preview": merge_preview,
                "can_proceed": can_proceed,
                "recommendations": {
                    "proceed_with_merge": can_proceed,
                    "complexity_warning": merge_preview.get("merge_feasibility", {}).get("complexity_increase") == "high",
                    "suggested_approach": merge_preview.get("merge_feasibility", {}).get("recommended_approach", "unknown")
                }
            }
        )
        
    except ValidationError as e:
        logger.warning(f"åˆå¹¶é¢„è§ˆè¾“å…¥éªŒè¯å¤±è´¥: {e}")
        raise handle_validation_error(e)
    except Exception as e:
        logger.error(f"âŒ é¢„è§ˆå·¥ä½œæµåˆå¹¶å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="é¢„è§ˆåˆå¹¶å¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )


@router.post("/{workflow_base_id}/execute-merge", response_model=BaseResponse)
async def execute_workflow_merge(
    workflow_base_id: uuid.UUID = Path(..., description="å·¥ä½œæµåŸºç¡€ID"),
    merge_request: Dict[str, Any] = Body(..., description="åˆå¹¶è¯·æ±‚æ•°æ®"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    æ‰§è¡Œå·¥ä½œæµåˆå¹¶æ“ä½œ
    
    Args:
        workflow_base_id: å·¥ä½œæµåŸºç¡€ID
        merge_request: åˆå¹¶è¯·æ±‚æ•°æ®ï¼ŒåŒ…å«selected_mergeså’Œmerge_config
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        åˆå¹¶æ‰§è¡Œç»“æœ
    """
    try:
        logger.info(f"ğŸ”„ æ‰§è¡Œå·¥ä½œæµåˆå¹¶: å·¥ä½œæµ={workflow_base_id}")
        
        # é¦–å…ˆéªŒè¯å·¥ä½œæµIDæ˜¯å¦å­˜åœ¨
        from ..repositories.base import BaseRepository
        db = BaseRepository("api").db
        
        workflow_check = await db.fetch_one(
            "SELECT workflow_id, name FROM workflow WHERE workflow_base_id = %s AND is_current_version = 1 AND is_deleted = 0",
            workflow_base_id
        )
        
        if not workflow_check:
            logger.warning(f"APIå±‚éªŒè¯å¤±è´¥: å·¥ä½œæµåŸºç¡€ID {workflow_base_id} ä¸å­˜åœ¨")
            return BaseResponse(
                success=False,
                message="å·¥ä½œæµä¸å­˜åœ¨",
                data={
                    "error_type": "workflow_not_found",
                    "workflow_base_id": str(workflow_base_id),
                    "suggestions": [
                        "è¯·æ£€æŸ¥å·¥ä½œæµIDæ˜¯å¦æ­£ç¡®",
                        "è¯¥å·¥ä½œæµå¯èƒ½å·²è¢«åˆ é™¤æˆ–ä¸æ˜¯å½“å‰ç‰ˆæœ¬",
                        "è¯·ä»å·¥ä½œæµåˆ—è¡¨ä¸­é€‰æ‹©æœ‰æ•ˆçš„å·¥ä½œæµ"
                    ]
                }
            )
        
        logger.info(f"âœ… å·¥ä½œæµéªŒè¯é€šè¿‡: {workflow_check['name']}")
        
        # éªŒè¯è¯·æ±‚æ•°æ®ç»“æ„
        if "selected_merges" not in merge_request:
            raise ValidationError("è¯·æä¾›selected_mergeså­—æ®µ")
        
        if "merge_config" not in merge_request:
            raise ValidationError("è¯·æä¾›merge_configå­—æ®µ")
        
        selected_merges = merge_request["selected_merges"]
        merge_config = merge_request["merge_config"]
        
        if not selected_merges:
            raise ValidationError("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªåˆå¹¶æ“ä½œ")
        
        # éªŒè¯åˆå¹¶é…ç½®
        if not merge_config.get("new_workflow_name"):
            raise ValidationError("è¯·æä¾›æ–°å·¥ä½œæµåç§°")
        
        logger.info(f"ğŸ”„ åˆå¹¶é…ç½®: {len(selected_merges)} ä¸ªæ“ä½œ, æ–°åç§°='{merge_config['new_workflow_name']}'")
        
        # æ‰§è¡Œåˆå¹¶
        merge_result = await merge_service.execute_workflow_merge(
            workflow_base_id, selected_merges, merge_config, current_user.user_id
        )
        
        if merge_result.get("success"):
            logger.info(f"âœ… å·¥ä½œæµåˆå¹¶æˆåŠŸ: æ–°å·¥ä½œæµID={merge_result.get('new_workflow_id')}")
            
            return BaseResponse(
                success=True,
                message=merge_result.get("message", "å·¥ä½œæµåˆå¹¶æˆåŠŸ"),
                data={
                    "merge_result": merge_result,
                    "new_workflow": {
                        "workflow_id": merge_result.get("new_workflow_id"),
                        "name": merge_result.get("new_workflow_name")
                    },
                    "statistics": merge_result.get("merge_statistics", {}),
                    "next_steps": {
                        "can_view_workflow": True,
                        "can_execute_workflow": True,
                        "workflow_url": f"/workflows/{merge_result.get('new_workflow_id')}"
                    }
                }
            )
        else:
            logger.error(f"âŒ å·¥ä½œæµåˆå¹¶å¤±è´¥: {merge_result.get('message')}")
            
            return BaseResponse(
                success=False,
                message=merge_result.get("message", "å·¥ä½œæµåˆå¹¶å¤±è´¥"),
                data={
                    "merge_result": merge_result,
                    "errors": merge_result.get("errors", []),
                    "warnings": merge_result.get("warnings", [])
                }
            )
        
    except ValidationError as e:
        logger.warning(f"åˆå¹¶æ‰§è¡Œè¾“å…¥éªŒè¯å¤±è´¥: {e}")
        raise handle_validation_error(e)
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå·¥ä½œæµåˆå¹¶å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="æ‰§è¡Œåˆå¹¶å¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )


@router.get("/{workflow_base_id}/merge-compatibility", response_model=BaseResponse)
async def check_merge_compatibility(
    workflow_base_id: uuid.UUID = Path(..., description="å·¥ä½œæµåŸºç¡€ID"),
    target_node_id: uuid.UUID = Query(..., description="ç›®æ ‡èŠ‚ç‚¹ID"),
    sub_workflow_id: uuid.UUID = Query(..., description="å­å·¥ä½œæµåŸºç¡€ID"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    æ£€æŸ¥ç‰¹å®šèŠ‚ç‚¹å’Œå­å·¥ä½œæµçš„åˆå¹¶å…¼å®¹æ€§
    
    Args:
        workflow_base_id: å·¥ä½œæµåŸºç¡€ID
        target_node_id: ç›®æ ‡èŠ‚ç‚¹ID
        sub_workflow_id: å­å·¥ä½œæµåŸºç¡€ID
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        å…¼å®¹æ€§æ£€æŸ¥ç»“æœ
    """
    try:
        logger.info(f"ğŸ” æ£€æŸ¥åˆå¹¶å…¼å®¹æ€§: å·¥ä½œæµ={workflow_base_id}, èŠ‚ç‚¹={target_node_id}, å­å·¥ä½œæµ={sub_workflow_id}")
        
        # è¿™é‡Œå¯ä»¥è°ƒç”¨WorkflowMergeServiceçš„å…¼å®¹æ€§æ£€æŸ¥æ–¹æ³•
        # ä¸ºç®€åŒ–èµ·è§ï¼Œå…ˆè¿”å›åŸºæœ¬çš„æ£€æŸ¥ç»“æœ
        
        compatibility_result = {
            "is_compatible": True,
            "compatibility_score": 0.85,
            "issues": [],
            "recommendations": [
                "å»ºè®®åœ¨åˆå¹¶å‰å¤‡ä»½åŸå·¥ä½œæµ",
                "å»ºè®®å…ˆåœ¨æµ‹è¯•ç¯å¢ƒä¸­éªŒè¯åˆå¹¶ç»“æœ"
            ],
            "impact_analysis": {
                "complexity_change": "medium",
                "estimated_nodes_added": 5,
                "estimated_execution_time_increase": "20%"
            }
        }
        
        return BaseResponse(
            success=True,
            message="å…¼å®¹æ€§æ£€æŸ¥å®Œæˆ",
            data={
                "compatibility": compatibility_result,
                "target_info": {
                    "workflow_base_id": str(workflow_base_id),
                    "target_node_id": str(target_node_id),
                    "sub_workflow_id": str(sub_workflow_id)
                }
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥åˆå¹¶å…¼å®¹æ€§å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="æ£€æŸ¥å…¼å®¹æ€§å¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )


@router.get("/templates/{template_id}/adoption-history", response_model=BaseResponse) 
async def get_template_adoption_history(
    template_id: uuid.UUID = Path(..., description="å·¥ä½œæµæ¨¡æ¿ID"),
    limit: int = Query(20, ge=1, le=100, description="ç»“æœæ•°é‡é™åˆ¶"),
    current_user: CurrentUser = Depends(get_current_user_context)
):
    """
    è·å–å·¥ä½œæµæ¨¡æ¿çš„é‡‡çº³å†å²
    
    Args:
        template_id: å·¥ä½œæµæ¨¡æ¿ID
        limit: ç»“æœæ•°é‡é™åˆ¶
        current_user: å½“å‰ç”¨æˆ·
        
    Returns:
        é‡‡çº³å†å²æ•°æ®
    """
    try:
        logger.info(f"ğŸ” è·å–æ¨¡æ¿é‡‡çº³å†å²: æ¨¡æ¿={template_id}")
        
        # æŸ¥è¯¢é‡‡çº³å†å²
        adoption_query = """
        SELECT 
            ts.subdivision_id,
            ts.subdivision_name,
            ts.subdivision_created_at,
            ts.status,
            
            -- åŸå§‹å·¥ä½œæµä¿¡æ¯
            pw.name as parent_workflow_name,
            pw.workflow_base_id as parent_workflow_id,
            
            -- é‡‡çº³è€…ä¿¡æ¯
            u.username as subdivider_name,
            
            -- å­å·¥ä½œæµå®ä¾‹çŠ¶æ€
            swi.status as instance_status,
            swi.completed_at as instance_completed_at
            
        FROM task_subdivision ts
        JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
        JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id  
        JOIN node n ON ni.node_id = n.node_id
        JOIN workflow pw ON n.workflow_base_id = pw.workflow_base_id
        JOIN "user" u ON ts.subdivider_id = u.user_id
        LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
        WHERE ts.sub_workflow_base_id = $1
        AND ts.is_deleted = FALSE
        ORDER BY ts.subdivision_created_at DESC
        LIMIT $2
        """
        
        adoptions = await merge_service.db.fetch_all(adoption_query, template_id, limit)
        
        # æ ¼å¼åŒ–é‡‡çº³å†å²æ•°æ®
        formatted_adoptions = []
        for adoption in adoptions:
            formatted_adoptions.append({
                "subdivision_id": str(adoption["subdivision_id"]),
                "subdivision_name": adoption["subdivision_name"],
                "created_at": adoption["subdivision_created_at"].isoformat() if adoption["subdivision_created_at"] else None,
                "status": adoption["status"],
                "parent_workflow": {
                    "workflow_id": str(adoption["parent_workflow_id"]),
                    "name": adoption["parent_workflow_name"]
                },
                "subdivider_name": adoption["subdivider_name"],
                "instance_info": {
                    "status": adoption["instance_status"],
                    "completed_at": adoption["instance_completed_at"].isoformat() if adoption["instance_completed_at"] else None
                }
            })
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats_query = """
        SELECT 
            COUNT(*) as total_adoptions,
            COUNT(CASE WHEN swi.status = 'completed' THEN 1 END) as completed_adoptions,
            COUNT(DISTINCT pw.workflow_base_id) as unique_parent_workflows
        FROM task_subdivision ts
        JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
        JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
        JOIN node n ON ni.node_id = n.node_id  
        JOIN workflow pw ON n.workflow_base_id = pw.workflow_base_id
        LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
        WHERE ts.sub_workflow_base_id = $1
        AND ts.is_deleted = FALSE
        """
        
        stats = await merge_service.db.fetch_one(stats_query, template_id)
        
        adoption_stats = {
            "total_adoptions": stats["total_adoptions"] if stats else 0,
            "completed_adoptions": stats["completed_adoptions"] if stats else 0,
            "success_rate": (stats["completed_adoptions"] / max(stats["total_adoptions"], 1)) if stats and stats["total_adoptions"] > 0 else 0,
            "unique_adopters": stats["unique_parent_workflows"] if stats else 0
        }
        
        logger.info(f"âœ… æ¨¡æ¿é‡‡çº³å†å²è·å–æˆåŠŸ: {adoption_stats['total_adoptions']} æ¡è®°å½•")
        
        return BaseResponse(
            success=True,
            message="è·å–é‡‡çº³å†å²æˆåŠŸ",
            data={
                "template_id": str(template_id),
                "adoptions": formatted_adoptions,
                "statistics": adoption_stats,
                "pagination": {
                    "limit": limit,
                    "returned_count": len(formatted_adoptions)
                }
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡æ¿é‡‡çº³å†å²å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="è·å–é‡‡çº³å†å²å¤±è´¥ï¼Œè¯·ç¨åå†è¯•"
        )