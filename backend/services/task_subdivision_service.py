"""
ä»»åŠ¡ç»†åˆ†æœåŠ¡
Task Subdivision Service
"""

import uuid
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List
from loguru import logger

from ..models.task_subdivision import (
    TaskSubdivisionCreate, TaskSubdivisionResponse, TaskSubdivisionStatus,
    WorkflowAdoptionCreate, WorkflowAdoptionResponse,
    SubdivisionPreviewResponse, WorkflowSubdivisionsResponse
)
from ..models.workflow import WorkflowCreate
from ..repositories.task_subdivision.task_subdivision_repository import (
    TaskSubdivisionRepository, WorkflowAdoptionRepository
)
from ..repositories.instance.task_instance_repository import TaskInstanceRepository
from ..services.workflow_service import WorkflowService
from ..services.node_service import NodeService
from ..services.execution_service import execution_engine
from ..utils.helpers import now_utc
from ..utils.exceptions import ValidationError


class TaskSubdivisionService:
    """ä»»åŠ¡ç»†åˆ†æœåŠ¡"""
    
    def __init__(self):
        self.subdivision_repo = TaskSubdivisionRepository()
        self.adoption_repo = WorkflowAdoptionRepository()
        self.task_repo = TaskInstanceRepository()
        self.workflow_service = WorkflowService()
        self.node_service = NodeService()
        
        # ğŸ”’ æ·»åŠ åº”ç”¨å±‚é”ï¼Œé˜²æ­¢å¹¶å‘é‡å¤åˆ›å»º
        self._subdivision_locks: Dict[str, asyncio.Lock] = {}
        self._locks_lock = asyncio.Lock()  # ä¿æŠ¤lockså­—å…¸æœ¬èº«
    
    async def create_task_subdivision(self, subdivision_data: TaskSubdivisionCreate,
                                    execute_immediately: bool = False) -> TaskSubdivisionResponse:
        """
        åˆ›å»ºä»»åŠ¡ç»†åˆ†ï¼ˆå¸¦é‡å¤åˆ›å»ºé˜²æŠ¤å’Œåº”ç”¨å±‚é”ï¼‰
        
        Args:
            subdivision_data: ç»†åˆ†æ•°æ®
            execute_immediately: æ˜¯å¦ç«‹å³æ‰§è¡Œå­å·¥ä½œæµ
            
        Returns:
            åˆ›å»ºçš„ç»†åˆ†å“åº”
        """
        # ğŸ”§ é˜²æŠ¤æœºåˆ¶1ï¼šä½¿ç”¨åº”ç”¨å±‚é”é˜²æ­¢ç«æ€æ¡ä»¶
        lock_key = f"{subdivision_data.original_task_id}_{subdivision_data.subdivider_id}_{subdivision_data.subdivision_name}"
        
        # è·å–æˆ–åˆ›å»ºé”
        async with self._locks_lock:
            if lock_key not in self._subdivision_locks:
                self._subdivision_locks[lock_key] = asyncio.Lock()
            lock = self._subdivision_locks[lock_key]
        
        async with lock:
            try:
                logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»ºä»»åŠ¡ç»†åˆ†: {subdivision_data.subdivision_name}")
                
                # åœ¨é”å†…æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„ç»†åˆ†è®°å½•
                existing_subdivision_query = """
                SELECT subdivision_id, status FROM task_subdivision 
                WHERE original_task_id = %s AND subdivider_id = %s 
                AND subdivision_name = %s AND status IN ('created', 'executing', 'completed')
                LIMIT 1
                """
                existing_subdivision = await self.subdivision_repo.db.fetch_one(
                    existing_subdivision_query, 
                    subdivision_data.original_task_id,
                    subdivision_data.subdivider_id,
                    subdivision_data.subdivision_name
                )
                
                if existing_subdivision:
                    logger.warning(f"ğŸ›¡ï¸ å‘ç°é‡å¤çš„ç»†åˆ†è¯·æ±‚: {subdivision_data.subdivision_name}")
                    logger.warning(f"   å·²å­˜åœ¨ç»†åˆ†ID: {existing_subdivision['subdivision_id']}")
                    logger.warning(f"   çŠ¶æ€: {existing_subdivision['status']}")
                    
                    # è¿”å›å·²å­˜åœ¨çš„ç»†åˆ†è®°å½•
                    subdivision_record = await self.subdivision_repo.get_subdivision_by_id(
                        existing_subdivision['subdivision_id']
                    )
                    if subdivision_record:
                        return await self._format_subdivision_response(subdivision_record)
                
                # ç»§ç»­æ­£å¸¸çš„åˆ›å»ºæµç¨‹...
                return await self._create_subdivision_internal(subdivision_data, execute_immediately)
                
            except Exception as e:
                logger.error(f"åˆ›å»ºä»»åŠ¡ç»†åˆ†å¤±è´¥: {e}")
                raise
            finally:
                # æ¸…ç†é”ï¼ˆå¯é€‰ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                async with self._locks_lock:
                    if lock_key in self._subdivision_locks:
                        # å¦‚æœå½“å‰æ²¡æœ‰å…¶ä»–åç¨‹åœ¨ç­‰å¾…è¿™ä¸ªé”ï¼Œå°±åˆ é™¤å®ƒ
                        if not lock.locked():
                            del self._subdivision_locks[lock_key]
    
    async def _create_subdivision_internal(self, subdivision_data: TaskSubdivisionCreate,
                                         execute_immediately: bool = False) -> TaskSubdivisionResponse:
        """å†…éƒ¨åˆ›å»ºç»†åˆ†æ–¹æ³•ï¼ˆæ— é”ç‰ˆæœ¬ï¼‰"""
        try:
            original_task = await self.task_repo.get_task_by_id(subdivision_data.original_task_id)
            if not original_task:
                raise ValidationError("åŸå§‹ä»»åŠ¡ä¸å­˜åœ¨")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šè¾“å‡ºä»»åŠ¡åˆ†é…å’Œå½“å‰ç”¨æˆ·ä¿¡æ¯
            # logger.info(f"ğŸ” ä»»åŠ¡æƒé™éªŒè¯è°ƒè¯•:")
            # logger.info(f"   - ä»»åŠ¡ID: {subdivision_data.original_task_id}")
            # logger.info(f"   - ä»»åŠ¡åˆ†é…ç”¨æˆ·ID: {original_task.get('assigned_user_id')}")
            # logger.info(f"   - å½“å‰ç”¨æˆ·ID: {subdivision_data.subdivider_id}")
            # logger.info(f"   - ä»»åŠ¡çŠ¶æ€: {original_task.get('status')}")
            # logger.info(f"   - ä»»åŠ¡æ ‡é¢˜: {original_task.get('task_title')}")
            
            # æ·»åŠ ç±»å‹è°ƒè¯•ä¿¡æ¯
            assigned_user_id = original_task.get('assigned_user_id')
            current_user_id = subdivision_data.subdivider_id
            logger.info(f"ğŸ”¬ ç±»å‹è°ƒè¯•:")
            # logger.info(f"   - ä»»åŠ¡åˆ†é…ç”¨æˆ·IDç±»å‹: {type(assigned_user_id)}")
            # logger.info(f"   - å½“å‰ç”¨æˆ·IDç±»å‹: {type(current_user_id)}")
            # logger.info(f"   - ä»»åŠ¡åˆ†é…ç”¨æˆ·IDå€¼: {repr(assigned_user_id)}")
            # logger.info(f"   - å½“å‰ç”¨æˆ·IDå€¼: {repr(current_user_id)}")
            # logger.info(f"   - ç›¸ç­‰æ¯”è¾ƒ: {assigned_user_id == current_user_id}")
            # logger.info(f"   - å­—ç¬¦ä¸²æ¯”è¾ƒ: {str(assigned_user_id) == str(current_user_id)}")
            
            # ä¿®å¤ç±»å‹ä¸åŒ¹é…é—®é¢˜ï¼šå°†ä¸¤ä¸ªå€¼éƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            if str(original_task.get('assigned_user_id')) != str(subdivision_data.subdivider_id):
                raise ValidationError("åªèƒ½ç»†åˆ†åˆ†é…ç»™è‡ªå·±çš„ä»»åŠ¡")
            
            # 2. åˆ›å»ºç»†åˆ†è®°å½•
            subdivision_record = await self.subdivision_repo.create_subdivision(subdivision_data)
            if not subdivision_record:
                raise ValueError("åˆ›å»ºç»†åˆ†è®°å½•å¤±è´¥")
            
            subdivision_id = subdivision_record['subdivision_id']
            
            # 3. åˆ›å»ºæˆ–ä½¿ç”¨å·²æœ‰çš„å­å·¥ä½œæµ
            if subdivision_data.sub_workflow_base_id:
                # ä½¿ç”¨å‰ç«¯å·²åˆ›å»ºçš„å·¥ä½œæµ
                logger.info(f"ğŸ”„ ä½¿ç”¨å‰ç«¯å·²åˆ›å»ºçš„å·¥ä½œæµ: {subdivision_data.sub_workflow_base_id}")
                sub_workflow = await self.workflow_service.get_workflow_by_base_id(subdivision_data.sub_workflow_base_id)
                if not sub_workflow:
                    raise ValueError(f"æŒ‡å®šçš„å·¥ä½œæµä¸å­˜åœ¨: {subdivision_data.sub_workflow_base_id}")
                    
                logger.info(f"âœ… æ‰¾åˆ°é¢„åˆ›å»ºçš„å·¥ä½œæµ: {sub_workflow.name}")
                
                # 4. ä¸ºå·²æœ‰å·¥ä½œæµæ·»åŠ èŠ‚ç‚¹å’Œè¿æ¥
                await self._create_subdivision_nodes_and_connections(
                    subdivision_data.sub_workflow_base_id,
                    subdivision_data.sub_workflow_data,
                    subdivision_data.subdivider_id,
                    subdivision_data.context_to_pass  # ä¼ é€’ä»»åŠ¡ä¸Šä¸‹æ–‡
                )
                
                sub_workflow_base_id = subdivision_data.sub_workflow_base_id
            else:
                # âš ï¸ è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå‰ç«¯åº”è¯¥æ€»æ˜¯é¢„åˆ›å»ºå·¥ä½œæµ
                logger.warning(f"âš ï¸ å‰ç«¯æ²¡æœ‰é¢„åˆ›å»ºå·¥ä½œæµï¼Œåç«¯å°†åˆ›å»ºæ–°å·¥ä½œæµ")
                logger.warning(f"   è¿™å¯èƒ½è¡¨æ˜å‰ç«¯å·¥ä½œæµåˆ›å»ºå¤±è´¥æˆ–IDä¼ é€’æœ‰é—®é¢˜")
                
                # åˆ›å»ºæ–°çš„å­å·¥ä½œæµï¼ˆä»…åŸºç¡€ä¿¡æ¯ï¼‰
                logger.info(f"ğŸ”„ åç«¯åˆ›å»ºæ–°çš„å­å·¥ä½œæµ")
                sub_workflow_create = WorkflowCreate(
                    name=subdivision_data.subdivision_name,  # ğŸ”§ ä½¿ç”¨ç»†åˆ†åç§°ä½œä¸ºå·¥ä½œæµåç§°
                    description=f"ä»ä»»åŠ¡ '{original_task.get('task_title', '')}' ç»†åˆ†è€Œæ¥\n\n{subdivision_data.subdivision_description}",
                    creator_id=subdivision_data.subdivider_id
                )
                
                sub_workflow = await self.workflow_service.create_workflow(sub_workflow_create)
                
                # 4. åˆ›å»ºå­å·¥ä½œæµçš„èŠ‚ç‚¹å’Œè¿æ¥
                await self._create_subdivision_nodes_and_connections(
                    sub_workflow.workflow_base_id,
                    subdivision_data.sub_workflow_data,
                    subdivision_data.subdivider_id,
                    subdivision_data.context_to_pass  # ä¼ é€’ä»»åŠ¡ä¸Šä¸‹æ–‡
                )
                
                sub_workflow_base_id = sub_workflow.workflow_base_id
            
            # 4. æ›´æ–°ç»†åˆ†è®°å½•çš„å·¥ä½œæµID
            await self.subdivision_repo.update_subdivision_workflow_ids(
                subdivision_id, sub_workflow_base_id
            )
            
            # 5. æ›´æ–°ç»†åˆ†çš„ä»»åŠ¡ä¸Šä¸‹æ–‡
            await self.subdivision_repo.update_subdivision_task_context(
                subdivision_id, original_task.get('task_description', '')
            )
            
            # 6. å¦‚æœéœ€è¦ç«‹å³æ‰§è¡Œï¼Œå¯åŠ¨å­å·¥ä½œæµå®ä¾‹
            sub_workflow_instance_id = None
            if execute_immediately:
                sub_workflow_instance_id = await self._execute_sub_workflow(
                    subdivision_id, sub_workflow_base_id, 
                    subdivision_data.subdivider_id, subdivision_data.context_to_pass
                )
            
            logger.info(f"âœ… ä»»åŠ¡ç»†åˆ†åˆ›å»ºæˆåŠŸ: {subdivision_id}")
            
            # 7. è¿”å›å“åº”
            return await self._format_subdivision_response(subdivision_record, {
                'original_task_title': original_task.get('task_title'),
                'sub_workflow_name': sub_workflow.name if hasattr(sub_workflow, 'name') else subdivision_data.subdivision_name,
                'sub_workflow_base_id': sub_workflow_base_id,  # ä½¿ç”¨ç»Ÿä¸€çš„å˜é‡
                'sub_workflow_instance_id': sub_workflow_instance_id
            })
            
        except Exception as e:
            logger.error(f"åˆ›å»ºä»»åŠ¡ç»†åˆ†å¤±è´¥: {e}")
            raise
    
    async def _execute_sub_workflow(self, subdivision_id: uuid.UUID, 
                                  sub_workflow_base_id: uuid.UUID,
                                  executor_id: uuid.UUID, 
                                  context_data: str) -> uuid.UUID:
        """æ‰§è¡Œå­å·¥ä½œæµï¼ˆå¸¦é‡å¤æ‰§è¡Œé˜²æŠ¤ï¼‰"""
        try:
            logger.info(f"ğŸš€ å¯åŠ¨å­å·¥ä½œæµæ‰§è¡Œ: {sub_workflow_base_id}")
            logger.info(f"   ä¸Šä¸‹æ–‡æ•°æ®: {context_data[:100]}..." if context_data else "   æ— ä¸Šä¸‹æ–‡æ•°æ®")
            
            # ğŸ”§ é˜²æŠ¤æœºåˆ¶1ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰å·¥ä½œæµå®ä¾‹
            existing_instance_query = """
            SELECT wi.workflow_instance_id, wi.status
            FROM workflow_instance wi
            WHERE wi.workflow_base_id = %s
            AND wi.workflow_instance_name LIKE %s
            AND wi.status IN ('running', 'pending', 'completed')
            ORDER BY wi.created_at DESC
            LIMIT 1
            """
            
            existing_instances = await self.workflow_service.workflow_repository.db.fetch_all(
                existing_instance_query,
                sub_workflow_base_id,
                f"ç»†åˆ†æ‰§è¡Œ_{subdivision_id}%"
            )
            
            if existing_instances:
                existing_instance = existing_instances[0]
                existing_instance_id = existing_instance['workflow_instance_id']
                existing_status = existing_instance['status']
                
                logger.warning(f"ğŸ›¡ï¸ å‘ç°å·²å­˜åœ¨çš„å·¥ä½œæµå®ä¾‹: {existing_instance_id}")
                logger.warning(f"   çŠ¶æ€: {existing_status}, ç»†åˆ†ID: {subdivision_id}")
                
                # å¦‚æœå®ä¾‹æ­£åœ¨è¿è¡Œæˆ–å·²å®Œæˆï¼Œç›´æ¥è¿”å›
                if existing_status in ['running', 'pending', 'completed']:
                    logger.warning(f"   è¿”å›å·²å­˜åœ¨çš„å®ä¾‹ï¼Œè·³è¿‡é‡å¤åˆ›å»º")
                    return uuid.UUID(str(existing_instance_id))
            
            # æ„é€ æ‰§è¡Œè¯·æ±‚
            from ..models.instance import WorkflowExecuteRequest
            execute_request = WorkflowExecuteRequest(
                workflow_base_id=sub_workflow_base_id,
                workflow_instance_name=f"ç»†åˆ†æ‰§è¡Œ_{subdivision_id}",
                input_data={},
                context_data={
                    "subdivision_context": context_data,
                    "subdivision_id": str(subdivision_id),
                    "execution_type": "task_subdivision"
                }
            )
            
            # ğŸ›¡ï¸ ä¿æŠ¤çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡ï¼šåˆ›å»ºå¿«ç…§
            # è·å–ç»†åˆ†è®°å½•ä»¥è·å¾—åŸå§‹ä»»åŠ¡ID
            subdivision_record = await self.subdivision_repo.get_subdivision_by_id(subdivision_id)
            if subdivision_record:
                original_task_id = subdivision_record['original_task_id']
                parent_workflow_id = await self._get_parent_workflow_id(original_task_id)
            else:
                parent_workflow_id = None
                logger.warning(f"âš ï¸ æ— æ³•è·å–ç»†åˆ†è®°å½•: {subdivision_id}")
            parent_context_snapshot = None
            
            if parent_workflow_id:
                from .workflow_execution_context import get_context_manager
                context_manager = get_context_manager()
                parent_context_snapshot = await context_manager.create_context_snapshot(parent_workflow_id)
                logger.info(f"ğŸ”’ å·²åˆ›å»ºçˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§: {parent_workflow_id}")
            
            try:
                # æ‰§è¡Œå·¥ä½œæµ
                result = await execution_engine.execute_workflow(execute_request, executor_id)
                
                logger.info(f"ğŸ” æ‰§è¡Œå¼•æ“è¿”å›ç»“æœ: {result}")
                logger.info(f"ğŸ” ç»“æœç±»å‹: {type(result)}")
            finally:
                # ğŸ”„ æ¢å¤çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ˆæ— è®ºå­å·¥ä½œæµæ˜¯å¦æˆåŠŸï¼‰
                if parent_workflow_id and parent_context_snapshot:
                    try:
                        await context_manager.restore_from_snapshot(parent_workflow_id, parent_context_snapshot)
                        logger.info(f"âœ… å·²æ¢å¤çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡: {parent_workflow_id}")
                    except Exception as restore_error:
                        logger.error(f"âŒ æ¢å¤çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {restore_error}")
                        # å°è¯•ä»æ•°æ®åº“æ¢å¤
                        try:
                            await context_manager._restore_context_from_database(parent_workflow_id)
                            logger.info(f"ğŸ”§ ä»æ•°æ®åº“æ¢å¤çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡æˆåŠŸ: {parent_workflow_id}")
                        except Exception as db_restore_error:
                            logger.error(f"âŒ ä»æ•°æ®åº“æ¢å¤çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡ä¹Ÿå¤±è´¥: {db_restore_error}")
            
            
            # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
            instance_id = None
            if result:
                if isinstance(result, dict):
                    if 'instance_id' in result:
                        instance_id = uuid.UUID(result['instance_id'])
                    elif 'workflow_instance_id' in result:
                        instance_id = uuid.UUID(result['workflow_instance_id'])
                elif hasattr(result, 'workflow_instance_id'):
                    instance_id = result.workflow_instance_id
                elif isinstance(result, str):
                    try:
                        instance_id = uuid.UUID(result)
                    except ValueError:
                        logger.error(f"æ— æ³•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºUUID: {result}")
            
            if instance_id:
                # æ›´æ–°ç»†åˆ†è®°å½•çš„å®ä¾‹ID
                await self.subdivision_repo.update_subdivision_workflow_ids(
                    subdivision_id, sub_workflow_base_id, instance_id
                )
                
                # æ³¨å†Œå·¥ä½œæµå®Œæˆå›è°ƒï¼Œç”¨äºè‡ªåŠ¨æäº¤ç»“æœç»™çˆ¶å·¥ä½œæµ
                await self._register_subdivision_completion_callback(
                    subdivision_id, instance_id, executor_id
                )
                
                logger.info(f"âœ… å­å·¥ä½œæµå¯åŠ¨æˆåŠŸ: {instance_id}")
                return instance_id
            else:
                logger.error(f"âŒ æ— æ³•ä»æ‰§è¡Œç»“æœä¸­æå–å®ä¾‹ID: {result}")
                raise ValueError("å­å·¥ä½œæµå¯åŠ¨å¤±è´¥")
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œå­å·¥ä½œæµå¤±è´¥: {e}")
            raise
    
    async def _register_subdivision_completion_callback(self, subdivision_id: uuid.UUID, 
                                                       workflow_instance_id: uuid.UUID,
                                                       executor_id: uuid.UUID):
        """æ³¨å†Œç»†åˆ†å·¥ä½œæµå®Œæˆå›è°ƒ"""
        try:
            logger.info(f"ğŸ”” æ³¨å†Œç»†åˆ†å·¥ä½œæµå®Œæˆå›è°ƒ: {workflow_instance_id}")
            
            # å¯¼å…¥ç›‘æ§æœåŠ¡æ¥æ³¨å†Œå›è°ƒ
            from ..services.monitoring_service import monitoring_service
            
            # åˆ›å»ºå›è°ƒå‡½æ•°
            async def subdivision_completion_callback(instance_id: uuid.UUID, final_status: str, results: dict):
                await self._handle_subdivision_completion(
                    subdivision_id, instance_id, final_status, results, executor_id
                )
            
            # æ³¨å†Œåˆ°ç›‘æ§æœåŠ¡
            await monitoring_service.register_workflow_completion_callback(
                workflow_instance_id, subdivision_completion_callback
            )
            
            logger.info(f"âœ… ç»†åˆ†å·¥ä½œæµå®Œæˆå›è°ƒæ³¨å†ŒæˆåŠŸ: {workflow_instance_id}")
            
        except Exception as e:
            logger.error(f"æ³¨å†Œç»†åˆ†å·¥ä½œæµå®Œæˆå›è°ƒå¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
    
    async def _handle_subdivision_completion(self, subdivision_id: uuid.UUID,
                                           workflow_instance_id: uuid.UUID,
                                           final_status: str,
                                           results: dict,
                                           executor_id: uuid.UUID):
        """å¤„ç†ç»†åˆ†å·¥ä½œæµå®Œæˆäº‹ä»¶"""
        try:
            logger.info(f"ğŸ¯ å¤„ç†ç»†åˆ†å·¥ä½œæµå®Œæˆäº‹ä»¶: {subdivision_id}")
            logger.info(f"   - å·¥ä½œæµå®ä¾‹ID: {workflow_instance_id}")
            logger.info(f"   - æœ€ç»ˆçŠ¶æ€: {final_status}")
            logger.info(f"   - ç»“æœæ•°æ®: {len(str(results))} å­—ç¬¦")
            
            # è·å–ç»†åˆ†ä¿¡æ¯
            subdivision = await self.subdivision_repo.get_subdivision_by_id(subdivision_id)
            if not subdivision:
                logger.error(f"æœªæ‰¾åˆ°ç»†åˆ†è®°å½•: {subdivision_id}")
                return
            
            original_task_id = subdivision['original_task_id']
            
            # æ›´æ–°ç»†åˆ†çŠ¶æ€
            if final_status == 'completed':
                await self._update_subdivision_status(subdivision_id, 'completed', results)
                
                # ğŸ”§ ä¿®æ”¹ï¼šä»…ä¿å­˜ç»“æœä¾›ç”¨æˆ·å‚è€ƒï¼Œä¸è‡ªåŠ¨æäº¤ä»»åŠ¡
                await self._save_subdivision_results_for_reference(
                    original_task_id, subdivision_id, results, executor_id
                )
            elif final_status in ['failed', 'timeout']:
                await self._update_subdivision_status(subdivision_id, 'failed', results)
                logger.error(f"ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {subdivision_id}, çŠ¶æ€: {final_status}")
            
        except Exception as e:
            logger.error(f"å¤„ç†ç»†åˆ†å·¥ä½œæµå®Œæˆäº‹ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    async def _update_subdivision_status(self, subdivision_id: uuid.UUID, 
                                       status: str, results: dict):
        """æ›´æ–°ç»†åˆ†çŠ¶æ€"""
        try:
            from ..models.task_subdivision import TaskSubdivisionStatus
            
            # æ„å»ºç»“æœæ‘˜è¦
            result_summary = self._generate_result_summary(results)
            
            # è½¬æ¢å­—ç¬¦ä¸²çŠ¶æ€ä¸ºæšä¸¾
            if status == 'completed':
                status_enum = TaskSubdivisionStatus.COMPLETED
            elif status == 'failed':
                status_enum = TaskSubdivisionStatus.FAILED
            elif status == 'executing':
                status_enum = TaskSubdivisionStatus.EXECUTING
            else:
                status_enum = TaskSubdivisionStatus.CREATED
            
            # æ›´æ–°ç»†åˆ†è®°å½•çŠ¶æ€
            await self.subdivision_repo.update_subdivision_status(subdivision_id, status_enum)
            
            # å•ç‹¬æ›´æ–°ç»“æœæ‘˜è¦ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
            if result_summary:
                update_data = {
                    'result_summary': result_summary,
                    'completed_at': now_utc() if status == 'completed' else None
                }
                await self.subdivision_repo.update(subdivision_id, update_data, id_column="subdivision_id")
            
            logger.info(f"âœ… æ›´æ–°ç»†åˆ†çŠ¶æ€æˆåŠŸ: {subdivision_id} -> {status}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç»†åˆ†çŠ¶æ€å¤±è´¥: {e}")
    
    def _serialize_for_json(self, obj):
        """JSONåºåˆ—åŒ–åŠ©æ‰‹å‡½æ•°ï¼Œå¤„ç†datetimeç­‰å¯¹è±¡"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, uuid.UUID):
            return str(obj)
        elif isinstance(obj, dict):
            return {key: self._serialize_for_json(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._serialize_for_json(item) for item in obj]
        else:
            return obj
    
    async def _save_subdivision_results_for_reference(self, original_task_id: uuid.UUID,
                                                     subdivision_id: uuid.UUID,
                                                     results: dict,
                                                     executor_id: uuid.UUID):
        """ä¿å­˜ç»†åˆ†å·¥ä½œæµç»“æœä½œä¸ºç”¨æˆ·å‚è€ƒï¼Œä¸è‡ªåŠ¨æäº¤ä»»åŠ¡"""
        try:
            logger.info(f"ğŸ’¾ ä¿å­˜ç»†åˆ†ç»“æœä¾›ç”¨æˆ·å‚è€ƒ: {original_task_id}")
            
            # æ¸…ç†resultsä¸­çš„datetimeå¯¹è±¡ï¼Œç¡®ä¿å¯ä»¥JSONåºåˆ—åŒ–
            clean_results = self._serialize_for_json(results)
            
            # ç”Ÿæˆç»“æœæ•°æ®
            result_data = {
                'subdivision_id': str(subdivision_id),
                'execution_results': clean_results,
                'completion_time': now_utc().isoformat(),
                'executed_by': str(executor_id),
                'is_reference_data': True,  # æ ‡è®°ä¸ºå‚è€ƒæ•°æ®
                'auto_submitted': False     # æœªè‡ªåŠ¨æäº¤
            }
            
            # æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬
            formatted_output = self._format_subdivision_output(clean_results)
            
            # å¯¼å…¥ä»»åŠ¡ä»“åº“æ¥æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡
            from ..repositories.instance.task_instance_repository import TaskInstanceRepository
            from ..models.instance import TaskInstanceUpdate
            
            task_repo = TaskInstanceRepository()
            
            # ä»…æ›´æ–°context_dataå’Œinstructionsï¼Œä¸æ”¹å˜ä»»åŠ¡çŠ¶æ€
            task_update = TaskInstanceUpdate(
                context_data=json.dumps(result_data, ensure_ascii=False, indent=2),
                instructions=f"ç»†åˆ†å·¥ä½œæµå·²å®Œæˆï¼Œç»“æœå¯ä½œä¸ºæäº¤å‚è€ƒã€‚\n\nã€å‚è€ƒç»“æœã€‘:\n{formatted_output}"
            )
            
            # æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆä¸æ”¹å˜çŠ¶æ€ï¼‰
            updated_task = await task_repo.update_task(original_task_id, task_update)
            
            if updated_task:
                logger.info(f"âœ… ç»†åˆ†ç»“æœå·²ä¿å­˜ä¾›ç”¨æˆ·å‚è€ƒ: {original_task_id}")
                logger.info(f"   - ä»»åŠ¡çŠ¶æ€: {updated_task.get('status')} (ä¿æŒä¸å˜)")
                logger.info(f"   - ç”¨æˆ·å¯åœ¨ä»»åŠ¡è¯¦æƒ…ä¸­æŸ¥çœ‹ç»†åˆ†ç»“æœå¹¶æ‰‹åŠ¨æäº¤")
                
                # ğŸ”§ é‡è¦ä¿®å¤ï¼šç»†åˆ†å·¥ä½œæµå®Œæˆååº”è¯¥æ›´æ–°çˆ¶èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
                # å³ä½¿ä»»åŠ¡çŠ¶æ€ä¿æŒä¸å˜ï¼ˆä¾›ç”¨æˆ·æ‰‹åŠ¨æäº¤ï¼‰ï¼ŒèŠ‚ç‚¹å®ä¾‹ä¹Ÿåº”è¯¥æ ‡è®°ä¸ºå®Œæˆ
                # è¿™æ ·åç»­èŠ‚ç‚¹æ‰èƒ½è¢«è§¦å‘
                try:
                    await self._update_parent_node_instance_status(original_task_id, formatted_output)
                    logger.info(f"âœ… çˆ¶èŠ‚ç‚¹å®ä¾‹çŠ¶æ€æ›´æ–°å®Œæˆ")
                except Exception as node_update_error:
                    logger.error(f"âŒ æ›´æ–°çˆ¶èŠ‚ç‚¹å®ä¾‹çŠ¶æ€å¤±è´¥: {node_update_error}")
                    import traceback
                    logger.error(f"èŠ‚ç‚¹çŠ¶æ€æ›´æ–°é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            else:
                logger.error(f"âŒ ä¿å­˜ç»†åˆ†ç»“æœå¤±è´¥: {original_task_id}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»†åˆ†ç»“æœå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    async def _update_parent_node_instance_status(self, original_task_id: uuid.UUID, output_data: str):
        """æ›´æ–°çˆ¶èŠ‚ç‚¹å®ä¾‹çŠ¶æ€ä¸ºå·²å®Œæˆ"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹æ›´æ–°çˆ¶èŠ‚ç‚¹å®ä¾‹çŠ¶æ€: ä»»åŠ¡ {original_task_id}")
            
            # 1. é€šè¿‡ä»»åŠ¡IDè·å–èŠ‚ç‚¹å®ä¾‹ID
            from ..repositories.instance.task_instance_repository import TaskInstanceRepository
            task_repo = TaskInstanceRepository()
            
            task_info = await task_repo.get_task_by_id(original_task_id)
            if not task_info:
                logger.error(f"âŒ æ— æ³•æ‰¾åˆ°åŸå§‹ä»»åŠ¡: {original_task_id}")
                return
            
            node_instance_id = task_info.get('node_instance_id')
            workflow_instance_id = task_info.get('workflow_instance_id')
            
            if not node_instance_id:
                logger.error(f"âŒ ä»»åŠ¡ {original_task_id} æ²¡æœ‰å…³è”çš„èŠ‚ç‚¹å®ä¾‹")
                return
            
            logger.info(f"   - èŠ‚ç‚¹å®ä¾‹ID: {node_instance_id}")
            logger.info(f"   - å·¥ä½œæµå®ä¾‹ID: {workflow_instance_id}")
            
            # 2. æ£€æŸ¥èŠ‚ç‚¹çš„æ‰€æœ‰ä»»åŠ¡æ˜¯å¦éƒ½å·²å®Œæˆ
            node_tasks = await task_repo.get_tasks_by_node_instance(uuid.UUID(node_instance_id))
            if not node_tasks:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ²¡æœ‰å…³è”çš„ä»»åŠ¡")
                return
            
            # ç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
            total_tasks = len(node_tasks)
            completed_tasks = [task for task in node_tasks if task.get('status') == 'completed']
            failed_tasks = [task for task in node_tasks if task.get('status') == 'failed']
            
            logger.info(f"   - èŠ‚ç‚¹ä»»åŠ¡ç»Ÿè®¡: æ€»è®¡ {total_tasks}, å®Œæˆ {len(completed_tasks)}, å¤±è´¥ {len(failed_tasks)}")
            
            # 3. æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
            # å¯¹äºç»†åˆ†å·¥ä½œæµå®Œæˆçš„æƒ…å†µï¼Œå³ä½¿åŸä»»åŠ¡è¿˜æœªæ‰‹åŠ¨æäº¤ï¼Œä¹Ÿåº”è¯¥æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
            # å› ä¸ºç»†åˆ†å·¥ä½œæµçš„å®Œæˆæ„å‘³ç€èŠ‚ç‚¹çš„å·¥ä½œå·²ç»å®Œæˆï¼Œåªæ˜¯ç­‰å¾…ç”¨æˆ·ç¡®è®¤
            should_update_node = False
            
            if len(completed_tasks) == total_tasks and len(failed_tasks) == 0:
                # æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆçš„æ ‡å‡†æƒ…å†µ
                should_update_node = True
                logger.info(f"ğŸ¯ èŠ‚ç‚¹ {node_instance_id} çš„æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆï¼Œæ›´æ–°èŠ‚ç‚¹çŠ¶æ€")
            elif len(completed_tasks) == total_tasks - 1 and len(failed_tasks) == 0:
                # ç»†åˆ†å·¥ä½œæµå®Œæˆçš„ç‰¹æ®Šæƒ…å†µï¼šåªæœ‰ä¸€ä¸ªä»»åŠ¡æœªå®Œæˆä½†æœ‰ç»†åˆ†ç»“æœ
                incomplete_tasks = [task for task in node_tasks if task.get('status') not in ['completed', 'failed']]
                if len(incomplete_tasks) == 1:
                    incomplete_task = incomplete_tasks[0]
                    # æ£€æŸ¥è¿™ä¸ªæœªå®Œæˆçš„ä»»åŠ¡æ˜¯å¦æœ‰ç»†åˆ†ç»“æœï¼ˆå³å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡ï¼‰
                    if str(incomplete_task.get('task_instance_id')) == str(original_task_id):
                        should_update_node = True
                        logger.info(f"ğŸ¯ èŠ‚ç‚¹ {node_instance_id} çš„ç»†åˆ†å·¥ä½œæµå·²å®Œæˆï¼Œæ›´æ–°èŠ‚ç‚¹çŠ¶æ€ï¼ˆä»»åŠ¡ç­‰å¾…æ‰‹åŠ¨æäº¤ï¼‰")
                        
            if should_update_node:
                
                # å¯¼å…¥èŠ‚ç‚¹å®ä¾‹ç›¸å…³æ¨¡å—
                from ..repositories.instance.node_instance_repository import NodeInstanceRepository
                from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
                
                node_instance_repo = NodeInstanceRepository()
                
                # å‡†å¤‡èŠ‚ç‚¹æ›´æ–°æ•°æ®
                node_update = NodeInstanceUpdate(
                    status=NodeInstanceStatus.COMPLETED,
                    output_data={
                        'subdivision_result': output_data,
                        'completed_by': 'task_subdivision',
                        'completion_time': now_utc().isoformat()
                    },
                    completed_at=now_utc()
                )
                
                # æ›´æ–°èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
                updated_node = await node_instance_repo.update_node_instance(
                    uuid.UUID(node_instance_id), node_update
                )
                
                if updated_node:
                    logger.info(f"âœ… èŠ‚ç‚¹å®ä¾‹çŠ¶æ€æ›´æ–°æˆåŠŸ: {node_instance_id}")
                    logger.info(f"   - æ–°çŠ¶æ€: COMPLETED")
                    logger.info(f"   - è¾“å‡ºæ•°æ®é•¿åº¦: {len(output_data)} å­—ç¬¦")
                    
                    # 4. é€šçŸ¥æ‰§è¡Œå¼•æ“æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å¯ä»¥ç»§ç»­æ‰§è¡Œ
                    await self._notify_workflow_engine_node_completion(
                        uuid.UUID(workflow_instance_id), uuid.UUID(node_instance_id)
                    )
                    
                else:
                    logger.error(f"âŒ èŠ‚ç‚¹å®ä¾‹çŠ¶æ€æ›´æ–°å¤±è´¥: {node_instance_id}")
            else:
                logger.info(f"â„¹ï¸ èŠ‚ç‚¹ {node_instance_id} è¿˜æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼Œæš‚ä¸æ›´æ–°èŠ‚ç‚¹çŠ¶æ€")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°çˆ¶èŠ‚ç‚¹å®ä¾‹çŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    async def _notify_workflow_engine_node_completion(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """é€šçŸ¥å·¥ä½œæµæ‰§è¡Œå¼•æ“èŠ‚ç‚¹å·²å®Œæˆ"""
        try:
            logger.info(f"ğŸ“¢ é€šçŸ¥æ‰§è¡Œå¼•æ“èŠ‚ç‚¹å®Œæˆ: å·¥ä½œæµ {workflow_instance_id}, èŠ‚ç‚¹ {node_instance_id}")
            
            # å¯¼å…¥æ‰§è¡Œå¼•æ“
            from ..services.execution_service import execution_engine
            
            # è§¦å‘å·¥ä½œæµçŠ¶æ€æ£€æŸ¥ï¼Œè®©æ‰§è¡Œå¼•æ“æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„èŠ‚ç‚¹å¯ä»¥æ‰§è¡Œ
            await execution_engine._check_workflow_completion(workflow_instance_id)
            
            logger.info(f"âœ… å·²é€šçŸ¥æ‰§è¡Œå¼•æ“æ£€æŸ¥å·¥ä½œæµçŠ¶æ€: {workflow_instance_id}")
            
        except Exception as e:
            logger.error(f"âŒ é€šçŸ¥æ‰§è¡Œå¼•æ“å¤±è´¥: {e}")
            # è¿™ä¸ªå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•é”™è¯¯
    
    def _generate_result_summary(self, results: dict) -> str:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        try:
            if not results:
                return "ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œæ— è¾“å‡ºæ•°æ®"
            
            # ç»Ÿè®¡ä»»åŠ¡å®Œæˆæƒ…å†µ
            total_tasks = results.get('total_tasks', 0)
            completed_tasks = results.get('completed_tasks', 0)
            
            summary_parts = [
                f"ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå®Œæˆ",
                f"æ€»ä»»åŠ¡æ•°: {total_tasks}",
                f"å®Œæˆä»»åŠ¡æ•°: {completed_tasks}"
            ]
            
            if 'final_output' in results:
                final_output = str(results['final_output'])
                if final_output:
                    summary_parts.append(f"æœ€ç»ˆè¾“å‡º: {final_output[:100]}...")
            
            return " | ".join(summary_parts)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç»“æœæ‘˜è¦å¤±è´¥: {e}")
            return "ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå®Œæˆ"
    
    def _format_subdivision_output(self, results: dict) -> str:
        """æ ¼å¼åŒ–ç»†åˆ†å·¥ä½œæµè¾“å‡ºä¸ºæ–‡æœ¬"""
        try:
            if not results:
                return "ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œä½†æ²¡æœ‰ç”Ÿæˆè¾“å‡ºæ•°æ®ã€‚"
            
            logger.info(f"ğŸ¨ æ ¼å¼åŒ–ç»†åˆ†å·¥ä½œæµè¾“å‡ºï¼Œç»“æœæ•°æ®é”®: {list(results.keys())}")
            
            output_parts = [f"=== {results.get('workflow_instance_id', 'å­å·¥ä½œæµ')} æ‰§è¡Œç»“æœ ===\n"]
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            if 'total_tasks' in results:
                output_parts.append(f"ğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
                output_parts.append(f"   â€¢ æ€»ä»»åŠ¡æ•°: {results.get('total_tasks', 0)}")
                output_parts.append(f"   â€¢ å®Œæˆä»»åŠ¡æ•°: {results.get('completed_tasks', 0)}")
                output_parts.append(f"   â€¢ å¤±è´¥ä»»åŠ¡æ•°: {results.get('failed_tasks', 0)}")
                output_parts.append(f"   â€¢ æ‰§è¡Œæ—¶é•¿: {results.get('execution_duration', 'N/A')}")
                output_parts.append("")
            
            # ğŸ”§ å¢å¼ºï¼šä¸»è¦è¾“å‡ºç»“æœï¼ˆä¼˜å…ˆæ˜¾ç¤ºç»“æŸèŠ‚ç‚¹çš„å®Œæ•´è¾“å‡ºï¼‰
            final_output = results.get('final_output', '')
            has_end_node_output = results.get('has_end_node_output', False)
            
            if final_output:
                if has_end_node_output:
                    output_parts.append("ğŸ“‹ å·¥ä½œæµæœ€ç»ˆç»“æœï¼ˆæ¥è‡ªç»“æŸèŠ‚ç‚¹ï¼‰:")
                else:
                    output_parts.append("ğŸ“‹ å·¥ä½œæµæœ€ç»ˆç»“æœï¼ˆæ¥è‡ªä»»åŠ¡è¾“å‡ºï¼‰:")
                
                # å¦‚æœæ˜¯é•¿æ–‡æœ¬ï¼Œè¿›è¡Œé€‚å½“çš„æ ¼å¼åŒ–
                if len(final_output) > 1000:
                    # æ˜¾ç¤ºå‰500å­—ç¬¦å’Œå200å­—ç¬¦
                    output_parts.append(final_output[:500])
                    output_parts.append("\n... [å†…å®¹è¿‡é•¿ï¼Œå·²çœç•¥éƒ¨åˆ†å†…å®¹] ...\n")
                    output_parts.append(final_output[-200:])
                else:
                    output_parts.append(final_output)
                output_parts.append("")
            
            # ğŸ”§ å¢å¼ºï¼šå¦‚æœæ²¡æœ‰è¯¦ç»†çš„æœ€ç»ˆè¾“å‡ºï¼Œæ˜¾ç¤ºå„ä¸ªä»»åŠ¡çš„è¯¦ç»†ç»“æœ
            if not final_output or len(final_output) < 50:
                if 'task_results' in results and isinstance(results['task_results'], list):
                    completed_task_results = [t for t in results['task_results'] if t.get('status') == 'completed']
                    
                    if completed_task_results:
                        output_parts.append("ğŸ“ å·²å®Œæˆä»»åŠ¡çš„è¯¦ç»†ç»“æœ:")
                        for i, task_result in enumerate(completed_task_results, 1):
                            if isinstance(task_result, dict):
                                task_title = task_result.get('title', f'ä»»åŠ¡ {i}')
                                task_output = task_result.get('output', 'æ— è¾“å‡º')
                                task_summary = task_result.get('result_summary', '')
                                
                                output_parts.append(f"   {i}. **{task_title}**")
                                
                                if task_summary:
                                    output_parts.append(f"      æ‘˜è¦: {task_summary}")
                                
                                if task_output and task_output != 'æ— è¾“å‡º':
                                    # é™åˆ¶å•ä¸ªä»»åŠ¡è¾“å‡ºçš„é•¿åº¦
                                    if len(str(task_output)) > 300:
                                        truncated_output = str(task_output)[:300] + "... [å·²æˆªæ–­]"
                                        output_parts.append(f"      ç»“æœ: {truncated_output}")
                                    else:
                                        output_parts.append(f"      ç»“æœ: {task_output}")
                                
                                output_parts.append("")
                            else:
                                output_parts.append(f"   {i}. {str(task_result)}")
                        output_parts.append("")
            
            # æ‰§è¡ŒçŠ¶æ€å’Œæ—¶é—´ä¿¡æ¯
            status = results.get('status', 'unknown')
            if status == 'completed':
                output_parts.append("âœ… ç»†åˆ†å·¥ä½œæµå·²æˆåŠŸå®Œæˆæ‰€æœ‰ä»»åŠ¡ã€‚")
            elif status == 'failed':
                output_parts.append("âŒ ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå¤±è´¥ã€‚")
            else:
                output_parts.append(f"â„¹ï¸ ç»†åˆ†å·¥ä½œæµçŠ¶æ€: {status}")
            
            # æ—¶é—´ä¿¡æ¯
            started_at = results.get('started_at')
            completed_at = results.get('completed_at')
            if started_at:
                output_parts.append(f"ğŸ• å¼€å§‹æ—¶é—´: {started_at}")
            if completed_at:
                output_parts.append(f"ğŸ• å®Œæˆæ—¶é—´: {completed_at}")
            
            formatted_output = "\n".join(output_parts)
            logger.info(f"âœ… ç»†åˆ†å·¥ä½œæµè¾“å‡ºæ ¼å¼åŒ–å®Œæˆï¼Œæ€»é•¿åº¦: {len(formatted_output)} å­—ç¬¦")
            
            return formatted_output
            
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–ç»†åˆ†å·¥ä½œæµè¾“å‡ºå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return f"ç»†åˆ†å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œä½†è¾“å‡ºæ ¼å¼åŒ–å¤±è´¥: {str(e)}"
    
    async def get_task_subdivisions(self, task_id: uuid.UUID) -> List[TaskSubdivisionResponse]:
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰ç»†åˆ†"""
        try:
            subdivisions = await self.subdivision_repo.get_subdivisions_by_task(task_id)
            
            return [
                await self._format_subdivision_response(subdivision)
                for subdivision in subdivisions
            ]
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡ç»†åˆ†åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def get_subdivision_workflow_instance(self, subdivision_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–ç»†åˆ†çš„å­å·¥ä½œæµå®ä¾‹ä¿¡æ¯"""
        try:
            logger.info(f"ğŸ“Š è·å–ç»†åˆ†å­å·¥ä½œæµå®ä¾‹ä¿¡æ¯: {subdivision_id}")
            
            # è·å–ç»†åˆ†è®°å½•
            subdivision = await self.subdivision_repo.get_subdivision_by_id(subdivision_id)
            if not subdivision:
                logger.warning(f"æœªæ‰¾åˆ°ç»†åˆ†è®°å½•: {subdivision_id}")
                return None
            
            # è·å–å­å·¥ä½œæµå®ä¾‹ID
            sub_workflow_instance_id = subdivision.get('sub_workflow_instance_id')
            if not sub_workflow_instance_id:
                logger.warning(f"ç»†åˆ†æ²¡æœ‰å…³è”çš„å­å·¥ä½œæµå®ä¾‹: {subdivision_id}")
                return None
            
            # ä»å·¥ä½œæµå®ä¾‹ä»“åº“è·å–å®ä¾‹ä¿¡æ¯
            from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
            workflow_instance_repo = WorkflowInstanceRepository()
            
            workflow_instance = await workflow_instance_repo.get_instance_by_id(
                uuid.UUID(sub_workflow_instance_id)
            )
            
            if workflow_instance:
                logger.info(f"âœ… æ‰¾åˆ°å­å·¥ä½œæµå®ä¾‹: {sub_workflow_instance_id}")
                logger.info(f"   - å®ä¾‹åç§°: {workflow_instance.get('workflow_instance_name')}")
                logger.info(f"   - çŠ¶æ€: {workflow_instance.get('status')}")
                return workflow_instance
            else:
                logger.warning(f"æœªæ‰¾åˆ°å­å·¥ä½œæµå®ä¾‹: {sub_workflow_instance_id}")
                return None
            
        except Exception as e:
            logger.error(f"è·å–ç»†åˆ†å­å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    async def get_workflow_subdivisions(self, workflow_base_id: uuid.UUID) -> WorkflowSubdivisionsResponse:
        """è·å–å·¥ä½œæµç›¸å…³çš„æ‰€æœ‰ç»†åˆ†ï¼ˆç”¨äºé¢„è§ˆï¼‰"""
        try:
            # è·å–å·¥ä½œæµä¿¡æ¯
            workflow = await self.workflow_service.get_workflow_by_base_id(workflow_base_id)
            if not workflow:
                raise ValidationError("å·¥ä½œæµä¸å­˜åœ¨")
            
            # è·å–ç›¸å…³ç»†åˆ†
            subdivisions = await self.subdivision_repo.get_subdivisions_by_workflow(workflow_base_id)
            
            # æ ¼å¼åŒ–å“åº”
            subdivision_previews = []
            for subdivision in subdivisions:
                total_nodes = subdivision.get('total_sub_nodes', 0)
                completed_nodes = subdivision.get('completed_sub_nodes', 0)
                success_rate = (completed_nodes / total_nodes * 100) if total_nodes > 0 else None
                
                preview = SubdivisionPreviewResponse(
                    subdivision_id=subdivision['subdivision_id'],
                    subdivision_name=subdivision['subdivision_name'],
                    subdivider_name=subdivision.get('subdivider_name', 'æœªçŸ¥'),
                    status=TaskSubdivisionStatus(subdivision['status']),
                    sub_workflow_name=subdivision.get('sub_workflow_name', ''),
                    total_nodes=total_nodes,
                    completed_nodes=completed_nodes,
                    success_rate=success_rate,
                    created_at=subdivision['subdivision_created_at'].isoformat(),
                    completed_at=subdivision['completed_at'].isoformat() if subdivision.get('completed_at') else None
                )
                subdivision_previews.append(preview)
            
            return WorkflowSubdivisionsResponse(
                workflow_base_id=workflow_base_id,
                workflow_name=workflow.name,
                subdivisions=subdivision_previews,
                total_count=len(subdivision_previews),
                completed_count=len([s for s in subdivision_previews if s.status == TaskSubdivisionStatus.COMPLETED])
            )
            
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµç»†åˆ†é¢„è§ˆå¤±è´¥: {e}")
            raise
    
    async def adopt_subdivision(self, adoption_data: WorkflowAdoptionCreate) -> WorkflowAdoptionResponse:
        """
        é‡‡çº³å­å·¥ä½œæµåˆ°åŸå§‹å·¥ä½œæµ
        
        Args:
            adoption_data: é‡‡çº³æ•°æ®
            
        Returns:
            é‡‡çº³å“åº”
        """
        try:
            logger.info(f"ğŸ”„ å¼€å§‹é‡‡çº³å­å·¥ä½œæµ: {adoption_data.subdivision_id}")
            
            # 1. éªŒè¯ç»†åˆ†å­˜åœ¨ä¸”å·²å®Œæˆ
            subdivision = await self.subdivision_repo.get_subdivision_by_id(adoption_data.subdivision_id)
            if not subdivision:
                raise ValidationError("ç»†åˆ†ä¸å­˜åœ¨")
            
            if subdivision['status'] != TaskSubdivisionStatus.COMPLETED.value:
                raise ValidationError("åªèƒ½é‡‡çº³å·²å®Œæˆçš„ç»†åˆ†")
            
            # 2. éªŒè¯ç›®æ ‡èŠ‚ç‚¹å­˜åœ¨ä¸”å±äºæŒ‡å®šå·¥ä½œæµ
            target_node = await self.node_service.get_node_by_id(adoption_data.target_node_id)
            if not target_node:
                raise ValidationError("ç›®æ ‡èŠ‚ç‚¹ä¸å­˜åœ¨")
            
            # 3. è·å–å­å·¥ä½œæµçš„èŠ‚ç‚¹å®šä¹‰
            sub_workflow_nodes = await self.node_service.get_workflow_nodes(
                subdivision['sub_workflow_base_id'], adoption_data.adopter_id
            )
            
            # 4. åœ¨ç›®æ ‡èŠ‚ç‚¹ä½ç½®æ·»åŠ å­å·¥ä½œæµçš„èŠ‚ç‚¹
            new_node_ids = await self._add_subdivision_nodes_to_workflow(
                adoption_data.original_workflow_base_id,
                adoption_data.target_node_id,
                sub_workflow_nodes,
                adoption_data.adoption_name,
                adoption_data.adopter_id
            )
            
            # 5. åˆ›å»ºé‡‡çº³è®°å½•
            adoption_record = await self.adoption_repo.create_adoption(adoption_data, new_node_ids)
            if not adoption_record:
                raise ValueError("åˆ›å»ºé‡‡çº³è®°å½•å¤±è´¥")
            
            logger.info(f"âœ… å­å·¥ä½œæµé‡‡çº³æˆåŠŸï¼Œæ–°å¢ {len(new_node_ids)} ä¸ªèŠ‚ç‚¹")
            
            # 6. è¿”å›å“åº”
            return WorkflowAdoptionResponse(
                adoption_id=adoption_record['adoption_id'],
                subdivision_id=adoption_data.subdivision_id,
                subdivision_name=subdivision.get('subdivision_name'),
                adopter_id=adoption_data.adopter_id,
                adopter_name=None,  # å¯ä»¥ä»ç”¨æˆ·ä¿¡æ¯è·å–
                adoption_name=adoption_data.adoption_name,
                target_node_id=adoption_data.target_node_id,
                new_nodes_count=len(new_node_ids),
                adopted_at=adoption_record['adopted_at'].isoformat()
            )
            
        except Exception as e:
            logger.error(f"é‡‡çº³å­å·¥ä½œæµå¤±è´¥: {e}")
            raise
    
    async def _add_subdivision_nodes_to_workflow(self, target_workflow_base_id: uuid.UUID,
                                               target_node_id: uuid.UUID,
                                               sub_nodes: List[Any],
                                               adoption_name: str,
                                               adopter_id: uuid.UUID) -> List[uuid.UUID]:
        """å°†å­å·¥ä½œæµçš„èŠ‚ç‚¹æ·»åŠ åˆ°ç›®æ ‡å·¥ä½œæµ"""
        try:
            # è¿™é‡Œæ˜¯å…·ä½“çš„èŠ‚ç‚¹æ·»åŠ é€»è¾‘
            # å®é™…å®ç°éœ€è¦ï¼š
            # 1. å°†å­å·¥ä½œæµçš„èŠ‚ç‚¹å¤åˆ¶åˆ°ç›®æ ‡å·¥ä½œæµ
            # 2. é‡æ–°æ˜ å°„èŠ‚ç‚¹è¿æ¥å…³ç³»
            # 3. å°†åŸç›®æ ‡èŠ‚ç‚¹æ›¿æ¢ä¸ºèŠ‚ç‚¹ç¾¤
            
            logger.info(f"ğŸ”„ å¼€å§‹æ·»åŠ  {len(sub_nodes)} ä¸ªèŠ‚ç‚¹åˆ°å·¥ä½œæµ")
            
            new_node_ids = []
            
            # ç®€åŒ–å®ç°ï¼šç›´æ¥åœ¨ç›®æ ‡èŠ‚ç‚¹åæ·»åŠ å­å·¥ä½œæµèŠ‚ç‚¹
            for i, sub_node in enumerate(sub_nodes):
                from ..models.node import NodeCreate
                
                new_node_create = NodeCreate(
                    workflow_base_id=target_workflow_base_id,
                    creator_id=adopter_id,
                    name=f"{adoption_name}_{i+1}",
                    type=sub_node.type,
                    task_description=sub_node.task_description,
                    position_x=sub_node.position_x + 100,  # åç§»ä½ç½®
                    position_y=sub_node.position_y + 100
                )
                
                new_node = await self.node_service.create_node(new_node_create, adopter_id)
                new_node_ids.append(new_node.node_base_id)
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(new_node_ids)} ä¸ªèŠ‚ç‚¹")
            return new_node_ids
            
        except Exception as e:
            logger.error(f"æ·»åŠ å­å·¥ä½œæµèŠ‚ç‚¹å¤±è´¥: {e}")
            raise
    
    async def _create_subdivision_nodes_and_connections(self, 
                                                       workflow_base_id: uuid.UUID,
                                                       sub_workflow_data: Dict[str, Any], 
                                                       creator_id: uuid.UUID,
                                                       task_context: str = "") -> None:
        """åˆ›å»ºç»†åˆ†å·¥ä½œæµçš„èŠ‚ç‚¹å’Œè¿æ¥ï¼ˆå¸¦é‡å¤åˆ›å»ºé˜²æŠ¤ï¼‰"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹ä¸ºå·¥ä½œæµ {workflow_base_id} åˆ›å»ºèŠ‚ç‚¹å’Œè¿æ¥")
            logger.info(f"ğŸ“‹ ä»»åŠ¡ä¸Šä¸‹æ–‡æ•°æ®é•¿åº¦: {len(task_context)} å­—ç¬¦")
            
            # ğŸ”§ é˜²æŠ¤æœºåˆ¶1ï¼šæ£€æŸ¥å·¥ä½œæµæ˜¯å¦å·²æœ‰èŠ‚ç‚¹
            existing_nodes_query = "SELECT COUNT(*) as node_count FROM node WHERE workflow_base_id = %s"
            existing_nodes_result = await self.node_service.node_repository.db.fetch_one(
                existing_nodes_query, workflow_base_id
            )
            existing_node_count = existing_nodes_result.get('node_count', 0) if existing_nodes_result else 0
            
            if existing_node_count > 0:
                logger.warning(f"ğŸ›¡ï¸ å·¥ä½œæµ {workflow_base_id} å·²æœ‰ {existing_node_count} ä¸ªèŠ‚ç‚¹ï¼Œè·³è¿‡é‡å¤åˆ›å»º")
                return
            
            # ä»ç»†åˆ†æ•°æ®ä¸­æå–èŠ‚ç‚¹å’Œè¿æ¥ä¿¡æ¯
            nodes_data = sub_workflow_data.get('nodes', [])
            connections_data = sub_workflow_data.get('connections', [])
            
            if not nodes_data:
                logger.warning("æ²¡æœ‰èŠ‚ç‚¹æ•°æ®ï¼Œè·³è¿‡èŠ‚ç‚¹åˆ›å»º")
                return
            
            logger.info(f"ğŸ“¦ å‡†å¤‡åˆ›å»º {len(nodes_data)} ä¸ªèŠ‚ç‚¹å’Œ {len(connections_data)} ä¸ªè¿æ¥")
            
            # 1. åˆ›å»ºèŠ‚ç‚¹
            node_id_mapping = {}  # ç”¨äºæ˜ å°„å‰ç«¯IDåˆ°åç«¯ID
            created_nodes = []
            
            for node_data in nodes_data:
                try:
                    # å¯¼å…¥èŠ‚ç‚¹åˆ›å»ºæ¨¡å‹
                    from ..models.node import NodeCreate
                    
                    # ç‰¹æ®Šå¤„ç†å¼€å§‹èŠ‚ç‚¹ï¼Œå°†ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯æ³¨å…¥åˆ°å¼€å§‹èŠ‚ç‚¹
                    task_description = node_data.get('task_description', '')
                    if node_data.get('type') == 'start' and task_context:
                        # å°†ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯æ³¨å…¥åˆ°å¼€å§‹èŠ‚ç‚¹çš„ä»»åŠ¡æè¿°ä¸­
                        task_description = f"{task_description}\n\n--- ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯ ---\n{task_context}"
                        logger.info(f"âœ… å·²å°†ä»»åŠ¡ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°å¼€å§‹èŠ‚ç‚¹: {node_data.get('name', 'å¼€å§‹èŠ‚ç‚¹')}")
                    
                    # åˆ›å»ºèŠ‚ç‚¹æ•°æ®
                    node_create = NodeCreate(
                        workflow_base_id=workflow_base_id,
                        name=node_data.get('name', 'æœªå‘½åèŠ‚ç‚¹'),
                        type=node_data.get('type', 'processor'),
                        task_description=task_description,
                        position_x=float(node_data.get('position_x', 0)),
                        position_y=float(node_data.get('position_y', 0)),
                        processor_id=node_data.get('processor_id')  # æ·»åŠ processor_id
                    )
                    
                    # è°ƒç”¨èŠ‚ç‚¹æœåŠ¡åˆ›å»ºèŠ‚ç‚¹
                    created_node = await self.node_service.create_node(node_create, creator_id)
                    
                    if created_node:
                        frontend_id = node_data.get('node_base_id') or node_data.get('id')
                        node_id_mapping[frontend_id] = created_node.node_base_id
                        created_nodes.append(created_node)
                        logger.debug(f"   âœ… èŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {created_node.name} ({frontend_id} -> {created_node.node_base_id})")
                    else:
                        logger.error(f"   âŒ èŠ‚ç‚¹åˆ›å»ºå¤±è´¥: {node_data.get('name')}")
                        
                except Exception as e:
                    logger.error(f"åˆ›å»ºèŠ‚ç‚¹å¤±è´¥: {node_data.get('name', 'æœªçŸ¥')}, é”™è¯¯: {e}")
                    # ç»§ç»­åˆ›å»ºå…¶ä»–èŠ‚ç‚¹ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue
            
            logger.info(f"âœ… æˆåŠŸåˆ›å»º {len(created_nodes)} ä¸ªèŠ‚ç‚¹")
            
            # 2. åˆ›å»ºè¿æ¥
            if connections_data and len(created_nodes) > 1:
                created_connections = 0
                
                for connection_data in connections_data:
                    try:
                        # å¯¼å…¥è¿æ¥åˆ›å»ºæ¨¡å‹
                        from ..models.node import NodeConnectionCreate
                        
                        # è·å–æ˜ å°„åçš„èŠ‚ç‚¹ID - ä¿®å¤å­—æ®µååŒ¹é…
                        from_node_frontend_id = connection_data.get('from_node_id') or connection_data.get('from')
                        to_node_frontend_id = connection_data.get('to_node_id') or connection_data.get('to')
                        
                        logger.debug(f"   ğŸ”— å¤„ç†è¿æ¥: {from_node_frontend_id} -> {to_node_frontend_id}")
                        
                        from_node_id = node_id_mapping.get(from_node_frontend_id)
                        to_node_id = node_id_mapping.get(to_node_frontend_id)
                        
                        if not from_node_id or not to_node_id:
                            logger.warning(f"è¿æ¥è·³è¿‡ï¼ŒèŠ‚ç‚¹IDæ˜ å°„å¤±è´¥: {from_node_frontend_id} -> {to_node_frontend_id}")
                            logger.warning(f"   å¯ç”¨æ˜ å°„: {list(node_id_mapping.keys())}")
                            continue
                        
                        # åˆ›å»ºè¿æ¥æ•°æ®
                        connection_create = NodeConnectionCreate(
                            from_node_base_id=from_node_id,
                            to_node_base_id=to_node_id,
                            workflow_base_id=workflow_base_id,
                            connection_type=connection_data.get('connection_type', 'normal')
                        )
                        
                        # è°ƒç”¨èŠ‚ç‚¹æœåŠ¡åˆ›å»ºè¿æ¥
                        created_connection = await self.node_service.create_node_connection(connection_create, creator_id)
                        
                        if created_connection:
                            created_connections += 1
                            logger.debug(f"   âœ… è¿æ¥åˆ›å»ºæˆåŠŸ: {from_node_id} -> {to_node_id}")
                        else:
                            logger.error(f"   âŒ è¿æ¥åˆ›å»ºå¤±è´¥: {from_node_id} -> {to_node_id}")
                            
                    except Exception as e:
                        logger.error(f"åˆ›å»ºè¿æ¥å¤±è´¥: {connection_data}, é”™è¯¯: {e}")
                        # ç»§ç»­åˆ›å»ºå…¶ä»–è¿æ¥
                        continue
                
                logger.info(f"âœ… æˆåŠŸåˆ›å»º {created_connections} ä¸ªè¿æ¥")
            else:
                logger.info("æ²¡æœ‰è¿æ¥æ•°æ®æˆ–èŠ‚ç‚¹ä¸è¶³ï¼Œè·³è¿‡è¿æ¥åˆ›å»º")
            
            logger.info(f"ğŸ‰ å·¥ä½œæµ {workflow_base_id} çš„èŠ‚ç‚¹å’Œè¿æ¥åˆ›å»ºå®Œæˆï¼Œä»»åŠ¡ä¸Šä¸‹æ–‡å·²æ³¨å…¥åˆ°å¼€å§‹èŠ‚ç‚¹")
            
        except Exception as e:
            logger.error(f"åˆ›å»ºç»†åˆ†å·¥ä½œæµèŠ‚ç‚¹å’Œè¿æ¥å¤±è´¥: {e}")
            # è¿™é‡Œå¯ä»¥è€ƒè™‘æ·»åŠ å›æ»šé€»è¾‘ï¼Œåˆ é™¤å·²åˆ›å»ºçš„èŠ‚ç‚¹
            raise
    
    async def _format_subdivision_response(self, subdivision: Dict[str, Any], 
                                         extra_data: Optional[Dict[str, Any]] = None) -> TaskSubdivisionResponse:
        """æ ¼å¼åŒ–ç»†åˆ†å“åº”"""
        extra_data = extra_data or {}
        
        total_nodes = subdivision.get('total_sub_nodes', 0)
        completed_nodes = subdivision.get('completed_sub_nodes', 0)
        
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†sub_workflow_base_idä¸ºNoneçš„æƒ…å†µ
        sub_workflow_base_id = extra_data.get('sub_workflow_base_id') or subdivision.get('sub_workflow_base_id')
        if sub_workflow_base_id is None:
            # å¦‚æœæ²¡æœ‰å·¥ä½œæµIDï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤çš„UUIDï¼ˆè¿™é€šå¸¸ä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†é˜²æŠ¤ï¼‰
            import uuid
            sub_workflow_base_id = uuid.uuid4()
            logger.warning(f"âš ï¸ ç»†åˆ†è®°å½• {subdivision['subdivision_id']} ç¼ºå°‘sub_workflow_base_idï¼Œä½¿ç”¨é»˜è®¤å€¼: {sub_workflow_base_id}")
        
        sub_workflow_instance_id = extra_data.get('sub_workflow_instance_id') or subdivision.get('sub_workflow_instance_id')
        
        return TaskSubdivisionResponse(
            subdivision_id=subdivision['subdivision_id'],
            original_task_id=subdivision['original_task_id'],
            original_task_title=extra_data.get('original_task_title') or subdivision.get('original_task_title'),
            subdivider_id=subdivision['subdivider_id'],
            subdivider_name=subdivision.get('subdivider_name'),
            sub_workflow_base_id=sub_workflow_base_id,
            sub_workflow_instance_id=sub_workflow_instance_id,
            subdivision_name=subdivision['subdivision_name'],
            subdivision_description=subdivision['subdivision_description'],
            status=TaskSubdivisionStatus(subdivision['status']),
            parent_task_description=subdivision.get('parent_task_description', ''),
            context_passed=subdivision.get('context_passed', ''),
            subdivision_created_at=subdivision['subdivision_created_at'].isoformat(),
            completed_at=subdivision['completed_at'].isoformat() if subdivision.get('completed_at') else None,
            sub_workflow_name=extra_data.get('sub_workflow_name') or subdivision.get('sub_workflow_name'),
            total_sub_nodes=total_nodes,
            completed_sub_nodes=completed_nodes
        )
    
    async def _get_parent_workflow_id(self, task_id: uuid.UUID) -> Optional[uuid.UUID]:
        """è·å–ä»»åŠ¡æ‰€å±çš„çˆ¶å·¥ä½œæµå®ä¾‹ID"""
        try:
            # é€šè¿‡ä»»åŠ¡IDè·å–å¯¹åº”çš„å·¥ä½œæµå®ä¾‹ID
            task = await self.task_repo.get_task_by_id(task_id)
            if task:
                return task.get('workflow_instance_id')
            return None
        except Exception as e:
            logger.error(f"è·å–çˆ¶å·¥ä½œæµIDå¤±è´¥: {e}")
            return None


# åˆ›å»ºä»»åŠ¡ç»†åˆ†æœåŠ¡å®ä¾‹
task_subdivision_service = TaskSubdivisionService()