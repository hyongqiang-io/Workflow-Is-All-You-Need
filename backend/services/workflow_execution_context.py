"""
å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†å•ä¸ªå·¥ä½œæµå®ä¾‹çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€çŠ¶æ€å’Œä¾èµ–å…³ç³»
ä¸€ä¸ªå·¥ä½œæµå®ä¾‹å¯¹åº”ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
"""

import uuid
from datetime import datetime
from typing import Dict, List, Any, Set, Optional, Union
import asyncio
import json
from dataclasses import dataclass, field
from loguru import logger

# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
from ..models.instance import WorkflowInstanceStatus, WorkflowInstanceUpdate


@dataclass
class ExecutionRecord:
    """èŠ‚ç‚¹æ‰§è¡Œè®°å½•"""
    execution_id: str  # æ‰§è¡ŒIDï¼Œæ ¼å¼ï¼š{path_id}:{node_instance_id}:{execution_count}
    path_id: str  # æ‰§è¡Œè·¯å¾„ID
    node_instance_id: uuid.UUID  # èŠ‚ç‚¹å®ä¾‹ID
    execution_count: int  # è¯¥èŠ‚ç‚¹çš„æ‰§è¡Œæ¬¡æ•°ï¼ˆä»1å¼€å§‹ï¼‰
    started_at: datetime  # å¼€å§‹æ—¶é—´
    completed_at: Optional[datetime] = None  # å®Œæˆæ—¶é—´
    status: str = "RUNNING"  # çŠ¶æ€ï¼šRUNNING, COMPLETED, FAILED
    input_data: Optional[Dict[str, Any]] = None  # è¾“å…¥æ•°æ®
    output_data: Optional[Dict[str, Any]] = None  # è¾“å‡ºæ•°æ®
    selected_next_nodes: Optional[List[uuid.UUID]] = field(default_factory=list)  # ç”¨æˆ·é€‰æ‹©çš„ä¸‹æ¸¸èŠ‚ç‚¹


@dataclass
class ExecutionPath:
    """æ‰§è¡Œè·¯å¾„"""
    path_id: str  # è·¯å¾„ID
    parent_path_id: Optional[str] = None  # çˆ¶è·¯å¾„IDï¼ˆåˆ†æ”¯æƒ…å†µä¸‹ï¼‰
    workflow_instance_id: uuid.UUID = None  # å·¥ä½œæµå®ä¾‹ID

    # è·¯å¾„çŠ¶æ€
    status: str = "ACTIVE"  # ACTIVE, COMPLETED, FAILED, MERGED
    created_at: datetime = field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None

    # è·¯å¾„æ‰§è¡Œä¿¡æ¯
    execution_sequence: List[str] = field(default_factory=list)  # æ‰§è¡Œè®°å½•IDåºåˆ—
    current_nodes: Set[uuid.UUID] = field(default_factory=set)  # å½“å‰æ‰§è¡Œä¸­çš„èŠ‚ç‚¹
    last_executed_node: Optional[uuid.UUID] = None  # æœ€åæ‰§è¡Œçš„èŠ‚ç‚¹

    # è·¯å¾„æ•°æ®ä¸Šä¸‹æ–‡
    path_context: Dict[str, Any] = field(default_factory=dict)  # è·¯å¾„ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ•°æ®
    accumulated_outputs: Dict[uuid.UUID, Any] = field(default_factory=dict)  # ç´¯ç§¯çš„èŠ‚ç‚¹è¾“å‡º

    # æ¡ä»¶è¾¹æ”¯æŒ
    pending_conditions: Dict[str, Any] = field(default_factory=dict)  # å¾…è¯„ä¼°çš„æ¡ä»¶
    user_selections: Dict[uuid.UUID, List[uuid.UUID]] = field(default_factory=dict)  # ç”¨æˆ·çš„æ¡ä»¶é€‰æ‹©

    # å¾ªç¯æ£€æµ‹
    visited_nodes: Set[uuid.UUID] = field(default_factory=set)  # å·²è®¿é—®çš„èŠ‚ç‚¹ï¼ˆç”¨äºå¾ªç¯æ£€æµ‹ï¼‰
    loop_count: Dict[uuid.UUID, int] = field(default_factory=dict)  # èŠ‚ç‚¹å¾ªç¯è®¡æ•°


def _serialize_for_json(obj):
    """å°†å¯¹è±¡åºåˆ—åŒ–ä¸ºJSONå…¼å®¹æ ¼å¼"""
    if isinstance(obj, uuid.UUID):
        return str(obj)
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, dict):
        result = {}
        for key, value in obj.items():
            serialized_key = str(key) if isinstance(key, uuid.UUID) else key
            serialized_value = _serialize_for_json(value)
            result[serialized_key] = serialized_value
        return result
    elif isinstance(obj, (list, tuple, set)):
        return [_serialize_for_json(item) for item in obj]
    elif hasattr(obj, '__dataclass_fields__'):
        # å¤„ç†dataclasså¯¹è±¡ï¼ˆå¦‚ExecutionPathï¼‰
        from dataclasses import asdict
        return _serialize_for_json(asdict(obj))
    else:
        return obj


class WorkflowExecutionContext:
    """å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨

    ç»Ÿä¸€ç®¡ç†ä¸€ä¸ªå·¥ä½œæµå®ä¾‹çš„ï¼š
    - æ‰§è¡Œä¸Šä¸‹æ–‡æ•°æ®
    - èŠ‚ç‚¹çŠ¶æ€ç®¡ç†
    - ä¾èµ–å…³ç³»ç®¡ç†
    - æ•°æ®æµç®¡ç†
    - æ¡ä»¶è¾¹æ”¯æŒå’Œå¤šè·¯å¾„æ‰§è¡Œ
    """

    def __init__(self, workflow_instance_id: uuid.UUID):
        self.workflow_instance_id = workflow_instance_id

        # é‡æ„åçš„æ‰§è¡Œä¸Šä¸‹æ–‡æ•°æ® - æ”¯æŒå¤šè·¯å¾„æ‰§è¡Œ
        self.execution_context = {
            'global_data': {},

            # å¤šè·¯å¾„æ‰§è¡Œæ”¯æŒ
            'execution_paths': {},  # path_id -> ExecutionPath
            'active_paths': set(),  # æ´»è·ƒè·¯å¾„IDé›†åˆ
            'completed_paths': set(),  # å·²å®Œæˆè·¯å¾„IDé›†åˆ
            'failed_paths': set(),  # å¤±è´¥è·¯å¾„IDé›†åˆ

            # èŠ‚ç‚¹æ‰§è¡Œè®°å½• - æ”¯æŒå¤šæ¬¡æ‰§è¡Œ
            'node_execution_records': {},  # node_instance_id -> [ExecutionRecord]
            'node_outputs_by_path': {},  # path_id -> {node_instance_id -> output_data}

            # å…¼å®¹æ€§å­—æ®µ - ä¿æŒå‘åå…¼å®¹
            'node_outputs': {},  # ä¸»è·¯å¾„çš„node_outputsï¼Œå‘åå…¼å®¹
            'execution_path': [],  # ä¸»è·¯å¾„çš„æ‰§è¡Œè·¯å¾„ï¼Œå‘åå…¼å®¹

            # æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª
            'current_executing_nodes': set(),  # å½“å‰æ‰§è¡Œä¸­çš„èŠ‚ç‚¹å®ä¾‹ID
            'completed_nodes': set(),  # å·²å®Œæˆçš„èŠ‚ç‚¹å®ä¾‹ID
            'failed_nodes': set(),  # å¤±è´¥çš„èŠ‚ç‚¹å®ä¾‹ID

            # ç³»ç»Ÿå­—æ®µ
            'execution_start_time': datetime.utcnow().isoformat(),
            'auto_save_counter': 0,
            'last_snapshot_time': datetime.utcnow().isoformat(),
            'persistence_enabled': True,
            'conditional_edges_enabled': True  # æ¡ä»¶è¾¹åŠŸèƒ½å¼€å…³
        }
        
        # èŠ‚ç‚¹ä¾èµ–å…³ç³»ç®¡ç† - ä½¿ç”¨node_instance_idä½œä¸ºkey
        self.node_dependencies: Dict[uuid.UUID, Dict[str, Any]] = {}
        
        # èŠ‚ç‚¹çŠ¶æ€ç®¡ç†
        self.node_states: Dict[uuid.UUID, str] = {}  # node_instance_id -> state
        
        # å¾…è§¦å‘çš„èŠ‚ç‚¹é˜Ÿåˆ—
        self.pending_triggers: Set[uuid.UUID] = set()
        
        # å¼‚æ­¥é”ç®¡ç†
        self._context_lock = asyncio.Lock()
        
        # å›è°ƒå‡½æ•°æ³¨å†Œ
        self.completion_callbacks: List[callable] = []
        
        logger.debug(f"ğŸ  åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡: {workflow_instance_id}")
    
    async def initialize_context(self, restore_from_snapshot: bool = False):
        """åˆå§‹åŒ–å·¥ä½œæµä¸Šä¸‹æ–‡"""
        async with self._context_lock:
            if restore_from_snapshot:
                # TODO: å®ç°å¿«ç…§æ¢å¤
                pass

            # ğŸ†• åˆå§‹åŒ–ä¸»æ‰§è¡Œè·¯å¾„
            main_path_id = f"main_{self.workflow_instance_id}"
            main_path = ExecutionPath(
                path_id=main_path_id,
                workflow_instance_id=self.workflow_instance_id,
                status="ACTIVE"
            )

            self.execution_context['execution_paths'][main_path_id] = main_path
            self.execution_context['active_paths'].add(main_path_id)
            self.execution_context['node_outputs_by_path'][main_path_id] = {}

            logger.info(f"ğŸ›¤ï¸ åˆå§‹åŒ–ä¸»æ‰§è¡Œè·¯å¾„: {main_path_id}")

            # ğŸ”§ æ–°å¢ï¼šè¯»å–å·¥ä½œæµå®ä¾‹çš„context_data
            try:
                from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
                workflow_repo = WorkflowInstanceRepository()
                workflow_instance = await workflow_repo.get_instance_by_id(self.workflow_instance_id)

                if workflow_instance and workflow_instance.get('context_data'):
                    # è§£æcontext_dataï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
                    context_data = workflow_instance['context_data']
                    if isinstance(context_data, str):
                        import json
                        try:
                            context_data = json.loads(context_data)
                        except json.JSONDecodeError:
                            logger.warning(f"âš ï¸ å·¥ä½œæµå®ä¾‹context_dataä¸æ˜¯æœ‰æ•ˆçš„JSON: {context_data}")
                            context_data = {}

                    # å°†context_dataå­˜å‚¨åˆ°global_dataä¸­
                    self.execution_context['global_data']['workflow_context_data'] = context_data
                    logger.info(f"ğŸ“‹ åŠ è½½å·¥ä½œæµä¸Šä¸‹æ–‡æ•°æ®: {context_data}")
                else:
                    logger.debug(f"ğŸ“‹ å·¥ä½œæµå®ä¾‹æ— context_data: {self.workflow_instance_id}")

            except Exception as e:
                logger.error(f"âŒ åŠ è½½å·¥ä½œæµä¸Šä¸‹æ–‡æ•°æ®å¤±è´¥: {e}")

            # è·å–å¼€å§‹èŠ‚ç‚¹ä¿¡æ¯
            start_node_info = await self._get_start_node_task_descriptions()
            self.execution_context['global_data']['start_node_descriptions'] = start_node_info

            logger.info(f"âœ… å·¥ä½œæµä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ: {self.workflow_instance_id}")
    
    async def register_node_dependencies(self, 
                                       node_instance_id: uuid.UUID,
                                       node_id: uuid.UUID,
                                       upstream_nodes: List[uuid.UUID]):
        """æ³¨å†ŒèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»ï¼ˆä¿®å¤ç‰ˆï¼šä½¿ç”¨node_statesæ£€æŸ¥å®ä¾‹çŠ¶æ€ï¼‰"""
        async with self._context_lock:
            # æ£€æŸ¥å·²å®Œæˆçš„ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ï¼ˆä½¿ç”¨node_statesï¼‰
            completed_upstream = set()
            
            for upstream_node_instance_id in upstream_nodes:
                if self.node_states.get(upstream_node_instance_id) == 'COMPLETED':
                    completed_upstream.add(upstream_node_instance_id)
                    logger.debug(f"  ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {upstream_node_instance_id} å·²å®Œæˆ")
            
            # è®¡ç®—æ˜¯å¦å‡†å¤‡æ‰§è¡Œ
            ready_to_execute = len(completed_upstream) == len(upstream_nodes)
            
            self.node_dependencies[node_instance_id] = {
                'node_id': node_id,
                'workflow_instance_id': self.workflow_instance_id,
                'upstream_nodes': upstream_nodes,
                'completed_upstream': completed_upstream,
                'ready_to_execute': ready_to_execute,
                'dependency_count': len(upstream_nodes)
            }
            
            # åˆå§‹åŒ–èŠ‚ç‚¹çŠ¶æ€ï¼ˆä½†ä¸è¦†ç›–å·²å­˜åœ¨çš„çŠ¶æ€ï¼‰
            if node_instance_id not in self.node_states:
                self.node_states[node_instance_id] = 'PENDING'
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœèŠ‚ç‚¹å‡†å¤‡æ‰§è¡Œä¸”çŠ¶æ€ä¸ºPENDINGï¼Œæ·»åŠ åˆ°pending_triggers
            current_state = self.node_states.get(node_instance_id, 'PENDING')
            
            if (ready_to_execute and 
                current_state == 'PENDING' and 
                node_instance_id not in self.pending_triggers):
                self.pending_triggers.add(node_instance_id)
                logger.debug(f"ğŸš€ [ä¾èµ–æ³¨å†Œ] èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²å‡†å¤‡æ‰§è¡Œï¼Œæ·»åŠ åˆ°å¾…è§¦å‘é˜Ÿåˆ— (çŠ¶æ€: {current_state})")
            
            logger.info(f"ğŸ“‹ [ä¾èµ–æ³¨å†Œ] èŠ‚ç‚¹å®ä¾‹ {node_instance_id}")
            logger.info(f"  - ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹æ€»æ•°: {len(upstream_nodes)}")
            logger.info(f"  - å·²å®Œæˆä¸Šæ¸¸å®ä¾‹: {len(completed_upstream)}")
            logger.info(f"  - å‡†å¤‡æ‰§è¡Œ: {ready_to_execute}")
            logger.info(f"  - å½“å‰ä¾èµ–å­—å…¸å¤§å°: {len(self.node_dependencies)}")
            if upstream_nodes:
                logger.info(f"  - ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹åˆ—è¡¨: {upstream_nodes}")
            if completed_upstream:
                logger.info(f"  - å·²å®Œæˆä¸Šæ¸¸å®ä¾‹åˆ—è¡¨: {list(completed_upstream)}")
    
    async def mark_node_executing(self, node_id: uuid.UUID, node_instance_id: uuid.UUID):
        """æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ"""
        async with self._context_lock:
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿å…³é”®é›†åˆå­—æ®µæ˜¯setç±»å‹
            if not isinstance(self.execution_context.get('current_executing_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤current_executing_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['current_executing_nodes'] = set(self.execution_context.get('current_executing_nodes', []))
            
            self.node_states[node_instance_id] = 'EXECUTING'
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†æ‰§è¡ŒçŠ¶æ€
            self.execution_context['current_executing_nodes'].add(node_instance_id)
            
            logger.trace(f"âš¡ æ ‡è®°èŠ‚ç‚¹å®ä¾‹æ‰§è¡Œ: {node_instance_id} (èŠ‚ç‚¹ID: {node_id})")
    
    async def mark_node_completed(self, node_id: uuid.UUID, node_instance_id: uuid.UUID, output_data: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å®Œæˆ"""
        async with self._context_lock:
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿å…³é”®é›†åˆå­—æ®µæ˜¯setç±»å‹ï¼ˆä¿®å¤JSONæ¢å¤åçš„ç±»å‹é—®é¢˜ï¼‰
            if not isinstance(self.execution_context.get('completed_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤completed_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['completed_nodes'] = set(self.execution_context.get('completed_nodes', []))
            if not isinstance(self.execution_context.get('current_executing_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤current_executing_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['current_executing_nodes'] = set(self.execution_context.get('current_executing_nodes', []))
            if not isinstance(self.execution_context.get('failed_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤failed_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['failed_nodes'] = set(self.execution_context.get('failed_nodes', []))
            
            # é˜²é‡å¤å¤„ç† - æ£€æŸ¥å†…å­˜çŠ¶æ€
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_instance_idæ£€æŸ¥å®ŒæˆçŠ¶æ€ï¼Œå› ä¸ºæˆ‘ä»¬ç®¡ç†çš„æ˜¯å®ä¾‹çŠ¶æ€
            if node_instance_id in self.execution_context['completed_nodes']:
                logger.warning(f"ğŸ”„ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²ç»åœ¨å†…å­˜ä¸­æ ‡è®°ä¸ºå®Œæˆï¼Œè·³è¿‡é‡å¤å¤„ç†")
                return
            
            # é˜²é‡å¤å¤„ç† - æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            try:
                existing_node = await node_repo.get_instance_by_id(node_instance_id)
                if existing_node and existing_node.get('status') == 'completed':
                    logger.warning(f"ğŸ”„ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} åœ¨æ•°æ®åº“ä¸­å·²ç»å®Œæˆï¼ŒåŒæ­¥å†…å­˜çŠ¶æ€")
                    # åŒæ­¥å†…å­˜çŠ¶æ€
                    self.node_states[node_instance_id] = 'COMPLETED'
                    # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†å®ŒæˆçŠ¶æ€
                    self.execution_context['completed_nodes'].add(node_instance_id)
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_instance_idå­˜å‚¨è¾“å‡ºæ•°æ®
                    self.execution_context['node_outputs'][node_instance_id] = output_data
                    self.execution_context['current_executing_nodes'].discard(node_instance_id)
                    return
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥èŠ‚ç‚¹æ•°æ®åº“çŠ¶æ€æ—¶å‡ºé”™: {e}")
            
            # æ›´æ–°çŠ¶æ€
            self.node_states[node_instance_id] = 'COMPLETED'
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†å®ŒæˆçŠ¶æ€ï¼Œä¿æŒä¸€è‡´æ€§
            self.execution_context['completed_nodes'].add(node_instance_id)
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_instance_idä½œä¸ºé”®å­˜å‚¨è¾“å‡ºæ•°æ®ï¼Œè¿™æ ·è·å–ä¸Šä¸‹æ–‡æ—¶èƒ½æ­£ç¡®åŒ¹é…
            self.execution_context['node_outputs'][node_instance_id] = output_data
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡ä¿®å¤] èŠ‚ç‚¹è¾“å‡ºå­˜å‚¨: {node_instance_id} -> {len(str(output_data))}å­—ç¬¦")
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡ä¿®å¤] å½“å‰æ‰€æœ‰è¾“å‡ºé”®: {list(self.execution_context['node_outputs'].keys())}")
            self.execution_context['execution_path'].append(str(node_instance_id))
            
            # ä»æ‰§è¡Œä¸­ç§»é™¤
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†æ‰§è¡ŒçŠ¶æ€
            self.execution_context['current_executing_nodes'].discard(node_instance_id)
            
            logger.info(f"ğŸ‰ èŠ‚ç‚¹å®Œæˆ: {node_id}")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            await self._update_database_node_state(node_instance_id, 'COMPLETED', output_data)

        # ğŸ”§ LinusåŸåˆ™ï¼šç»Ÿä¸€è§¦å‘é€»è¾‘ï¼Œæ¶ˆé™¤é‡å¤çš„è§¦å‘æ–¹æ³•
        triggered_nodes = await self._trigger_downstream_nodes_unified(node_instance_id)

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å·¥ä½œæµæ˜¯å¦å·²å®Œæˆ
        await self._check_workflow_completion()

        # é€šçŸ¥å›è°ƒ
        if triggered_nodes:
            await self._notify_completion_callbacks(triggered_nodes)

        # æ£€æŸ¥å·¥ä½œæµå®Œæˆ
        await self._check_workflow_completion()
    
    async def mark_node_failed(self, node_id: uuid.UUID, node_instance_id: uuid.UUID, error_info: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å¤±è´¥"""
        async with self._context_lock:
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿å…³é”®é›†åˆå­—æ®µæ˜¯setç±»å‹
            if not isinstance(self.execution_context.get('failed_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤failed_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['failed_nodes'] = set(self.execution_context.get('failed_nodes', []))
            if not isinstance(self.execution_context.get('current_executing_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤current_executing_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['current_executing_nodes'] = set(self.execution_context.get('current_executing_nodes', []))
            
            self.node_states[node_instance_id] = 'FAILED'
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†å¤±è´¥çŠ¶æ€ï¼Œä¿æŒä¸€è‡´æ€§
            self.execution_context['failed_nodes'].add(node_instance_id)
            self.execution_context['current_executing_nodes'].discard(node_instance_id)
            
            logger.error(f"âŒ èŠ‚ç‚¹å®ä¾‹å¤±è´¥: {node_instance_id} (èŠ‚ç‚¹ID: {node_id}) - {error_info}")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            await self._update_database_node_state(node_instance_id, 'FAILED', None, error_info)
    
    def is_node_ready_to_execute(self, node_instance_id: uuid.UUID) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ"""
        deps = self.node_dependencies.get(node_instance_id)
        if not deps:
            return False
        
        return deps.get('ready_to_execute', False)
    
    def get_node_state(self, node_instance_id: uuid.UUID) -> str:
        """è·å–èŠ‚ç‚¹çŠ¶æ€"""
        return self.node_states.get(node_instance_id, 'UNKNOWN')
    
    async def get_node_execution_context(self, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹çš„æ‰§è¡Œä¸Šä¸‹æ–‡æ•°æ®"""
        async with self._context_lock:
            # è·å–èŠ‚ç‚¹ä¾èµ–ä¿¡æ¯
            deps = self.node_dependencies.get(node_instance_id)
            if not deps:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ²¡æœ‰ä¾èµ–ä¿¡æ¯")
                return {
                    'immediate_upstream_results': {},
                    'all_upstream_results': {},
                    'workflow_global': {
                        'global_data': self.execution_context.get('global_data', {}),
                        'workflow_instance_id': str(self.workflow_instance_id),
                        'execution_start_time': self.execution_context.get('execution_start_time'),
                        'execution_path': self.execution_context.get('execution_path', [])
                    },
                    'upstream_node_count': 0,
                    'all_upstream_node_count': 0,
                    'current_node': {}
                }
            
            upstream_nodes = deps.get('upstream_nodes', [])
            logger.debug(f"ğŸ” [ä¸Šä¸‹æ–‡æ„å»º] èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æœ‰ {len(upstream_nodes)} ä¸ªç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹")
            
            # æ”¶é›†ç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹çš„è¾“å‡ºæ•°æ®
            immediate_upstream_results = {}
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] å¼€å§‹æ”¶é›†ç›´æ¥ä¸Šæ¸¸è¾“å‡ºï¼Œä¸Šæ¸¸èŠ‚ç‚¹æ•°: {len(upstream_nodes)}")
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹IDs: {upstream_nodes}")
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] å¯ç”¨è¾“å‡ºæ•°æ®é”®: {list(self.execution_context['node_outputs'].keys())}")
            
            for upstream_node_instance_id in upstream_nodes:
                logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] æ£€æŸ¥ä¸Šæ¸¸èŠ‚ç‚¹: {upstream_node_instance_id}")
                if upstream_node_instance_id in self.execution_context['node_outputs']:
                    output_data = self.execution_context['node_outputs'][upstream_node_instance_id]
                    logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] âœ… æ‰¾åˆ°è¾“å‡ºæ•°æ®: {len(str(output_data))}å­—ç¬¦")

                    # ğŸ†• è·å–ä¸Šæ¸¸ä»»åŠ¡çš„æäº¤é™„ä»¶
                    upstream_task_attachments = []
                    try:
                        logger.info(f"ğŸ“ [é™„ä»¶æ”¶é›†] å¼€å§‹ä¸ºä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_instance_id} æ”¶é›†ä»»åŠ¡é™„ä»¶")

                        # é€šè¿‡node_instance_idè·å–task_instance_id
                        from ..repositories.instance.task_instance_repository import TaskInstanceRepository
                        task_repo = TaskInstanceRepository()
                        task_query = "SELECT task_instance_id FROM task_instance WHERE node_instance_id = %s AND is_deleted = FALSE"
                        task_result = await task_repo.db.fetch_one(task_query, upstream_node_instance_id)

                        logger.info(f"ğŸ“ [é™„ä»¶æ”¶é›†] æŸ¥è¯¢ç»“æœ: {task_result}")

                        if task_result:
                            task_instance_id = task_result['task_instance_id']
                            logger.info(f"ğŸ“ [é™„ä»¶æ”¶é›†] æ‰¾åˆ°ä¸Šæ¸¸ä»»åŠ¡: {task_instance_id}")

                            # è·å–ä»»åŠ¡æäº¤çš„é™„ä»¶
                            from ..services.file_association_service import FileAssociationService
                            file_service = FileAssociationService()
                            task_attachments = await file_service.get_task_instance_files(task_instance_id)

                            logger.info(f"ğŸ“ [é™„ä»¶æ”¶é›†] åŸå§‹é™„ä»¶æ•°æ®: {task_attachments}")

                            # æ ¼å¼åŒ–é™„ä»¶ä¿¡æ¯
                            for attachment in task_attachments:
                                upstream_task_attachments.append({
                                    'file_id': str(attachment.get('file_id')),
                                    'filename': attachment.get('filename', ''),
                                    'original_filename': attachment.get('original_filename', ''),
                                    'file_size': attachment.get('file_size', 0),
                                    'content_type': attachment.get('content_type', ''),
                                    'attachment_type': attachment.get('attachment_type', 'input'),
                                    'source': 'task_submission'  # æ ‡è®°æ¥æºä¸ºä»»åŠ¡æäº¤
                                })

                            logger.info(f"ğŸ“ [é™„ä»¶æ”¶é›†] æ”¶é›†åˆ° {len(upstream_task_attachments)} ä¸ªä¸Šæ¸¸ä»»åŠ¡é™„ä»¶")
                        else:
                            logger.info(f"ğŸ“ [é™„ä»¶æ”¶é›†] æœªæ‰¾åˆ°ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_instance_id} å¯¹åº”çš„ä»»åŠ¡å®ä¾‹")

                    except Exception as e:
                        logger.warning(f"âš ï¸ è·å–ä¸Šæ¸¸ä»»åŠ¡é™„ä»¶å¤±è´¥: {e}")
                        import traceback
                        logger.warning(f"âš ï¸ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

                    # ğŸ†• è·å–ä¸Šæ¸¸èŠ‚ç‚¹çš„å®Œæ•´é™„ä»¶ä¿¡æ¯ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹ç»‘å®šå’Œä»»åŠ¡æäº¤ï¼‰
                    upstream_node_attachments = await self._get_node_attachments(upstream_node_instance_id)
                    logger.info(f"ğŸ”§ [immediateä¸Šä¸‹æ–‡] èŠ‚ç‚¹ {upstream_node_instance_id} è·å–åˆ°çš„å®Œæ•´é™„ä»¶: {len(upstream_node_attachments)} ä¸ª")
                    for i, att in enumerate(upstream_node_attachments):
                        logger.debug(f"   é™„ä»¶ #{i+1}: {att.get('filename')} (ç±»å‹: {att.get('association_type')})")

                    # é€šè¿‡upstream_node_instance_idè·å–å¯¹åº”çš„node_idæ¥æŸ¥è¯¢èŠ‚ç‚¹åç§°
                    upstream_deps = self.node_dependencies.get(upstream_node_instance_id)
                    if upstream_deps:
                        upstream_node_id = upstream_deps.get('node_id')
                        node_name = await self._get_node_name_by_id(upstream_node_id) if upstream_node_id else None
                    else:
                        node_name = None

                    upstream_key = node_name or f'èŠ‚ç‚¹å®ä¾‹_{str(upstream_node_instance_id)[:8]}'
                    immediate_upstream_results[upstream_key] = {
                        'node_instance_id': str(upstream_node_instance_id),
                        'node_name': node_name or f'èŠ‚ç‚¹å®ä¾‹_{str(upstream_node_instance_id)[:8]}',
                        'output_data': output_data,
                        'task_attachments': upstream_task_attachments,  # ğŸ†• æ·»åŠ ä»»åŠ¡é™„ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
                        'attachments': upstream_node_attachments,  # ğŸ†• æ·»åŠ å®Œæ•´çš„èŠ‚ç‚¹é™„ä»¶ä¿¡æ¯
                        'status': 'completed'
                    }
                    logger.info(f"  âœ… æ·»åŠ ä¸Šæ¸¸è¾“å‡º: {upstream_key} -> {len(str(output_data))}å­—ç¬¦, {len(upstream_task_attachments)}ä¸ªä»»åŠ¡é™„ä»¶, {len(upstream_node_attachments)}ä¸ªæ€»é™„ä»¶")
                    logger.info(f"ğŸ”§ [immediateä¸Šä¸‹æ–‡] æœ€ç»ˆå­˜å‚¨åˆ°immediate_upstream_resultsçš„é™„ä»¶æ•°é‡: {len(immediate_upstream_results[upstream_key]['attachments'])}")
                else:
                    logger.warning(f"  âš ï¸ ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {upstream_node_instance_id} çš„è¾“å‡ºæ•°æ®ä¸å­˜åœ¨")
                    logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] âŒ æœªæ‰¾åˆ° {upstream_node_instance_id} çš„è¾“å‡ºæ•°æ®")
            
            # ğŸ†• æ”¶é›†æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹ï¼ˆå…¨å±€ä¸Šä¸‹æ–‡ï¼‰
            all_upstream_results = await self._collect_all_upstream_results(node_instance_id)
            logger.debug(f"ğŸŒ [å…¨å±€ä¸Šä¸‹æ–‡] ä¸ºèŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ”¶é›†äº† {len(all_upstream_results)} ä¸ªå…¨å±€ä¸Šæ¸¸ç»“æœ")
            
            # å½“å‰èŠ‚ç‚¹ä¿¡æ¯
            current_node_name = await self._get_node_name_by_id(deps.get('node_id'))
            
            context_data = {
                'immediate_upstream_results': immediate_upstream_results,
                'all_upstream_results': all_upstream_results,  # ğŸ†• æ–°å¢å…¨å±€ä¸Šæ¸¸ç»“æœ
                'workflow_global': {
                    'global_data': self.execution_context.get('global_data', {}),
                    'workflow_instance_id': str(self.workflow_instance_id),
                    'execution_start_time': self.execution_context.get('execution_start_time'),
                    'execution_path': self.execution_context.get('execution_path', [])
                },
                'upstream_node_count': len(upstream_nodes),
                'all_upstream_node_count': len(all_upstream_results),  # ğŸ†• æ–°å¢å…¨å±€ä¸Šæ¸¸è®¡æ•°
                'current_node': {
                    'node_instance_id': str(node_instance_id),
                    'node_id': str(deps.get('node_id')),
                    'node_name': current_node_name,
                    'status': self.get_node_state(node_instance_id)
                }
            }
            
            logger.debug(f"âœ… [ä¸Šä¸‹æ–‡æ„å»º] ä¸ºèŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ„å»ºäº†åŒ…å« {len(immediate_upstream_results)} ä¸ªç›´æ¥ä¸Šæ¸¸å’Œ {len(all_upstream_results)} ä¸ªå…¨å±€ä¸Šæ¸¸ç»“æœçš„ä¸Šä¸‹æ–‡")
            return context_data
    
    async def _check_and_trigger_downstream_nodes(self, completed_node_instance_id: uuid.UUID) -> List[uuid.UUID]:
        """æ£€æŸ¥å¹¶è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆï¼šé˜²æ­¢ç«æ€å’Œé‡å¤è§¦å‘ï¼Œç»Ÿä¸€ä½¿ç”¨node_instance_idï¼‰"""
        triggered_nodes = []
        
        # ğŸ”’ ä½¿ç”¨é”ä¿æŠ¤æ•´ä¸ªæ£€æŸ¥å’Œè§¦å‘è¿‡ç¨‹ï¼Œé˜²æ­¢ç«æ€æ¡ä»¶
        async with self._context_lock:
            logger.info(f"ğŸ” [ä¸‹æ¸¸è§¦å‘] æ£€æŸ¥å®ŒæˆèŠ‚ç‚¹å®ä¾‹ {completed_node_instance_id} çš„ä¸‹æ¸¸ä¾èµ–...")
            logger.info(f"   - å½“å‰æ³¨å†Œçš„èŠ‚ç‚¹ä¾èµ–æ•°é‡: {len(self.node_dependencies)}")
            logger.info(f"   - ä¾èµ–å­—å…¸çš„keyåˆ—è¡¨: {list(self.node_dependencies.keys())}")
            
            for node_instance_id, deps in self.node_dependencies.items():
                logger.info(f"   - æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹ {node_instance_id}ï¼Œä¸Šæ¸¸ä¾èµ–: {deps['upstream_nodes']}")
                logger.info(f"     - èŠ‚ç‚¹çŠ¶æ€: {self.node_states.get(node_instance_id, 'UNKNOWN')}")
                logger.info(f"     - å·²å®Œæˆä¸Šæ¸¸: {len(deps.get('completed_upstream', set()))}/{len(deps['upstream_nodes'])}")
                
                # ğŸ”§ ä¿®å¤ï¼šUUIDç±»å‹ä¸€è‡´æ€§æ¯”è¾ƒï¼Œå…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
                completed_node_str = str(completed_node_instance_id)
                upstream_nodes_str = [str(x) for x in deps['upstream_nodes']]
                
                if completed_node_str in upstream_nodes_str:
                    logger.info(f"âœ… [ä¸‹æ¸¸è§¦å‘] æ‰¾åˆ°ä¸‹æ¸¸èŠ‚ç‚¹å®ä¾‹: {node_instance_id}")
                    logger.info(f"   - å½“å‰çŠ¶æ€: {self.node_states.get(node_instance_id, 'UNKNOWN')}")
                    logger.info(f"   - ä¾èµ–çŠ¶æ€: {len(deps.get('completed_upstream', set()))}/{len(deps['upstream_nodes'])}")
                    
                    # ğŸ”’ å…ˆæ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²ç»è¢«è§¦å‘æˆ–æ­£åœ¨æ‰§è¡Œ
                    if deps.get('ready_to_execute', False):
                        logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²ç»è¢«æ ‡è®°ä¸ºå‡†å¤‡æ‰§è¡Œï¼Œè·³è¿‡")
                        continue
                        
                    # æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹æ˜¯å¦æ­£åœ¨æ‰§è¡Œï¼ˆä½¿ç”¨node_statesæ£€æŸ¥ï¼‰
                    if self.node_states.get(node_instance_id) == 'EXECUTING':
                        logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡")
                        continue
                        
                    # æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹æ˜¯å¦å·²å®Œæˆï¼ˆä½¿ç”¨node_statesæ£€æŸ¥å®ä¾‹çº§åˆ«çŠ¶æ€ï¼‰
                    if self.node_states.get(node_instance_id) == 'COMPLETED':
                        logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²å®Œæˆï¼Œè·³è¿‡")
                        continue
                    
                    # æ ‡è®°ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹å®Œæˆ
                    if completed_node_instance_id not in deps['completed_upstream']:
                        deps['completed_upstream'].add(completed_node_instance_id)
                        logger.info(f"  âœ… æ ‡è®°ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {completed_node_instance_id} ä¸ºå·²å®Œæˆ")
                    else:
                        logger.debug(f"  â„¹ï¸ ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {completed_node_instance_id} å·²ç»æ ‡è®°ä¸ºå®Œæˆ")
                    
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿å®Œæˆçš„èŠ‚ç‚¹çŠ¶æ€ç«‹å³æ›´æ–°
                    if self.node_states.get(completed_node_instance_id) != 'COMPLETED':
                        self.node_states[completed_node_instance_id] = 'COMPLETED'
                        logger.info(f"  ğŸ”§ å¼ºåˆ¶åŒæ­¥èŠ‚ç‚¹çŠ¶æ€: {completed_node_instance_id} -> COMPLETED")
                    
                    # ä¸¥æ ¼æ£€æŸ¥æ‰€æœ‰ä¸Šæ¸¸æ˜¯å¦éƒ½å®Œæˆ
                    total_upstream = len(deps['upstream_nodes'])
                    completed_upstream = len(deps['completed_upstream'])
                    
                    logger.info(f"  ğŸ“Š èŠ‚ç‚¹å®ä¾‹ {node_instance_id} ä¾èµ–çŠ¶æ€: {completed_upstream}/{total_upstream}")
                    
                    # åªæœ‰å½“æ‰€æœ‰ä¸Šæ¸¸éƒ½å®Œæˆæ—¶æ‰è§¦å‘
                    if completed_upstream == total_upstream and total_upstream > 0:
                        # ğŸ”§ ä¿®å¤ï¼šç®€åŒ–æ£€æŸ¥é€»è¾‘ï¼Œç›´æ¥åŸºäºcompleted_upstreamé›†åˆ
                        # å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ä¸Šé¢å¼ºåˆ¶åŒæ­¥äº†çŠ¶æ€ï¼Œä¸éœ€è¦åŒé‡æ£€æŸ¥
                        all_upstream_completed_verified = True
                        logger.info(f"  âœ… æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹å·²å®Œæˆï¼Œå‡†å¤‡è§¦å‘: {node_instance_id}")
                        
                        if all_upstream_completed_verified:
                            # é˜²æ­¢é‡å¤è§¦å‘çš„æœ€ç»ˆæ£€æŸ¥
                            if node_instance_id not in self.pending_triggers:
                                deps['ready_to_execute'] = True
                                self.pending_triggers.add(node_instance_id)
                                triggered_nodes.append(node_instance_id)
                                
                                logger.debug(f"ğŸš€ è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹å®ä¾‹: {node_instance_id} (ä¾èµ–å·²å…¨éƒ¨æ»¡è¶³: {deps['upstream_nodes']})")
                            else:
                                logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²åœ¨pending_triggersä¸­ï¼Œé¿å…é‡å¤è§¦å‘")
                    else:
                        logger.trace(f"  â³ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} ä¾èµ–æœªæ»¡è¶³ï¼Œç­‰å¾…æ›´å¤šä¸Šæ¸¸å®Œæˆ")
                else:
                    # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°ä¸‹æ¸¸èŠ‚ç‚¹
                    logger.info(f"   â¡ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} ä¸ä¾èµ–äºå®ŒæˆèŠ‚ç‚¹ {completed_node_instance_id}")
                    logger.info(f"      ä¸Šæ¸¸ä¾èµ–ç±»å‹æ£€æŸ¥: {[type(x) for x in deps['upstream_nodes']]}")
                    logger.info(f"      å®ŒæˆèŠ‚ç‚¹ç±»å‹: {type(completed_node_instance_id)}")
                    logger.info(f"      UUIDæ¯”è¾ƒç»“æœ: {[str(x) == str(completed_node_instance_id) for x in deps['upstream_nodes']]}")
            
            logger.info(f"ğŸ¯ [ä¸‹æ¸¸è§¦å‘] è§¦å‘æ£€æŸ¥å®Œæˆï¼Œå…±è§¦å‘ {len(triggered_nodes)} ä¸ªä¸‹æ¸¸èŠ‚ç‚¹å®ä¾‹")
            if triggered_nodes:
                logger.info(f"   - è§¦å‘çš„èŠ‚ç‚¹å®ä¾‹: {triggered_nodes}")
            else:
                logger.info(f"   - åŸå› åˆ†æ: å¯èƒ½æ˜¯ä¾èµ–æœªå®Œå…¨æ»¡è¶³ï¼Œæˆ–èŠ‚ç‚¹å·²å¤„ç†ï¼Œæˆ–æ²¡æœ‰ä¸‹æ¸¸èŠ‚ç‚¹")
        
        return triggered_nodes
    
    async def scan_and_trigger_ready_nodes(self) -> List[uuid.UUID]:
        """æ‰«æå¹¶è§¦å‘æ‰€æœ‰å‡†å¤‡å¥½æ‰§è¡Œçš„èŠ‚ç‚¹ï¼ˆç”¨äºä¸Šä¸‹æ–‡æ¢å¤åä¸»åŠ¨æ‰«æï¼‰"""
        async with self._context_lock:
            logger.info(f"ğŸ” [ä¸»åŠ¨æ‰«æ] å¼€å§‹æ‰«æå‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹...")
            logger.info(f"   - å½“å‰èŠ‚ç‚¹ä¾èµ–æ•°é‡: {len(self.node_dependencies)}")
            logger.info(f"   - pending_triggersä¸­çš„èŠ‚ç‚¹: {len(self.pending_triggers)}")
            
            # ğŸ”§ ä¿®å¤Bad Tasteï¼šç›´æ¥è¿”å›pending_triggersä¸­çš„èŠ‚ç‚¹ï¼Œè¿™äº›å°±æ˜¯å‡†å¤‡æ‰§è¡Œçš„
            # ä¸è¦é‡å¤æ‰«æå’Œæ·»åŠ é€»è¾‘ï¼Œpending_triggerså°±æ˜¯æˆ‘ä»¬çš„"ready nodes"é˜Ÿåˆ—
            ready_nodes = list(self.pending_triggers)
            
            # é¢å¤–æ‰«æå…¶ä»–å¯èƒ½é—æ¼çš„å‡†å¤‡æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆæ²¡åœ¨pending_triggersä¸­çš„ï¼‰
            for node_instance_id, deps in self.node_dependencies.items():
                node_state = self.node_states.get(node_instance_id, 'UNKNOWN')
                ready_status = deps.get('ready_to_execute', False)
                
                logger.debug(f"   æ£€æŸ¥èŠ‚ç‚¹ {node_instance_id}: çŠ¶æ€={node_state}, å‡†å¤‡æ‰§è¡Œ={ready_status}")
                
                # å¦‚æœèŠ‚ç‚¹å‡†å¤‡å¥½ä½†ä¸åœ¨pending_triggersä¸­ï¼Œæ·»åŠ è¿›å»
                if (ready_status and 
                    node_state == 'PENDING' and 
                    node_instance_id not in self.pending_triggers):
                    
                    self.pending_triggers.add(node_instance_id)
                    ready_nodes.append(node_instance_id)
                    logger.info(f"ğŸ” [é—æ¼å‘ç°] æ·»åŠ å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹: {node_instance_id}")
            
            logger.info(f"âœ… [ä¸»åŠ¨æ‰«æ] å®Œæˆï¼Œå…±å‘ç° {len(ready_nodes)} ä¸ªå‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹")
            if ready_nodes:
                logger.info(f"   - å‡†å¤‡æ‰§è¡ŒèŠ‚ç‚¹: {ready_nodes}")
            
            return ready_nodes

    async def get_ready_nodes(self) -> List[uuid.UUID]:
        """è·å–å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆï¼šä¸»åŠ¨æ‰«æå‡†å¤‡å¥½çš„èŠ‚ç‚¹ï¼‰"""
        async with self._context_lock:
            # 1. å…ˆè·å–pending_triggersä¸­çš„èŠ‚ç‚¹
            ready_nodes = list(self.pending_triggers)
            self.pending_triggers.clear()
            
            # 2. ğŸ”§ ä¿®å¤ï¼šä¸»åŠ¨æ‰«ææ‰€æœ‰ä¾èµ–å…³ç³»ï¼Œæ‰¾å‡ºå‡†å¤‡æ‰§è¡Œä½†æœªåœ¨pending_triggersä¸­çš„èŠ‚ç‚¹
            for node_instance_id, deps in self.node_dependencies.items():
                node_state = self.node_states.get(node_instance_id, 'UNKNOWN')
                
                # ğŸ”§ ä¿®å¤ï¼šç°åœ¨çŠ¶æ€éƒ½æ˜¯å¤§å†™çš„ï¼Œç®€åŒ–æ£€æŸ¥
                if (deps.get('ready_to_execute', False) and 
                    node_instance_id not in ready_nodes and
                    node_state == 'PENDING'):
                    
                    ready_nodes.append(node_instance_id)
                    logger.debug(f"ğŸ” [ä¸»åŠ¨æ‰«æ] å‘ç°å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹: {node_instance_id} (çŠ¶æ€: {node_state})")
            
            if ready_nodes:
                logger.info(f"ğŸš€ [å‡†å¤‡æ‰§è¡Œ] å…±å‘ç° {len(ready_nodes)} ä¸ªå‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹: {ready_nodes}")
            else:
                logger.trace(f"â³ [å‡†å¤‡æ‰§è¡Œ] æš‚æ— å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹")
                # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ‰€æœ‰èŠ‚ç‚¹çš„å‡†å¤‡çŠ¶æ€
                for node_instance_id, deps in self.node_dependencies.items():
                    node_state = self.node_states.get(node_instance_id, 'UNKNOWN')
                    ready_status = deps.get('ready_to_execute', False)
                    logger.trace(f"   - èŠ‚ç‚¹ {node_instance_id}: çŠ¶æ€={node_state}, å‡†å¤‡æ‰§è¡Œ={ready_status}")
            
            return ready_nodes
    
    async def build_node_context(self, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ„å»ºèŠ‚ç‚¹æ‰§è¡Œä¸Šä¸‹æ–‡"""
        deps = self.node_dependencies.get(node_instance_id, {})
        upstream_nodes = deps.get('upstream_nodes', [])
        
        # æ”¶é›†ä¸Šæ¸¸è¾“å‡º
        upstream_context = {}
        for upstream_node_instance_id in upstream_nodes:
            if upstream_node_instance_id in self.execution_context['node_outputs']:
                # é€šè¿‡upstream_node_instance_idè·å–å¯¹åº”çš„node_idæ¥æŸ¥è¯¢èŠ‚ç‚¹åç§°
                upstream_deps = self.node_dependencies.get(upstream_node_instance_id)
                if upstream_deps:
                    upstream_node_id = upstream_deps.get('node_id')
                    node_name = await self._get_node_name_by_id(upstream_node_id) if upstream_node_id else None
                else:
                    node_name = None
                    
                key = node_name or str(upstream_node_instance_id)
                upstream_context[key] = {
                    'node_instance_id': str(upstream_node_instance_id),
                    'output_data': self.execution_context['node_outputs'][upstream_node_instance_id],
                    'status': 'completed'
                }
        
        return {
            'node_instance_id': str(node_instance_id),
            'upstream_outputs': upstream_context,
            'workflow_context': {
                'workflow_instance_id': str(self.workflow_instance_id),
                'execution_start_time': self.execution_context['execution_start_time'],
                'execution_path': self.execution_context['execution_path'],
                'global_data': self.execution_context['global_data']
            },
            'context_built_at': datetime.utcnow().isoformat()
        }
    
    async def get_workflow_status(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        total_nodes = len(self.node_dependencies)  # åŸºäºnode_instance_idçš„æ€»æ•°

        # ğŸ†• å¦‚æœå¯ç”¨äº†æ¡ä»¶è¾¹ï¼Œä½¿ç”¨åŸºäºè·¯å¾„çš„çŠ¶æ€åˆ¤æ–­
        if self.execution_context.get('conditional_edges_enabled', True):
            is_completed = await self.is_workflow_completed()

            # è·å–è·¯å¾„ç»Ÿè®¡ä¿¡æ¯
            active_paths = len(self.execution_context.get('active_paths', set()))
            completed_paths = len(self.execution_context.get('completed_paths', set()))
            failed_paths = len(self.execution_context.get('failed_paths', set()))

            # ç»Ÿè®¡èŠ‚ç‚¹å®ä¾‹çŠ¶æ€ï¼ˆç”¨äºè¯¦ç»†ä¿¡æ¯ï¼‰
            completed_node_instances = len([nid for nid, state in self.node_states.items() if state == 'COMPLETED'])
            failed_node_instances = len([nid for nid, state in self.node_states.items() if state == 'FAILED'])
            executing_node_instances = len([nid for nid, state in self.node_states.items() if state == 'EXECUTING'])

            logger.debug(f"ğŸ“Š [è·¯å¾„çŠ¶æ€è®¡ç®—] å·¥ä½œæµçŠ¶æ€ç»Ÿè®¡:")
            logger.debug(f"   - æ´»è·ƒè·¯å¾„: {active_paths}, å®Œæˆè·¯å¾„: {completed_paths}, å¤±è´¥è·¯å¾„: {failed_paths}")
            logger.debug(f"   - æ€»èŠ‚ç‚¹å®ä¾‹: {total_nodes}")
            logger.debug(f"   - å·²å®ŒæˆèŠ‚ç‚¹å®ä¾‹: {completed_node_instances}")
            logger.debug(f"   - æ‰§è¡Œä¸­èŠ‚ç‚¹å®ä¾‹: {executing_node_instances}")
            logger.debug(f"   - å¤±è´¥èŠ‚ç‚¹å®ä¾‹: {failed_node_instances}")

            # åŸºäºè·¯å¾„çŠ¶æ€ç¡®å®šå·¥ä½œæµçŠ¶æ€
            if failed_paths > 0:
                status = 'FAILED'
            elif is_completed:
                status = 'COMPLETED'
            elif active_paths > 0:
                status = 'RUNNING'
            else:
                status = 'UNKNOWN'

            logger.debug(f"ğŸ“Š [è·¯å¾„çŠ¶æ€è®¡ç®—] æœ€ç»ˆçŠ¶æ€: {status}")

            return {
                'status': status,
                'total_nodes': total_nodes,
                'completed_nodes': completed_node_instances,
                'failed_nodes': failed_node_instances,
                'executing_nodes': executing_node_instances,
                'active_paths': active_paths,
                'completed_paths': completed_paths,
                'failed_paths': failed_paths,
                'is_path_based': True
            }

        # ä¼ ç»Ÿçš„åŸºäºèŠ‚ç‚¹è®¡æ•°çš„é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
        else:
            # ç»Ÿè®¡å·²å®Œæˆçš„èŠ‚ç‚¹å®ä¾‹æ•°é‡ï¼ˆé€šè¿‡node_statesæ£€æŸ¥ï¼‰
            completed_node_instances = len([nid for nid, state in self.node_states.items() if state == 'COMPLETED'])
            failed_node_instances = len([nid for nid, state in self.node_states.items() if state == 'FAILED'])
            executing_node_instances = len([nid for nid, state in self.node_states.items() if state == 'EXECUTING'])

            # ä¹Ÿä¿ç•™åŸæœ‰çš„node_idçº§åˆ«çš„ç»Ÿè®¡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            completed_nodes = len(self.execution_context['completed_nodes'])
            failed_nodes = len(self.execution_context['failed_nodes'])
            executing_nodes = len(self.execution_context['current_executing_nodes'])

            logger.debug(f"ğŸ“Š [ä¼ ç»ŸçŠ¶æ€è®¡ç®—] å·¥ä½œæµçŠ¶æ€ç»Ÿè®¡:")
            logger.debug(f"   - æ€»èŠ‚ç‚¹å®ä¾‹: {total_nodes}")
            logger.debug(f"   - å·²å®ŒæˆèŠ‚ç‚¹å®ä¾‹: {completed_node_instances}")
            logger.debug(f"   - æ‰§è¡Œä¸­èŠ‚ç‚¹å®ä¾‹: {executing_node_instances}")
            logger.debug(f"   - å¤±è´¥èŠ‚ç‚¹å®ä¾‹: {failed_node_instances}")
            logger.debug(f"   - å·²å®ŒæˆèŠ‚ç‚¹(node_id): {completed_nodes}")

            if failed_node_instances > 0:
                status = 'FAILED'
            elif completed_node_instances == total_nodes and total_nodes > 0:
                status = 'COMPLETED'
            elif executing_node_instances > 0 or (total_nodes - completed_node_instances - failed_node_instances) > 0:
                status = 'RUNNING'
            else:
                status = 'UNKNOWN'

            logger.debug(f"ğŸ“Š [ä¼ ç»ŸçŠ¶æ€è®¡ç®—] æœ€ç»ˆçŠ¶æ€: {status}")

            return {
                'status': status,
                'total_nodes': total_nodes,
                'completed_nodes': completed_node_instances,  # ä½¿ç”¨èŠ‚ç‚¹å®ä¾‹çº§åˆ«çš„ç»Ÿè®¡
                'failed_nodes': failed_node_instances,
                'executing_nodes': executing_node_instances,
                'pending_nodes': total_nodes - completed_node_instances - failed_node_instances - executing_node_instances,
                # ä¿ç•™åŸæœ‰å­—æ®µç”¨äºå…¼å®¹æ€§
                'completed_nodes_by_id': completed_nodes,
                'failed_nodes_by_id': failed_nodes,
                'executing_nodes_by_id': executing_nodes,
                'is_path_based': False
            }
    
    def register_completion_callback(self, callback: callable):
        """æ³¨å†Œå®Œæˆå›è°ƒ"""
        self.completion_callbacks.append(callback)
    
    async def _notify_completion_callbacks(self, triggered_nodes: List[uuid.UUID]):
        """é€šçŸ¥å›è°ƒå‡½æ•°"""
        logger.debug(f"ğŸ”” [å›è°ƒé€šçŸ¥] å¼€å§‹é€šçŸ¥ {len(self.completion_callbacks)} ä¸ªå›è°ƒå‡½æ•°")
        logger.debug(f"   - å·¥ä½œæµID: {self.workflow_instance_id}")
        logger.debug(f"   - è§¦å‘çš„èŠ‚ç‚¹: {triggered_nodes}")
        
        for i, callback in enumerate(self.completion_callbacks):
            callback_name = getattr(callback, '__name__', f'callback_{i}')
            try:
                logger.debug(f"ğŸ”” [å›è°ƒé€šçŸ¥] æ‰§è¡Œå›è°ƒ #{i+1}: {callback_name}")
                if asyncio.iscoroutinefunction(callback):
                    await callback(self.workflow_instance_id, triggered_nodes)
                else:
                    callback(self.workflow_instance_id, triggered_nodes)
                logger.debug(f"âœ… [å›è°ƒé€šçŸ¥] å›è°ƒ #{i+1} æ‰§è¡ŒæˆåŠŸ: {callback_name}")
            except Exception as e:
                logger.error(f"âŒ [å›è°ƒé€šçŸ¥] å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {callback_name}")
                logger.error(f"   - é”™è¯¯: {e}")
                import traceback
                logger.error(f"   - å †æ ˆ: {traceback.format_exc()}")
        
        logger.debug(f"ğŸ”” [å›è°ƒé€šçŸ¥] æ‰€æœ‰å›è°ƒé€šçŸ¥å®Œæˆ")
    
    async def _check_workflow_completion(self):
        """æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å®Œæˆ"""
        status_info = await self.get_workflow_status()
        if status_info['status'] in ['COMPLETED', 'FAILED']:
            logger.info(f"ğŸ å·¥ä½œæµ {self.workflow_instance_id} æ‰§è¡Œå®Œæˆ: {status_info['status']}")
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„å·¥ä½œæµçŠ¶æ€
            try:
                from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
                from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus
                from ..utils.helpers import now_utc
                
                workflow_repo = WorkflowInstanceRepository()
                
                # ç¡®å®šæœ€ç»ˆçŠ¶æ€
                final_status = WorkflowInstanceStatus.COMPLETED if status_info['status'] == 'COMPLETED' else WorkflowInstanceStatus.FAILED
                
                # æ›´æ–°å·¥ä½œæµå®ä¾‹çŠ¶æ€
                update_data = WorkflowInstanceUpdate(
                    status=final_status,
                    completed_at=now_utc() if final_status == WorkflowInstanceStatus.COMPLETED else None,
                    error_message=status_info.get('error_message') if final_status == WorkflowInstanceStatus.FAILED else None
                )
                
                result = await workflow_repo.update_instance(self.workflow_instance_id, update_data)
                if result:
                    logger.info(f"âœ… å·¥ä½œæµçŠ¶æ€å·²æ›´æ–°åˆ°æ•°æ®åº“: {self.workflow_instance_id} -> {final_status.value}")
                else:
                    logger.error(f"âŒ å·¥ä½œæµçŠ¶æ€æ›´æ–°å¤±è´¥: {self.workflow_instance_id}")
                    
            except Exception as e:
                logger.error(f"âŒ æ›´æ–°å·¥ä½œæµçŠ¶æ€åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                import traceback
                logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _update_database_node_state(self, node_instance_id: uuid.UUID, 
                                        state: str, output_data: Optional[Dict[str, Any]] = None,
                                        error_info: Optional[Dict[str, Any]] = None):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„èŠ‚ç‚¹çŠ¶æ€"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
            
            node_repo = NodeInstanceRepository()
            
            # å…ˆæ£€æŸ¥èŠ‚ç‚¹å®ä¾‹æ˜¯å¦å­˜åœ¨
            existing_node = await node_repo.get_instance_by_id(node_instance_id)
            if not existing_node:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨ï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°: {node_instance_id}")
                return
            
            status_mapping = {
                'COMPLETED': NodeInstanceStatus.COMPLETED,
                'FAILED': NodeInstanceStatus.FAILED,
                'EXECUTING': NodeInstanceStatus.RUNNING,
                'PENDING': NodeInstanceStatus.PENDING
            }
            
            db_status = status_mapping.get(state, NodeInstanceStatus.PENDING)
            
            # å‡†å¤‡è¾“å‡ºæ•°æ® - ç›´æ¥ä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œä¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            output_data_dict = output_data if output_data else None
            
            update_data = NodeInstanceUpdate(
                status=db_status,
                output_data=output_data_dict,
                error_message=error_info.get('message') if error_info else None
            )
            
            result = await node_repo.update_node_instance(node_instance_id, update_data)
            if result:
                logger.debug(f"âœ… æ•°æ®åº“çŠ¶æ€æ›´æ–°æˆåŠŸ: {node_instance_id} -> {state}")
            else:
                logger.warning(f"âš ï¸ æ•°æ®åº“çŠ¶æ€æ›´æ–°è¿”å›ç©ºç»“æœ: {node_instance_id} -> {state}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“çŠ¶æ€æ›´æ–°å¤±è´¥: {node_instance_id} -> {state}: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æµç¨‹ç»§ç»­
    
    async def _get_start_node_task_descriptions(self) -> Dict[str, Any]:
        """è·å–å¼€å§‹èŠ‚ç‚¹ä»»åŠ¡æè¿°"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            
            node_instance_repo = NodeInstanceRepository()
            
            query = """
                SELECT ni.node_instance_id, ni.node_id, n.name as node_name,
                       n.task_description, ni.task_description as instance_task_description
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.workflow_instance_id = $1
                AND LOWER(n.type) = 'start'
                AND ni.is_deleted = FALSE
                AND n.is_deleted = FALSE
                ORDER BY ni.created_at ASC
            """
            
            start_nodes = await node_instance_repo.db.fetch_all(query, self.workflow_instance_id)
            
            start_node_info = {}
            for node in start_nodes:
                node_id = str(node['node_id'])
                task_description = (
                    node.get('instance_task_description') or 
                    node.get('task_description') or 
                    f"å¼€å§‹èŠ‚ç‚¹ {node.get('node_name', 'æœªå‘½å')} çš„ä»»åŠ¡"
                )
                
                start_node_info[node_id] = {
                    'node_instance_id': str(node['node_instance_id']),
                    'node_name': node.get('node_name', 'æœªå‘½å'),
                    'task_description': task_description
                }
            
            return start_node_info
            
        except Exception as e:
            logger.error(f"è·å–å¼€å§‹èŠ‚ç‚¹ä»»åŠ¡æè¿°å¤±è´¥: {e}")
            return {}
    
    async def _get_node_name_by_id(self, node_id: uuid.UUID) -> str:
        """æ ¹æ®node_idè·å–èŠ‚ç‚¹åç§°"""
        try:
            from ..repositories.node.node_repository import NodeRepository
            node_repo = NodeRepository()
            
            query = "SELECT name FROM node WHERE node_id = $1"
            result = await node_repo.db.fetch_one(query, node_id)
            return result['name'] if result else None
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹åç§°å¤±è´¥: {e}")
            return None

    async def _collect_all_upstream_results(self, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ”¶é›†æŒ‡å®šèŠ‚ç‚¹çš„æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹ç»“æœï¼ˆå…¨å±€ä¸Šä¸‹æ–‡ï¼‰
        
        ä½¿ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢æ”¶é›†ä»å·¥ä½œæµå¼€å§‹åˆ°å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰å·²å®ŒæˆèŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®å’Œé™„ä»¶
        """
        all_upstream = {}
        visited = set()
        
        async def dfs_collect(current_node_instance_id: uuid.UUID):
            """æ·±åº¦ä¼˜å…ˆæœç´¢æ”¶é›†ä¸Šæ¸¸èŠ‚ç‚¹æ•°æ®"""
            if current_node_instance_id in visited:
                return
            
            visited.add(current_node_instance_id)
            deps = self.node_dependencies.get(current_node_instance_id)
            
            if not deps:
                return
            
            # é€’å½’æ”¶é›†ä¸Šæ¸¸èŠ‚ç‚¹çš„ä¸Šæ¸¸
            for upstream_node_instance_id in deps.get('upstream_nodes', []):
                await dfs_collect(upstream_node_instance_id)
                
                # å¦‚æœä¸Šæ¸¸èŠ‚ç‚¹æœ‰è¾“å‡ºæ•°æ®ï¼Œæ”¶é›†å®ƒ
                if upstream_node_instance_id in self.execution_context['node_outputs']:
                    output_data = self.execution_context['node_outputs'][upstream_node_instance_id]
                    
                    # è·å–èŠ‚ç‚¹åç§°
                    upstream_deps = self.node_dependencies.get(upstream_node_instance_id)
                    if upstream_deps:
                        upstream_node_id = upstream_deps.get('node_id')
                        node_name = await self._get_node_name_by_id(upstream_node_id) if upstream_node_id else None
                    else:
                        node_name = None
                    
                    # ğŸ†• æ”¶é›†èŠ‚ç‚¹ç›¸å…³çš„é™„ä»¶
                    node_attachments = await self._get_node_attachments(upstream_node_instance_id)
                    logger.info(f"ğŸŒ [å…¨å±€æ”¶é›†è°ƒè¯•] èŠ‚ç‚¹ {upstream_node_instance_id} æ”¶é›†åˆ° {len(node_attachments)} ä¸ªé™„ä»¶")
                    for i, att in enumerate(node_attachments):
                        logger.debug(f"   å…¨å±€é™„ä»¶ #{i+1}: {att.get('filename')} (ç±»å‹: {att.get('association_type')})")

                    # ä½¿ç”¨èŠ‚ç‚¹å®ä¾‹IDä½œä¸ºå”¯ä¸€é”®ï¼Œé¿å…åŒåèŠ‚ç‚¹å†²çª
                    unique_key = f"{str(upstream_node_instance_id)[:8]}_{node_name or 'unknown'}"
                    all_upstream[unique_key] = {
                        'node_instance_id': str(upstream_node_instance_id),
                        'node_id': str(upstream_deps.get('node_id')) if upstream_deps else None,
                        'node_name': node_name or f'èŠ‚ç‚¹å®ä¾‹_{str(upstream_node_instance_id)[:8]}',
                        'output_data': output_data,
                        'status': 'completed',
                        'completed_at': self.execution_context.get('node_completion_times', {}).get(str(upstream_node_instance_id)),
                        'execution_order': len(all_upstream) + 1,  # æŒ‰å‘ç°é¡ºåºç¼–å·
                        'attachments': node_attachments  # ğŸ†• èŠ‚ç‚¹ç›¸å…³é™„ä»¶
                    }
                    logger.info(f"ğŸŒ [å…¨å±€æ”¶é›†è°ƒè¯•] å­˜å‚¨åˆ°all_upstream[{unique_key}]çš„é™„ä»¶æ•°é‡: {len(all_upstream[unique_key]['attachments'])}")
                    logger.debug(f"ğŸŒ [å…¨å±€æ”¶é›†] æ·»åŠ ä¸Šæ¸¸èŠ‚ç‚¹: {unique_key} -> è¾“å‡º:{len(str(output_data))}å­—ç¬¦, é™„ä»¶:{len(node_attachments)}ä¸ª")
        
        # ä»å½“å‰èŠ‚ç‚¹å¼€å§‹æ”¶é›†æ‰€æœ‰ä¸Šæ¸¸
        await dfs_collect(node_instance_id)
        
        logger.debug(f"ğŸŒ [å…¨å±€æ”¶é›†å®Œæˆ] ä¸ºèŠ‚ç‚¹ {node_instance_id} æ”¶é›†äº† {len(all_upstream)} ä¸ªå…¨å±€ä¸Šæ¸¸èŠ‚ç‚¹")
        return all_upstream

    async def _get_node_attachments(self, node_instance_id: uuid.UUID) -> List[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹å®ä¾‹ç›¸å…³çš„æ‰€æœ‰é™„ä»¶ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹ç»‘å®šé™„ä»¶å’Œä»»åŠ¡æäº¤é™„ä»¶ï¼‰"""
        try:
            attachments = []
            logger.info(f"ğŸ”— [èŠ‚ç‚¹é™„ä»¶æ”¶é›†] å¼€å§‹ä¸ºèŠ‚ç‚¹ {node_instance_id} æ”¶é›†æ‰€æœ‰é™„ä»¶")

            # å¯¼å…¥æ–‡ä»¶å…³è”æœåŠ¡
            from ..services.file_association_service import FileAssociationService
            file_service = FileAssociationService()

            # è·å–èŠ‚ç‚¹ç»‘å®šçš„é™„ä»¶
            try:
                node_files = await file_service.get_node_instance_files(node_instance_id)
                logger.info(f"ğŸ”— [èŠ‚ç‚¹é™„ä»¶æ”¶é›†] è·å–åˆ° {len(node_files)} ä¸ªèŠ‚ç‚¹ç»‘å®šé™„ä»¶")
                for i, file_info in enumerate(node_files):
                    attachments.append({
                        'file_id': file_info['file_id'],
                        'filename': file_info['original_filename'],
                        'file_size': file_info['file_size'],
                        'content_type': file_info['content_type'],
                        'created_at': file_info['created_at'],
                        'association_type': 'node_binding',
                        'association_id': str(node_instance_id)
                    })
                    logger.debug(f"   - èŠ‚ç‚¹ç»‘å®šé™„ä»¶ #{i+1}: {file_info['original_filename']}")
            except Exception as e:
                logger.warning(f"è·å–èŠ‚ç‚¹ {node_instance_id} ç»‘å®šé™„ä»¶å¤±è´¥: {e}")

            # ğŸ†• è·å–è¯¥èŠ‚ç‚¹æ‰§è¡Œçš„ä»»åŠ¡æäº¤çš„é™„ä»¶
            try:
                # æŸ¥è¯¢è¯¥èŠ‚ç‚¹å®ä¾‹å¯¹åº”çš„ä»»åŠ¡
                from ..repositories.instance.task_instance_repository import TaskInstanceRepository
                task_repo = TaskInstanceRepository()

                # è·å–èŠ‚ç‚¹çš„ä»»åŠ¡
                tasks = await task_repo.get_tasks_by_node_instance(node_instance_id)
                logger.info(f"ğŸ”— [èŠ‚ç‚¹é™„ä»¶æ”¶é›†] è¯¥èŠ‚ç‚¹æœ‰ {len(tasks)} ä¸ªä»»åŠ¡")

                for task_idx, task in enumerate(tasks):
                    task_id = task.get('task_instance_id')
                    if task_id:
                        logger.info(f"ğŸ”— [èŠ‚ç‚¹é™„ä»¶æ”¶é›†] æ£€æŸ¥ä»»åŠ¡ #{task_idx+1}: {task_id}")
                        # è·å–ä»»åŠ¡æäº¤çš„é™„ä»¶
                        task_files = await file_service.get_task_instance_files(task_id)
                        logger.info(f"ğŸ”— [èŠ‚ç‚¹é™„ä»¶æ”¶é›†] ä»»åŠ¡ {task_id} æœ‰ {len(task_files)} ä¸ªæäº¤é™„ä»¶")

                        for file_idx, file_info in enumerate(task_files):
                            attachments.append({
                                'file_id': file_info['file_id'],
                                'filename': file_info['original_filename'],
                                'file_size': file_info['file_size'],
                                'content_type': file_info['content_type'],
                                'created_at': file_info['created_at'],
                                'association_type': 'task_submission',
                                'association_id': str(task_id),
                                'task_title': task.get('task_title', 'æœªçŸ¥ä»»åŠ¡')
                            })
                            logger.debug(f"   - ä»»åŠ¡æäº¤é™„ä»¶ #{file_idx+1}: {file_info['original_filename']} (æ¥è‡ªä»»åŠ¡: {task.get('task_title', 'æœªçŸ¥ä»»åŠ¡')})")
            except Exception as e:
                logger.warning(f"è·å–èŠ‚ç‚¹ {node_instance_id} ä»»åŠ¡é™„ä»¶å¤±è´¥: {e}")
                import traceback
                logger.warning(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

            logger.info(f"ğŸ”— [èŠ‚ç‚¹é™„ä»¶æ”¶é›†] èŠ‚ç‚¹ {node_instance_id} å…±æ”¶é›†åˆ° {len(attachments)} ä¸ªé™„ä»¶")
            return attachments

        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹é™„ä»¶å¤±è´¥: {e}")
            return []
    
    def cleanup(self):
        """æ¸…ç†ä¸Šä¸‹æ–‡èµ„æº"""
        logger.info(f"ğŸ§¹ æ¸…ç†å·¥ä½œæµä¸Šä¸‹æ–‡: {self.workflow_instance_id}")
        self.execution_context.clear()
        self.node_dependencies.clear()
        self.node_states.clear()
        self.pending_triggers.clear()
        self.completion_callbacks.clear()

    # ==================== ğŸ†• æ¡ä»¶è¾¹å’Œå¤šè·¯å¾„æ‰§è¡Œæ”¯æŒ ====================

    async def create_execution_path(self, parent_path_id: Optional[str] = None) -> str:
        """åˆ›å»ºæ–°çš„æ‰§è¡Œè·¯å¾„ï¼ˆç”¨äºåˆ†æ”¯ï¼‰"""
        async with self._context_lock:
            path_id = f"path_{uuid.uuid4().hex[:8]}_{len(self.execution_context['execution_paths'])}"

            new_path = ExecutionPath(
                path_id=path_id,
                parent_path_id=parent_path_id,
                workflow_instance_id=self.workflow_instance_id,
                status="ACTIVE"
            )

            # å¦‚æœæœ‰çˆ¶è·¯å¾„ï¼Œç»§æ‰¿å…¶ä¸Šä¸‹æ–‡
            if parent_path_id and parent_path_id in self.execution_context['execution_paths']:
                parent_path = self.execution_context['execution_paths'][parent_path_id]
                new_path.path_context = parent_path.path_context.copy()
                new_path.accumulated_outputs = parent_path.accumulated_outputs.copy()
                logger.info(f"ğŸŒ¿ åˆ›å»ºåˆ†æ”¯è·¯å¾„: {path_id} (çˆ¶è·¯å¾„: {parent_path_id})")
            else:
                logger.info(f"ğŸ›¤ï¸ åˆ›å»ºç‹¬ç«‹è·¯å¾„: {path_id}")

            self.execution_context['execution_paths'][path_id] = new_path
            self.execution_context['active_paths'].add(path_id)
            self.execution_context['node_outputs_by_path'][path_id] = {}

            return path_id

    async def record_node_execution(self, path_id: str, node_instance_id: uuid.UUID,
                                  input_data: Dict[str, Any],
                                  selected_next_nodes: Optional[List[uuid.UUID]] = None) -> ExecutionRecord:
        """è®°å½•èŠ‚ç‚¹æ‰§è¡Œ"""
        async with self._context_lock:
            if path_id not in self.execution_context['execution_paths']:
                raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {path_id}")

            path = self.execution_context['execution_paths'][path_id]

            # è®¡ç®—æ‰§è¡Œæ¬¡æ•°
            existing_records = self.execution_context['node_execution_records'].get(node_instance_id, [])
            execution_count = len([r for r in existing_records if r.path_id == path_id]) + 1

            # åˆ›å»ºæ‰§è¡Œè®°å½•
            execution_id = f"{path_id}:{node_instance_id}:{execution_count}"
            record = ExecutionRecord(
                execution_id=execution_id,
                path_id=path_id,
                node_instance_id=node_instance_id,
                execution_count=execution_count,
                started_at=datetime.utcnow(),
                input_data=input_data,
                selected_next_nodes=selected_next_nodes or []
            )

            # å­˜å‚¨è®°å½•
            if node_instance_id not in self.execution_context['node_execution_records']:
                self.execution_context['node_execution_records'][node_instance_id] = []
            self.execution_context['node_execution_records'][node_instance_id].append(record)

            # æ›´æ–°è·¯å¾„çŠ¶æ€
            path.execution_sequence.append(execution_id)
            path.current_nodes.add(node_instance_id)
            path.last_executed_node = node_instance_id

            # å¾ªç¯æ£€æµ‹
            if node_instance_id in path.visited_nodes:
                path.loop_count[node_instance_id] = path.loop_count.get(node_instance_id, 0) + 1
                logger.warning(f"ğŸ”„ æ£€æµ‹åˆ°èŠ‚ç‚¹å¾ªç¯: {node_instance_id} (ç¬¬{path.loop_count[node_instance_id]}æ¬¡)")
            else:
                path.visited_nodes.add(node_instance_id)

            logger.info(f"ğŸ“ è®°å½•èŠ‚ç‚¹æ‰§è¡Œ: {execution_id}")
            return record

    async def complete_node_execution(self, path_id: str, node_instance_id: uuid.UUID,
                                    output_data: Dict[str, Any], status: str = "COMPLETED") -> bool:
        """å®ŒæˆèŠ‚ç‚¹æ‰§è¡Œï¼Œè®°å½•è¾“å‡ºæ•°æ®"""
        async with self._context_lock:
            if path_id not in self.execution_context['execution_paths']:
                logger.error(f"è·¯å¾„ä¸å­˜åœ¨: {path_id}")
                return False

            path = self.execution_context['execution_paths'][path_id]

            # æŸ¥æ‰¾æœ€æ–°çš„æ‰§è¡Œè®°å½•
            records = self.execution_context['node_execution_records'].get(node_instance_id, [])
            latest_record = None
            for record in reversed(records):
                if record.path_id == path_id and record.status == "RUNNING":
                    latest_record = record
                    break

            if not latest_record:
                logger.error(f"æœªæ‰¾åˆ°è¿è¡Œä¸­çš„æ‰§è¡Œè®°å½•: {path_id}:{node_instance_id}")
                return False

            # æ›´æ–°è®°å½•
            latest_record.completed_at = datetime.utcnow()
            latest_record.output_data = output_data
            latest_record.status = status

            # æ›´æ–°è·¯å¾„æ•°æ®
            path.current_nodes.discard(node_instance_id)
            path.accumulated_outputs[node_instance_id] = output_data

            # å­˜å‚¨åˆ°è·¯å¾„ç‰¹å®šçš„è¾“å‡ºä¸­
            self.execution_context['node_outputs_by_path'][path_id][node_instance_id] = output_data

            # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯ä¸»è·¯å¾„ï¼Œä¹Ÿæ›´æ–°ä¼ ç»Ÿçš„node_outputs
            main_path_id = f"main_{self.workflow_instance_id}"
            if path_id == main_path_id:
                self.execution_context['node_outputs'][node_instance_id] = output_data

            logger.info(f"âœ… å®ŒæˆèŠ‚ç‚¹æ‰§è¡Œ: {latest_record.execution_id} (çŠ¶æ€: {status})")
            return True

    async def evaluate_conditional_edges(self, path_id: str, node_instance_id: uuid.UUID,
                                       connections: List[Dict[str, Any]]) -> List[uuid.UUID]:
        """è¯„ä¼°æ¡ä»¶è¾¹ï¼Œè¿”å›åº”è¯¥æ¿€æ´»çš„ä¸‹æ¸¸èŠ‚ç‚¹"""
        async with self._context_lock:
            if path_id not in self.execution_context['execution_paths']:
                return []

            path = self.execution_context['execution_paths'][path_id]
            activated_nodes = []

            # è·å–èŠ‚ç‚¹è¾“å‡ºæ•°æ®ç”¨äºæ¡ä»¶è¯„ä¼°
            node_output = path.accumulated_outputs.get(node_instance_id, {})

            for connection in connections:
                to_node_base_id = connection.get('to_node_base_id')
                condition_config = connection.get('condition_config', {})

                # ğŸ”§ LinusåŸåˆ™ï¼šæ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ
                # æ‰€æœ‰è¾¹éƒ½æ˜¯æ¡ä»¶è¾¹ï¼Œå›ºå®šè¾¹å°±æ˜¯æ¡ä»¶æ°¸è¿œä¸ºtrueçš„è¾¹
                should_activate = await self._evaluate_condition(condition_config, node_output, path)
                if should_activate:
                    activated_nodes.append(to_node_base_id)
                    logger.debug(f"âœ… è¾¹æ¿€æ´»: {node_instance_id} -> {to_node_base_id}")
                else:
                    logger.debug(f"âŒ è¾¹æœªæ¿€æ´»: {node_instance_id} -> {to_node_base_id}")

            return activated_nodes

    async def _evaluate_condition(self, condition_config: Dict[str, Any],
                                node_output: Dict[str, Any],
                                path: ExecutionPath) -> bool:
        """è¯„ä¼°å•ä¸ªæ¡ä»¶ï¼ˆä½¿ç”¨æ¡ä»¶è¯„ä¼°å¼•æ“ï¼‰"""
        if not condition_config:
            return True  # ç©ºæ¡ä»¶é»˜è®¤ä¸ºtrue

        # ä½¿ç”¨æ¡ä»¶è¯„ä¼°å¼•æ“
        from .condition_evaluation_engine import get_condition_engine

        engine = get_condition_engine()

        # æ„å»ºè¯„ä¼°ä¸Šä¸‹æ–‡
        context = {
            'node_output': node_output,
            'path_data': path.path_context,
            'global_data': self.execution_context['global_data'],
            'accumulated_outputs': path.accumulated_outputs,
            'user_selections': path.user_selections,
            'current_node_id': path.last_executed_node
        }

        return await engine.evaluate_condition(condition_config, context)

    async def _trigger_downstream_nodes_unified(self, completed_node_instance_id: uuid.UUID) -> List[uuid.UUID]:
        """
        ç»Ÿä¸€çš„ä¸‹æ¸¸èŠ‚ç‚¹è§¦å‘é€»è¾‘ - LinusåŸåˆ™ï¼šæ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ

        å”¯ä¸€çš„è§¦å‘æ–¹æ³•ï¼šåŸºäºä¾èµ–æ»¡è¶³æ£€æŸ¥
        ä¸å†åŒºåˆ†å›ºå®šè¾¹ã€æ¡ä»¶è¾¹ã€å›ç¯ç­‰ç‰¹æ®Šæƒ…å†µ
        """
        triggered_nodes = []

        async with self._context_lock:
            logger.info(f"ğŸ” [ç»Ÿä¸€è§¦å‘] æ£€æŸ¥å®ŒæˆèŠ‚ç‚¹ {completed_node_instance_id} çš„ä¸‹æ¸¸ä¾èµ–...")

            # æ ¸å¿ƒé€»è¾‘ï¼šéå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³
            for node_instance_id, deps in self.node_dependencies.items():
                # UUIDç±»å‹ä¸€è‡´æ€§æ¯”è¾ƒ
                completed_node_str = str(completed_node_instance_id)
                upstream_nodes_str = [str(x) for x in deps['upstream_nodes']]

                if completed_node_str in upstream_nodes_str:
                    logger.debug(f"âœ… [ç»Ÿä¸€è§¦å‘] å‘ç°ä¾èµ–å…³ç³»: {node_instance_id} ä¾èµ–äº {completed_node_instance_id}")

                    # æ ‡è®°ä¸Šæ¸¸èŠ‚ç‚¹å®Œæˆ
                    deps['completed_upstream'].add(completed_node_instance_id)

                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä¸Šæ¸¸ä¾èµ–éƒ½å®Œæˆ
                    all_upstream_completed = len(deps['completed_upstream']) >= len(deps['upstream_nodes'])

                    if all_upstream_completed and self.node_states.get(node_instance_id) == 'PENDING':
                        # ğŸ”§ ç®€åŒ–ï¼šåªè¦ä¾èµ–æ»¡è¶³å°±è§¦å‘ï¼Œæ¡ä»¶è¯„ä¼°åœ¨å®é™…æ‰§è¡Œæ—¶è¿›è¡Œ
                        self.node_states[node_instance_id] = 'READY'
                        triggered_nodes.append(node_instance_id)
                        logger.info(f"ğŸš€ [ç»Ÿä¸€è§¦å‘] è§¦å‘èŠ‚ç‚¹: {node_instance_id}")

            logger.info(f"âœ… [ç»Ÿä¸€è§¦å‘] å…±è§¦å‘ {len(triggered_nodes)} ä¸ªèŠ‚ç‚¹")

        return triggered_nodes

    # ğŸ—‘ï¸ åºŸå¼ƒçš„æ–¹æ³• - å·²è¢«ç»Ÿä¸€çš„è§¦å‘é€»è¾‘æ›¿ä»£
    # ä¿ç•™ä»¥é˜²å‘åå…¼å®¹éœ€è¦ï¼Œä½†ä¸æ¨èä½¿ç”¨

    async def _check_and_trigger_downstream_nodes_with_conditions(self, completed_node_instance_id: uuid.UUID) -> List[uuid.UUID]:
        """æ£€æŸ¥å¹¶è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆæ”¯æŒæ¡ä»¶è¾¹å’Œå›ç¯ï¼‰"""
        triggered_nodes = []

        async with self._context_lock:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ‰§è¡Œè·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°åŸå§‹é€»è¾‘
            main_path_id = f"main_{self.workflow_instance_id}"
            if main_path_id not in self.execution_context.get('execution_paths', {}):
                logger.warning(f"ä¸»è·¯å¾„ä¸å­˜åœ¨ï¼Œå›é€€åˆ°åŸå§‹è§¦å‘é€»è¾‘: {main_path_id}")
                return await self._check_and_trigger_downstream_nodes(completed_node_instance_id)

            # è·å–å®ŒæˆèŠ‚ç‚¹çš„ä¿¡æ¯
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            node_instance = await node_repo.get_instance_by_id(completed_node_instance_id)
            if not node_instance:
                logger.error(f"èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨: {completed_node_instance_id}")
                return []

            node_id = node_instance['node_id']

            # è·å–èŠ‚ç‚¹çš„ä¸‹æ¸¸è¿æ¥ï¼ˆåŒ…å«æ¡ä»¶é…ç½®ï¼‰
            from ..services.execution_service import ExecutionEngine
            execution_engine = ExecutionEngine()
            connections = await execution_engine._get_next_nodes(node_id)

            if not connections:
                logger.debug(f"èŠ‚ç‚¹ {node_id} æ²¡æœ‰ä¸‹æ¸¸è¿æ¥")
                return []

            # è¯„ä¼°æ¡ä»¶è¾¹ï¼Œè·å–åº”è¯¥æ¿€æ´»çš„ä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆæ”¯æŒå›ç¯ï¼‰
            activated_results = await self.handle_conditional_edges_with_loops(main_path_id, completed_node_instance_id, connections)

            logger.info(f"ğŸ”— æ¡ä»¶è¾¹è¯„ä¼°å®Œæˆ: {len(activated_results)} ä¸ªä¸‹æ¸¸ç›®æ ‡è¢«æ¿€æ´»")

            # ğŸ†• "å¥½å“å‘³"é‡æ„ï¼šç»Ÿä¸€å¤„ç†æ™®é€šèŠ‚ç‚¹å’Œå›ç¯èŠ‚ç‚¹
            for activated_target in activated_results:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°åˆ›å»ºçš„å›ç¯èŠ‚ç‚¹å®ä¾‹ï¼ˆUUIDæ ¼å¼ï¼‰
                if isinstance(activated_target, uuid.UUID):
                    # å›ç¯èŠ‚ç‚¹ï¼šç›´æ¥è§¦å‘ï¼ˆä¾èµ–å…³ç³»å·²åœ¨åˆ›å»ºæ—¶æ³¨å†Œï¼‰
                    if activated_target in self.node_dependencies:
                        if activated_target not in self.pending_triggers:
                            self.node_dependencies[activated_target]['ready_to_execute'] = True
                            self.pending_triggers.add(activated_target)
                            triggered_nodes.append(activated_target)
                            logger.info(f"ğŸ”„ ç›´æ¥è§¦å‘å›ç¯èŠ‚ç‚¹å®ä¾‹: {activated_target}")
                        else:
                            logger.debug(f"å›ç¯èŠ‚ç‚¹å®ä¾‹ {activated_target} å·²åœ¨å¾…è§¦å‘é˜Ÿåˆ—ä¸­")
                    else:
                        logger.warning(f"âš ï¸ å›ç¯èŠ‚ç‚¹å®ä¾‹ {activated_target} ä¾èµ–å…³ç³»æœªæ³¨å†Œ")
                else:
                    # æ™®é€šèŠ‚ç‚¹ï¼šæŸ¥æ‰¾ç°æœ‰å®ä¾‹å¹¶æ£€æŸ¥ä¾èµ–
                    await self._trigger_normal_downstream_node(activated_target, connections, completed_node_instance_id, triggered_nodes)

            logger.info(f"ğŸ¯ æ¡ä»¶è¾¹è§¦å‘å®Œæˆï¼Œå…±è§¦å‘ {len(triggered_nodes)} ä¸ªä¸‹æ¸¸èŠ‚ç‚¹")

        return triggered_nodes

    async def _trigger_normal_downstream_node(self, to_node_base_id: uuid.UUID, connections: List[Dict[str, Any]],
                                           completed_node_instance_id: uuid.UUID, triggered_nodes: List[uuid.UUID]):
        """è§¦å‘æ™®é€šä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆéå›ç¯ï¼‰"""
        # æŸ¥æ‰¾å¯¹åº”çš„è¿æ¥ä¿¡æ¯
        to_node_id = None
        for connection in connections:
            if connection['to_node_base_id'] == to_node_base_id:
                to_node_id = connection['to_node_id']
                break

        if not to_node_id:
            logger.warning(f"æœªæ‰¾åˆ°èŠ‚ç‚¹ {to_node_base_id} çš„è¿æ¥ä¿¡æ¯")
            return

        # æŸ¥æ‰¾å¯¹åº”çš„èŠ‚ç‚¹å®ä¾‹
        downstream_node_instance_id = None
        for node_instance_id, deps in self.node_dependencies.items():
            if deps.get('node_id') == to_node_id:
                downstream_node_instance_id = node_instance_id
                break

        if not downstream_node_instance_id:
            logger.warning(f"æœªæ‰¾åˆ°èŠ‚ç‚¹ {to_node_id} çš„å®ä¾‹")
            return

        deps = self.node_dependencies[downstream_node_instance_id]

        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        current_state = self.node_states.get(downstream_node_instance_id, 'PENDING')
        if current_state in ['EXECUTING', 'COMPLETED']:
            logger.debug(f"èŠ‚ç‚¹å®ä¾‹ {downstream_node_instance_id} çŠ¶æ€ä¸º {current_state}ï¼Œè·³è¿‡")
            return

        # æ ‡è®°ä¸Šæ¸¸èŠ‚ç‚¹å®Œæˆ
        if completed_node_instance_id not in deps['completed_upstream']:
            deps['completed_upstream'].add(completed_node_instance_id)

        # æ£€æŸ¥æ‰€æœ‰ä¸Šæ¸¸ä¾èµ–æ˜¯å¦æ»¡è¶³
        total_upstream = len(deps['upstream_nodes'])
        completed_upstream = len(deps['completed_upstream'])

        if completed_upstream == total_upstream and total_upstream > 0:
            if downstream_node_instance_id not in self.pending_triggers:
                deps['ready_to_execute'] = True
                self.pending_triggers.add(downstream_node_instance_id)
                triggered_nodes.append(downstream_node_instance_id)

                logger.info(f"ğŸš€ æ¡ä»¶è¾¹è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹: {downstream_node_instance_id}")

    async def handle_loop_back_execution(self, path_id: str, loop_back_node_base_id: uuid.UUID,
                                       loop_trigger_node_instance_id: uuid.UUID) -> uuid.UUID:
        """å¤„ç†å›ç¯æ‰§è¡Œï¼Œåˆ›å»ºæ–°çš„èŠ‚ç‚¹å®ä¾‹"""
        async with self._context_lock:
            if path_id not in self.execution_context['execution_paths']:
                raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {path_id}")

            path = self.execution_context['execution_paths'][path_id]

            # æ£€æŸ¥å¾ªç¯é™åˆ¶
            loop_count = path.loop_count.get(loop_back_node_base_id, 0)
            max_loops = 10  # æœ€å¤§å¾ªç¯æ¬¡æ•°é™åˆ¶

            if loop_count >= max_loops:
                logger.error(f"âŒ èŠ‚ç‚¹ {loop_back_node_base_id} å¾ªç¯æ¬¡æ•°è¶…é™: {loop_count} >= {max_loops}")
                raise ValueError(f"èŠ‚ç‚¹å¾ªç¯æ¬¡æ•°è¶…é™: {loop_count}")

            # æ›´æ–°å¾ªç¯è®¡æ•°
            path.loop_count[loop_back_node_base_id] = loop_count + 1

            # ğŸ†• åˆ›å»ºæ–°çš„èŠ‚ç‚¹å®ä¾‹æ¥å¤„ç†å›ç¯
            new_node_instance_id = await self._create_loop_node_instance(
                loop_back_node_base_id,
                path_id,
                loop_count + 1,
                loop_trigger_node_instance_id
            )

            logger.info(f"ğŸ”„ åˆ›å»ºå›ç¯èŠ‚ç‚¹å®ä¾‹: {new_node_instance_id} (ç¬¬{loop_count + 1}æ¬¡å¾ªç¯)")

            return new_node_instance_id

    async def _create_loop_node_instance(self, node_base_id: uuid.UUID, path_id: str,
                                       loop_iteration: int, trigger_node_instance_id: uuid.UUID) -> uuid.UUID:
        """ä¸ºå›ç¯åˆ›å»ºæ–°çš„èŠ‚ç‚¹å®ä¾‹"""
        try:
            # è·å–åŸå§‹èŠ‚ç‚¹ä¿¡æ¯
            from ..repositories.node.node_repository import NodeRepository
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceCreate, NodeInstanceStatus

            node_repo = NodeRepository()
            node_instance_repo = NodeInstanceRepository()

            # è·å–èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
            node_query = """
                SELECT * FROM node
                WHERE node_base_id = %s AND is_current_version = TRUE AND is_deleted = FALSE
            """
            node_info = await node_repo.db.fetch_one(node_query, node_base_id)

            if not node_info:
                raise ValueError(f"èŠ‚ç‚¹ä¸å­˜åœ¨: {node_base_id}")

            # ç”Ÿæˆæ–°çš„èŠ‚ç‚¹å®ä¾‹ID
            new_node_instance_id = uuid.uuid4()

            # åˆ›å»ºæ–°çš„èŠ‚ç‚¹å®ä¾‹æ•°æ®
            loop_instance_data = NodeInstanceCreate(
                node_instance_id=new_node_instance_id,
                workflow_instance_id=self.workflow_instance_id,
                node_id=node_info['node_id'],
                node_base_id=node_base_id,
                node_instance_name=f"{node_info['name']}_loop_{loop_iteration}",
                task_description=node_info.get('task_description', ''),
                status=NodeInstanceStatus.PENDING,
                # ğŸ†• å›ç¯ç‰¹æœ‰å­—æ®µ
                loop_iteration=loop_iteration,
                parent_instance_id=trigger_node_instance_id,
                execution_path_id=path_id
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            result = await node_instance_repo.create_node_instance(loop_instance_data)

            if result:
                # åœ¨æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­æ³¨å†Œæ–°å®ä¾‹
                await self._register_loop_node_dependencies(new_node_instance_id, node_info['node_id'])

                logger.info(f"âœ… å›ç¯èŠ‚ç‚¹å®ä¾‹åˆ›å»ºæˆåŠŸ: {new_node_instance_id}")
                return new_node_instance_id
            else:
                raise ValueError("åˆ›å»ºå›ç¯èŠ‚ç‚¹å®ä¾‹å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå›ç¯èŠ‚ç‚¹å®ä¾‹å¤±è´¥: {e}")
            raise

    async def _register_loop_node_dependencies(self, loop_node_instance_id: uuid.UUID, node_id: uuid.UUID):
        """æ³¨å†Œå›ç¯èŠ‚ç‚¹å®ä¾‹çš„ä¾èµ–å…³ç³»

        ğŸ”§ å…³é”®ä¿®å¤ï¼šå›ç¯èŠ‚ç‚¹åº”è¯¥ä¾èµ–äºè§¦å‘å›ç¯çš„èŠ‚ç‚¹å®ä¾‹ï¼Œè€Œä¸æ˜¯åŸå§‹çš„ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹
        è¿™æ ·æ‰èƒ½æ­£ç¡®å½¢æˆå›ç¯çš„æ‰§è¡Œé“¾æ¡
        """
        try:
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šè·å–è§¦å‘å›ç¯çš„èŠ‚ç‚¹å®ä¾‹ID
            # ä»loop_node_instance_idæ‰¾åˆ°parent_instance_idï¼ˆè§¦å‘å›ç¯çš„èŠ‚ç‚¹ï¼‰
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()

            loop_node_data = await node_repo.get_instance_by_id(loop_node_instance_id)
            if not loop_node_data:
                raise ValueError(f"æ— æ³•è·å–å›ç¯èŠ‚ç‚¹å®ä¾‹æ•°æ®: {loop_node_instance_id}")

            # è·å–è§¦å‘å›ç¯çš„çˆ¶èŠ‚ç‚¹å®ä¾‹IDï¼ˆè¿™æ˜¯çœŸæ­£çš„ä¸Šæ¸¸ä¾èµ–ï¼‰
            parent_instance_id = loop_node_data.get('parent_instance_id')
            if not parent_instance_id:
                logger.warning(f"âš ï¸ å›ç¯èŠ‚ç‚¹ {loop_node_instance_id} æ²¡æœ‰çˆ¶å®ä¾‹IDï¼Œä½¿ç”¨åŸå§‹ä¸Šæ¸¸ä¾èµ–")
                # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼šè·å–èŠ‚ç‚¹å®šä¹‰çš„ä¸Šæ¸¸ä¾èµ–
                upstream_node_instances = await self._get_original_upstream_instances(node_id)
            else:
                # ğŸ†• æ­£ç¡®çš„å›ç¯ä¾èµ–ï¼šåªä¾èµ–äºè§¦å‘å›ç¯çš„èŠ‚ç‚¹å®ä¾‹
                upstream_node_instances = [parent_instance_id]
                logger.info(f"ğŸ”„ è®¾ç½®å›ç¯ä¾èµ–: {loop_node_instance_id} -> ä¾èµ–äºè§¦å‘èŠ‚ç‚¹ {parent_instance_id}")

            # æ³¨å†Œä¾èµ–å…³ç³»
            await self.register_node_dependencies(
                loop_node_instance_id,
                node_id,
                upstream_node_instances
            )

            logger.info(f"ğŸ“‹ æ³¨å†Œå›ç¯èŠ‚ç‚¹ä¾èµ–å®Œæˆ: {loop_node_instance_id} -> {len(upstream_node_instances)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹")

        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œå›ç¯èŠ‚ç‚¹ä¾èµ–å¤±è´¥: {e}")
            raise

    async def _get_original_upstream_instances(self, node_id: uuid.UUID) -> List[uuid.UUID]:
        """è·å–èŠ‚ç‚¹çš„åŸå§‹ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        try:
            from ..services.execution_service import ExecutionEngine
            execution_engine = ExecutionEngine()
            return await execution_engine._get_upstream_node_instances(node_id, self.workflow_instance_id)
        except Exception as e:
            logger.error(f"è·å–åŸå§‹ä¸Šæ¸¸ä¾èµ–å¤±è´¥: {e}")
            return []

    async def handle_conditional_edges_with_loops(self, path_id: str, completed_node_instance_id: uuid.UUID,
                                                connections: List[Dict[str, Any]]) -> List[uuid.UUID]:
        """è¯„ä¼°æ¡ä»¶è¾¹ï¼Œæ”¯æŒå›ç¯å¤„ç†"""
        async with self._context_lock:
            if path_id not in self.execution_context['execution_paths']:
                return []

            path = self.execution_context['execution_paths'][path_id]
            activated_nodes = []

            # è·å–èŠ‚ç‚¹è¾“å‡ºæ•°æ®ç”¨äºæ¡ä»¶è¯„ä¼°
            node_output = path.accumulated_outputs.get(completed_node_instance_id, {})

            for connection in connections:
                to_node_base_id = connection.get('to_node_base_id')
                condition_config = connection.get('condition_config', {})

                # ğŸ”§ LinusåŸåˆ™ï¼šæ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ
                # æ‰€æœ‰è¾¹éƒ½æ˜¯æ¡ä»¶è¾¹ï¼Œå›ºå®šè¾¹å°±æ˜¯æ¡ä»¶æ°¸è¿œä¸ºtrueçš„è¾¹
                should_activate = await self._evaluate_condition(condition_config, node_output, path)
                if should_activate:
                    # ğŸ”§ ç®€åŒ–å›ç¯å¤„ç†ï¼šé€šè¿‡çŠ¶æ€é‡ç½®è€Œä¸æ˜¯åˆ›å»ºæ–°å®ä¾‹
                    if to_node_base_id in path.visited_nodes:
                        logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å›ç¯: {completed_node_instance_id} -> {to_node_base_id}")
                        # ç®€å•çš„å›ç¯ï¼šé‡ç½®èŠ‚ç‚¹çŠ¶æ€åˆ°PENDINGï¼Œå…è®¸é‡æ–°æ‰§è¡Œ
                        await self._reset_node_for_loop(to_node_base_id, path)

                    activated_nodes.append(to_node_base_id)
                    logger.debug(f"âœ… è¾¹æ¿€æ´»: {completed_node_instance_id} -> {to_node_base_id}")
                else:
                    logger.debug(f"âŒ è¾¹æœªæ¿€æ´»: {completed_node_instance_id} -> {to_node_base_id}")

            return activated_nodes

    async def _reset_node_for_loop(self, node_base_id: uuid.UUID, path: ExecutionPath):
        """
        ç®€åŒ–çš„å›ç¯å¤„ç†ï¼šé‡ç½®èŠ‚ç‚¹çŠ¶æ€è€Œä¸æ˜¯åˆ›å»ºæ–°å®ä¾‹

        LinusåŸåˆ™ï¼šå›ç¯ä¸åº”è¯¥æ˜¯ç‰¹æ®Šæƒ…å†µï¼Œåªæ˜¯çŠ¶æ€çš„é‡ç½®
        """
        try:
            # æ‰¾åˆ°è¿™ä¸ªnode_base_idå¯¹åº”çš„å®ä¾‹
            for node_instance_id, deps in self.node_dependencies.items():
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªnode_base_idçš„å®ä¾‹
                if str(deps.get('node_id')) == str(node_base_id):
                    # é‡ç½®çŠ¶æ€åˆ°PENDINGï¼Œå…è®¸é‡æ–°æ‰§è¡Œ
                    self.node_states[node_instance_id] = 'PENDING'

                    # ä»å®Œæˆé›†åˆä¸­ç§»é™¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    self.execution_context.get('completed_nodes', set()).discard(node_instance_id)

                    # é‡ç½®ä¾èµ–æ»¡è¶³çŠ¶æ€
                    deps['ready_to_execute'] = False
                    deps['completed_upstream'] = set()

                    # å¢åŠ å¾ªç¯è®¡æ•°
                    path.loop_count[node_base_id] = path.loop_count.get(node_base_id, 0) + 1

                    logger.info(f"ğŸ”„ é‡ç½®èŠ‚ç‚¹çŠ¶æ€ç”¨äºå›ç¯: {node_instance_id} (ç¬¬{path.loop_count[node_base_id]}æ¬¡)")
                    break

        except Exception as e:
            logger.error(f"âŒ é‡ç½®èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")

    async def _check_workflow_completion(self):
        """
        æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å·²å®Œæˆ - Linuså¼ç®€åŒ–ç‰ˆæœ¬

        æ¯æ¬¡èŠ‚ç‚¹å®Œæˆåè°ƒç”¨ï¼Œç¡®ä¿åŠæ—¶æ›´æ–°å·¥ä½œæµçŠ¶æ€
        """
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from .execution_service import execution_engine
            await execution_engine._check_and_update_workflow_status(self.workflow_instance_id)
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å¤±è´¥: {e}")

    async def get_node_execution_history(self, node_base_id: uuid.UUID, path_id: Optional[str] = None) -> List[ExecutionRecord]:
        """è·å–èŠ‚ç‚¹çš„æ‰§è¡Œå†å²è®°å½•"""
        async with self._context_lock:
            all_records = []

            # éå†æ‰€æœ‰æ‰§è¡Œè®°å½•ï¼Œæ‰¾åˆ°åŒ¹é…çš„èŠ‚ç‚¹
            for node_instance_id, records in self.execution_context['node_execution_records'].items():
                for record in records:
                    # é€šè¿‡node_instance_idæŸ¥æ‰¾å¯¹åº”çš„node_base_id
                    if (path_id is None or record.path_id == path_id):
                        # è¿™é‡Œéœ€è¦æŸ¥è¯¢node_instanceå¯¹åº”çš„node_base_id
                        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›è®°å½•
                        all_records.append(record)

            # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
            all_records.sort(key=lambda r: r.started_at)

            return all_records

    async def handle_user_path_selection(self, node_instance_id: uuid.UUID,
                                       available_paths: List[Dict[str, Any]],
                                       user_selected_paths: List[uuid.UUID]) -> List[str]:
        """å¤„ç†ç”¨æˆ·è·¯å¾„é€‰æ‹©ï¼Œå¯èƒ½åˆ›å»ºå¤šä¸ªåˆ†æ”¯è·¯å¾„"""
        async with self._context_lock:
            created_paths = []

            # æ‰¾åˆ°å½“å‰èŠ‚ç‚¹æ‰€åœ¨çš„è·¯å¾„
            current_path_id = None
            for path_id, path in self.execution_context['execution_paths'].items():
                if node_instance_id in path.current_nodes:
                    current_path_id = path_id
                    break

            if not current_path_id:
                logger.error(f"æœªæ‰¾åˆ°èŠ‚ç‚¹çš„å½“å‰è·¯å¾„: {node_instance_id}")
                return []

            # è®°å½•ç”¨æˆ·é€‰æ‹©
            path = self.execution_context['execution_paths'][current_path_id]
            path.user_selections[node_instance_id] = user_selected_paths

            # æ ¹æ®ç”¨æˆ·é€‰æ‹©åˆ›å»ºè·¯å¾„
            if len(user_selected_paths) <= 1:
                # å•é€‰æˆ–æ— é€‰æ‹©ï¼Œç»§ç»­å½“å‰è·¯å¾„
                created_paths.append(current_path_id)
            else:
                # å¤šé€‰ï¼Œåˆ›å»ºåˆ†æ”¯è·¯å¾„
                for selected_node in user_selected_paths:
                    branch_path_id = await self.create_execution_path(current_path_id)
                    created_paths.append(branch_path_id)

                # æ ‡è®°åŸè·¯å¾„ä¸ºå·²åˆ†æ”¯
                path.status = "BRANCHED"
                self.execution_context['active_paths'].discard(current_path_id)

            logger.info(f"ğŸ”€ ç”¨æˆ·è·¯å¾„é€‰æ‹©å¤„ç†å®Œæˆ: {len(created_paths)} ä¸ªè·¯å¾„")
            return created_paths

    async def check_path_completion(self, path_id: str) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦å®Œæˆ"""
        async with self._context_lock:
            if path_id not in self.execution_context['execution_paths']:
                return False

            path = self.execution_context['execution_paths'][path_id]

            # å¦‚æœè·¯å¾„ä¸­æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹ï¼Œä¸”æ²¡æœ‰ç­‰å¾…æ¡ä»¶çš„èŠ‚ç‚¹ï¼Œåˆ™è®¤ä¸ºè·¯å¾„å®Œæˆ
            if not path.current_nodes and not path.pending_conditions:
                path.status = "COMPLETED"
                path.completed_at = datetime.utcnow()
                self.execution_context['active_paths'].discard(path_id)
                self.execution_context['completed_paths'].add(path_id)

                logger.info(f"ğŸ è·¯å¾„å®Œæˆ: {path_id}")
                return True

            return False

    async def is_workflow_completed(self) -> bool:
        """æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å®Œæˆï¼ˆåŸºäºè·¯å¾„çŠ¶æ€è€ŒéèŠ‚ç‚¹è®¡æ•°ï¼‰"""
        async with self._context_lock:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ‰§è¡Œè·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹é€»è¾‘
            if not self.execution_context.get('execution_paths') or not self.execution_context.get('active_paths'):
                logger.debug("æ‰§è¡Œè·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼ ç»ŸèŠ‚ç‚¹çŠ¶æ€åˆ¤æ–­å·¥ä½œæµå®Œæˆ")
                # ä¼ ç»Ÿæ–¹å¼ï¼šæ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹æ˜¯å¦éƒ½å®Œæˆ
                if not self.node_states:
                    return False

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½å®Œæˆæˆ–å¤±è´¥
                all_finished = all(state in ['COMPLETED', 'FAILED'] for state in self.node_states.values())
                has_completed = any(state == 'COMPLETED' for state in self.node_states.values())

                logger.debug(f"ä¼ ç»Ÿå®Œæˆæ£€æŸ¥: å…¨éƒ¨ç»“æŸ={all_finished}, æœ‰å®Œæˆ={has_completed}")
                return all_finished and has_completed

            # å¦‚æœæ²¡æœ‰æ´»è·ƒè·¯å¾„ï¼Œå·¥ä½œæµå®Œæˆ
            active_count = len(self.execution_context['active_paths'])
            completed_count = len(self.execution_context['completed_paths'])
            failed_count = len(self.execution_context['failed_paths'])

            logger.debug(f"ğŸ” å·¥ä½œæµå®Œæˆæ£€æŸ¥: æ´»è·ƒè·¯å¾„={active_count}, å®Œæˆè·¯å¾„={completed_count}, å¤±è´¥è·¯å¾„={failed_count}")

            # å·¥ä½œæµå®Œæˆçš„æ¡ä»¶ï¼šæ²¡æœ‰æ´»è·ƒè·¯å¾„
            if active_count == 0 and (completed_count > 0 or failed_count > 0):
                logger.info(f"ğŸ¯ å·¥ä½œæµå®Œæˆåˆ¤å®š: æ‰€æœ‰è·¯å¾„å·²ç»“æŸ")
                return True

            return False

    def cleanup(self):
        """æ¸…ç†ä¸Šä¸‹æ–‡èµ„æº"""
        logger.info(f"ğŸ§¹ æ¸…ç†å·¥ä½œæµä¸Šä¸‹æ–‡: {self.workflow_instance_id}")
        self.execution_context.clear()
        self.node_dependencies.clear()
        self.node_states.clear()
        self.pending_triggers.clear()


# å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥å‚
class WorkflowExecutionContextManager:
    """å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥å‚
    
    ç®¡ç†å¤šä¸ªå·¥ä½œæµå®ä¾‹çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    ä¸€ä¸ªå·¥ä½œæµå®ä¾‹å¯¹åº”ä¸€ä¸ªWorkflowExecutionContext
    æ”¯æŒæŒä¹…åŒ–å’Œè‡ªåŠ¨æ¢å¤
    """
    
    def __init__(self):
        self.contexts: Dict[uuid.UUID, WorkflowExecutionContext] = {}
        self._contexts_lock = asyncio.Lock()
        # ä¸Šä¸‹æ–‡è®¿é—®æ—¶é—´è·Ÿè¸ªï¼ˆç”¨äºLRUæ¸…ç†ï¼‰
        self._last_access: Dict[uuid.UUID, datetime] = {}
        # ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€è·Ÿè¸ª
        self._context_health: Dict[uuid.UUID, Dict[str, Any]] = {}
        # æŒä¹…åŒ–é…ç½®
        self._persistence_enabled = True
        self._auto_recovery_enabled = True
        self._auto_save_interval = 30  # ç§’
        self._max_memory_contexts = 1000  # æœ€å¤§å†…å­˜ä¸­ä¿å­˜çš„ä¸Šä¸‹æ–‡æ•°
        self._context_ttl = 3600  # ä¸Šä¸‹æ–‡ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰- 1å°æ—¶
        self._health_check_interval = 300  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰- 5åˆ†é’Ÿ (ä»1åˆ†é’Ÿå¢åŠ åˆ°5åˆ†é’Ÿ)
        self._context_grace_period = 180  # æ–°æ¢å¤ä¸Šä¸‹æ–‡çš„å®½é™æœŸï¼ˆç§’ï¼‰- 3åˆ†é’Ÿ
        # åå°ä»»åŠ¡å¼•ç”¨
        self._background_task = None
        self._health_check_task = None
        self._task_started = False
        # ä¸Šä¸‹æ–‡æ¢å¤æ—¶é—´è·Ÿè¸ª
        self._context_restored_at = {}  # workflow_id -> datetime
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'context_recoveries': 0,
            'context_losses': 0,
            'health_check_failures': 0,
            'persistence_failures': 0
        }
    
    async def _ensure_background_task(self):
        """ç¡®ä¿åå°æŒä¹…åŒ–ä»»åŠ¡å·²å¯åŠ¨"""
        if not self._task_started:
            try:
                self._background_task = asyncio.create_task(self._background_persistence_task())
                self._health_check_task = asyncio.create_task(self._background_health_check_task())
                self._task_started = True
                logger.info("ğŸ”„ å¯åŠ¨åå°ä¸Šä¸‹æ–‡æŒä¹…åŒ–ä»»åŠ¡")
                logger.info("ğŸ¥ å¯åŠ¨åå°ä¸Šä¸‹æ–‡å¥åº·æ£€æŸ¥ä»»åŠ¡")
            except RuntimeError as e:
                if "no running event loop" in str(e):
                    logger.warning("âš ï¸ äº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œå»¶è¿Ÿå¯åŠ¨åå°ä»»åŠ¡")
                else:
                    raise
    
    async def _background_health_check_task(self):
        """åå°ä¸Šä¸‹æ–‡å¥åº·æ£€æŸ¥ä»»åŠ¡"""
        logger.info("ğŸ¥ åå°å¥åº·æ£€æŸ¥ä»»åŠ¡å¼€å§‹è¿è¡Œ")
        while True:
            try:
                await asyncio.sleep(self._health_check_interval)
                await self._perform_health_check()
            except asyncio.CancelledError:
                logger.info("ğŸ›‘ åå°å¥åº·æ£€æŸ¥ä»»åŠ¡è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"åå°å¥åº·æ£€æŸ¥ä»»åŠ¡å¼‚å¸¸: {e}")
                self._stats['health_check_failures'] += 1
    
    async def _perform_health_check(self):
        """æ‰§è¡Œä¸Šä¸‹æ–‡å¥åº·æ£€æŸ¥"""
        try:
            current_time = datetime.utcnow()
            contexts_to_check = []
            
            # å¤åˆ¶ä¸Šä¸‹æ–‡åˆ—è¡¨é¿å…å¹¶å‘ä¿®æ”¹
            async with self._contexts_lock:
                contexts_to_check = list(self.contexts.items())
            
            logger.debug(f"ğŸ¥ å¼€å§‹å¥åº·æ£€æŸ¥ï¼Œæ£€æŸ¥ {len(contexts_to_check)} ä¸ªä¸Šä¸‹æ–‡")
            
            expired_contexts = []
            unhealthy_contexts = []
            
            for workflow_id, context in contexts_to_check:
                # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¿‡æœŸ
                last_access = self._last_access.get(workflow_id, current_time)
                age_seconds = (current_time - last_access).total_seconds()
                
                # å¥åº·çŠ¶æ€æ£€æŸ¥
                health_info = await self._check_context_health(workflow_id, context)
                self._context_health[workflow_id] = health_info
                
                if age_seconds > self._context_ttl:
                    expired_contexts.append(workflow_id)
                elif not health_info['healthy']:
                    unhealthy_contexts.append(workflow_id)
            
            # å¤„ç†è¿‡æœŸä¸Šä¸‹æ–‡
            for workflow_id in expired_contexts:
                await self._handle_expired_context(workflow_id)
            
            # å¤„ç†ä¸å¥åº·ä¸Šä¸‹æ–‡
            for workflow_id in unhealthy_contexts:
                await self._handle_unhealthy_context(workflow_id)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            if expired_contexts or unhealthy_contexts:
                logger.info(f"ğŸ¥ å¥åº·æ£€æŸ¥å®Œæˆ - è¿‡æœŸ: {len(expired_contexts)}, ä¸å¥åº·: {len(unhealthy_contexts)}")
            
        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥æ‰§è¡Œå¤±è´¥: {e}")
            self._stats['health_check_failures'] += 1
    
    async def _check_context_health(self, workflow_id: uuid.UUID, context: WorkflowExecutionContext) -> Dict[str, Any]:
        """æ£€æŸ¥å•ä¸ªä¸Šä¸‹æ–‡çš„å¥åº·çŠ¶æ€"""
        try:
            current_time = datetime.utcnow()
            health_info = {
                'healthy': True,
                'issues': [],
                'last_check': current_time.isoformat(),
                'node_count': len(context.node_dependencies),
                'completed_nodes': len(context.execution_context.get('completed_nodes', set())),
                'executing_nodes': len(context.execution_context.get('current_executing_nodes', set())),
                'failed_nodes': len(context.execution_context.get('failed_nodes', set()))
            }
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å®½é™æœŸå†…ï¼ˆæ–°æ¢å¤çš„ä¸Šä¸‹æ–‡ç»™äºˆå®½é™æœŸï¼‰
            restored_at = self._context_restored_at.get(workflow_id)
            in_grace_period = False
            if restored_at:
                grace_age = (current_time - restored_at).total_seconds()
                in_grace_period = grace_age < self._context_grace_period
                health_info['in_grace_period'] = in_grace_period
                health_info['grace_remaining_seconds'] = max(0, self._context_grace_period - grace_age)
            
            # æ£€æŸ¥1: ä¸Šä¸‹æ–‡æ•°æ®å®Œæ•´æ€§ï¼ˆå®½é™æœŸå†…ä¸æ£€æŸ¥ï¼‰
            if not in_grace_period and not context.execution_context:
                health_info['healthy'] = False
                health_info['issues'].append('execution_context_empty')
            elif in_grace_period and not context.execution_context:
                health_info['issues'].append('execution_context_empty_grace_period')
            
            # æ£€æŸ¥2: èŠ‚ç‚¹ä¾èµ–å…³ç³»ä¸€è‡´æ€§ï¼ˆå®½é™æœŸå†…ä¸æ£€æŸ¥ï¼‰
            if not in_grace_period and not context.node_dependencies:
                health_info['healthy'] = False
                health_info['issues'].append('node_dependencies_empty')
            elif in_grace_period and not context.node_dependencies:
                health_info['issues'].append('node_dependencies_empty_grace_period')
            
            # æ£€æŸ¥3: ä¸æ•°æ®åº“çŠ¶æ€ä¸€è‡´æ€§
            if await self._check_database_consistency(workflow_id, context):
                health_info['issues'].append('database_inconsistency')
                # ä¸æ ‡è®°ä¸ºä¸å¥åº·ï¼Œå› ä¸ºè¿™å¯ä»¥è‡ªåŠ¨ä¿®å¤
            
            return health_info
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€å¤±è´¥ {workflow_id}: {e}")
            return {
                'healthy': False,
                'issues': ['health_check_failed'],
                'error': str(e),
                'last_check': datetime.utcnow().isoformat()
            }
    
    async def _check_database_consistency(self, workflow_id: uuid.UUID, context: WorkflowExecutionContext) -> bool:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸æ•°æ®åº“çš„ä¸€è‡´æ€§"""
        try:
            # ç®€åŒ–æ£€æŸ¥ï¼šæ¯”è¾ƒå†…å­˜ä¸­çš„å®ŒæˆèŠ‚ç‚¹ä¸æ•°æ®åº“ä¸­çš„å®ŒæˆèŠ‚ç‚¹
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            
            db_completed_nodes = await node_repo.get_completed_nodes_by_workflow(workflow_id)
            memory_completed_nodes = context.execution_context.get('completed_nodes', set())
            
            db_node_ids = set(str(node_id) for node_id in db_completed_nodes)
            memory_node_ids = set(str(node_id) for node_id in memory_completed_nodes)
            
            if db_node_ids != memory_node_ids:
                logger.warning(f"âš ï¸ ä¸Šä¸‹æ–‡ä¸æ•°æ®åº“ä¸ä¸€è‡´ {workflow_id}")
                logger.warning(f"   æ•°æ®åº“å®ŒæˆèŠ‚ç‚¹: {len(db_node_ids)} ä¸ª")
                logger.warning(f"   å†…å­˜å®ŒæˆèŠ‚ç‚¹: {len(memory_node_ids)} ä¸ª")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®åº“ä¸€è‡´æ€§å¤±è´¥ {workflow_id}: {e}")
            return False
    
    async def _handle_expired_context(self, workflow_id: uuid.UUID):
        """å¤„ç†è¿‡æœŸçš„ä¸Šä¸‹æ–‡"""
        try:
            # æŒä¹…åŒ–åç§»é™¤
            if workflow_id in self.contexts:
                await self._persist_context_to_database(workflow_id, self.contexts[workflow_id])
                
            async with self._contexts_lock:
                if workflow_id in self.contexts:
                    self.contexts[workflow_id].cleanup()
                    del self.contexts[workflow_id]
                if workflow_id in self._last_access:
                    del self._last_access[workflow_id]
                if workflow_id in self._context_health:
                    del self._context_health[workflow_id]
            
            logger.info(f"ğŸ•’ è¿‡æœŸä¸Šä¸‹æ–‡å·²æ¸…ç†: {workflow_id}")
            
        except Exception as e:
            logger.error(f"å¤„ç†è¿‡æœŸä¸Šä¸‹æ–‡å¤±è´¥ {workflow_id}: {e}")
    
    async def _handle_unhealthy_context(self, workflow_id: uuid.UUID):
        """å¤„ç†ä¸å¥åº·çš„ä¸Šä¸‹æ–‡"""
        try:
            health_info = self._context_health.get(workflow_id, {})
            issues = health_info.get('issues', [])
            
            logger.warning(f"âš ï¸ å‘ç°ä¸å¥åº·ä¸Šä¸‹æ–‡: {workflow_id}")
            logger.warning(f"   é—®é¢˜: {issues}")
            
            # å°è¯•ä¿®å¤
            if 'database_inconsistency' in issues:
                await self._repair_context_from_database(workflow_id)
            elif 'execution_context_empty' in issues or 'node_dependencies_empty' in issues:
                # ä¸¥é‡é—®é¢˜ï¼Œé‡æ–°ä»æ•°æ®åº“æ„å»º
                logger.info(f"ğŸ”§ é‡æ–°æ„å»ºä¸å¥åº·ä¸Šä¸‹æ–‡: {workflow_id}")
                await self.remove_context(workflow_id)
                # ä¸‹æ¬¡è®¿é—®æ—¶ä¼šè‡ªåŠ¨ä»æ•°æ®åº“æ¢å¤
            
            self._stats['context_losses'] += 1
            
        except Exception as e:
            logger.error(f"å¤„ç†ä¸å¥åº·ä¸Šä¸‹æ–‡å¤±è´¥ {workflow_id}: {e}")
    
    async def _repair_context_from_database(self, workflow_id: uuid.UUID):
        """ä»æ•°æ®åº“ä¿®å¤ä¸Šä¸‹æ–‡"""
        try:
            if workflow_id not in self.contexts:
                return
                
            context = self.contexts[workflow_id]
            
            # ä»æ•°æ®åº“é‡æ–°åŒæ­¥èŠ‚ç‚¹çŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            nodes = await node_repo.get_instances_by_workflow_instance(workflow_id)
            
            # é‡æ–°åŒæ­¥å®Œæˆçš„èŠ‚ç‚¹
            completed_nodes = set()
            for node in nodes:
                node_instance_id = node['node_instance_id']
                status = node.get('status', 'pending')
                
                if status == 'completed':
                    completed_nodes.add(node_instance_id)
            
            # æ›´æ–°å†…å­˜çŠ¶æ€
            context.execution_context['completed_nodes'] = completed_nodes
            
            logger.info(f"ğŸ”§ å·²ä¿®å¤ä¸Šä¸‹æ–‡æ•°æ®åº“ä¸ä¸€è‡´: {workflow_id}")
            logger.info(f"   åŒæ­¥äº† {len(completed_nodes)} ä¸ªå®ŒæˆèŠ‚ç‚¹")
            
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“ä¿®å¤ä¸Šä¸‹æ–‡å¤±è´¥ {workflow_id}: {e}")
    
    def get_health_stats(self) -> Dict[str, Any]:
        """è·å–å¥åº·ç»Ÿè®¡ä¿¡æ¯"""
        current_time = datetime.utcnow()
        
        stats = {
            **self._stats,
            'total_contexts': len(self.contexts),
            'healthy_contexts': sum(1 for h in self._context_health.values() if h.get('healthy', False)),
            'unhealthy_contexts': sum(1 for h in self._context_health.values() if not h.get('healthy', True)),
            'average_context_age_minutes': 0,
            'oldest_context_age_minutes': 0
        }
        
        if self._last_access:
            ages = [(current_time - access_time).total_seconds() / 60 
                   for access_time in self._last_access.values()]
            stats['average_context_age_minutes'] = round(sum(ages) / len(ages), 2)
            stats['oldest_context_age_minutes'] = round(max(ages), 2)
        
        return stats
    
    async def _background_persistence_task(self):
        """åå°æŒä¹…åŒ–ä»»åŠ¡"""
        logger.info("ğŸ”„ åå°æŒä¹…åŒ–ä»»åŠ¡å¼€å§‹è¿è¡Œ")
        while True:
            try:
                await asyncio.sleep(self._auto_save_interval)
                await self._persist_all_contexts()
            except asyncio.CancelledError:
                logger.info("ğŸ›‘ åå°æŒä¹…åŒ–ä»»åŠ¡è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"åå°æŒä¹…åŒ–ä»»åŠ¡å¼‚å¸¸: {e}")
    
    async def shutdown(self):
        """å…³é—­ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self._background_task and not self._background_task.done():
            self._background_task.cancel()
            try:
                await self._background_task
            except asyncio.CancelledError:
                pass
        
        if self._health_check_task and not self._health_check_task.done():
            self._health_check_task.cancel()
            try:
                await self._health_check_task
            except asyncio.CancelledError:
                pass
                
        logger.info("ğŸ›‘ ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²å…³é—­")
    
    async def _persist_all_contexts(self):
        """æŒä¹…åŒ–æ‰€æœ‰æ´»è·ƒä¸Šä¸‹æ–‡"""
        if not self._persistence_enabled:
            return
            
        contexts_to_persist = []
        async with self._contexts_lock:
            contexts_to_persist = list(self.contexts.items())
        
        for workflow_id, context in contexts_to_persist:
            try:
                await self._persist_context_to_database(workflow_id, context)
            except Exception as e:
                logger.error(f"æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¤±è´¥ {workflow_id}: {e}")
    
    async def _persist_context_to_database(self, workflow_instance_id: uuid.UUID, context: WorkflowExecutionContext):
        """å°†ä¸Šä¸‹æ–‡æŒä¹…åŒ–åˆ°æ•°æ®åº“"""
        try:
            # åºåˆ—åŒ–ä¸Šä¸‹æ–‡æ•°æ®
            context_data = {
                'workflow_instance_id': str(workflow_instance_id),
                'execution_context': _serialize_for_json(context.execution_context),
                'node_dependencies': _serialize_for_json(context.node_dependencies),
                'node_states': _serialize_for_json(context.node_states),
                'last_updated': datetime.utcnow().isoformat()
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨workflow_instanceè¡¨çš„context_snapshotå­—æ®µï¼‰
            from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
            workflow_repo = WorkflowInstanceRepository()
            
            await workflow_repo.update_context_snapshot(workflow_instance_id, context_data)
            logger.trace(f"âœ… ä¸Šä¸‹æ–‡æŒä¹…åŒ–å®Œæˆ: {workflow_instance_id}")
            
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ä¸Šä¸‹æ–‡åˆ°æ•°æ®åº“å¤±è´¥ {workflow_instance_id}: {e}")
    
    async def _ensure_memory_limit(self):
        """ç¡®ä¿å†…å­˜ä¸­çš„ä¸Šä¸‹æ–‡æ•°é‡ä¸è¶…è¿‡é™åˆ¶"""
        if len(self.contexts) <= self._max_memory_contexts:
            return
            
        # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åºï¼Œç§»é™¤æœ€è€çš„ä¸Šä¸‹æ–‡
        sorted_contexts = sorted(
            self._last_access.items(),
            key=lambda x: x[1]
        )
        
        contexts_to_remove = sorted_contexts[:len(self.contexts) - self._max_memory_contexts + 100]  # å¤šåˆ é™¤100ä¸ªï¼Œé¿å…é¢‘ç¹æ¸…ç†
        
        async with self._contexts_lock:
            for workflow_id, _ in contexts_to_remove:
                if workflow_id in self.contexts:
                    # å…ˆæŒä¹…åŒ–å†åˆ é™¤
                    await self._persist_context_to_database(workflow_id, self.contexts[workflow_id])
                    self.contexts[workflow_id].cleanup()
                    del self.contexts[workflow_id]
                    del self._last_access[workflow_id]
                    logger.info(f"ğŸ§¹ å†…å­˜æ¸…ç†ï¼šç§»é™¤ä¸Šä¸‹æ–‡ {workflow_id}")
    
    async def get_context(self, workflow_instance_id: uuid.UUID) -> Optional[WorkflowExecutionContext]:
        """è·å–å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒè‡ªåŠ¨æ¢å¤å’Œå†…å­˜ç®¡ç†ï¼‰"""
        # ç¡®ä¿åå°ä»»åŠ¡å·²å¯åŠ¨
        await self._ensure_background_task()
        
        # æ›´æ–°è®¿é—®æ—¶é—´
        self._last_access[workflow_instance_id] = datetime.utcnow()
        
        # ä¼˜å…ˆä»å†…å­˜è·å–
        if workflow_instance_id in self.contexts:
            return self.contexts[workflow_instance_id]
        
        # ä»æ•°æ®åº“æ¢å¤
        if self._auto_recovery_enabled:
            logger.info(f"ğŸ”„ å†…å­˜ä¸­æœªæ‰¾åˆ°ä¸Šä¸‹æ–‡ï¼Œå°è¯•ä»æ•°æ®åº“æ¢å¤: {workflow_instance_id}")
            context = await self._restore_context_from_database(workflow_instance_id)
            
            if context:
                # æ£€æŸ¥å†…å­˜é™åˆ¶ï¼Œå¿…è¦æ—¶æ¸…ç†
                await self._ensure_memory_limit()
                async with self._contexts_lock:
                    self.contexts[workflow_instance_id] = context
                logger.info(f"âœ… æˆåŠŸä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡: {workflow_instance_id}")
                return context
        
        return None
    
    async def remove_context(self, workflow_instance_id: uuid.UUID):
        """ç§»é™¤å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡"""
        async with self._contexts_lock:
            if workflow_instance_id in self.contexts:
                context = self.contexts[workflow_instance_id]
                context.cleanup()
                del self.contexts[workflow_instance_id]
                
                # æ¸…ç†æ¢å¤æ—¶é—´è·Ÿè¸ª
                if workflow_instance_id in self._context_restored_at:
                    del self._context_restored_at[workflow_instance_id]
                    
                # æ¸…ç†å¥åº·çŠ¶æ€è·Ÿè¸ª
                if workflow_instance_id in self._context_health:
                    del self._context_health[workflow_instance_id]
                    
                # æ¸…ç†æœ€åè®¿é—®æ—¶é—´è·Ÿè¸ª
                if workflow_instance_id in self._last_access:
                    del self._last_access[workflow_instance_id]
                    
                logger.info(f"ğŸ—‘ï¸ ç§»é™¤å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡: {workflow_instance_id}")
    
    def get_all_contexts(self) -> List[WorkflowExecutionContext]:
        """è·å–æ‰€æœ‰ä¸Šä¸‹æ–‡"""
        return list(self.contexts.values())
    
    def register_completion_callback(self, callback):
        """æ³¨å†Œå®Œæˆå›è°ƒå‡½æ•°åˆ°æ‰€æœ‰ç°æœ‰å’Œæœªæ¥çš„ä¸Šä¸‹æ–‡"""
        # å°†å›è°ƒæ·»åŠ åˆ°æ‰€æœ‰ç°æœ‰ä¸Šä¸‹æ–‡
        for context in self.contexts.values():
            if callback not in context.completion_callbacks:
                context.completion_callbacks.append(callback)
        
        # ä¿å­˜å›è°ƒå‡½æ•°ï¼Œä»¥ä¾¿ä¸ºæ–°åˆ›å»ºçš„ä¸Šä¸‹æ–‡è‡ªåŠ¨æ³¨å†Œ
        if not hasattr(self, '_global_callbacks'):
            self._global_callbacks = []
        if callback not in self._global_callbacks:
            self._global_callbacks.append(callback)
            logger.debug(f"ğŸ“ æ³¨å†Œå…¨å±€å®Œæˆå›è°ƒå‡½æ•°: {callback.__name__ if hasattr(callback, '__name__') else 'anonymous'}")
    
    async def get_or_create_context(self, workflow_instance_id: uuid.UUID) -> WorkflowExecutionContext:
        """è·å–æˆ–åˆ›å»ºå·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼Œè‡ªåŠ¨æ³¨å†Œå…¨å±€å›è°ƒï¼‰"""
        # ç¡®ä¿åå°ä»»åŠ¡å·²å¯åŠ¨
        await self._ensure_background_task()
        
        async with self._contexts_lock:
            if workflow_instance_id not in self.contexts:
                context = WorkflowExecutionContext(workflow_instance_id)
                
                # ä¸ºæ–°ä¸Šä¸‹æ–‡æ³¨å†Œæ‰€æœ‰å…¨å±€å›è°ƒ
                if hasattr(self, '_global_callbacks'):
                    for callback in self._global_callbacks:
                        if callback not in context.completion_callbacks:
                            context.completion_callbacks.append(callback)
                
                self.contexts[workflow_instance_id] = context
                # æ›´æ–°è®¿é—®æ—¶é—´
                self._last_access[workflow_instance_id] = datetime.utcnow()
                logger.info(f"ğŸ†• åˆ›å»ºæ–°çš„å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡: {workflow_instance_id}")
            
            return self.contexts[workflow_instance_id]
    
    async def initialize_workflow_context(self, workflow_instance_id: uuid.UUID):
        """åˆå§‹åŒ–å·¥ä½œæµä¸Šä¸‹æ–‡"""
        context = await self.get_or_create_context(workflow_instance_id)
        await context.initialize_context()
    
    async def get_task_context_data(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ•°æ®"""
        context = await self.get_context(workflow_instance_id)
        if context:
            return await context.get_node_execution_context(node_instance_id)
        return {}
    
    async def mark_node_completed(self, workflow_instance_id: uuid.UUID, node_id: uuid.UUID, 
                                node_instance_id: uuid.UUID, output_data: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å®Œæˆ"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await context.mark_node_completed(node_id, node_instance_id, output_data)
    
    async def mark_node_failed(self, workflow_instance_id: uuid.UUID, node_id: uuid.UUID, 
                             node_instance_id: uuid.UUID, error_info: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å¤±è´¥"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await context.mark_node_failed(node_id, node_instance_id, error_info)
    
    @property
    def node_completion_status(self) -> Dict[uuid.UUID, str]:
        """è·å–æ‰€æœ‰èŠ‚ç‚¹çš„å®ŒæˆçŠ¶æ€ï¼ˆå…¼å®¹æ€§å±æ€§ï¼‰"""
        if not hasattr(self, '_node_completion_status'):
            self._node_completion_status = {}
        return self._node_completion_status
    
    async def register_node_dependencies(self, workflow_instance_id: uuid.UUID, 
                                       node_instance_id: uuid.UUID, node_id: uuid.UUID, 
                                       upstream_nodes: List[uuid.UUID]):
        """æ³¨å†ŒèŠ‚ç‚¹ä¾èµ–å…³ç³»"""
        context = await self.get_or_create_context(workflow_instance_id)
        await context.register_node_dependencies(node_instance_id, node_id, upstream_nodes)
    
    def print_dependency_summary(self, workflow_instance_id: uuid.UUID):
        """æ‰“å°ä¾èµ–å…³ç³»æ‘˜è¦"""
        context = self.contexts.get(workflow_instance_id)
        if context:
            logger.info(f"ğŸ“Š å·¥ä½œæµ {workflow_instance_id} ä¾èµ–å…³ç³»æ‘˜è¦:")
            logger.info(f"   - èŠ‚ç‚¹æ€»æ•°: {len(context.node_dependencies)}")
            logger.info(f"   - å·²å®ŒæˆèŠ‚ç‚¹: {len(context.execution_context.get('completed_nodes', set()))}")
            logger.info(f"   - æ‰§è¡Œä¸­èŠ‚ç‚¹: {len(context.execution_context.get('current_executing_nodes', set()))}")
            logger.info(f"   - å¤±è´¥èŠ‚ç‚¹: {len(context.execution_context.get('failed_nodes', set()))}")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å·¥ä½œæµ {workflow_instance_id} çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")
    
    def get_node_dependency_info(self, node_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹ä¾èµ–ä¿¡æ¯"""
        for workflow_context in self.contexts.values():
            if node_instance_id in workflow_context.node_dependencies:
                return workflow_context.node_dependencies[node_instance_id]
        return None
    
    def is_node_ready_to_execute(self, node_instance_id: uuid.UUID) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ"""
        for workflow_context in self.contexts.values():
            if node_instance_id in workflow_context.node_dependencies:
                return workflow_context.is_node_ready_to_execute(node_instance_id)
        return False
    
    async def mark_node_executing(self, workflow_instance_id: uuid.UUID, node_id: uuid.UUID, 
                                 node_instance_id: uuid.UUID):
        """æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await context.mark_node_executing(node_id, node_instance_id)
    
    async def cleanup_workflow_context(self, workflow_instance_id: uuid.UUID):
        """æ¸…ç†å·¥ä½œæµä¸Šä¸‹æ–‡"""
        await self.remove_context(workflow_instance_id)
    
    async def get_node_upstream_context(self, workflow_instance_id: uuid.UUID, 
                                       node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹ä¸Šæ¸¸ä¸Šä¸‹æ–‡æ•°æ®"""
        context = await self.get_context(workflow_instance_id)
        if context:
            return await context.get_node_execution_context(node_instance_id)
        return {}
    
    async def sync_workflow_instance_status(self, workflow_instance_id: uuid.UUID):
        """æ‰‹åŠ¨åŒæ­¥å·¥ä½œæµå®ä¾‹çŠ¶æ€ï¼ˆå…¬å…±æ¥å£ï¼‰"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await self._sync_workflow_instance_status(workflow_instance_id, context)
        else:
            logger.warning(f"âš ï¸ æ— æ³•åŒæ­¥çŠ¶æ€ï¼Œå·¥ä½œæµä¸Šä¸‹æ–‡ä¸å­˜åœ¨: {workflow_instance_id}")

    async def scan_and_trigger_ready_nodes(self, workflow_instance_id: uuid.UUID) -> List[uuid.UUID]:
        """æ‰«æå¹¶è§¦å‘å·¥ä½œæµä¸­æ‰€æœ‰å‡†å¤‡å¥½æ‰§è¡Œçš„èŠ‚ç‚¹"""
        context = await self.get_context(workflow_instance_id)
        if context:
            return await context.scan_and_trigger_ready_nodes()
        return []

    async def ensure_context_lifecycle_consistency(self, workflow_instance_id: uuid.UUID):
        """ç¡®ä¿ä¸Šä¸‹æ–‡ç”Ÿå‘½å‘¨æœŸä¸€è‡´æ€§"""
        # ç¡®ä¿å·¥ä½œæµä¸Šä¸‹æ–‡å­˜åœ¨
        await self.get_or_create_context(workflow_instance_id)
    
    async def _sync_workflow_instance_status(self, workflow_instance_id: uuid.UUID, context: WorkflowExecutionContext):
        """åŒæ­¥å·¥ä½œæµå®ä¾‹çŠ¶æ€"""
        try:
            logger.info(f"ğŸ”„ [çŠ¶æ€åŒæ­¥] å¼€å§‹åŒæ­¥å·¥ä½œæµå®ä¾‹çŠ¶æ€: {workflow_instance_id}")
            
            # è·å–å½“å‰å·¥ä½œæµçŠ¶æ€
            workflow_status = await context.get_workflow_status()
            current_status = workflow_status['status']  # COMPLETED, RUNNING, FAILED, UNKNOWN
            
            logger.info(f"   - ä¸Šä¸‹æ–‡è®¡ç®—çŠ¶æ€: {current_status}")
            logger.info(f"   - æ€»èŠ‚ç‚¹: {workflow_status['total_nodes']}")
            logger.info(f"   - å·²å®Œæˆ: {workflow_status['completed_nodes']}")
            logger.info(f"   - æ‰§è¡Œä¸­: {workflow_status['executing_nodes']}")
            logger.info(f"   - å¤±è´¥: {workflow_status['failed_nodes']}")
            
            # è·å–æ•°æ®åº“ä¸­çš„å·¥ä½œæµå®ä¾‹çŠ¶æ€
            from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
            from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus
            from ..utils.helpers import now_utc
            
            workflow_repo = WorkflowInstanceRepository()
            workflow_instance = await workflow_repo.get_instance_by_id(workflow_instance_id)
            
            if not workflow_instance:
                logger.warning(f"âš ï¸ å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {workflow_instance_id}")
                return
                
            db_status = workflow_instance.get('status', 'unknown')
            logger.info(f"   - æ•°æ®åº“çŠ¶æ€: {db_status}")
            
            # ç¡®å®šéœ€è¦æ›´æ–°çš„çŠ¶æ€
            target_status = None
            update_data = {}
            
            if current_status == 'COMPLETED' and db_status != 'completed':
                target_status = WorkflowInstanceStatus.COMPLETED
                update_data['completed_at'] = now_utc()
                logger.info(f"âœ… éœ€è¦æ›´æ–°çŠ¶æ€: {db_status} -> completed")
                
            elif current_status == 'FAILED' and db_status != 'failed':
                target_status = WorkflowInstanceStatus.FAILED
                update_data['completed_at'] = now_utc()
                logger.info(f"âŒ éœ€è¦æ›´æ–°çŠ¶æ€: {db_status} -> failed")
                
            elif current_status == 'RUNNING' and db_status not in ['running', 'pending']:
                target_status = WorkflowInstanceStatus.RUNNING
                logger.info(f"ğŸ”„ éœ€è¦æ›´æ–°çŠ¶æ€: {db_status} -> running")
                
            elif workflow_status['executing_nodes'] > 0 and db_status not in ['running']:
                # å¦‚æœæœ‰èŠ‚ç‚¹æ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿å·¥ä½œæµçŠ¶æ€ä¸ºrunning
                target_status = WorkflowInstanceStatus.RUNNING
                logger.info(f"ğŸ”„ æœ‰æ‰§è¡Œä¸­èŠ‚ç‚¹ï¼Œéœ€è¦æ›´æ–°çŠ¶æ€: {db_status} -> running")
                
            elif (workflow_status['completed_nodes'] > 0 and 
                  workflow_status['executing_nodes'] == 0 and 
                  workflow_status['failed_nodes'] == 0 and
                  workflow_status['completed_nodes'] < workflow_status['total_nodes'] and
                  db_status in ['completed', 'failed', 'cancelled']):
                # éƒ¨åˆ†å®Œæˆä½†å·¥ä½œæµè¢«æ ‡è®°ä¸ºæœ€ç»ˆçŠ¶æ€ï¼Œéœ€è¦æ¢å¤ä¸ºrunning
                target_status = WorkflowInstanceStatus.RUNNING
                logger.info(f"ğŸ”„ éƒ¨åˆ†å®Œæˆå·¥ä½œæµéœ€è¦æ¢å¤è¿è¡Œ: {db_status} -> running")
            
            # æ‰§è¡ŒçŠ¶æ€æ›´æ–°
            if target_status:
                update_data['status'] = target_status
                update_data['updated_at'] = now_utc()
                
                result = await workflow_repo.update_instance(workflow_instance_id, WorkflowInstanceUpdate(**update_data))
                if result:
                    logger.info(f"âœ… [çŠ¶æ€åŒæ­¥] å·¥ä½œæµå®ä¾‹çŠ¶æ€å·²æ›´æ–°: {workflow_instance_id} -> {target_status.value}")
                else:
                    logger.error(f"âŒ [çŠ¶æ€åŒæ­¥] å·¥ä½œæµå®ä¾‹çŠ¶æ€æ›´æ–°å¤±è´¥: {workflow_instance_id}")
            else:
                logger.info(f"â„¹ï¸ [çŠ¶æ€åŒæ­¥] å·¥ä½œæµå®ä¾‹çŠ¶æ€æ— éœ€æ›´æ–°: {db_status}")
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥å·¥ä½œæµå®ä¾‹çŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

    async def _restore_context_from_database(self, workflow_instance_id: uuid.UUID) -> Optional[WorkflowExecutionContext]:
        """ä»æ•°æ®åº“æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼Œä¼˜å…ˆä»å¿«ç…§æ¢å¤ï¼‰"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹ä»æ•°æ®åº“æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡: {workflow_instance_id}")
            
            # 1. æ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨
            from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
            workflow_repo = WorkflowInstanceRepository()
            workflow = await workflow_repo.get_instance_by_id(workflow_instance_id)
            
            if not workflow:
                logger.warning(f"âŒ å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨ï¼Œæ— æ³•æ¢å¤ä¸Šä¸‹æ–‡: {workflow_instance_id}")
                return None
            
            # 2. ä¼˜å…ˆå°è¯•ä»å¿«ç…§æ¢å¤
            context_snapshot = await workflow_repo.get_latest_context_snapshot(workflow_instance_id)
            
            if context_snapshot:
                logger.info(f"ğŸ“¸ å‘ç°ä¸Šä¸‹æ–‡å¿«ç…§ï¼Œä»å¿«ç…§æ¢å¤: {context_snapshot.get('snapshot_id')}")
                context = await self._restore_from_snapshot(workflow_instance_id, context_snapshot)
                if context:
                    return context
                else:
                    logger.warning(f"âš ï¸ ä»å¿«ç…§æ¢å¤å¤±è´¥ï¼Œå›é€€åˆ°æ•°æ®åº“é‡å»º")
            
            # 3. å¿«ç…§ä¸å­˜åœ¨æˆ–æ¢å¤å¤±è´¥ï¼Œä»æ•°æ®åº“é‡å»º
            logger.info(f"ğŸ”§ ä»æ•°æ®åº“é‡å»ºä¸Šä¸‹æ–‡: {workflow_instance_id}")
            return await self._rebuild_from_database(workflow_instance_id, workflow)
            
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡å¤±è´¥: {workflow_instance_id}, é”™è¯¯: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None
    
    async def _rebuild_node_dependencies(self, context: WorkflowExecutionContext, workflow_instance_id: uuid.UUID):
        """é‡å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»"""
        try:
            logger.info(f"ğŸ”§ å¼€å§‹é‡å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»: {workflow_instance_id}")
            
            # è·å–æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..repositories.base import BaseRepository
            node_repo = NodeInstanceRepository()
            nodes = await node_repo.get_instances_by_workflow_instance(workflow_instance_id)
            
            logger.info(f"ğŸ“‹ å‘ç° {len(nodes)} ä¸ªèŠ‚ç‚¹å®ä¾‹ï¼Œå¼€å§‹é‡å»ºä¾èµ–å…³ç³»...")
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šå…ˆåŒæ­¥æ‰€æœ‰å·²å®ŒæˆèŠ‚ç‚¹çš„çŠ¶æ€åˆ°å†…å­˜
            for node in nodes:
                node_instance_id = node['node_instance_id']
                node_id = node['node_id']
                status = node.get('status', 'pending')
                
                # åŒæ­¥èŠ‚ç‚¹çŠ¶æ€åˆ°å†…å­˜ - ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸ºå¤§å†™çŠ¶æ€
                context.node_states[node_instance_id] = status.upper()
                
                # å¦‚æœèŠ‚ç‚¹å·²å®Œæˆï¼Œç¡®ä¿åœ¨completed_nodesé›†åˆä¸­
                if status.upper() == 'COMPLETED':
                    context.execution_context.setdefault('completed_nodes', set()).add(node_instance_id)
                    logger.debug(f"ğŸ”„ åŒæ­¥å·²å®ŒæˆèŠ‚ç‚¹çŠ¶æ€åˆ°å†…å­˜: {node.get('node_instance_name', 'æœªçŸ¥')} -> {status.upper()}")
                else:
                    logger.debug(f"ğŸ”„ åŒæ­¥èŠ‚ç‚¹çŠ¶æ€åˆ°å†…å­˜: {node.get('node_instance_name', 'æœªçŸ¥')} -> {status.upper()}")
            
            # ç„¶åé‡å»ºä¾èµ–å…³ç³»
            for node in nodes:
                node_instance_id = node['node_instance_id']
                node_id = node['node_id']
                node_name = node.get('node_instance_name', 'æœªçŸ¥')
                
                try:
                    # è·å–ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹IDs - ä½¿ç”¨node_repoçš„æ•°æ®åº“è¿æ¥
                    upstream_query = """
                        SELECT DISTINCT ni.node_instance_id, ni.created_at
                        FROM node_connection nc
                        JOIN node_instance ni ON ni.node_id = nc.from_node_id
                        WHERE nc.to_node_id = $1 
                          AND ni.workflow_instance_id = $2
                          AND ni.is_deleted = FALSE
                        ORDER BY ni.created_at ASC
                    """
                    upstream_results = await node_repo.db.fetch_all(upstream_query, node_id, workflow_instance_id)
                    upstream_node_instance_ids = [result['node_instance_id'] for result in upstream_results]
                    
                    # æ³¨å†Œä¾èµ–å…³ç³»
                    await context.register_node_dependencies(
                        node_instance_id, node_id, upstream_node_instance_ids
                    )
                    
                    if upstream_node_instance_ids:
                        logger.debug(f"âœ… é‡å»ºèŠ‚ç‚¹ {node_name} ä¾èµ–: {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹")
                    
                except Exception as e:
                    logger.error(f"âŒ é‡å»ºèŠ‚ç‚¹ {node_name} ä¾èµ–å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… ä¾èµ–å…³ç³»é‡å»ºå®Œæˆï¼Œæ€»å…±å¤„ç† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            logger.info(f"   - æœ€ç»ˆèŠ‚ç‚¹ä¾èµ–æ•°é‡: {len(context.node_dependencies)}")
            logger.info(f"   - å†…å­˜ä¸­å·²å®ŒæˆèŠ‚ç‚¹: {len(context.execution_context.get('completed_nodes', set()))}")
            
        except Exception as e:
            logger.error(f"âŒ é‡å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    async def _restore_from_snapshot(self, workflow_instance_id: uuid.UUID, snapshot: Dict[str, Any]) -> Optional[WorkflowExecutionContext]:
        """ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡"""
        try:
            context_data = snapshot.get('context_data', {})
            if isinstance(context_data, str):
                import json
                context_data = json.loads(context_data)
            
            # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡å®ä¾‹
            context = WorkflowExecutionContext(workflow_instance_id)
            await context.initialize_context()
            
            # æ¢å¤æ‰§è¡Œä¸Šä¸‹æ–‡
            if 'execution_context' in context_data:
                exec_context = context_data['execution_context']
                context.execution_context.update(exec_context)
                
                # è½¬æ¢é›†åˆç±»å‹
                if 'completed_nodes' in exec_context:
                    context.execution_context['completed_nodes'] = set(uuid.UUID(n) for n in exec_context['completed_nodes'])
                if 'failed_nodes' in exec_context:
                    context.execution_context['failed_nodes'] = set(uuid.UUID(n) for n in exec_context['failed_nodes'])
                if 'current_executing_nodes' in exec_context:
                    context.execution_context['current_executing_nodes'] = set(uuid.UUID(n) for n in exec_context['current_executing_nodes'])
            
            # æ¢å¤èŠ‚ç‚¹çŠ¶æ€
            if 'node_states' in context_data:
                node_states = context_data['node_states']
                for node_id_str, state in node_states.items():
                    node_id = uuid.UUID(node_id_str)
                    context.node_states[node_id] = state
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šä»æ•°æ®åº“é‡å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»ï¼Œè€Œä¸æ˜¯ä»å¿«ç…§æ¢å¤
            # è¿™ç¡®ä¿ä¾èµ–å…³ç³»æ˜¯æœ€æ–°çš„ï¼Œå³ä½¿å¿«ç…§æ•°æ®è¿‡æœŸ
            await self._rebuild_node_dependencies(context, workflow_instance_id)
            
            # ğŸš€ æ–°å¢ï¼šä¸»åŠ¨æ‰«æå¹¶è§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹
            triggered_nodes = await context.scan_and_trigger_ready_nodes()
            if triggered_nodes:
                logger.info(f"ğŸ¯ [å¿«ç…§æ¢å¤] æ¢å¤åç«‹å³è§¦å‘ {len(triggered_nodes)} ä¸ªå‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹")
            
            # ğŸ”§ æ–°å¢ï¼šåŒæ­¥å·¥ä½œæµå®ä¾‹çŠ¶æ€
            await self._sync_workflow_instance_status(workflow_instance_id, context)
            
            # ğŸ”§ ä¿®å¤å…³é”®é—®é¢˜ï¼šç¡®ä¿ä»æ•°æ®åº“æ¢å¤æ—¶ä¹Ÿèƒ½æ³¨å†Œå…¨å±€å›è°ƒ
            # æ³¨å†Œå…¨å±€å›è°ƒ
            if hasattr(self, '_global_callbacks'):
                logger.info(f"ğŸ”§ [å¿«ç…§æ¢å¤] æ³¨å†Œ {len(self._global_callbacks)} ä¸ªå…¨å±€å›è°ƒåˆ°æ¢å¤çš„ä¸Šä¸‹æ–‡")
                for i, callback in enumerate(self._global_callbacks):
                    callback_name = getattr(callback, '__name__', f'callback_{i}')
                    if callback not in context.completion_callbacks:
                        context.completion_callbacks.append(callback)
                        logger.debug(f"   - å·²æ³¨å†Œå›è°ƒ: {callback_name}")
                    else:
                        logger.debug(f"   - è·³è¿‡é‡å¤å›è°ƒ: {callback_name}")
            else:
                logger.warning(f"âš ï¸ [å¿«ç…§æ¢å¤] æœªæ‰¾åˆ°å…¨å±€å›è°ƒåˆ—è¡¨ï¼Œè¿™å¯èƒ½å¯¼è‡´ENDèŠ‚ç‚¹æ— æ³•æ­£ç¡®æ‰§è¡Œ")
                logger.warning(f"   - å»ºè®®æ£€æŸ¥ExecutionServiceæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–å¹¶æ³¨å†Œäº†å›è°ƒ")
            
            logger.info(f"ğŸ”§ [å¿«ç…§æ¢å¤] æœ€ç»ˆä¸Šä¸‹æ–‡å›è°ƒæ•°é‡: {len(context.completion_callbacks)}")
            
            logger.info(f"âœ… ä»å¿«ç…§æˆåŠŸæ¢å¤ä¸Šä¸‹æ–‡: {workflow_instance_id}")
            logger.info(f"   - å·²å®ŒæˆèŠ‚ç‚¹: {len(context.execution_context.get('completed_nodes', set()))}")
            logger.info(f"   - èŠ‚ç‚¹ä¾èµ–æ•°: {len(context.node_dependencies)}")
            logger.info(f"   - æ³¨å†Œçš„å›è°ƒ: {len(context.completion_callbacks)}")
            
            # è®°å½•æ¢å¤æ—¶é—´ï¼Œç”¨äºå¥åº·æ£€æŸ¥å®½é™æœŸ
            self._context_restored_at[workflow_instance_id] = datetime.utcnow()
            
            return context
            
        except Exception as e:
            logger.error(f"ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None
    
    async def _rebuild_from_database(self, workflow_instance_id: uuid.UUID, workflow: Dict[str, Any]) -> Optional[WorkflowExecutionContext]:
        """ä»æ•°æ®åº“é‡å»ºä¸Šä¸‹æ–‡ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        try:
            # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡å®ä¾‹
            context = WorkflowExecutionContext(workflow_instance_id)
            await context.initialize_context()
            
            # æ¢å¤èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            nodes = await node_repo.get_instances_by_workflow_instance(workflow_instance_id)
            
            logger.info(f"ğŸ“‹ å‘ç° {len(nodes)} ä¸ªèŠ‚ç‚¹å®ä¾‹ï¼Œå¼€å§‹é‡å»ºçŠ¶æ€...")
            
            completed_count = 0
            for node in nodes:
                node_instance_id = node['node_instance_id']
                node_id = node['node_id'] 
                node_name = node.get('node_instance_name', 'æœªçŸ¥')
                status = node.get('status', 'pending')
                
                # æ¢å¤èŠ‚ç‚¹çŠ¶æ€åˆ°å†…å­˜ - ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸ºå¤§å†™çŠ¶æ€
                context.node_states[node_instance_id] = status.upper()
                
                # å¦‚æœèŠ‚ç‚¹å·²å®Œæˆï¼Œæ¢å¤å…¶è¾“å‡ºæ•°æ®
                if status.upper() == 'COMPLETED':
                    output_data = {
                        'status': 'completed',
                        'node_name': node_name,
                        'completed_at': str(node.get('completed_at', '')),
                        'output_data': node.get('output_data', {})
                    }
                    
                    # æ ‡è®°èŠ‚ç‚¹å®Œæˆ
                    await context.mark_node_completed(node_id, node_instance_id, output_data)
                    completed_count += 1
                    logger.debug(f"âœ… æ¢å¤å·²å®ŒæˆèŠ‚ç‚¹: {node_name}")
                
                elif status.upper() == 'RUNNING':
                    # æ ‡è®°èŠ‚ç‚¹æ­£åœ¨æ‰§è¡Œ
                    await context.mark_node_executing(node_id, node_instance_id)
                    logger.debug(f"ğŸ”„ æ¢å¤æ‰§è¡Œä¸­èŠ‚ç‚¹: {node_name}")
                else:
                    logger.debug(f"ğŸ”„ æ¢å¤èŠ‚ç‚¹çŠ¶æ€: {node_name} -> {status.upper()}")
            
            logger.info(f"ğŸ¯ ä¸Šä¸‹æ–‡çŠ¶æ€æ¢å¤å®Œæˆ: {completed_count} ä¸ªå·²å®ŒæˆèŠ‚ç‚¹å·²æ¢å¤")
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šé‡å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»
            await self._rebuild_node_dependencies(context, workflow_instance_id)
            
            # ğŸš€ æ–°å¢ï¼šä¸»åŠ¨æ‰«æå¹¶è§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹
            triggered_nodes = await context.scan_and_trigger_ready_nodes()
            if triggered_nodes:
                logger.info(f"ğŸ¯ [ä¸Šä¸‹æ–‡æ¢å¤] æ¢å¤åç«‹å³è§¦å‘ {len(triggered_nodes)} ä¸ªå‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹")
            
            # ğŸ”§ æ–°å¢ï¼šåŒæ­¥å·¥ä½œæµå®ä¾‹çŠ¶æ€
            await self._sync_workflow_instance_status(workflow_instance_id, context)
            
            # ğŸ”§ ä¿®å¤å…³é”®é—®é¢˜ï¼šç¡®ä¿ä»æ•°æ®åº“é‡å»ºæ—¶ä¹Ÿèƒ½æ³¨å†Œå…¨å±€å›è°ƒ
            # æ³¨å†Œå…¨å±€å›è°ƒ
            if hasattr(self, '_global_callbacks'):
                logger.info(f"ğŸ”§ [æ•°æ®åº“é‡å»º] æ³¨å†Œ {len(self._global_callbacks)} ä¸ªå…¨å±€å›è°ƒåˆ°é‡å»ºçš„ä¸Šä¸‹æ–‡")
                for i, callback in enumerate(self._global_callbacks):
                    callback_name = getattr(callback, '__name__', f'callback_{i}')
                    if callback not in context.completion_callbacks:
                        context.completion_callbacks.append(callback)
                        logger.debug(f"   - å·²æ³¨å†Œå›è°ƒ: {callback_name}")
                    else:
                        logger.debug(f"   - è·³è¿‡é‡å¤å›è°ƒ: {callback_name}")
            else:
                logger.warning(f"âš ï¸ [æ•°æ®åº“é‡å»º] æœªæ‰¾åˆ°å…¨å±€å›è°ƒåˆ—è¡¨ï¼Œè¿™å¯èƒ½å¯¼è‡´ENDèŠ‚ç‚¹æ— æ³•æ­£ç¡®æ‰§è¡Œ")
                logger.warning(f"   - å»ºè®®æ£€æŸ¥ExecutionServiceæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–å¹¶æ³¨å†Œäº†å›è°ƒ")
            
            logger.info(f"ğŸ”§ [æ•°æ®åº“é‡å»º] æœ€ç»ˆä¸Šä¸‹æ–‡å›è°ƒæ•°é‡: {len(context.completion_callbacks)}")
            
            # è®°å½•æ¢å¤æ—¶é—´ï¼Œç”¨äºå¥åº·æ£€æŸ¥å®½é™æœŸ
            self._context_restored_at[workflow_instance_id] = datetime.utcnow()
            
            # 4. æŒä¹…åŒ–ä¸Šä¸‹æ–‡çŠ¶æ€
            if self._persistence_enabled:
                await self._persist_context_snapshot(workflow_instance_id, context)
            
            return context
            
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡å¤±è´¥: {workflow_instance_id}, é”™è¯¯: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    async def _persist_context_snapshot(self, workflow_instance_id: uuid.UUID, context: WorkflowExecutionContext):
        """æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¿«ç…§åˆ°æ•°æ®åº“"""
        try:
            if not self._persistence_enabled:
                return
                
            logger.debug(f"ğŸ’¾ æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¿«ç…§: {workflow_instance_id}")
            
            # æ„é€ å¿«ç…§æ•°æ®
            snapshot_data = {
                'workflow_instance_id': str(workflow_instance_id),
                'execution_context': _serialize_for_json(context.execution_context),
                'node_states': {str(k): v for k, v in context.node_states.items()},
                'completed_nodes_count': len(context.execution_context.get('completed_nodes', set())),
                'snapshot_time': datetime.utcnow().isoformat(),
                'context_version': '2.0'  # ç‰ˆæœ¬æ ‡è¯†
            }
            
            # è¿™é‡Œå¯ä»¥å­˜å‚¨åˆ°Redisæˆ–æ•°æ®åº“è¡¨ä¸­
            # æš‚æ—¶ä½¿ç”¨æ—¥å¿—è®°å½•ï¼ˆç”Ÿäº§ç¯å¢ƒä¸­å¯æ›¿æ¢ä¸ºå®é™…å­˜å‚¨ï¼‰
            logger.debug(f"ğŸ“Š ä¸Šä¸‹æ–‡å¿«ç…§æ•°æ®: {len(str(snapshot_data))} å­—ç¬¦")
            
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¿«ç…§å¤±è´¥: {e}")
    
    async def create_context_snapshot(self, workflow_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """åˆ›å»ºä¸Šä¸‹æ–‡å¿«ç…§ï¼ˆç”¨äºç»†åˆ†å·¥ä½œæµéš”ç¦»ï¼‰"""
        context = await self.get_context(workflow_instance_id)
        if not context:
            return None
            
        return {
            'workflow_instance_id': str(workflow_instance_id),
            'execution_context': _serialize_for_json(context.execution_context),
            'node_states': {str(k): v for k, v in context.node_states.items()},
            'node_dependencies': {str(k): v for k, v in context.node_dependencies.items()},
            'snapshot_time': datetime.utcnow().isoformat()
        }
    
    async def restore_from_snapshot(self, workflow_instance_id: uuid.UUID, snapshot: Dict[str, Any]):
        """ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç»†åˆ†å·¥ä½œæµéš”ç¦»ï¼‰"""
        try:
            # logger.info(f"ğŸ”„ ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡: {workflow_instance_id}")
            
            async with self._contexts_lock:
                if workflow_instance_id not in self.contexts:
                    context = WorkflowExecutionContext(workflow_instance_id)
                    self.contexts[workflow_instance_id] = context
                else:
                    context = self.contexts[workflow_instance_id]
                
                # æ¢å¤æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œç¡®ä¿é›†åˆç±»å‹å­—æ®µæ­£ç¡®æ¢å¤
                execution_context = snapshot.get('execution_context', {})
                context.execution_context.update(execution_context)
                
                # ä¿®å¤ï¼šç¡®ä¿å…³é”®çš„é›†åˆå­—æ®µè¢«æ­£ç¡®æ¢å¤ä¸ºsetç±»å‹ï¼ˆJSONåºåˆ—åŒ–ä¼šå°†setè½¬ä¸ºlistï¼‰
                if 'completed_nodes' in execution_context:
                    context.execution_context['completed_nodes'] = set(execution_context['completed_nodes'])
                if 'current_executing_nodes' in execution_context:
                    context.execution_context['current_executing_nodes'] = set(execution_context['current_executing_nodes'])
                if 'failed_nodes' in execution_context:
                    context.execution_context['failed_nodes'] = set(execution_context['failed_nodes'])
                
                # æ¢å¤èŠ‚ç‚¹çŠ¶æ€
                node_states = snapshot.get('node_states', {})
                for node_id_str, state in node_states.items():
                    context.node_states[uuid.UUID(node_id_str)] = state
                
                # logger.info(f"âœ… ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡æˆåŠŸ: {workflow_instance_id}")
                
        except Exception as e:
            logger.error(f"âŒ ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    async def check_context_health(self, workflow_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€"""
        try:
            context = self.contexts.get(workflow_instance_id)
            
            if not context:
                return {
                    'healthy': False,
                    'status': 'context_missing',
                    'message': 'ä¸Šä¸‹æ–‡ä¸å­˜åœ¨äºå†…å­˜ä¸­',
                    'auto_recovery_available': self._auto_recovery_enabled
                }
            
            # æ£€æŸ¥å†…å­˜çŠ¶æ€ä¸æ•°æ®åº“çŠ¶æ€ä¸€è‡´æ€§
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            db_nodes = await node_repo.get_instances_by_workflow_instance(workflow_instance_id)
            
            db_completed_count = sum(1 for node in db_nodes if node.get('status') == 'completed')
            memory_completed_count = len(context.execution_context.get('completed_nodes', set()))
            
            consistent = db_completed_count == memory_completed_count
            
            return {
                'healthy': consistent,
                'status': 'consistent' if consistent else 'inconsistent',
                'memory_completed_nodes': memory_completed_count,
                'db_completed_nodes': db_completed_count,
                'total_nodes': len(db_nodes),
                'context_size': len(context.node_dependencies),
                'last_activity': context.execution_context.get('last_snapshot_time')
            }
            
        except Exception as e:
            return {
                'healthy': False,
                'status': 'check_failed',
                'error': str(e)
            }


# å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
_global_context_manager: Optional[WorkflowExecutionContextManager] = None

def get_context_manager() -> WorkflowExecutionContextManager:
    """è·å–å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    global _global_context_manager
    if _global_context_manager is None:
        _global_context_manager = WorkflowExecutionContextManager()
        logger.debug("ğŸŒ åˆå§‹åŒ–å…¨å±€å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    return _global_context_manager