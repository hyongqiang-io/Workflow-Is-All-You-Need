"""
å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†å•ä¸ªå·¥ä½œæµå®ä¾‹çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€çŠ¶æ€å’Œä¾èµ–å…³ç³»
ä¸€ä¸ªå·¥ä½œæµå®ä¾‹å¯¹åº”ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
"""

import uuid
from datetime import datetime
from typing import Dict, List, Any, Set, Optional
import asyncio
import json
from loguru import logger

# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
from ..models.instance import WorkflowInstanceStatus, WorkflowInstanceUpdate


def _serialize_for_json(obj):
    """å°†å¯¹è±¡åºåˆ—åŒ–ä¸ºJSONå…¼å®¹æ ¼å¼"""
    if isinstance(obj, uuid.UUID):
        return str(obj)
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, dict):
        result = {}
        for key, value in obj.items():
            serialized_key = str(key) if isinstance(key, uuid.UUID) else key
            serialized_value = _serialize_for_json(value)
            result[serialized_key] = serialized_value
        return result
    elif isinstance(obj, (list, tuple, set)):
        return [_serialize_for_json(item) for item in obj]
    else:
        return obj


class WorkflowExecutionContext:
    """å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†ä¸€ä¸ªå·¥ä½œæµå®ä¾‹çš„ï¼š
    - æ‰§è¡Œä¸Šä¸‹æ–‡æ•°æ®
    - èŠ‚ç‚¹çŠ¶æ€ç®¡ç† 
    - ä¾èµ–å…³ç³»ç®¡ç†
    - æ•°æ®æµç®¡ç†
    """
    
    def __init__(self, workflow_instance_id: uuid.UUID):
        self.workflow_instance_id = workflow_instance_id
        
        # æ‰§è¡Œä¸Šä¸‹æ–‡æ•°æ®
        self.execution_context = {
            'global_data': {},
            'node_outputs': {},  # node_instance_id -> output_data
            'execution_path': [],  # å·²æ‰§è¡Œçš„èŠ‚ç‚¹è·¯å¾„ (node_instance_id)
            'execution_start_time': datetime.utcnow().isoformat(),
            'current_executing_nodes': set(),  # å½“å‰æ‰§è¡Œä¸­çš„èŠ‚ç‚¹å®ä¾‹ID (node_instance_id)
            'completed_nodes': set(),  # å·²å®Œæˆçš„èŠ‚ç‚¹å®ä¾‹ID (node_instance_id)
            'failed_nodes': set(),  # å¤±è´¥çš„èŠ‚ç‚¹å®ä¾‹ID (node_instance_id)
            'auto_save_counter': 0,
            'last_snapshot_time': datetime.utcnow().isoformat(),
            'persistence_enabled': True
        }
        
        # èŠ‚ç‚¹ä¾èµ–å…³ç³»ç®¡ç† - ä½¿ç”¨node_instance_idä½œä¸ºkey
        self.node_dependencies: Dict[uuid.UUID, Dict[str, Any]] = {}
        
        # èŠ‚ç‚¹çŠ¶æ€ç®¡ç†
        self.node_states: Dict[uuid.UUID, str] = {}  # node_instance_id -> state
        
        # å¾…è§¦å‘çš„èŠ‚ç‚¹é˜Ÿåˆ—
        self.pending_triggers: Set[uuid.UUID] = set()
        
        # å¼‚æ­¥é”ç®¡ç†
        self._context_lock = asyncio.Lock()
        
        # å›è°ƒå‡½æ•°æ³¨å†Œ
        self.completion_callbacks: List[callable] = []
        
        logger.debug(f"ğŸ  åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡: {workflow_instance_id}")
    
    async def initialize_context(self, restore_from_snapshot: bool = False):
        """åˆå§‹åŒ–å·¥ä½œæµä¸Šä¸‹æ–‡"""
        async with self._context_lock:
            if restore_from_snapshot:
                # TODO: å®ç°å¿«ç…§æ¢å¤
                pass
            
            # è·å–å¼€å§‹èŠ‚ç‚¹ä¿¡æ¯
            start_node_info = await self._get_start_node_task_descriptions()
            self.execution_context['global_data']['start_node_descriptions'] = start_node_info
            
            logger.info(f"âœ… å·¥ä½œæµä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ: {self.workflow_instance_id}")
    
    async def register_node_dependencies(self, 
                                       node_instance_id: uuid.UUID,
                                       node_id: uuid.UUID,
                                       upstream_nodes: List[uuid.UUID]):
        """æ³¨å†ŒèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»ï¼ˆä¿®å¤ç‰ˆï¼šä½¿ç”¨node_statesæ£€æŸ¥å®ä¾‹çŠ¶æ€ï¼‰"""
        async with self._context_lock:
            # æ£€æŸ¥å·²å®Œæˆçš„ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ï¼ˆä½¿ç”¨node_statesï¼‰
            completed_upstream = set()
            
            for upstream_node_instance_id in upstream_nodes:
                if self.node_states.get(upstream_node_instance_id) == 'COMPLETED':
                    completed_upstream.add(upstream_node_instance_id)
                    logger.debug(f"  ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {upstream_node_instance_id} å·²å®Œæˆ")
            
            # è®¡ç®—æ˜¯å¦å‡†å¤‡æ‰§è¡Œ
            ready_to_execute = len(completed_upstream) == len(upstream_nodes)
            
            self.node_dependencies[node_instance_id] = {
                'node_id': node_id,
                'workflow_instance_id': self.workflow_instance_id,
                'upstream_nodes': upstream_nodes,
                'completed_upstream': completed_upstream,
                'ready_to_execute': ready_to_execute,
                'dependency_count': len(upstream_nodes)
            }
            
            # åˆå§‹åŒ–èŠ‚ç‚¹çŠ¶æ€
            self.node_states[node_instance_id] = 'PENDING'
            
            logger.info(f"ğŸ“‹ [ä¾èµ–æ³¨å†Œ] èŠ‚ç‚¹å®ä¾‹ {node_instance_id}")
            logger.info(f"  - ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹æ€»æ•°: {len(upstream_nodes)}")
            logger.info(f"  - å·²å®Œæˆä¸Šæ¸¸å®ä¾‹: {len(completed_upstream)}")
            logger.info(f"  - å‡†å¤‡æ‰§è¡Œ: {ready_to_execute}")
            logger.info(f"  - å½“å‰ä¾èµ–å­—å…¸å¤§å°: {len(self.node_dependencies)}")
            if upstream_nodes:
                logger.info(f"  - ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹åˆ—è¡¨: {upstream_nodes}")
            if completed_upstream:
                logger.info(f"  - å·²å®Œæˆä¸Šæ¸¸å®ä¾‹åˆ—è¡¨: {list(completed_upstream)}")
    
    async def mark_node_executing(self, node_id: uuid.UUID, node_instance_id: uuid.UUID):
        """æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ"""
        async with self._context_lock:
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿å…³é”®é›†åˆå­—æ®µæ˜¯setç±»å‹
            if not isinstance(self.execution_context.get('current_executing_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤current_executing_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['current_executing_nodes'] = set(self.execution_context.get('current_executing_nodes', []))
            
            self.node_states[node_instance_id] = 'EXECUTING'
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†æ‰§è¡ŒçŠ¶æ€
            self.execution_context['current_executing_nodes'].add(node_instance_id)
            
            logger.trace(f"âš¡ æ ‡è®°èŠ‚ç‚¹å®ä¾‹æ‰§è¡Œ: {node_instance_id} (èŠ‚ç‚¹ID: {node_id})")
    
    async def mark_node_completed(self, node_id: uuid.UUID, node_instance_id: uuid.UUID, output_data: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å®Œæˆ"""
        async with self._context_lock:
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿å…³é”®é›†åˆå­—æ®µæ˜¯setç±»å‹ï¼ˆä¿®å¤JSONæ¢å¤åçš„ç±»å‹é—®é¢˜ï¼‰
            if not isinstance(self.execution_context.get('completed_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤completed_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['completed_nodes'] = set(self.execution_context.get('completed_nodes', []))
            if not isinstance(self.execution_context.get('current_executing_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤current_executing_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['current_executing_nodes'] = set(self.execution_context.get('current_executing_nodes', []))
            if not isinstance(self.execution_context.get('failed_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤failed_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['failed_nodes'] = set(self.execution_context.get('failed_nodes', []))
            
            # é˜²é‡å¤å¤„ç† - æ£€æŸ¥å†…å­˜çŠ¶æ€
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_instance_idæ£€æŸ¥å®ŒæˆçŠ¶æ€ï¼Œå› ä¸ºæˆ‘ä»¬ç®¡ç†çš„æ˜¯å®ä¾‹çŠ¶æ€
            if node_instance_id in self.execution_context['completed_nodes']:
                logger.warning(f"ğŸ”„ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²ç»åœ¨å†…å­˜ä¸­æ ‡è®°ä¸ºå®Œæˆï¼Œè·³è¿‡é‡å¤å¤„ç†")
                return
            
            # é˜²é‡å¤å¤„ç† - æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            try:
                existing_node = await node_repo.get_instance_by_id(node_instance_id)
                if existing_node and existing_node.get('status') == 'completed':
                    logger.warning(f"ğŸ”„ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} åœ¨æ•°æ®åº“ä¸­å·²ç»å®Œæˆï¼ŒåŒæ­¥å†…å­˜çŠ¶æ€")
                    # åŒæ­¥å†…å­˜çŠ¶æ€
                    self.node_states[node_instance_id] = 'COMPLETED'
                    # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†å®ŒæˆçŠ¶æ€
                    self.execution_context['completed_nodes'].add(node_instance_id)
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_instance_idå­˜å‚¨è¾“å‡ºæ•°æ®
                    self.execution_context['node_outputs'][node_instance_id] = output_data
                    self.execution_context['current_executing_nodes'].discard(node_instance_id)
                    return
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥èŠ‚ç‚¹æ•°æ®åº“çŠ¶æ€æ—¶å‡ºé”™: {e}")
            
            # æ›´æ–°çŠ¶æ€
            self.node_states[node_instance_id] = 'COMPLETED'
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†å®ŒæˆçŠ¶æ€ï¼Œä¿æŒä¸€è‡´æ€§
            self.execution_context['completed_nodes'].add(node_instance_id)
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_instance_idä½œä¸ºé”®å­˜å‚¨è¾“å‡ºæ•°æ®ï¼Œè¿™æ ·è·å–ä¸Šä¸‹æ–‡æ—¶èƒ½æ­£ç¡®åŒ¹é…
            self.execution_context['node_outputs'][node_instance_id] = output_data
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡ä¿®å¤] èŠ‚ç‚¹è¾“å‡ºå­˜å‚¨: {node_instance_id} -> {len(str(output_data))}å­—ç¬¦")
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡ä¿®å¤] å½“å‰æ‰€æœ‰è¾“å‡ºé”®: {list(self.execution_context['node_outputs'].keys())}")
            self.execution_context['execution_path'].append(str(node_instance_id))
            
            # ä»æ‰§è¡Œä¸­ç§»é™¤
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†æ‰§è¡ŒçŠ¶æ€
            self.execution_context['current_executing_nodes'].discard(node_instance_id)
            
            logger.info(f"ğŸ‰ èŠ‚ç‚¹å®Œæˆ: {node_id}")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            await self._update_database_node_state(node_instance_id, 'COMPLETED', output_data)
        
        # æ£€æŸ¥å¹¶è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆåœ¨é”å¤–æ‰§è¡Œé¿å…æ­»é”ï¼‰
        triggered_nodes = await self._check_and_trigger_downstream_nodes(node_instance_id)
        
        # é€šçŸ¥å›è°ƒ
        if triggered_nodes:
            await self._notify_completion_callbacks(triggered_nodes)
        
        # æ£€æŸ¥å·¥ä½œæµå®Œæˆ
        await self._check_workflow_completion()
    
    async def mark_node_failed(self, node_id: uuid.UUID, node_instance_id: uuid.UUID, error_info: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å¤±è´¥"""
        async with self._context_lock:
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿å…³é”®é›†åˆå­—æ®µæ˜¯setç±»å‹
            if not isinstance(self.execution_context.get('failed_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤failed_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['failed_nodes'] = set(self.execution_context.get('failed_nodes', []))
            if not isinstance(self.execution_context.get('current_executing_nodes'), set):
                logger.warning("ğŸ”§ ä¿®å¤current_executing_nodesç±»å‹ä»liståˆ°set")
                self.execution_context['current_executing_nodes'] = set(self.execution_context.get('current_executing_nodes', []))
            
            self.node_states[node_instance_id] = 'FAILED'
            # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨node_instance_idç®¡ç†å¤±è´¥çŠ¶æ€ï¼Œä¿æŒä¸€è‡´æ€§
            self.execution_context['failed_nodes'].add(node_instance_id)
            self.execution_context['current_executing_nodes'].discard(node_instance_id)
            
            logger.error(f"âŒ èŠ‚ç‚¹å®ä¾‹å¤±è´¥: {node_instance_id} (èŠ‚ç‚¹ID: {node_id}) - {error_info}")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            await self._update_database_node_state(node_instance_id, 'FAILED', None, error_info)
    
    def is_node_ready_to_execute(self, node_instance_id: uuid.UUID) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ"""
        deps = self.node_dependencies.get(node_instance_id)
        if not deps:
            return False
        
        return deps.get('ready_to_execute', False)
    
    def get_node_state(self, node_instance_id: uuid.UUID) -> str:
        """è·å–èŠ‚ç‚¹çŠ¶æ€"""
        return self.node_states.get(node_instance_id, 'UNKNOWN')
    
    async def get_node_execution_context(self, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹çš„æ‰§è¡Œä¸Šä¸‹æ–‡æ•°æ®"""
        async with self._context_lock:
            # è·å–èŠ‚ç‚¹ä¾èµ–ä¿¡æ¯
            deps = self.node_dependencies.get(node_instance_id)
            if not deps:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ²¡æœ‰ä¾èµ–ä¿¡æ¯")
                return {
                    'immediate_upstream_results': {},
                    'workflow_global': {
                        'global_data': self.execution_context.get('global_data', {}),
                        'workflow_instance_id': str(self.workflow_instance_id),
                        'execution_start_time': self.execution_context.get('execution_start_time'),
                        'execution_path': self.execution_context.get('execution_path', [])
                    },
                    'upstream_node_count': 0,
                    'current_node': {}
                }
            
            upstream_nodes = deps.get('upstream_nodes', [])
            logger.debug(f"ğŸ” [ä¸Šä¸‹æ–‡æ„å»º] èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æœ‰ {len(upstream_nodes)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹")
            
            # æ”¶é›†ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹çš„è¾“å‡ºæ•°æ®
            immediate_upstream_results = {}
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] å¼€å§‹æ”¶é›†ä¸Šæ¸¸è¾“å‡ºï¼Œä¸Šæ¸¸èŠ‚ç‚¹æ•°: {len(upstream_nodes)}")
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹IDs: {upstream_nodes}")
            logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] å¯ç”¨è¾“å‡ºæ•°æ®é”®: {list(self.execution_context['node_outputs'].keys())}")
            
            for upstream_node_instance_id in upstream_nodes:
                logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] æ£€æŸ¥ä¸Šæ¸¸èŠ‚ç‚¹: {upstream_node_instance_id}")
                if upstream_node_instance_id in self.execution_context['node_outputs']:
                    output_data = self.execution_context['node_outputs'][upstream_node_instance_id]
                    logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] âœ… æ‰¾åˆ°è¾“å‡ºæ•°æ®: {len(str(output_data))}å­—ç¬¦")
                    # é€šè¿‡upstream_node_instance_idè·å–å¯¹åº”çš„node_idæ¥æŸ¥è¯¢èŠ‚ç‚¹åç§°
                    upstream_deps = self.node_dependencies.get(upstream_node_instance_id)
                    if upstream_deps:
                        upstream_node_id = upstream_deps.get('node_id')
                        node_name = await self._get_node_name_by_id(upstream_node_id) if upstream_node_id else None
                    else:
                        node_name = None
                    
                    upstream_key = node_name or f'èŠ‚ç‚¹å®ä¾‹_{str(upstream_node_instance_id)[:8]}'
                    immediate_upstream_results[upstream_key] = {
                        'node_instance_id': str(upstream_node_instance_id),
                        'node_name': node_name or f'èŠ‚ç‚¹å®ä¾‹_{str(upstream_node_instance_id)[:8]}',
                        'output_data': output_data,
                        'status': 'completed'
                    }
                    logger.debug(f"  âœ… æ·»åŠ ä¸Šæ¸¸è¾“å‡º: {upstream_key} -> {len(str(output_data))}å­—ç¬¦")
                else:
                    logger.warning(f"  âš ï¸ ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {upstream_node_instance_id} çš„è¾“å‡ºæ•°æ®ä¸å­˜åœ¨")
                    logger.debug(f"ğŸ”§ [ä¸Šä¸‹æ–‡è·å–] âŒ æœªæ‰¾åˆ° {upstream_node_instance_id} çš„è¾“å‡ºæ•°æ®")
            
            # å½“å‰èŠ‚ç‚¹ä¿¡æ¯
            current_node_name = await self._get_node_name_by_id(deps.get('node_id'))
            
            context_data = {
                'immediate_upstream_results': immediate_upstream_results,
                'workflow_global': {
                    'global_data': self.execution_context.get('global_data', {}),
                    'workflow_instance_id': str(self.workflow_instance_id),
                    'execution_start_time': self.execution_context.get('execution_start_time'),
                    'execution_path': self.execution_context.get('execution_path', [])
                },
                'upstream_node_count': len(upstream_nodes),
                'current_node': {
                    'node_instance_id': str(node_instance_id),
                    'node_id': str(deps.get('node_id')),
                    'node_name': current_node_name,
                    'status': self.get_node_state(node_instance_id)
                }
            }
            
            logger.debug(f"âœ… [ä¸Šä¸‹æ–‡æ„å»º] ä¸ºèŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ„å»ºäº†åŒ…å« {len(immediate_upstream_results)} ä¸ªä¸Šæ¸¸ç»“æœçš„ä¸Šä¸‹æ–‡")
            return context_data
    
    async def _check_and_trigger_downstream_nodes(self, completed_node_instance_id: uuid.UUID) -> List[uuid.UUID]:
        """æ£€æŸ¥å¹¶è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆï¼šé˜²æ­¢ç«æ€å’Œé‡å¤è§¦å‘ï¼Œç»Ÿä¸€ä½¿ç”¨node_instance_idï¼‰"""
        triggered_nodes = []
        
        # ğŸ”’ ä½¿ç”¨é”ä¿æŠ¤æ•´ä¸ªæ£€æŸ¥å’Œè§¦å‘è¿‡ç¨‹ï¼Œé˜²æ­¢ç«æ€æ¡ä»¶
        async with self._context_lock:
            logger.info(f"ğŸ” [ä¸‹æ¸¸è§¦å‘] æ£€æŸ¥å®ŒæˆèŠ‚ç‚¹å®ä¾‹ {completed_node_instance_id} çš„ä¸‹æ¸¸ä¾èµ–...")
            logger.info(f"   - å½“å‰æ³¨å†Œçš„èŠ‚ç‚¹ä¾èµ–æ•°é‡: {len(self.node_dependencies)}")
            logger.info(f"   - ä¾èµ–å­—å…¸çš„keyåˆ—è¡¨: {list(self.node_dependencies.keys())}")
            
            for node_instance_id, deps in self.node_dependencies.items():
                logger.info(f"   - æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹ {node_instance_id}ï¼Œä¸Šæ¸¸ä¾èµ–: {deps['upstream_nodes']}")
                logger.info(f"     - èŠ‚ç‚¹çŠ¶æ€: {self.node_states.get(node_instance_id, 'UNKNOWN')}")
                logger.info(f"     - å·²å®Œæˆä¸Šæ¸¸: {len(deps.get('completed_upstream', set()))}/{len(deps['upstream_nodes'])}")
                
                if completed_node_instance_id in deps['upstream_nodes']:
                    logger.info(f"âœ… [ä¸‹æ¸¸è§¦å‘] æ‰¾åˆ°ä¸‹æ¸¸èŠ‚ç‚¹å®ä¾‹: {node_instance_id}")
                    logger.info(f"   - å½“å‰çŠ¶æ€: {self.node_states.get(node_instance_id, 'UNKNOWN')}")
                    logger.info(f"   - ä¾èµ–çŠ¶æ€: {len(deps.get('completed_upstream', set()))}/{len(deps['upstream_nodes'])}")
                    
                    # ğŸ”’ å…ˆæ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²ç»è¢«è§¦å‘æˆ–æ­£åœ¨æ‰§è¡Œ
                    if deps.get('ready_to_execute', False):
                        logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²ç»è¢«æ ‡è®°ä¸ºå‡†å¤‡æ‰§è¡Œï¼Œè·³è¿‡")
                        continue
                        
                    # æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹æ˜¯å¦æ­£åœ¨æ‰§è¡Œï¼ˆä½¿ç”¨node_statesæ£€æŸ¥ï¼‰
                    if self.node_states.get(node_instance_id) == 'EXECUTING':
                        logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡")
                        continue
                        
                    # æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹æ˜¯å¦å·²å®Œæˆï¼ˆä½¿ç”¨node_statesæ£€æŸ¥å®ä¾‹çº§åˆ«çŠ¶æ€ï¼‰
                    if self.node_states.get(node_instance_id) == 'COMPLETED':
                        logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²å®Œæˆï¼Œè·³è¿‡")
                        continue
                    
                    # æ ‡è®°ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹å®Œæˆ
                    if completed_node_instance_id not in deps['completed_upstream']:
                        deps['completed_upstream'].add(completed_node_instance_id)
                        logger.info(f"  âœ… æ ‡è®°ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {completed_node_instance_id} ä¸ºå·²å®Œæˆ")
                    else:
                        logger.debug(f"  â„¹ï¸ ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {completed_node_instance_id} å·²ç»æ ‡è®°ä¸ºå®Œæˆ")
                    
                    # ä¸¥æ ¼æ£€æŸ¥æ‰€æœ‰ä¸Šæ¸¸æ˜¯å¦éƒ½å®Œæˆ
                    total_upstream = len(deps['upstream_nodes'])
                    completed_upstream = len(deps['completed_upstream'])
                    
                    logger.info(f"  ğŸ“Š èŠ‚ç‚¹å®ä¾‹ {node_instance_id} ä¾èµ–çŠ¶æ€: {completed_upstream}/{total_upstream}")
                    
                    # åªæœ‰å½“æ‰€æœ‰ä¸Šæ¸¸éƒ½å®Œæˆæ—¶æ‰è§¦å‘
                    if completed_upstream == total_upstream and total_upstream > 0:
                        # åŒé‡æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹éƒ½å·²å®Œæˆï¼ˆä½¿ç”¨node_statesæ£€æŸ¥ï¼‰
                        all_upstream_completed_verified = True
                        for upstream_instance_id in deps['upstream_nodes']:
                            if self.node_states.get(upstream_instance_id) != 'COMPLETED':
                                logger.trace(f"  âŒ ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ {upstream_instance_id} çŠ¶æ€ä¸º {self.node_states.get(upstream_instance_id, 'UNKNOWN')}ï¼Œç­‰å¾…")
                                all_upstream_completed_verified = False
                                break
                        
                        if all_upstream_completed_verified:
                            # é˜²æ­¢é‡å¤è§¦å‘çš„æœ€ç»ˆæ£€æŸ¥
                            if node_instance_id not in self.pending_triggers:
                                deps['ready_to_execute'] = True
                                self.pending_triggers.add(node_instance_id)
                                triggered_nodes.append(node_instance_id)
                                
                                logger.debug(f"ğŸš€ è§¦å‘ä¸‹æ¸¸èŠ‚ç‚¹å®ä¾‹: {node_instance_id} (ä¾èµ–å·²å…¨éƒ¨æ»¡è¶³: {deps['upstream_nodes']})")
                            else:
                                logger.trace(f"  âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} å·²åœ¨pending_triggersä¸­ï¼Œé¿å…é‡å¤è§¦å‘")
                    else:
                        logger.trace(f"  â³ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} ä¾èµ–æœªæ»¡è¶³ï¼Œç­‰å¾…æ›´å¤šä¸Šæ¸¸å®Œæˆ")
            
            logger.info(f"ğŸ¯ [ä¸‹æ¸¸è§¦å‘] è§¦å‘æ£€æŸ¥å®Œæˆï¼Œå…±è§¦å‘ {len(triggered_nodes)} ä¸ªä¸‹æ¸¸èŠ‚ç‚¹å®ä¾‹")
            if triggered_nodes:
                logger.info(f"   - è§¦å‘çš„èŠ‚ç‚¹å®ä¾‹: {triggered_nodes}")
            else:
                logger.info(f"   - åŸå› åˆ†æ: å¯èƒ½æ˜¯ä¾èµ–æœªå®Œå…¨æ»¡è¶³ï¼Œæˆ–èŠ‚ç‚¹å·²å¤„ç†ï¼Œæˆ–æ²¡æœ‰ä¸‹æ¸¸èŠ‚ç‚¹")
        
        return triggered_nodes
    
    async def get_ready_nodes(self) -> List[uuid.UUID]:
        """è·å–å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹"""
        ready_nodes = list(self.pending_triggers)
        self.pending_triggers.clear()
        return ready_nodes
    
    async def build_node_context(self, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ„å»ºèŠ‚ç‚¹æ‰§è¡Œä¸Šä¸‹æ–‡"""
        deps = self.node_dependencies.get(node_instance_id, {})
        upstream_nodes = deps.get('upstream_nodes', [])
        
        # æ”¶é›†ä¸Šæ¸¸è¾“å‡º
        upstream_context = {}
        for upstream_node_instance_id in upstream_nodes:
            if upstream_node_instance_id in self.execution_context['node_outputs']:
                # é€šè¿‡upstream_node_instance_idè·å–å¯¹åº”çš„node_idæ¥æŸ¥è¯¢èŠ‚ç‚¹åç§°
                upstream_deps = self.node_dependencies.get(upstream_node_instance_id)
                if upstream_deps:
                    upstream_node_id = upstream_deps.get('node_id')
                    node_name = await self._get_node_name_by_id(upstream_node_id) if upstream_node_id else None
                else:
                    node_name = None
                    
                key = node_name or str(upstream_node_instance_id)
                upstream_context[key] = {
                    'node_instance_id': str(upstream_node_instance_id),
                    'output_data': self.execution_context['node_outputs'][upstream_node_instance_id],
                    'status': 'completed'
                }
        
        return {
            'node_instance_id': str(node_instance_id),
            'upstream_outputs': upstream_context,
            'workflow_context': {
                'workflow_instance_id': str(self.workflow_instance_id),
                'execution_start_time': self.execution_context['execution_start_time'],
                'execution_path': self.execution_context['execution_path'],
                'global_data': self.execution_context['global_data']
            },
            'context_built_at': datetime.utcnow().isoformat()
        }
    
    async def get_workflow_status(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        total_nodes = len(self.node_dependencies)  # åŸºäºnode_instance_idçš„æ€»æ•°
        
        # ç»Ÿè®¡å·²å®Œæˆçš„èŠ‚ç‚¹å®ä¾‹æ•°é‡ï¼ˆé€šè¿‡node_statesæ£€æŸ¥ï¼‰
        completed_node_instances = len([nid for nid, state in self.node_states.items() if state == 'COMPLETED'])
        failed_node_instances = len([nid for nid, state in self.node_states.items() if state == 'FAILED'])
        executing_node_instances = len([nid for nid, state in self.node_states.items() if state == 'EXECUTING'])
        
        # ä¹Ÿä¿ç•™åŸæœ‰çš„node_idçº§åˆ«çš„ç»Ÿè®¡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        completed_nodes = len(self.execution_context['completed_nodes'])
        failed_nodes = len(self.execution_context['failed_nodes'])
        executing_nodes = len(self.execution_context['current_executing_nodes'])
        
        logger.debug(f"ğŸ“Š [çŠ¶æ€è®¡ç®—] å·¥ä½œæµçŠ¶æ€ç»Ÿè®¡:")
        logger.debug(f"   - æ€»èŠ‚ç‚¹å®ä¾‹: {total_nodes}")
        logger.debug(f"   - å·²å®ŒæˆèŠ‚ç‚¹å®ä¾‹: {completed_node_instances}")
        logger.debug(f"   - æ‰§è¡Œä¸­èŠ‚ç‚¹å®ä¾‹: {executing_node_instances}")
        logger.debug(f"   - å¤±è´¥èŠ‚ç‚¹å®ä¾‹: {failed_node_instances}")
        logger.debug(f"   - å·²å®ŒæˆèŠ‚ç‚¹(node_id): {completed_nodes}")
        
        if failed_node_instances > 0:
            status = 'FAILED'
        elif completed_node_instances == total_nodes and total_nodes > 0:
            status = 'COMPLETED'
        elif executing_node_instances > 0 or (total_nodes - completed_node_instances - failed_node_instances) > 0:
            status = 'RUNNING'
        else:
            status = 'UNKNOWN'
        
        logger.debug(f"ğŸ“Š [çŠ¶æ€è®¡ç®—] æœ€ç»ˆçŠ¶æ€: {status}")
        
        return {
            'status': status,
            'total_nodes': total_nodes,
            'completed_nodes': completed_node_instances,  # ä½¿ç”¨èŠ‚ç‚¹å®ä¾‹çº§åˆ«çš„ç»Ÿè®¡
            'failed_nodes': failed_node_instances,
            'executing_nodes': executing_node_instances,
            'pending_nodes': total_nodes - completed_node_instances - failed_node_instances - executing_node_instances,
            # ä¿ç•™åŸæœ‰å­—æ®µç”¨äºå…¼å®¹æ€§
            'completed_nodes_by_id': completed_nodes,
            'failed_nodes_by_id': failed_nodes,
            'executing_nodes_by_id': executing_nodes
        }
    
    def register_completion_callback(self, callback: callable):
        """æ³¨å†Œå®Œæˆå›è°ƒ"""
        self.completion_callbacks.append(callback)
    
    async def _notify_completion_callbacks(self, triggered_nodes: List[uuid.UUID]):
        """é€šçŸ¥å›è°ƒå‡½æ•°"""
        for callback in self.completion_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(self.workflow_instance_id, triggered_nodes)
                else:
                    callback(self.workflow_instance_id, triggered_nodes)
            except Exception as e:
                logger.error(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    async def _check_workflow_completion(self):
        """æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å®Œæˆ"""
        status_info = await self.get_workflow_status()
        if status_info['status'] in ['COMPLETED', 'FAILED']:
            logger.info(f"ğŸ å·¥ä½œæµ {self.workflow_instance_id} æ‰§è¡Œå®Œæˆ: {status_info['status']}")
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„å·¥ä½œæµçŠ¶æ€
            try:
                from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
                from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus
                from ..utils.helpers import now_utc
                
                workflow_repo = WorkflowInstanceRepository()
                
                # ç¡®å®šæœ€ç»ˆçŠ¶æ€
                final_status = WorkflowInstanceStatus.COMPLETED if status_info['status'] == 'COMPLETED' else WorkflowInstanceStatus.FAILED
                
                # æ›´æ–°å·¥ä½œæµå®ä¾‹çŠ¶æ€
                update_data = WorkflowInstanceUpdate(
                    status=final_status,
                    completed_at=now_utc() if final_status == WorkflowInstanceStatus.COMPLETED else None,
                    error_message=status_info.get('error_message') if final_status == WorkflowInstanceStatus.FAILED else None
                )
                
                result = await workflow_repo.update_instance(self.workflow_instance_id, update_data)
                if result:
                    logger.info(f"âœ… å·¥ä½œæµçŠ¶æ€å·²æ›´æ–°åˆ°æ•°æ®åº“: {self.workflow_instance_id} -> {final_status.value}")
                else:
                    logger.error(f"âŒ å·¥ä½œæµçŠ¶æ€æ›´æ–°å¤±è´¥: {self.workflow_instance_id}")
                    
            except Exception as e:
                logger.error(f"âŒ æ›´æ–°å·¥ä½œæµçŠ¶æ€åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                import traceback
                logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _update_database_node_state(self, node_instance_id: uuid.UUID, 
                                        state: str, output_data: Optional[Dict[str, Any]] = None,
                                        error_info: Optional[Dict[str, Any]] = None):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„èŠ‚ç‚¹çŠ¶æ€"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
            
            node_repo = NodeInstanceRepository()
            
            # å…ˆæ£€æŸ¥èŠ‚ç‚¹å®ä¾‹æ˜¯å¦å­˜åœ¨
            existing_node = await node_repo.get_instance_by_id(node_instance_id)
            if not existing_node:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨ï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°: {node_instance_id}")
                return
            
            status_mapping = {
                'COMPLETED': NodeInstanceStatus.COMPLETED,
                'FAILED': NodeInstanceStatus.FAILED,
                'EXECUTING': NodeInstanceStatus.RUNNING,
                'PENDING': NodeInstanceStatus.PENDING
            }
            
            db_status = status_mapping.get(state, NodeInstanceStatus.PENDING)
            
            # å‡†å¤‡è¾“å‡ºæ•°æ® - ç›´æ¥ä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œä¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            output_data_dict = output_data if output_data else None
            
            update_data = NodeInstanceUpdate(
                status=db_status,
                output_data=output_data_dict,
                error_message=error_info.get('message') if error_info else None
            )
            
            result = await node_repo.update_node_instance(node_instance_id, update_data)
            if result:
                logger.debug(f"âœ… æ•°æ®åº“çŠ¶æ€æ›´æ–°æˆåŠŸ: {node_instance_id} -> {state}")
            else:
                logger.warning(f"âš ï¸ æ•°æ®åº“çŠ¶æ€æ›´æ–°è¿”å›ç©ºç»“æœ: {node_instance_id} -> {state}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“çŠ¶æ€æ›´æ–°å¤±è´¥: {node_instance_id} -> {state}: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æµç¨‹ç»§ç»­
    
    async def _get_start_node_task_descriptions(self) -> Dict[str, Any]:
        """è·å–å¼€å§‹èŠ‚ç‚¹ä»»åŠ¡æè¿°"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            
            node_instance_repo = NodeInstanceRepository()
            
            query = """
                SELECT ni.node_instance_id, ni.node_id, n.name as node_name,
                       n.task_description, ni.task_description as instance_task_description
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.workflow_instance_id = $1
                AND LOWER(n.type) = 'start'
                AND ni.is_deleted = FALSE
                AND n.is_deleted = FALSE
                ORDER BY ni.created_at ASC
            """
            
            start_nodes = await node_instance_repo.db.fetch_all(query, self.workflow_instance_id)
            
            start_node_info = {}
            for node in start_nodes:
                node_id = str(node['node_id'])
                task_description = (
                    node.get('instance_task_description') or 
                    node.get('task_description') or 
                    f"å¼€å§‹èŠ‚ç‚¹ {node.get('node_name', 'æœªå‘½å')} çš„ä»»åŠ¡"
                )
                
                start_node_info[node_id] = {
                    'node_instance_id': str(node['node_instance_id']),
                    'node_name': node.get('node_name', 'æœªå‘½å'),
                    'task_description': task_description
                }
            
            return start_node_info
            
        except Exception as e:
            logger.error(f"è·å–å¼€å§‹èŠ‚ç‚¹ä»»åŠ¡æè¿°å¤±è´¥: {e}")
            return {}
    
    async def _get_node_name_by_id(self, node_id: uuid.UUID) -> str:
        """æ ¹æ®node_idè·å–èŠ‚ç‚¹åç§°"""
        try:
            from ..repositories.node.node_repository import NodeRepository
            node_repo = NodeRepository()
            
            query = "SELECT name FROM node WHERE node_id = $1"
            result = await node_repo.db.fetch_one(query, node_id)
            return result['name'] if result else None
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹åç§°å¤±è´¥: {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†ä¸Šä¸‹æ–‡èµ„æº"""
        logger.info(f"ğŸ§¹ æ¸…ç†å·¥ä½œæµä¸Šä¸‹æ–‡: {self.workflow_instance_id}")
        self.execution_context.clear()
        self.node_dependencies.clear()
        self.node_states.clear()
        self.pending_triggers.clear()
        self.completion_callbacks.clear()


# å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥å‚
class WorkflowExecutionContextManager:
    """å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥å‚
    
    ç®¡ç†å¤šä¸ªå·¥ä½œæµå®ä¾‹çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    ä¸€ä¸ªå·¥ä½œæµå®ä¾‹å¯¹åº”ä¸€ä¸ªWorkflowExecutionContext
    æ”¯æŒæŒä¹…åŒ–å’Œè‡ªåŠ¨æ¢å¤
    """
    
    def __init__(self):
        self.contexts: Dict[uuid.UUID, WorkflowExecutionContext] = {}
        self._contexts_lock = asyncio.Lock()
        self._persistence_enabled = True
        self._auto_recovery_enabled = True
    
    
    async def get_context(self, workflow_instance_id: uuid.UUID) -> Optional[WorkflowExecutionContext]:
        """è·å–å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒè‡ªåŠ¨æ¢å¤ï¼‰"""
        context = self.contexts.get(workflow_instance_id)
        
        # å¦‚æœä¸Šä¸‹æ–‡ä¸å­˜åœ¨ä¸”å¯ç”¨äº†è‡ªåŠ¨æ¢å¤ï¼Œå°è¯•ä»æ•°æ®åº“æ¢å¤
        if context is None and self._auto_recovery_enabled:
            logger.info(f"ğŸ”§ ä¸Šä¸‹æ–‡ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ•°æ®åº“è‡ªåŠ¨æ¢å¤: {workflow_instance_id}")
            context = await self._restore_context_from_database(workflow_instance_id)
            
        return context
    
    async def remove_context(self, workflow_instance_id: uuid.UUID):
        """ç§»é™¤å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡"""
        async with self._contexts_lock:
            if workflow_instance_id in self.contexts:
                context = self.contexts[workflow_instance_id]
                context.cleanup()
                del self.contexts[workflow_instance_id]
                logger.info(f"ğŸ—‘ï¸ ç§»é™¤å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡: {workflow_instance_id}")
    
    def get_all_contexts(self) -> List[WorkflowExecutionContext]:
        """è·å–æ‰€æœ‰ä¸Šä¸‹æ–‡"""
        return list(self.contexts.values())
    
    def register_completion_callback(self, callback):
        """æ³¨å†Œå®Œæˆå›è°ƒå‡½æ•°åˆ°æ‰€æœ‰ç°æœ‰å’Œæœªæ¥çš„ä¸Šä¸‹æ–‡"""
        # å°†å›è°ƒæ·»åŠ åˆ°æ‰€æœ‰ç°æœ‰ä¸Šä¸‹æ–‡
        for context in self.contexts.values():
            if callback not in context.completion_callbacks:
                context.completion_callbacks.append(callback)
        
        # ä¿å­˜å›è°ƒå‡½æ•°ï¼Œä»¥ä¾¿ä¸ºæ–°åˆ›å»ºçš„ä¸Šä¸‹æ–‡è‡ªåŠ¨æ³¨å†Œ
        if not hasattr(self, '_global_callbacks'):
            self._global_callbacks = []
        if callback not in self._global_callbacks:
            self._global_callbacks.append(callback)
            logger.debug(f"ğŸ“ æ³¨å†Œå…¨å±€å®Œæˆå›è°ƒå‡½æ•°: {callback.__name__ if hasattr(callback, '__name__') else 'anonymous'}")
    
    async def get_or_create_context(self, workflow_instance_id: uuid.UUID) -> WorkflowExecutionContext:
        """è·å–æˆ–åˆ›å»ºå·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼Œè‡ªåŠ¨æ³¨å†Œå…¨å±€å›è°ƒï¼‰"""
        async with self._contexts_lock:
            if workflow_instance_id not in self.contexts:
                context = WorkflowExecutionContext(workflow_instance_id)
                
                # ä¸ºæ–°ä¸Šä¸‹æ–‡æ³¨å†Œæ‰€æœ‰å…¨å±€å›è°ƒ
                if hasattr(self, '_global_callbacks'):
                    for callback in self._global_callbacks:
                        if callback not in context.completion_callbacks:
                            context.completion_callbacks.append(callback)
                
                self.contexts[workflow_instance_id] = context
                logger.info(f"ğŸ†• åˆ›å»ºæ–°çš„å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡: {workflow_instance_id}")
            
            return self.contexts[workflow_instance_id]
    
    async def initialize_workflow_context(self, workflow_instance_id: uuid.UUID):
        """åˆå§‹åŒ–å·¥ä½œæµä¸Šä¸‹æ–‡"""
        context = await self.get_or_create_context(workflow_instance_id)
        await context.initialize_context()
    
    async def get_task_context_data(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ•°æ®"""
        context = await self.get_context(workflow_instance_id)
        if context:
            return await context.get_node_execution_context(node_instance_id)
        return {}
    
    async def mark_node_completed(self, workflow_instance_id: uuid.UUID, node_id: uuid.UUID, 
                                node_instance_id: uuid.UUID, output_data: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å®Œæˆ"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await context.mark_node_completed(node_id, node_instance_id, output_data)
    
    async def mark_node_failed(self, workflow_instance_id: uuid.UUID, node_id: uuid.UUID, 
                             node_instance_id: uuid.UUID, error_info: Dict[str, Any]):
        """æ ‡è®°èŠ‚ç‚¹å¤±è´¥"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await context.mark_node_failed(node_id, node_instance_id, error_info)
    
    @property
    def node_completion_status(self) -> Dict[uuid.UUID, str]:
        """è·å–æ‰€æœ‰èŠ‚ç‚¹çš„å®ŒæˆçŠ¶æ€ï¼ˆå…¼å®¹æ€§å±æ€§ï¼‰"""
        if not hasattr(self, '_node_completion_status'):
            self._node_completion_status = {}
        return self._node_completion_status
    
    async def register_node_dependencies(self, workflow_instance_id: uuid.UUID, 
                                       node_instance_id: uuid.UUID, node_id: uuid.UUID, 
                                       upstream_nodes: List[uuid.UUID]):
        """æ³¨å†ŒèŠ‚ç‚¹ä¾èµ–å…³ç³»"""
        context = await self.get_or_create_context(workflow_instance_id)
        await context.register_node_dependencies(node_instance_id, node_id, upstream_nodes)
    
    def print_dependency_summary(self, workflow_instance_id: uuid.UUID):
        """æ‰“å°ä¾èµ–å…³ç³»æ‘˜è¦"""
        context = self.contexts.get(workflow_instance_id)
        if context:
            logger.info(f"ğŸ“Š å·¥ä½œæµ {workflow_instance_id} ä¾èµ–å…³ç³»æ‘˜è¦:")
            logger.info(f"   - èŠ‚ç‚¹æ€»æ•°: {len(context.node_dependencies)}")
            logger.info(f"   - å·²å®ŒæˆèŠ‚ç‚¹: {len(context.execution_context.get('completed_nodes', set()))}")
            logger.info(f"   - æ‰§è¡Œä¸­èŠ‚ç‚¹: {len(context.execution_context.get('current_executing_nodes', set()))}")
            logger.info(f"   - å¤±è´¥èŠ‚ç‚¹: {len(context.execution_context.get('failed_nodes', set()))}")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å·¥ä½œæµ {workflow_instance_id} çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")
    
    def get_node_dependency_info(self, node_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹ä¾èµ–ä¿¡æ¯"""
        for workflow_context in self.contexts.values():
            if node_instance_id in workflow_context.node_dependencies:
                return workflow_context.node_dependencies[node_instance_id]
        return None
    
    def is_node_ready_to_execute(self, node_instance_id: uuid.UUID) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ"""
        for workflow_context in self.contexts.values():
            if node_instance_id in workflow_context.node_dependencies:
                return workflow_context.is_node_ready_to_execute(node_instance_id)
        return False
    
    async def mark_node_executing(self, workflow_instance_id: uuid.UUID, node_id: uuid.UUID, 
                                 node_instance_id: uuid.UUID):
        """æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ"""
        context = await self.get_context(workflow_instance_id)
        if context:
            await context.mark_node_executing(node_id, node_instance_id)
    
    async def cleanup_workflow_context(self, workflow_instance_id: uuid.UUID):
        """æ¸…ç†å·¥ä½œæµä¸Šä¸‹æ–‡"""
        await self.remove_context(workflow_instance_id)
    
    async def get_node_upstream_context(self, workflow_instance_id: uuid.UUID, 
                                       node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹ä¸Šæ¸¸ä¸Šä¸‹æ–‡æ•°æ®"""
        context = await self.get_context(workflow_instance_id)
        if context:
            return await context.get_node_execution_context(node_instance_id)
        return {}
    
    async def ensure_context_lifecycle_consistency(self, workflow_instance_id: uuid.UUID):
        """ç¡®ä¿ä¸Šä¸‹æ–‡ç”Ÿå‘½å‘¨æœŸä¸€è‡´æ€§"""
        # ç¡®ä¿å·¥ä½œæµä¸Šä¸‹æ–‡å­˜åœ¨
        await self.get_or_create_context(workflow_instance_id)
    
    async def _restore_context_from_database(self, workflow_instance_id: uuid.UUID) -> Optional[WorkflowExecutionContext]:
        """ä»æ•°æ®åº“æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ˆæ ¸å¿ƒæ¢å¤é€»è¾‘ï¼‰"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹ä»æ•°æ®åº“æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡: {workflow_instance_id}")
            
            # 1. æ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨
            from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
            workflow_repo = WorkflowInstanceRepository()
            workflow = await workflow_repo.get_instance_by_id(workflow_instance_id)
            
            if not workflow:
                logger.warning(f"âŒ å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨ï¼Œæ— æ³•æ¢å¤ä¸Šä¸‹æ–‡: {workflow_instance_id}")
                return None
            
            # 2. åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡å®ä¾‹
            async with self._contexts_lock:
                context = WorkflowExecutionContext(workflow_instance_id)
                await context.initialize_context()
                self.contexts[workflow_instance_id] = context
                logger.info(f"âœ… åˆ›å»ºç©ºç™½ä¸Šä¸‹æ–‡æˆåŠŸ: {workflow_instance_id}")
            
            # 3. æ¢å¤èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            nodes = await node_repo.get_instances_by_workflow_instance(workflow_instance_id)
            
            logger.info(f"ğŸ“‹ å‘ç° {len(nodes)} ä¸ªèŠ‚ç‚¹å®ä¾‹ï¼Œå¼€å§‹æ¢å¤çŠ¶æ€...")
            
            completed_count = 0
            for node in nodes:
                node_instance_id = node['node_instance_id']
                node_id = node['node_id'] 
                node_name = node.get('node_instance_name', 'æœªçŸ¥')
                status = node.get('status', 'pending')
                
                # æ¢å¤èŠ‚ç‚¹çŠ¶æ€åˆ°å†…å­˜
                context.node_states[node_instance_id] = status
                
                # å¦‚æœèŠ‚ç‚¹å·²å®Œæˆï¼Œæ¢å¤å…¶è¾“å‡ºæ•°æ®
                if status == 'completed':
                    output_data = {
                        'status': 'completed',
                        'node_name': node_name,
                        'completed_at': str(node.get('completed_at', '')),
                        'output_data': node.get('output_data', {})
                    }
                    
                    # æ ‡è®°èŠ‚ç‚¹å®Œæˆ
                    await context.mark_node_completed(node_id, node_instance_id, output_data)
                    completed_count += 1
                    logger.debug(f"âœ… æ¢å¤å·²å®ŒæˆèŠ‚ç‚¹: {node_name}")
                
                elif status == 'running':
                    # æ ‡è®°èŠ‚ç‚¹æ­£åœ¨æ‰§è¡Œ
                    await context.mark_node_executing(node_id, node_instance_id)
                    logger.debug(f"ğŸ”„ æ¢å¤æ‰§è¡Œä¸­èŠ‚ç‚¹: {node_name}")
            
            logger.info(f"ğŸ¯ ä¸Šä¸‹æ–‡æ¢å¤å®Œæˆ: {completed_count} ä¸ªå·²å®ŒæˆèŠ‚ç‚¹å·²æ¢å¤")
            
            # 4. æŒä¹…åŒ–ä¸Šä¸‹æ–‡çŠ¶æ€
            if self._persistence_enabled:
                await self._persist_context_snapshot(workflow_instance_id, context)
            
            return context
            
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡å¤±è´¥: {workflow_instance_id}, é”™è¯¯: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    async def _persist_context_snapshot(self, workflow_instance_id: uuid.UUID, context: WorkflowExecutionContext):
        """æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¿«ç…§åˆ°æ•°æ®åº“"""
        try:
            if not self._persistence_enabled:
                return
                
            logger.debug(f"ğŸ’¾ æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¿«ç…§: {workflow_instance_id}")
            
            # æ„é€ å¿«ç…§æ•°æ®
            snapshot_data = {
                'workflow_instance_id': str(workflow_instance_id),
                'execution_context': _serialize_for_json(context.execution_context),
                'node_states': {str(k): v for k, v in context.node_states.items()},
                'completed_nodes_count': len(context.execution_context.get('completed_nodes', set())),
                'snapshot_time': datetime.utcnow().isoformat(),
                'context_version': '2.0'  # ç‰ˆæœ¬æ ‡è¯†
            }
            
            # è¿™é‡Œå¯ä»¥å­˜å‚¨åˆ°Redisæˆ–æ•°æ®åº“è¡¨ä¸­
            # æš‚æ—¶ä½¿ç”¨æ—¥å¿—è®°å½•ï¼ˆç”Ÿäº§ç¯å¢ƒä¸­å¯æ›¿æ¢ä¸ºå®é™…å­˜å‚¨ï¼‰
            logger.debug(f"ğŸ“Š ä¸Šä¸‹æ–‡å¿«ç…§æ•°æ®: {len(str(snapshot_data))} å­—ç¬¦")
            
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¿«ç…§å¤±è´¥: {e}")
    
    async def create_context_snapshot(self, workflow_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """åˆ›å»ºä¸Šä¸‹æ–‡å¿«ç…§ï¼ˆç”¨äºç»†åˆ†å·¥ä½œæµéš”ç¦»ï¼‰"""
        context = await self.get_context(workflow_instance_id)
        if not context:
            return None
            
        return {
            'workflow_instance_id': str(workflow_instance_id),
            'execution_context': _serialize_for_json(context.execution_context),
            'node_states': {str(k): v for k, v in context.node_states.items()},
            'node_dependencies': {str(k): v for k, v in context.node_dependencies.items()},
            'snapshot_time': datetime.utcnow().isoformat()
        }
    
    async def restore_from_snapshot(self, workflow_instance_id: uuid.UUID, snapshot: Dict[str, Any]):
        """ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç»†åˆ†å·¥ä½œæµéš”ç¦»ï¼‰"""
        try:
            logger.info(f"ğŸ”„ ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡: {workflow_instance_id}")
            
            async with self._contexts_lock:
                if workflow_instance_id not in self.contexts:
                    context = WorkflowExecutionContext(workflow_instance_id)
                    self.contexts[workflow_instance_id] = context
                else:
                    context = self.contexts[workflow_instance_id]
                
                # æ¢å¤æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œç¡®ä¿é›†åˆç±»å‹å­—æ®µæ­£ç¡®æ¢å¤
                execution_context = snapshot.get('execution_context', {})
                context.execution_context.update(execution_context)
                
                # ä¿®å¤ï¼šç¡®ä¿å…³é”®çš„é›†åˆå­—æ®µè¢«æ­£ç¡®æ¢å¤ä¸ºsetç±»å‹ï¼ˆJSONåºåˆ—åŒ–ä¼šå°†setè½¬ä¸ºlistï¼‰
                if 'completed_nodes' in execution_context:
                    context.execution_context['completed_nodes'] = set(execution_context['completed_nodes'])
                if 'current_executing_nodes' in execution_context:
                    context.execution_context['current_executing_nodes'] = set(execution_context['current_executing_nodes'])
                if 'failed_nodes' in execution_context:
                    context.execution_context['failed_nodes'] = set(execution_context['failed_nodes'])
                
                # æ¢å¤èŠ‚ç‚¹çŠ¶æ€
                node_states = snapshot.get('node_states', {})
                for node_id_str, state in node_states.items():
                    context.node_states[uuid.UUID(node_id_str)] = state
                
                logger.info(f"âœ… ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡æˆåŠŸ: {workflow_instance_id}")
                
        except Exception as e:
            logger.error(f"âŒ ä»å¿«ç…§æ¢å¤ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    async def check_context_health(self, workflow_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡å¥åº·çŠ¶æ€"""
        try:
            context = self.contexts.get(workflow_instance_id)
            
            if not context:
                return {
                    'healthy': False,
                    'status': 'context_missing',
                    'message': 'ä¸Šä¸‹æ–‡ä¸å­˜åœ¨äºå†…å­˜ä¸­',
                    'auto_recovery_available': self._auto_recovery_enabled
                }
            
            # æ£€æŸ¥å†…å­˜çŠ¶æ€ä¸æ•°æ®åº“çŠ¶æ€ä¸€è‡´æ€§
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            db_nodes = await node_repo.get_instances_by_workflow_instance(workflow_instance_id)
            
            db_completed_count = sum(1 for node in db_nodes if node.get('status') == 'completed')
            memory_completed_count = len(context.execution_context.get('completed_nodes', set()))
            
            consistent = db_completed_count == memory_completed_count
            
            return {
                'healthy': consistent,
                'status': 'consistent' if consistent else 'inconsistent',
                'memory_completed_nodes': memory_completed_count,
                'db_completed_nodes': db_completed_count,
                'total_nodes': len(db_nodes),
                'context_size': len(context.node_dependencies),
                'last_activity': context.execution_context.get('last_snapshot_time')
            }
            
        except Exception as e:
            return {
                'healthy': False,
                'status': 'check_failed',
                'error': str(e)
            }


# å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
_global_context_manager: Optional[WorkflowExecutionContextManager] = None

def get_context_manager() -> WorkflowExecutionContextManager:
    """è·å–å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    global _global_context_manager
    if _global_context_manager is None:
        _global_context_manager = WorkflowExecutionContextManager()
        logger.debug("ğŸŒ åˆå§‹åŒ–å…¨å±€å·¥ä½œæµæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    return _global_context_manager