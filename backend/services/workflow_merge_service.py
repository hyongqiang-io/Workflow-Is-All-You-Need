"""
å·¥ä½œæµåˆå¹¶æœåŠ¡ - Subdivision Tree Merge Service

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»æœ€ä½å±‚å¼€å§‹é€å±‚åˆå¹¶subdivision
2. å°†çˆ¶èŠ‚ç‚¹ç”¨å­å·¥ä½œæµæ›¿æ¢
3. å»é™¤å­å·¥ä½œæµçš„å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
4. é‡æ–°è¿æ¥ä¸Šä¸‹æ¸¸èŠ‚ç‚¹
5. ç”Ÿæˆæ–°çš„å·¥ä½œæµæ¨¡æ¿
"""

import uuid
from typing import Dict, Any, List, Optional, Set, Tuple
from dataclasses import dataclass
from loguru import logger

from ..repositories.base import BaseRepository
from ..utils.helpers import now_utc
from .subdivision_tree_builder import SubdivisionTree, SubdivisionNode


@dataclass
class MergeCandidate:
    """åˆå¹¶å€™é€‰é¡¹"""
    subdivision_id: str
    parent_subdivision_id: Optional[str]
    workflow_instance_id: str
    workflow_base_id: str
    node_name: str
    depth: int
    can_merge: bool = True
    merge_reason: str = ""


@dataclass
class MergeOperation:
    """åˆå¹¶æ“ä½œ"""
    target_node_id: str  # è¢«æ›¿æ¢çš„çˆ¶èŠ‚ç‚¹ID
    sub_workflow_id: str  # æ›¿æ¢ç”¨çš„å­å·¥ä½œæµID
    subdivision_id: str  # å¯¹åº”çš„subdivision ID
    depth: int  # åˆå¹¶æ·±åº¦


class WorkflowMergeService:
    """å·¥ä½œæµåˆå¹¶æœåŠ¡"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_merge").db
    
    async def get_merge_candidates(self, workflow_instance_id: uuid.UUID) -> List[MergeCandidate]:
        """
        è·å–å¯åˆå¹¶çš„subdivisionåˆ—è¡¨
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            
        Returns:
            åˆå¹¶å€™é€‰é¡¹åˆ—è¡¨ï¼ŒæŒ‰æ·±åº¦ä»é«˜åˆ°ä½æ’åºï¼ˆä»å¶å­èŠ‚ç‚¹å¼€å§‹ï¼‰
        """
        try:
            logger.info(f"ğŸ” è·å–åˆå¹¶å€™é€‰: {workflow_instance_id}")
            
            # ä½¿ç”¨subdivision tree builderè·å–æ ‘ç»“æ„
            from .workflow_template_connection_service import WorkflowTemplateConnectionService
            connection_service = WorkflowTemplateConnectionService()
            
            subdivisions_data = await connection_service._get_all_subdivisions_simple(workflow_instance_id)
            
            if not subdivisions_data:
                logger.info(f"æ— subdivisionæ•°æ®: {workflow_instance_id}")
                return []
            
            tree = SubdivisionTree().build_from_subdivisions(subdivisions_data)
            candidates = []
            
            # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹å¹¶æŒ‰æ·±åº¦æ’åº
            all_nodes = tree.get_all_nodes()
            # ä»æœ€æ·±å±‚å¼€å§‹ï¼ˆå¶å­èŠ‚ç‚¹ä¼˜å…ˆåˆå¹¶ï¼‰
            sorted_nodes = sorted(all_nodes, key=lambda n: n.depth, reverse=True)
            
            for node in sorted_nodes:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
                can_merge, reason = await self._check_merge_feasibility(node)
                
                candidate = MergeCandidate(
                    subdivision_id=node.subdivision_id,
                    parent_subdivision_id=node.parent_id,
                    workflow_instance_id=node.workflow_instance_id or "",
                    workflow_base_id=node.workflow_base_id,
                    node_name=node.node_name,
                    depth=node.depth,
                    can_merge=can_merge,
                    merge_reason=reason
                )
                candidates.append(candidate)
            
            logger.info(f"âœ… æ‰¾åˆ° {len(candidates)} ä¸ªåˆå¹¶å€™é€‰")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ è·å–åˆå¹¶å€™é€‰å¤±è´¥: {e}")
            raise
    
    async def execute_merge(self, workflow_instance_id: uuid.UUID, 
                          selected_merges: List[str], 
                          creator_id: uuid.UUID) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥ä½œæµåˆå¹¶
        
        Args:
            workflow_instance_id: ä¸»å·¥ä½œæµå®ä¾‹ID
            selected_merges: é€‰ä¸­çš„subdivision IDåˆ—è¡¨
            creator_id: åˆå¹¶æ“ä½œæ‰§è¡Œè€…ID
            
        Returns:
            åˆå¹¶ç»“æœä¿¡æ¯
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹å·¥ä½œæµåˆå¹¶: {workflow_instance_id}")
            logger.info(f"é€‰ä¸­çš„subdivisions: {selected_merges}")
            logger.info(f"åˆå¹¶æ‰§è¡Œè€…: {creator_id}")
            
            # 1. è·å–åˆå¹¶å€™é€‰é¡¹å¹¶ç­›é€‰
            candidates = await self.get_merge_candidates(workflow_instance_id)
            
            # æ”¯æŒé€šè¿‡subdivision_idæˆ–workflow_instance_idåŒ¹é…
            selected_candidates = []
            for c in candidates:
                # å¯ä»¥é€šè¿‡subdivision_idæˆ–å¯¹åº”çš„workflow_instance_idé€‰æ‹©
                if c.subdivision_id in selected_merges or c.workflow_instance_id in selected_merges:
                    selected_candidates.append(c)
            
            logger.info(f"ğŸ“‹ å€™é€‰åŒ¹é…ç»“æœ: {len(selected_candidates)}/{len(candidates)} ä¸ªå€™é€‰è¢«é€‰ä¸­")
            
            if not selected_candidates:
                return {"success": False, "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆå¹¶å€™é€‰"}
            
            # 2. æŒ‰æ·±åº¦æ’åºï¼ˆä»æœ€æ·±å±‚å¼€å§‹åˆå¹¶ï¼‰
            selected_candidates.sort(key=lambda c: c.depth, reverse=True)
            
            # 3. åˆ›å»ºæ–°çš„å·¥ä½œæµæ¨¡æ¿
            new_workflow_base_id = uuid.uuid4()
            merge_operations = []
            
            # 4. é€å±‚æ‰§è¡Œåˆå¹¶ï¼Œæ”¶é›†èŠ‚ç‚¹å’Œè¿æ¥æ•°æ®
            all_merged_nodes = []
            all_merged_connections = []
            
            for candidate in selected_candidates:
                if not candidate.can_merge:
                    logger.warning(f"âš ï¸ è·³è¿‡ä¸å¯åˆå¹¶çš„èŠ‚ç‚¹: {candidate.subdivision_id} - {candidate.merge_reason}")
                    continue
                
                logger.info(f"ğŸ”„ åˆå¹¶å±‚çº§ {candidate.depth}: {candidate.node_name}")
                
                # æ‰§è¡Œå•ä¸ªåˆå¹¶æ“ä½œ
                merge_result = await self._execute_single_merge(candidate, new_workflow_base_id)
                
                if merge_result['success']:
                    merge_operations.append(merge_result['operation'])
                    all_merged_nodes.extend(merge_result['merged_nodes'])
                    all_merged_connections.extend(merge_result['merged_connections'])
                else:
                    logger.error(f"âŒ åˆå¹¶å¤±è´¥: {candidate.subdivision_id} - {merge_result['error']}")
            
            # 5. ç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶å·¥ä½œæµ
            if merge_operations:
                final_workflow = await self._finalize_merged_workflow(
                    workflow_instance_id, new_workflow_base_id, merge_operations, 
                    creator_id, all_merged_nodes, all_merged_connections
                )
                
                return {
                    "success": True,
                    "new_workflow_base_id": str(new_workflow_base_id),
                    "merged_count": len(merge_operations),
                    "merge_operations": [op.__dict__ for op in merge_operations],
                    "final_workflow": final_workflow
                }
            else:
                return {"success": False, "message": "æ²¡æœ‰æˆåŠŸæ‰§è¡Œä»»ä½•åˆå¹¶æ“ä½œ"}
                
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµåˆå¹¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _check_merge_feasibility(self, node: SubdivisionNode) -> Tuple[bool, str]:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¯ä»¥åˆå¹¶"""
        try:
            if not node.workflow_instance_id:
                return False, "ç¼ºå°‘å­å·¥ä½œæµå®ä¾‹"
            
            # æ£€æŸ¥å­å·¥ä½œæµçŠ¶æ€
            workflow_status = await self.db.fetch_one("""
                SELECT status FROM workflow_instance WHERE workflow_instance_id = %s
            """, node.workflow_instance_id)
            
            if not workflow_status:
                return False, "å­å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨"
            
            if workflow_status['status'] not in ['completed', 'draft']:
                return False, f"å­å·¥ä½œæµçŠ¶æ€ä¸å…è®¸åˆå¹¶: {workflow_status['status']}"
            
            return True, "å¯ä»¥åˆå¹¶"
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥åˆå¹¶å¯è¡Œæ€§å¤±è´¥: {e}")
            return False, f"æ£€æŸ¥å¤±è´¥: {str(e)}"
    
    async def _execute_single_merge(self, candidate: MergeCandidate, 
                                  new_workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªsubdivisionçš„åˆå¹¶"""
        try:
            logger.info(f"ğŸ”§ æ‰§è¡Œå•ä¸ªåˆå¹¶: {candidate.node_name}")
            
            # è·å–å­å·¥ä½œæµçš„æ‰€æœ‰èŠ‚ç‚¹
            sub_workflow_id = await self.db.fetch_one("""
                SELECT workflow_id FROM workflow 
                WHERE workflow_base_id = %s 
                AND is_current_version = TRUE
            """, candidate.workflow_base_id)
            
            if not sub_workflow_id:
                raise Exception(f"æ‰¾ä¸åˆ°å­å·¥ä½œæµ: {candidate.workflow_base_id}")
            
            sub_workflow_id = sub_workflow_id['workflow_id']
            
            # è·å–å­å·¥ä½œæµçš„èŠ‚ç‚¹ï¼ˆæ’é™¤å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹ï¼‰
            nodes_query = """
            SELECT node_id, node_base_id, name, type, task_description, 
                   position_x, position_y, version
            FROM node 
            WHERE workflow_id = %s 
            AND is_deleted = FALSE 
            AND type NOT IN ('start', 'end')
            ORDER BY name
            """
            
            sub_nodes = await self.db.fetch_all(nodes_query, sub_workflow_id)
            logger.info(f"ğŸ“‹ å­å·¥ä½œæµæœ‰ {len(sub_nodes)} ä¸ªå¯åˆå¹¶èŠ‚ç‚¹")
            
            # è·å–å­å·¥ä½œæµçš„è¿æ¥
            connections_query = """
            SELECT from_node_id, to_node_id, condition_config
            FROM node_connection 
            WHERE workflow_id = %s
            """
            
            sub_connections = await self.db.fetch_all(connections_query, sub_workflow_id)
            logger.info(f"ğŸ”— å­å·¥ä½œæµæœ‰ {len(sub_connections)} ä¸ªè¿æ¥")
            
            operation = MergeOperation(
                target_node_id=candidate.subdivision_id,
                sub_workflow_id=candidate.workflow_base_id,
                subdivision_id=candidate.subdivision_id,
                depth=candidate.depth
            )
            
            return {
                "success": True,
                "operation": operation,
                "merged_nodes": sub_nodes,
                "merged_connections": sub_connections
            }
            
        except Exception as e:
            logger.error(f"âŒ å•ä¸ªåˆå¹¶æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _finalize_merged_workflow(self, original_workflow_id: uuid.UUID, 
                                      new_workflow_base_id: uuid.UUID, 
                                      merge_operations: List[MergeOperation],
                                      creator_id: uuid.UUID,
                                      merged_nodes: List[Dict[str, Any]],
                                      merged_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å®Œæˆåˆå¹¶åçš„å·¥ä½œæµç”Ÿæˆ"""
        try:
            logger.info(f"ğŸ¯ å®Œæˆåˆå¹¶å·¥ä½œæµç”Ÿæˆ: {new_workflow_base_id}")
            
            # è·å–çˆ¶å·¥ä½œæµåç§°
            parent_workflow = await self.db.fetch_one("""
                SELECT w.name, w.workflow_base_id 
                FROM workflow_instance wi
                JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id
                WHERE wi.workflow_instance_id = %s 
                AND w.is_current_version = TRUE
            """, original_workflow_id)
            
            parent_name = parent_workflow['name'] if parent_workflow else "Unknown_Workflow"
            
            # ç”Ÿæˆåˆå¹¶åºå·
            existing_merges = await self.db.fetch_all("""
                SELECT name FROM workflow 
                WHERE name LIKE %s AND is_deleted = FALSE
                ORDER BY created_at
            """, f"{parent_name}_åˆå¹¶_%")
            
            merge_number = len(existing_merges) + 1
            
            new_workflow_id = uuid.uuid4()
            merged_name = f"{parent_name}_åˆå¹¶_{merge_number}"
            merged_description = f"åˆå¹¶äº†{len(merge_operations)}ä¸ªsubdivisionçš„å·¥ä½œæµï¼ŒåŸºäº{parent_name}"
            
            await self.db.execute("""
                INSERT INTO workflow (
                    workflow_id, workflow_base_id, name, description, 
                    creator_id, is_current_version, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, new_workflow_id, new_workflow_base_id, merged_name, merged_description,
                 creator_id, True, now_utc())
            
            logger.info(f"âœ… åˆ›å»ºåˆå¹¶å·¥ä½œæµè®°å½•: {merged_name}")
            
            # å¤åˆ¶çˆ¶å·¥ä½œæµçš„åŸºç¡€ç»“æ„ï¼ˆå¼€å§‹å’Œç»“æŸèŠ‚ç‚¹ï¼‰
            parent_workflow_id = await self.db.fetch_one("""
                SELECT w.workflow_id FROM workflow_instance wi
                JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id
                WHERE wi.workflow_instance_id = %s 
                AND w.is_current_version = TRUE
            """, original_workflow_id)
            
            if parent_workflow_id:
                parent_workflow_id = parent_workflow_id['workflow_id']
                
                # å¤åˆ¶çˆ¶å·¥ä½œæµçš„å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
                parent_nodes = await self.db.fetch_all("""
                    SELECT node_id, node_base_id, name, type, task_description, 
                           position_x, position_y, version
                    FROM node 
                    WHERE workflow_id = %s AND is_deleted = FALSE
                    AND type IN ('start', 'end')
                """, parent_workflow_id)
                
                logger.info(f"ğŸ“‹ å¤åˆ¶çˆ¶å·¥ä½œæµçš„ {len(parent_nodes)} ä¸ªåŸºç¡€èŠ‚ç‚¹")
                
                # åˆ›å»ºèŠ‚ç‚¹IDæ˜ å°„
                node_id_mapping = {}
                
                # å¤åˆ¶åŸºç¡€èŠ‚ç‚¹
                for node in parent_nodes:
                    new_node_id = uuid.uuid4()
                    new_node_base_id = uuid.uuid4()
                    node_id_mapping[node['node_id']] = new_node_id
                    
                    await self.db.execute("""
                        INSERT INTO node (
                            node_id, node_base_id, workflow_id, workflow_base_id,
                            name, type, task_description, position_x, position_y,
                            version, is_current_version, created_at, is_deleted
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                         node['name'], node['type'], node['task_description'], 
                         node['position_x'], node['position_y'], 1, True, now_utc(), False)
                
                # å¤åˆ¶åˆå¹¶çš„èŠ‚ç‚¹
                for node in merged_nodes:
                    new_node_id = uuid.uuid4()
                    new_node_base_id = uuid.uuid4()
                    node_id_mapping[node['node_id']] = new_node_id
                    
                    await self.db.execute("""
                        INSERT INTO node (
                            node_id, node_base_id, workflow_id, workflow_base_id,
                            name, type, task_description, position_x, position_y,
                            version, is_current_version, created_at, is_deleted
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                         node['name'], node['type'], node['task_description'], 
                         node['position_x'], node['position_y'], 1, True, now_utc(), False)
                
                logger.info(f"âœ… å¤åˆ¶äº† {len(merged_nodes)} ä¸ªåˆå¹¶èŠ‚ç‚¹")
                
                # å¤åˆ¶è¿æ¥
                connections_copied = 0
                for connection in merged_connections:
                    if (connection['from_node_id'] in node_id_mapping and 
                        connection['to_node_id'] in node_id_mapping):
                        
                        await self.db.execute("""
                            INSERT INTO node_connection (
                                from_node_id, to_node_id, workflow_id,
                                condition_config, created_at
                            ) VALUES (%s, %s, %s, %s, %s)
                        """, node_id_mapping[connection['from_node_id']],
                             node_id_mapping[connection['to_node_id']],
                             new_workflow_id, connection.get('condition_config'),
                             now_utc())
                        connections_copied += 1
                
                logger.info(f"âœ… å¤åˆ¶äº† {connections_copied} ä¸ªè¿æ¥")
                
                total_nodes = len(parent_nodes) + len(merged_nodes)
                
                return {
                    "workflow_id": str(new_workflow_id),
                    "workflow_base_id": str(new_workflow_base_id),
                    "name": merged_name,
                    "description": merged_description,
                    "nodes_count": total_nodes,
                    "connections_count": connections_copied,
                    "merge_operations_count": len(merge_operations)
                }
            else:
                raise Exception("æ— æ³•æ‰¾åˆ°çˆ¶å·¥ä½œæµä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"âŒ å®Œæˆåˆå¹¶å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
            raise