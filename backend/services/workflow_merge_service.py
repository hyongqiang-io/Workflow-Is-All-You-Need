"""
å·¥ä½œæµåˆå¹¶æœåŠ¡
Workflow Merge Service

å¤„ç†å·¥ä½œæµæ¨¡æ¿é—´çš„åˆå¹¶æ“ä½œï¼ŒåŒ…æ‹¬èŠ‚ç‚¹æ›¿æ¢ã€è¿æ¥é‡æ„å’Œæ–°å·¥ä½œæµç”Ÿæˆ
"""

import uuid
from typing import Optional, Dict, Any, List, Tuple
from loguru import logger
from datetime import datetime

from ..repositories.base import BaseRepository
from ..repositories.workflow.workflow_repository import WorkflowRepository
from ..repositories.node.node_repository import NodeRepository
from ..models.workflow import WorkflowCreate
from ..models.node import NodeCreate, NodeType
from ..utils.helpers import now_utc


class WorkflowMergeService:
    """å·¥ä½œæµåˆå¹¶æœåŠ¡"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_merge").db
        self.workflow_repo = WorkflowRepository()
        self.node_repo = NodeRepository()
    
    async def preview_workflow_merge(
        self, 
        parent_workflow_id: uuid.UUID,
        merge_candidates: List[Dict[str, Any]],
        user_id: uuid.UUID
    ) -> Dict[str, Any]:
        """
        é¢„è§ˆå·¥ä½œæµåˆå¹¶ç»“æœ
        
        Args:
            parent_workflow_id: çˆ¶å·¥ä½œæµåŸºç¡€ID
            merge_candidates: åˆå¹¶å€™é€‰åˆ—è¡¨
            user_id: ç”¨æˆ·ID
            
        Returns:
            åˆå¹¶é¢„è§ˆæ•°æ®
        """
        try:
            logger.info(f"ğŸ” é¢„è§ˆå·¥ä½œæµåˆå¹¶: çˆ¶å·¥ä½œæµ={parent_workflow_id}, å€™é€‰æ•°={len(merge_candidates)}")
            
            # è·å–çˆ¶å·¥ä½œæµè¯¦ç»†ä¿¡æ¯
            parent_workflow = await self._get_workflow_structure(parent_workflow_id)
            if not parent_workflow:
                raise ValueError("çˆ¶å·¥ä½œæµä¸å­˜åœ¨")
            
            # åˆ†ææ¯ä¸ªåˆå¹¶å€™é€‰
            merge_previews = []
            for candidate in merge_candidates:
                preview = await self._analyze_merge_candidate(
                    parent_workflow, candidate, user_id
                )
                merge_previews.append(preview)
            
            # ç”Ÿæˆæ•´ä½“åˆå¹¶é¢„è§ˆ
            overall_preview = self._build_merge_preview(
                parent_workflow, merge_previews
            )
            
            logger.info(f"âœ… å·¥ä½œæµåˆå¹¶é¢„è§ˆå®Œæˆ: {len(merge_previews)} ä¸ªå€™é€‰")
            
            return overall_preview
            
        except Exception as e:
            logger.error(f"âŒ é¢„è§ˆå·¥ä½œæµåˆå¹¶å¤±è´¥: {e}")
            raise
    
    async def execute_workflow_merge(
        self,
        parent_workflow_id: uuid.UUID,
        selected_merges: List[Dict[str, Any]],
        merge_config: Dict[str, Any],
        user_id: uuid.UUID
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥ä½œæµåˆå¹¶æ“ä½œ
        
        Args:
            parent_workflow_id: çˆ¶å·¥ä½œæµåŸºç¡€ID
            selected_merges: é€‰ä¸­çš„åˆå¹¶æ“ä½œåˆ—è¡¨
            merge_config: åˆå¹¶é…ç½®
            user_id: ç”¨æˆ·ID
            
        Returns:
            åˆå¹¶ç»“æœ
        """
        try:
            logger.info(f"ğŸ”„ [MERGE-START] å¼€å§‹æ‰§è¡Œå·¥ä½œæµåˆå¹¶")
            logger.info(f"ğŸ“‹ [MERGE-PARAMS] çˆ¶å·¥ä½œæµID: {parent_workflow_id}")
            logger.info(f"ğŸ“‹ [MERGE-PARAMS] åˆå¹¶æ“ä½œæ•°: {len(selected_merges)}")
            logger.info(f"ğŸ“‹ [MERGE-PARAMS] ç”¨æˆ·ID: {user_id}")
            logger.info(f"ğŸ“‹ [MERGE-PARAMS] åˆå¹¶é…ç½®: {merge_config}")
            
            for i, merge in enumerate(selected_merges):
                logger.info(f"ğŸ“‹ [MERGE-PARAMS] åˆå¹¶æ“ä½œ {i+1}: {merge}")
            
            # è·å–çˆ¶å·¥ä½œæµç»“æ„
            logger.info(f"ğŸ” [STEP-1] è·å–çˆ¶å·¥ä½œæµç»“æ„...")
            parent_workflow = await self._get_workflow_structure(parent_workflow_id)
            if not parent_workflow:
                logger.error(f"âŒ [STEP-1-FAILED] çˆ¶å·¥ä½œæµä¸å­˜åœ¨: {parent_workflow_id}")
                return {
                    "success": False,
                    "message": f"å·¥ä½œæµä¸å­˜åœ¨",
                    "errors": [f"å·¥ä½œæµID {parent_workflow_id} ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤"],
                    "warnings": ["è¯·æ£€æŸ¥å·¥ä½œæµIDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–é€‰æ‹©å…¶ä»–æœ‰æ•ˆçš„å·¥ä½œæµ"]
                }
            
            logger.info(f"âœ… [STEP-1-SUCCESS] çˆ¶å·¥ä½œæµç»“æ„è·å–æˆåŠŸ:")
            logger.info(f"   - å·¥ä½œæµåç§°: {parent_workflow['workflow']['name']}")
            logger.info(f"   - èŠ‚ç‚¹æ•°é‡: {len(parent_workflow['nodes'])}")
            logger.info(f"   - è¿æ¥æ•°é‡: {len(parent_workflow['connections'])}")
            
            # éªŒè¯åˆå¹¶é…ç½®
            logger.info(f"ğŸ” [STEP-2] éªŒè¯åˆå¹¶é…ç½®...")
            validation_result = await self._validate_merge_operations(
                parent_workflow, selected_merges, user_id
            )
            
            if not validation_result["valid"]:
                logger.error(f"âŒ [STEP-2-FAILED] åˆå¹¶éªŒè¯å¤±è´¥:")
                logger.error(f"   - é”™è¯¯: {validation_result['errors']}")
                logger.error(f"   - è­¦å‘Š: {validation_result['warnings']}")
                return {
                    "success": False,
                    "message": "åˆå¹¶éªŒè¯å¤±è´¥",
                    "errors": validation_result["errors"],
                    "warnings": validation_result["warnings"]
                }
            
            logger.info(f"âœ… [STEP-2-SUCCESS] åˆå¹¶éªŒè¯é€šè¿‡")
            if validation_result["warnings"]:
                logger.warning(f"âš ï¸ [STEP-2-WARNINGS] éªŒè¯è­¦å‘Š: {validation_result['warnings']}")
            
            # æ‰§è¡Œåˆå¹¶æ“ä½œ
            logger.info(f"ğŸš€ [STEP-3] æ‰§è¡Œåˆå¹¶æ“ä½œ...")
            merge_result = await self._perform_merge(
                parent_workflow, selected_merges, merge_config, user_id
            )
            
            if merge_result.get("success"):
                logger.info(f"âœ… [MERGE-SUCCESS] å·¥ä½œæµåˆå¹¶æ‰§è¡Œå®Œæˆ:")
                logger.info(f"   - æ–°å·¥ä½œæµID: {merge_result.get('new_workflow_id')}")
                logger.info(f"   - æ–°å·¥ä½œæµåç§°: {merge_result.get('new_workflow_name')}")
                logger.info(f"   - åˆå¹¶ç»Ÿè®¡: {merge_result.get('merge_statistics')}")
            else:
                logger.error(f"âŒ [MERGE-FAILED] åˆå¹¶æ“ä½œå¤±è´¥: {merge_result.get('message')}")
            
            return merge_result
            
        except Exception as e:
            logger.error(f"âŒ [MERGE-EXCEPTION] æ‰§è¡Œå·¥ä½œæµåˆå¹¶å¼‚å¸¸: {e}")
            logger.error(f"âŒ [MERGE-EXCEPTION] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ [MERGE-EXCEPTION] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def _get_workflow_structure(self, workflow_base_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–å·¥ä½œæµçš„å®Œæ•´ç»“æ„ä¿¡æ¯"""
        try:
            logger.info(f"ğŸ” [GET-WORKFLOW-STRUCTURE] è·å–å·¥ä½œæµç»“æ„: {workflow_base_id}")
            
            # è·å–å·¥ä½œæµåŸºæœ¬ä¿¡æ¯
            logger.info(f"   ğŸ“‹ æŸ¥è¯¢å·¥ä½œæµåŸºæœ¬ä¿¡æ¯...")
            workflow_query = """
            SELECT 
                w.workflow_id,
                w.workflow_base_id,
                w.name,
                w.description,
                w.creator_id,
                w.version
            FROM workflow w
            WHERE w.workflow_base_id = %s
            AND w.is_current_version = TRUE
            AND w.is_deleted = FALSE
            """
            
            workflow = await self.db.fetch_one(workflow_query, workflow_base_id)
            if not workflow:
                logger.error(f"   âŒ å·¥ä½œæµä¸å­˜åœ¨æˆ–æ— å½“å‰ç‰ˆæœ¬: {workflow_base_id}")
                return None
            
            logger.info(f"   âœ… å·¥ä½œæµåŸºæœ¬ä¿¡æ¯è·å–æˆåŠŸ:")
            logger.info(f"     - ID: {workflow['workflow_id']}")
            logger.info(f"     - åç§°: {workflow['name']}")
            logger.info(f"     - ç‰ˆæœ¬: {workflow['version']}")
            logger.info(f"     - åˆ›å»ºè€…: {workflow['creator_id']}")
            
            # è·å–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«processorï¼‰
            logger.info(f"   ğŸ“‹ æŸ¥è¯¢å·¥ä½œæµèŠ‚ç‚¹...")
            nodes_query = """
            SELECT 
                n.node_id,
                n.node_base_id,
                n.name,
                n.type,
                n.task_description,
                n.position_x,
                n.position_y
            FROM node n
            WHERE n.workflow_base_id = %s
            AND n.is_current_version = TRUE
            AND n.is_deleted = FALSE
            ORDER BY n.created_at
            """
            
            nodes = await self.db.fetch_all(nodes_query, workflow_base_id)
            logger.info(f"   âœ… å·¥ä½œæµèŠ‚ç‚¹æŸ¥è¯¢å®Œæˆ: {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
            for i, node in enumerate(nodes):
                logger.info(f"     èŠ‚ç‚¹ {i+1}: {node['name']} (ç±»å‹: {node['type']}, ID: {node['node_id']})")
            
            # è·å–æ‰€æœ‰è¿æ¥
            logger.info(f"   ğŸ“‹ æŸ¥è¯¢å·¥ä½œæµè¿æ¥...")
            connections_query = """
            SELECT 
                CONCAT(nc.from_node_id, '-', nc.to_node_id) as connection_id,
                nc.from_node_id,
                nc.to_node_id,
                nc.connection_type,
                fn.node_base_id as from_node_base_id,
                fn.name as from_node_name,
                tn.node_base_id as to_node_base_id,
                tn.name as to_node_name
            FROM node_connection nc
            JOIN node fn ON nc.from_node_id = fn.node_id
            JOIN node tn ON nc.to_node_id = tn.node_id
            WHERE nc.workflow_id = %s
            ORDER BY nc.created_at
            """
            
            connections = await self.db.fetch_all(connections_query, workflow["workflow_id"])
            logger.info(f"   âœ… å·¥ä½œæµè¿æ¥æŸ¥è¯¢å®Œæˆ: {len(connections)} ä¸ªè¿æ¥")
            
            for i, conn in enumerate(connections):
                logger.info(f"     è¿æ¥ {i+1}: {conn['from_node_name']} -> {conn['to_node_name']} (ç±»å‹: {conn['connection_type']})")
            
            result = {
                "workflow": workflow,
                "nodes": [dict(node) for node in nodes],
                "connections": [dict(conn) for conn in connections]
            }
            
            logger.info(f"âœ… [GET-WORKFLOW-STRUCTURE-SUCCESS] å·¥ä½œæµç»“æ„è·å–å®Œæˆ:")
            logger.info(f"   - èŠ‚ç‚¹æ€»æ•°: {len(result['nodes'])}")
            logger.info(f"   - è¿æ¥æ€»æ•°: {len(result['connections'])}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ [GET-WORKFLOW-STRUCTURE-EXCEPTION] è·å–å·¥ä½œæµç»“æ„å¼‚å¸¸: {workflow_base_id}, {e}")
            logger.error(f"âŒ [GET-WORKFLOW-STRUCTURE-EXCEPTION] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ [GET-WORKFLOW-STRUCTURE-EXCEPTION] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return None
    
    async def _analyze_merge_candidate(
        self,
        parent_workflow: Dict[str, Any],
        candidate: Dict[str, Any],
        user_id: uuid.UUID
    ) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªåˆå¹¶å€™é€‰"""
        try:
            parent_workflow_id = candidate["parent_workflow_id"]
            sub_workflow_id = candidate["sub_workflow_id"]
            replaceable_node_id = candidate["replaceable_node"]["node_base_id"]
            
            # è·å–å­å·¥ä½œæµç»“æ„
            sub_workflow = await self._get_workflow_structure(uuid.UUID(sub_workflow_id))
            if not sub_workflow:
                return {
                    "candidate_id": candidate.get("subdivision_id"),
                    "valid": False,
                    "error": "å­å·¥ä½œæµä¸å­˜åœ¨",
                    "preview": None
                }
            
            # æ‰¾åˆ°è¦æ›¿æ¢çš„èŠ‚ç‚¹
            target_node = None
            for node in parent_workflow["nodes"]:
                if str(node["node_base_id"]) == replaceable_node_id:
                    target_node = node
                    break
            
            if not target_node:
                return {
                    "candidate_id": candidate.get("subdivision_id"),
                    "valid": False,
                    "error": "ç›®æ ‡èŠ‚ç‚¹ä¸å­˜åœ¨",
                    "preview": None
                }
            
            # åˆ†ææ›¿æ¢åçš„ç»“æ„å˜åŒ–
            merge_preview = self._calculate_merge_impact(
                parent_workflow, sub_workflow, target_node
            )
            
            return {
                "candidate_id": candidate.get("subdivision_id"),
                "valid": True,
                "target_node": {
                    "node_base_id": str(target_node["node_base_id"]),
                    "name": target_node["name"],
                    "type": target_node["type"]
                },
                "replacement_info": {
                    "sub_workflow_name": sub_workflow["workflow"]["name"],
                    "nodes_to_add": len(sub_workflow["nodes"]),
                    "connections_to_add": len(sub_workflow["connections"])
                },
                "preview": merge_preview
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æåˆå¹¶å€™é€‰å¤±è´¥: {e}")
            return {
                "candidate_id": candidate.get("subdivision_id"),
                "valid": False,
                "error": str(e),
                "preview": None
            }
    
    def _calculate_merge_impact(
        self,
        parent_workflow: Dict[str, Any],
        sub_workflow: Dict[str, Any],
        target_node: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è®¡ç®—åˆå¹¶å¯¹å·¥ä½œæµç»“æ„çš„å½±å“"""
        try:
            # åˆ†æè¿æ¥å˜åŒ–
            incoming_connections = []
            outgoing_connections = []
            
            target_node_id = str(target_node["node_id"])
            
            # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è¾“å…¥å’Œè¾“å‡ºè¿æ¥
            for conn in parent_workflow["connections"]:
                if str(conn["to_node_id"]) == target_node_id:
                    incoming_connections.append(conn)
                elif str(conn["from_node_id"]) == target_node_id:
                    outgoing_connections.append(conn)
            
            # åˆ†æå­å·¥ä½œæµçš„å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
            start_nodes = [n for n in sub_workflow["nodes"] if n["type"] == "start"]
            end_nodes = [n for n in sub_workflow["nodes"] if n["type"] == "end"]
            
            # è®¡ç®—æ–°çš„è¿æ¥ç­–ç•¥
            connection_strategy = self._plan_connection_strategy(
                incoming_connections, outgoing_connections, start_nodes, end_nodes
            )
            
            return {
                "nodes_removed": 1,  # ç›®æ ‡èŠ‚ç‚¹
                "nodes_added": len(sub_workflow["nodes"]),
                "connections_removed": len(incoming_connections) + len(outgoing_connections),
                "connections_added": len(sub_workflow["connections"]) + len(connection_strategy["bridge_connections"]),
                "connection_strategy": connection_strategy,
                "complexity_increase": len(sub_workflow["nodes"]) - 1,
                "estimated_execution_time_change": self._estimate_execution_time_change(
                    target_node, sub_workflow["nodes"]
                )
            }
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—åˆå¹¶å½±å“å¤±è´¥: {e}")
            return {
                "nodes_removed": 0,
                "nodes_added": 0,
                "connections_removed": 0,
                "connections_added": 0,
                "error": str(e)
            }
    
    def _plan_connection_strategy(
        self,
        incoming_connections: List[Dict[str, Any]],
        outgoing_connections: List[Dict[str, Any]],
        start_nodes: List[Dict[str, Any]],
        end_nodes: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """è§„åˆ’è¿æ¥ç­–ç•¥"""
        try:
            bridge_connections = []
            
            # è¾“å…¥è¿æ¥ç­–ç•¥ï¼šè¿æ¥åˆ°å­å·¥ä½œæµçš„å¼€å§‹èŠ‚ç‚¹
            for incoming in incoming_connections:
                if start_nodes:
                    # å¦‚æœæœ‰å¤šä¸ªå¼€å§‹èŠ‚ç‚¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                    target_start = start_nodes[0]
                    bridge_connections.append({
                        "type": "incoming_bridge",
                        "from_node_id": incoming["from_node_id"],
                        "to_node_id": target_start["node_id"],
                        "connection_type": incoming["connection_type"]
                    })
            
            # è¾“å‡ºè¿æ¥ç­–ç•¥ï¼šä»å­å·¥ä½œæµçš„ç»“æŸèŠ‚ç‚¹è¿æ¥
            for outgoing in outgoing_connections:
                if end_nodes:
                    # å¦‚æœæœ‰å¤šä¸ªç»“æŸèŠ‚ç‚¹ï¼Œæ¯ä¸ªéƒ½è¦è¿æ¥
                    for end_node in end_nodes:
                        bridge_connections.append({
                            "type": "outgoing_bridge", 
                            "from_node_id": end_node["node_id"],
                            "to_node_id": outgoing["to_node_id"],
                            "connection_type": outgoing["connection_type"]
                        })
            
            return {
                "bridge_connections": bridge_connections,
                "strategy": "replace_with_subflow",
                "start_nodes_count": len(start_nodes),
                "end_nodes_count": len(end_nodes),
                "connection_complexity": len(bridge_connections)
            }
            
        except Exception as e:
            logger.error(f"âŒ è§„åˆ’è¿æ¥ç­–ç•¥å¤±è´¥: {e}")
            return {
                "bridge_connections": [],
                "strategy": "error",
                "error": str(e)
            }
    
    def _estimate_execution_time_change(
        self,
        original_node: Dict[str, Any],
        replacement_nodes: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ä¼°ç®—æ‰§è¡Œæ—¶é—´å˜åŒ–"""
        try:
            # ç®€å•çš„å¯å‘å¼ä¼°ç®—
            original_complexity = 1  # å•ä¸ªèŠ‚ç‚¹çš„å¤æ‚åº¦
            replacement_complexity = len(replacement_nodes)
            
            # æ ¹æ®èŠ‚ç‚¹ç±»å‹è°ƒæ•´å¤æ‚åº¦
            for node in replacement_nodes:
                if node["type"] in ["start", "end"]:
                    replacement_complexity -= 0.1  # å¼€å§‹ç»“æŸèŠ‚ç‚¹å¤æ‚åº¦è¾ƒä½
                elif node["type"] == "processor":
                    replacement_complexity += 0.5  # å¤„ç†èŠ‚ç‚¹å¤æ‚åº¦è¾ƒé«˜
            
            change_factor = replacement_complexity / original_complexity
            
            return {
                "original_complexity": original_complexity,
                "new_complexity": replacement_complexity,
                "change_factor": change_factor,
                "estimated_change": "increase" if change_factor > 1.2 else "similar" if change_factor > 0.8 else "decrease"
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¼°ç®—æ‰§è¡Œæ—¶é—´å˜åŒ–å¤±è´¥: {e}")
            return {
                "estimated_change": "unknown",
                "error": str(e)
            }
    
    def _build_merge_preview(
        self,
        parent_workflow: Dict[str, Any],
        merge_previews: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ„å»ºæ•´ä½“åˆå¹¶é¢„è§ˆ"""
        try:
            valid_merges = [p for p in merge_previews if p["valid"]]
            invalid_merges = [p for p in merge_previews if not p["valid"]]
            
            # è®¡ç®—æ•´ä½“å½±å“
            total_nodes_added = sum(p["replacement_info"]["nodes_to_add"] for p in valid_merges)
            total_nodes_removed = len(valid_merges)  # æ¯ä¸ªåˆå¹¶æ“ä½œç§»é™¤ä¸€ä¸ªèŠ‚ç‚¹
            total_connections_added = sum(p["replacement_info"]["connections_to_add"] for p in valid_merges)
            
            return {
                "parent_workflow": {
                    "workflow_base_id": str(parent_workflow["workflow"]["workflow_base_id"]),
                    "name": parent_workflow["workflow"]["name"],
                    "current_nodes": len(parent_workflow["nodes"]),
                    "current_connections": len(parent_workflow["connections"])
                },
                "merge_summary": {
                    "total_merge_candidates": len(merge_previews),
                    "valid_merges": len(valid_merges),
                    "invalid_merges": len(invalid_merges),
                    "net_nodes_change": total_nodes_added - total_nodes_removed,
                    "net_connections_change": total_connections_added
                },
                "valid_merge_previews": valid_merges,
                "invalid_merge_previews": invalid_merges,
                "merge_feasibility": {
                    "can_proceed": len(valid_merges) > 0,
                    "complexity_increase": "high" if total_nodes_added > 20 else "medium" if total_nodes_added > 10 else "low",
                    "recommended_approach": self._recommend_merge_approach(valid_merges)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºåˆå¹¶é¢„è§ˆå¤±è´¥: {e}")
            return {
                "error": str(e),
                "can_proceed": False
            }
    
    def _recommend_merge_approach(self, valid_merges: List[Dict[str, Any]]) -> str:
        """æ¨èåˆå¹¶æ–¹å¼"""
        if len(valid_merges) == 0:
            return "no_merge_possible"
        elif len(valid_merges) == 1:
            return "single_merge"
        elif len(valid_merges) <= 3:
            return "batch_merge"
        else:
            return "phased_merge"
    
    async def _validate_merge_operations(
        self,
        parent_workflow: Dict[str, Any],
        selected_merges: List[Dict[str, Any]],
        user_id: uuid.UUID
    ) -> Dict[str, Any]:
        """éªŒè¯åˆå¹¶æ“ä½œçš„æœ‰æ•ˆæ€§"""
        try:
            errors = []
            warnings = []
            
            # æ£€æŸ¥æƒé™
            if str(parent_workflow["workflow"]["creator_id"]) != str(user_id):
                errors.append("ç”¨æˆ·æ— æƒé™ä¿®æ”¹æ­¤å·¥ä½œæµ")
            
            # æ£€æŸ¥èŠ‚ç‚¹å†²çª
            target_nodes = set()
            for merge in selected_merges:
                node_id = merge.get("target_node_id")
                if node_id in target_nodes:
                    errors.append(f"èŠ‚ç‚¹ {node_id} è¢«å¤šä¸ªåˆå¹¶æ“ä½œé€‰ä¸­")
                target_nodes.add(node_id)
            
            # æ£€æŸ¥å­å·¥ä½œæµçš„å­˜åœ¨æ€§
            for merge in selected_merges:
                sub_workflow_id = merge.get("sub_workflow_id")
                if sub_workflow_id:
                    sub_workflow = await self._get_workflow_structure(uuid.UUID(sub_workflow_id))
                    if not sub_workflow:
                        errors.append(f"å­å·¥ä½œæµ {sub_workflow_id} ä¸å­˜åœ¨")
            
            # å¤æ‚åº¦æ£€æŸ¥
            total_new_nodes = sum(merge.get("nodes_to_add", 0) for merge in selected_merges)
            if total_new_nodes > 50:
                warnings.append(f"åˆå¹¶åå°†å¢åŠ  {total_new_nodes} ä¸ªèŠ‚ç‚¹ï¼Œå·¥ä½œæµå¤æ‚åº¦è¾ƒé«˜")
            
            return {
                "valid": len(errors) == 0,
                "errors": errors,
                "warnings": warnings
            }
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯åˆå¹¶æ“ä½œå¤±è´¥: {e}")
            return {
                "valid": False,
                "errors": [f"éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}"],
                "warnings": []
            }
    
    async def _perform_merge(
        self,
        parent_workflow: Dict[str, Any],
        selected_merges: List[Dict[str, Any]],
        merge_config: Dict[str, Any],
        user_id: uuid.UUID
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå®é™…çš„åˆå¹¶æ“ä½œ"""
        try:
            logger.info(f"ğŸ”„ [MERGE-PERFORM-START] å¼€å§‹æ‰§è¡Œå®é™…åˆå¹¶æ“ä½œ")
            
            # åˆ›å»ºæ–°çš„å·¥ä½œæµ
            new_workflow_name = merge_config.get("new_workflow_name", 
                f"{parent_workflow['workflow']['name']}_merged_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            
            logger.info(f"ğŸ“ [MERGE-PERFORM-STEP1] å‡†å¤‡åˆ›å»ºæ–°å·¥ä½œæµ:")
            logger.info(f"   - æ–°å·¥ä½œæµåç§°: {new_workflow_name}")
            logger.info(f"   - åŸºäºçˆ¶å·¥ä½œæµ: {parent_workflow['workflow']['name']}")
            logger.info(f"   - åˆ›å»ºè€…ID: {user_id}")
            
            new_workflow_data = WorkflowCreate(
                name=new_workflow_name,
                description=f"é€šè¿‡æ¨¡æ¿è¿æ¥åˆå¹¶ç”Ÿæˆçš„å·¥ä½œæµï¼ŒåŸºäº {parent_workflow['workflow']['name']}",
                creator_id=user_id
            )
            
            # åˆ›å»ºå·¥ä½œæµ
            logger.info(f"ğŸ”§ [MERGE-PERFORM-STEP2] æ­£åœ¨åˆ›å»ºæ–°å·¥ä½œæµ...")
            new_workflow = await self.workflow_repo.create_workflow(new_workflow_data)
            
            if not new_workflow:
                logger.error(f"âŒ [MERGE-PERFORM-STEP2-FAILED] åˆ›å»ºæ–°å·¥ä½œæµå¤±è´¥")
                return {
                    "success": False,
                    "message": "åˆ›å»ºæ–°å·¥ä½œæµå¤±è´¥",
                    "error": "workflow_creation_failed"
                }
            
            logger.info(f"âœ… [MERGE-PERFORM-STEP2-SUCCESS] æ–°å·¥ä½œæµåˆ›å»ºæˆåŠŸ:")
            logger.info(f"   - æ–°å·¥ä½œæµID: {new_workflow['workflow_id']}")
            logger.info(f"   - æ–°å·¥ä½œæµåŸºç¡€ID: {new_workflow['workflow_base_id']}")
            
            # å¤åˆ¶å¹¶ä¿®æ”¹èŠ‚ç‚¹å’Œè¿æ¥
            logger.info(f"ğŸ”§ [MERGE-PERFORM-STEP3] å¼€å§‹æ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢...")
            merge_result = await self._execute_node_replacement(
                parent_workflow, 
                selected_merges, 
                new_workflow["workflow_base_id"],
                user_id
            )
            
            logger.info(f"âœ… [MERGE-PERFORM-STEP3-SUCCESS] èŠ‚ç‚¹æ›¿æ¢å®Œæˆ:")
            logger.info(f"   - åˆ›å»ºèŠ‚ç‚¹æ•°: {merge_result.get('nodes_created', 0)}")
            logger.info(f"   - åˆ›å»ºè¿æ¥æ•°: {merge_result.get('connections_created', 0)}")
            logger.info(f"   - æ›¿æ¢èŠ‚ç‚¹æ•°: {merge_result.get('nodes_replaced', 0)}")
            
            logger.info(f"ğŸ‰ [MERGE-PERFORM-SUCCESS] åˆå¹¶æ“ä½œæ‰§è¡ŒæˆåŠŸ")
            return {
                "success": True,
                "message": "å·¥ä½œæµåˆå¹¶æˆåŠŸ",
                "new_workflow_id": str(new_workflow["workflow_base_id"]),
                "new_workflow_name": new_workflow_name,
                "merge_statistics": merge_result
            }
            
        except Exception as e:
            logger.error(f"âŒ [MERGE-PERFORM-EXCEPTION] æ‰§è¡Œåˆå¹¶æ“ä½œå¼‚å¸¸: {e}")
            logger.error(f"âŒ [MERGE-PERFORM-EXCEPTION] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ [MERGE-PERFORM-EXCEPTION] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return {
                "success": False,
                "message": f"åˆå¹¶æ“ä½œå¤±è´¥: {str(e)}",
                "error": str(e)
            }
    
    async def _execute_node_replacement(
        self,
        parent_workflow: Dict[str, Any],
        selected_merges: List[Dict[str, Any]],
        new_workflow_base_id: uuid.UUID,
        user_id: uuid.UUID
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢æ“ä½œï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼Œé¿å…å†—ä½™å¼€å§‹/ç»“æŸèŠ‚ç‚¹ï¼‰"""
        try:
            logger.info(f"ğŸ”„ [NODE-REPLACEMENT-START] å¼€å§‹æ™ºèƒ½èŠ‚ç‚¹æ›¿æ¢æ“ä½œ")
            logger.info(f"   - æ–°å·¥ä½œæµåŸºç¡€ID: {new_workflow_base_id}")
            logger.info(f"   - åˆå¹¶æ“ä½œæ•°: {len(selected_merges)}")
            
            nodes_created = 0
            connections_created = 0
            nodes_replaced = 0
            
            # è·å–è¦æ›¿æ¢çš„èŠ‚ç‚¹IDé›†åˆ
            replaced_node_ids = set()
            replacement_mapping = {}  # åŸèŠ‚ç‚¹base_id -> æ›¿æ¢ä¿¡æ¯æ˜ å°„
            
            logger.info(f"ğŸ” [NODE-REPLACEMENT-STEP1] æ™ºèƒ½å¤„ç†å­å·¥ä½œæµèŠ‚ç‚¹æ›¿æ¢...")
            
            for i, merge in enumerate(selected_merges):
                logger.info(f"ğŸ”§ [NODE-REPLACEMENT-MERGE-{i+1}] å¤„ç†åˆå¹¶æ“ä½œ {i+1}:")
                target_node_id = merge["target_node_id"]
                sub_workflow_id = merge["sub_workflow_id"]
                
                logger.info(f"   - ç›®æ ‡èŠ‚ç‚¹ID: {target_node_id}")
                logger.info(f"   - å­å·¥ä½œæµID: {sub_workflow_id}")
                
                replaced_node_ids.add(target_node_id)
                
                # è·å–å­å·¥ä½œæµç»“æ„
                logger.info(f"   ğŸ“‹ è·å–å­å·¥ä½œæµç»“æ„...")
                sub_workflow = await self._get_workflow_structure(uuid.UUID(sub_workflow_id))
                
                if not sub_workflow:
                    logger.error(f"   âŒ å­å·¥ä½œæµä¸å­˜åœ¨: {sub_workflow_id}")
                    continue
                
                logger.info(f"   âœ… å­å·¥ä½œæµç»“æ„è·å–æˆåŠŸ:")
                logger.info(f"     - å­å·¥ä½œæµåç§°: {sub_workflow['workflow']['name']}")
                logger.info(f"     - å­å·¥ä½œæµèŠ‚ç‚¹æ•°: {len(sub_workflow['nodes'])}")
                logger.info(f"     - å­å·¥ä½œæµè¿æ¥æ•°: {len(sub_workflow['connections'])}")
                
                if sub_workflow:
                    # ğŸ¯ æ™ºèƒ½èŠ‚ç‚¹å¤„ç†ï¼šæ’é™¤å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹ï¼Œåªå¤åˆ¶å¤„ç†èŠ‚ç‚¹
                    logger.info(f"   ğŸ§  æ™ºèƒ½è¿‡æ»¤èŠ‚ç‚¹ç±»å‹...")
                    
                    start_nodes = [n for n in sub_workflow["nodes"] if n["type"] == "start"]
                    end_nodes = [n for n in sub_workflow["nodes"] if n["type"] == "end"]
                    process_nodes = [n for n in sub_workflow["nodes"] if n["type"] not in ["start", "end"]]
                    
                    logger.info(f"     - å¼€å§‹èŠ‚ç‚¹: {len(start_nodes)} ä¸ª (å°†è¢«æ’é™¤)")
                    logger.info(f"     - ç»“æŸèŠ‚ç‚¹: {len(end_nodes)} ä¸ª (å°†è¢«æ’é™¤)")
                    logger.info(f"     - å¤„ç†èŠ‚ç‚¹: {len(process_nodes)} ä¸ª (å°†è¢«å¤åˆ¶)")
                    
                    # å¤åˆ¶å¤„ç†èŠ‚ç‚¹åˆ°æ–°å·¥ä½œæµï¼ˆæ’é™¤start/endèŠ‚ç‚¹é¿å…å†—ä½™ï¼‰
                    logger.info(f"   ğŸ”§ å¤åˆ¶å¤„ç†èŠ‚ç‚¹åˆ°æ–°å·¥ä½œæµ...")
                    node_id_mapping = {}
                    
                    for j, sub_node in enumerate(process_nodes):
                        logger.info(f"     ğŸ“ åˆ›å»ºå¤„ç†èŠ‚ç‚¹ {j+1}: {sub_node['name']} (ç±»å‹: {sub_node['type']})")
                        
                        new_node_data = NodeCreate(
                            name=sub_node["name"],
                            type=NodeType(sub_node["type"]),
                            task_description=sub_node.get("task_description", ""),
                            workflow_base_id=new_workflow_base_id,
                            position_x=sub_node.get("position_x", 0),
                            position_y=sub_node.get("position_y", 0),
                            creator_id=user_id
                        )
                        
                        try:
                            created_node = await self.node_repo.create_node(new_node_data)
                            if not created_node:
                                logger.error(f"     âŒ èŠ‚ç‚¹åˆ›å»ºå¤±è´¥: {sub_node['name']}")
                                continue
                                
                            node_id_mapping[str(sub_node["node_id"])] = created_node["node_id"]
                            nodes_created += 1
                            logger.info(f"     âœ… å¤„ç†èŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {created_node['node_id']}")
                        except Exception as e:
                            logger.error(f"     âŒ èŠ‚ç‚¹åˆ›å»ºå¼‚å¸¸: {e}")
                            continue
                    
                    logger.info(f"   âœ… å¤„ç†èŠ‚ç‚¹å¤åˆ¶å®Œæˆ: åˆ›å»ºäº† {len(node_id_mapping)} ä¸ªèŠ‚ç‚¹")
                    
                    # ğŸ”— æ™ºèƒ½è¿æ¥å¤„ç†ï¼šåªå¤„ç†å¤„ç†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
                    logger.info(f"   ğŸ”— æ™ºèƒ½å¤„ç†å†…éƒ¨è¿æ¥...")
                    internal_connections = 0
                    
                    for k, sub_conn in enumerate(sub_workflow["connections"]):
                        from_node_id = node_id_mapping.get(str(sub_conn["from_node_id"]))
                        to_node_id = node_id_mapping.get(str(sub_conn["to_node_id"]))
                        
                        # åªå¤„ç†ä¸¤ç«¯éƒ½æ˜¯å¤„ç†èŠ‚ç‚¹çš„è¿æ¥
                        if from_node_id and to_node_id:
                            logger.info(f"     ğŸ”— åˆ›å»ºå†…éƒ¨è¿æ¥ {k+1}: {from_node_id} -> {to_node_id}")
                            
                            try:
                                await self._create_node_connection(
                                    from_node_id, to_node_id, sub_conn["connection_type"]
                                )
                                connections_created += 1
                                internal_connections += 1
                                logger.info(f"     âœ… å†…éƒ¨è¿æ¥åˆ›å»ºæˆåŠŸ")
                            except Exception as e:
                                logger.error(f"     âŒ å†…éƒ¨è¿æ¥åˆ›å»ºå¤±è´¥: {e}")
                        else:
                            # è·³è¿‡ä¸å¼€å§‹/ç»“æŸèŠ‚ç‚¹ç›¸å…³çš„è¿æ¥
                            logger.debug(f"     â­ï¸ è·³è¿‡è¾¹ç•Œè¿æ¥: {sub_conn['from_node_id']} -> {sub_conn['to_node_id']}")
                    
                    logger.info(f"   âœ… å†…éƒ¨è¿æ¥å¤„ç†å®Œæˆ: åˆ›å»ºäº† {internal_connections} ä¸ªè¿æ¥")
                    
                    # ğŸ¯ æ™ºèƒ½æ›¿æ¢æ˜ å°„ï¼šæ‰¾åˆ°å…¥å£å’Œå‡ºå£èŠ‚ç‚¹
                    entry_nodes = self._find_entry_nodes(sub_workflow["nodes"], sub_workflow["connections"], node_id_mapping)
                    exit_nodes = self._find_exit_nodes(sub_workflow["nodes"], sub_workflow["connections"], node_id_mapping)
                    
                    replacement_mapping[target_node_id] = {
                        "entry_nodes": entry_nodes,  # æ›¿æ¢åçš„å…¥å£èŠ‚ç‚¹
                        "exit_nodes": exit_nodes,    # æ›¿æ¢åçš„å‡ºå£èŠ‚ç‚¹
                        "all_process_nodes": list(node_id_mapping.values())
                    }
                    
                    logger.info(f"   ğŸ“‹ æ™ºèƒ½æ›¿æ¢æ˜ å°„å®Œæˆ:")
                    logger.info(f"     - å…¥å£èŠ‚ç‚¹: {len(entry_nodes)} ä¸ª")
                    logger.info(f"     - å‡ºå£èŠ‚ç‚¹: {len(exit_nodes)} ä¸ª") 
                    logger.info(f"     - å¤„ç†èŠ‚ç‚¹æ€»æ•°: {len(node_id_mapping.values())} ä¸ª")
            
            logger.info(f"ğŸ” [NODE-REPLACEMENT-STEP2] å¤åˆ¶çˆ¶å·¥ä½œæµä¸­æœªè¢«æ›¿æ¢çš„èŠ‚ç‚¹...")
            
            # å¤åˆ¶çˆ¶å·¥ä½œæµä¸­æœªè¢«æ›¿æ¢çš„èŠ‚ç‚¹
            parent_node_mapping = {}
            for i, parent_node in enumerate(parent_workflow["nodes"]):
                if str(parent_node["node_base_id"]) not in replaced_node_ids:
                    logger.info(f"   ğŸ“ å¤åˆ¶çˆ¶èŠ‚ç‚¹ {i+1}: {parent_node['name']} (ç±»å‹: {parent_node['type']})")
                    
                    new_node_data = NodeCreate(
                        name=parent_node["name"],
                        type=NodeType(parent_node["type"]),
                        task_description=parent_node.get("task_description", ""),
                        workflow_base_id=new_workflow_base_id,
                        position_x=parent_node.get("position_x", 0),
                        position_y=parent_node.get("position_y", 0),
                        creator_id=user_id
                    )
                    
                    try:
                        created_node = await self.node_repo.create_node(new_node_data)
                        if created_node:
                            parent_node_mapping[str(parent_node["node_id"])] = created_node["node_id"]
                            nodes_created += 1
                            logger.info(f"   âœ… çˆ¶èŠ‚ç‚¹å¤åˆ¶æˆåŠŸ: {created_node['node_id']}")
                        else:
                            logger.error(f"   âŒ çˆ¶èŠ‚ç‚¹å¤åˆ¶å¤±è´¥: {parent_node['name']}")
                    except Exception as e:
                        logger.error(f"   âŒ çˆ¶èŠ‚ç‚¹å¤åˆ¶å¼‚å¸¸: {e}")
                else:
                    logger.info(f"   â­ï¸ è·³è¿‡è¢«æ›¿æ¢çš„èŠ‚ç‚¹: {parent_node['name']}")
            
            logger.info(f"âœ… [NODE-REPLACEMENT-STEP2-COMPLETE] çˆ¶èŠ‚ç‚¹å¤åˆ¶å®Œæˆ: {len(parent_node_mapping)} ä¸ªèŠ‚ç‚¹")
            
            logger.info(f"ğŸ” [NODE-REPLACEMENT-STEP3] æ™ºèƒ½é‡å»ºè¿æ¥å…³ç³»...")
            
            # æ™ºèƒ½é‡å»ºè¿æ¥å…³ç³»
            bridge_connections = await self._rebuild_intelligent_connections(
                parent_workflow["connections"], 
                parent_node_mapping, 
                replacement_mapping, 
                parent_workflow["nodes"]
            )
            connections_created += bridge_connections
            
            nodes_replaced = len(replaced_node_ids)
            
            logger.info(f"ğŸ‰ [NODE-REPLACEMENT-SUCCESS] æ™ºèƒ½èŠ‚ç‚¹æ›¿æ¢æ“ä½œå®Œæˆ:")
            logger.info(f"   - åˆ›å»ºèŠ‚ç‚¹æ•°: {nodes_created}")
            logger.info(f"   - åˆ›å»ºè¿æ¥æ•°: {connections_created}")
            logger.info(f"   - æ›¿æ¢èŠ‚ç‚¹æ•°: {nodes_replaced}")
            logger.info(f"   - æ›¿æ¢æ“ä½œæ•°: {len(selected_merges)}")
            
            return {
                "nodes_created": nodes_created,
                "connections_created": connections_created,
                "nodes_replaced": nodes_replaced,
                "replacement_operations": len(selected_merges)
            }
            
        except Exception as e:
            logger.error(f"âŒ [NODE-REPLACEMENT-EXCEPTION] æ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢å¼‚å¸¸: {e}")
            logger.error(f"âŒ [NODE-REPLACEMENT-EXCEPTION] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ [NODE-REPLACEMENT-EXCEPTION] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise
    
    def _find_entry_nodes(
        self, 
        sub_nodes: List[Dict[str, Any]], 
        sub_connections: List[Dict[str, Any]], 
        node_id_mapping: Dict[str, uuid.UUID]
    ) -> List[uuid.UUID]:
        """
        æ‰¾åˆ°å­å·¥ä½œæµä¸­çš„å…¥å£èŠ‚ç‚¹ï¼ˆåº”è¯¥æ¥æ”¶æ¥è‡ªçˆ¶å·¥ä½œæµçš„è¾“å…¥ï¼‰
        
        å…¥å£èŠ‚ç‚¹å®šä¹‰ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¤„ç†æ— è¿æ¥æƒ…å†µï¼‰ï¼š
        1. ç›´æ¥è¿æ¥åˆ°startèŠ‚ç‚¹çš„å¤„ç†èŠ‚ç‚¹
        2. å¦‚æœæ²¡æœ‰startèŠ‚ç‚¹ï¼Œåˆ™æ˜¯æ²¡æœ‰å‰ç½®èŠ‚ç‚¹çš„å¤„ç†èŠ‚ç‚¹
        3. å¦‚æœæ²¡æœ‰è¿æ¥æ•°æ®ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•ç¡®å®šå…¥å£èŠ‚ç‚¹
        """
        try:
            entry_nodes = []
            start_nodes = [n for n in sub_nodes if n["type"] == "start"]
            process_nodes = [n for n in sub_nodes if n["type"] not in ["start", "end"]]
            
            logger.debug(f"   ğŸ” åˆ†æå…¥å£èŠ‚ç‚¹: {len(start_nodes)} ä¸ªstartèŠ‚ç‚¹, {len(sub_connections)} ä¸ªè¿æ¥")
            
            # æƒ…å†µ1: æœ‰è¿æ¥æ•°æ®ä¸”æœ‰startèŠ‚ç‚¹
            if sub_connections and start_nodes:
                logger.debug(f"   ğŸ“¡ æƒ…å†µ1: åŸºäºstartèŠ‚ç‚¹è¿æ¥åˆ†æ")
                for start_node in start_nodes:
                    for conn in sub_connections:
                        if (str(conn["from_node_id"]) == str(start_node["node_id"]) and
                            str(conn["to_node_id"]) in node_id_mapping):
                            target_node_id = node_id_mapping[str(conn["to_node_id"])]
                            if target_node_id not in entry_nodes:
                                entry_nodes.append(target_node_id)
                                logger.debug(f"     âœ… æ‰¾åˆ°startè¿æ¥çš„å…¥å£èŠ‚ç‚¹")
                                
            # æƒ…å†µ2: æœ‰è¿æ¥æ•°æ®ä½†æ²¡æœ‰startèŠ‚ç‚¹
            elif sub_connections and not start_nodes:
                logger.debug(f"   ğŸ“¡ æƒ…å†µ2: åŸºäºå‰ç½®èŠ‚ç‚¹åˆ†æ")
                for node_id, mapped_id in node_id_mapping.items():
                    has_predecessor = False
                    for conn in sub_connections:
                        if (str(conn["to_node_id"]) == node_id and
                            str(conn["from_node_id"]) in node_id_mapping):
                            has_predecessor = True
                            break
                    
                    if not has_predecessor:
                        entry_nodes.append(mapped_id)
                        logger.debug(f"     âœ… æ‰¾åˆ°æ— å‰ç½®çš„å…¥å£èŠ‚ç‚¹")
                        
            # æƒ…å†µ3: æ²¡æœ‰è¿æ¥æ•°æ® - å¯å‘å¼æ–¹æ³•
            else:
                logger.warning(f"   âš ï¸ æƒ…å†µ3: æ— è¿æ¥æ•°æ®ï¼Œä½¿ç”¨å¯å‘å¼å…¥å£èŠ‚ç‚¹è¯†åˆ«")
                
                # å¯å‘å¼ç­–ç•¥1: å¦‚æœæœ‰startèŠ‚ç‚¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªç´§è·Ÿçš„å¤„ç†èŠ‚ç‚¹
                if start_nodes and process_nodes:
                    logger.debug(f"     ğŸ¯ å¯å‘å¼ç­–ç•¥1: é€‰æ‹©startèŠ‚ç‚¹åçš„å¤„ç†èŠ‚ç‚¹")
                    # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ—©çš„å¤„ç†èŠ‚ç‚¹ä½œä¸ºå…¥å£
                    sorted_process_nodes = sorted(process_nodes, key=lambda x: x.get('created_at', ''))
                    if sorted_process_nodes:
                        first_process_node_id = str(sorted_process_nodes[0]["node_id"])
                        if first_process_node_id in node_id_mapping:
                            entry_nodes.append(node_id_mapping[first_process_node_id])
                            logger.debug(f"     âœ… å¯å‘å¼é€‰æ‹©: {sorted_process_nodes[0]['name']}")
                
                # å¯å‘å¼ç­–ç•¥2: æ²¡æœ‰startèŠ‚ç‚¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¤„ç†èŠ‚ç‚¹
                elif process_nodes:
                    logger.debug(f"     ğŸ¯ å¯å‘å¼ç­–ç•¥2: é€‰æ‹©ç¬¬ä¸€ä¸ªå¤„ç†èŠ‚ç‚¹")
                    # æŒ‰èŠ‚ç‚¹ä½ç½®æˆ–åˆ›å»ºæ—¶é—´é€‰æ‹©
                    sorted_nodes = sorted(process_nodes, key=lambda x: (
                        x.get('position_x', 0), 
                        x.get('position_y', 0),
                        x.get('created_at', '')
                    ))
                    
                    if sorted_nodes:
                        first_node_id = str(sorted_nodes[0]["node_id"])
                        if first_node_id in node_id_mapping:
                            entry_nodes.append(node_id_mapping[first_node_id])
                            logger.debug(f"     âœ… å¯å‘å¼é€‰æ‹©: {sorted_nodes[0]['name']}")
                
                # å¯å‘å¼ç­–ç•¥3: å…œåº•æ–¹æ¡ˆ - é€‰æ‹©æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ä¸­çš„ç¬¬ä¸€ä¸ª
                if not entry_nodes and node_id_mapping:
                    logger.debug(f"     ğŸ¯ å¯å‘å¼ç­–ç•¥3: å…œåº•é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹")
                    first_mapped_id = next(iter(node_id_mapping.values()))
                    entry_nodes.append(first_mapped_id)
                    logger.debug(f"     âœ… å…œåº•é€‰æ‹©å®Œæˆ")
            
            logger.info(f"   ğŸ¯ å…¥å£èŠ‚ç‚¹è¯†åˆ«å®Œæˆ: {len(entry_nodes)} ä¸ª")
            return entry_nodes
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾å…¥å£èŠ‚ç‚¹å¤±è´¥: {e}")
            # å…œåº•ç­–ç•¥ï¼šå¦‚æœæœ‰æ˜ å°„èŠ‚ç‚¹ï¼Œè‡³å°‘è¿”å›ä¸€ä¸ª
            if node_id_mapping:
                logger.warning(f"   ğŸš‘ å¯ç”¨å…œåº•å…¥å£èŠ‚ç‚¹ç­–ç•¥")
                return [next(iter(node_id_mapping.values()))]
            return []
    
    def _find_exit_nodes(
        self, 
        sub_nodes: List[Dict[str, Any]], 
        sub_connections: List[Dict[str, Any]], 
        node_id_mapping: Dict[str, uuid.UUID]
    ) -> List[uuid.UUID]:
        """
        æ‰¾åˆ°å­å·¥ä½œæµä¸­çš„å‡ºå£èŠ‚ç‚¹ï¼ˆåº”è¯¥è¾“å‡ºåˆ°çˆ¶å·¥ä½œæµçš„åç»­èŠ‚ç‚¹ï¼‰
        
        å‡ºå£èŠ‚ç‚¹å®šä¹‰ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¤„ç†æ— è¿æ¥æƒ…å†µï¼‰ï¼š
        1. ç›´æ¥è¿æ¥åˆ°endèŠ‚ç‚¹çš„å¤„ç†èŠ‚ç‚¹
        2. å¦‚æœæ²¡æœ‰endèŠ‚ç‚¹ï¼Œåˆ™æ˜¯æ²¡æœ‰åç»­èŠ‚ç‚¹çš„å¤„ç†èŠ‚ç‚¹
        3. å¦‚æœæ²¡æœ‰è¿æ¥æ•°æ®ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•ç¡®å®šå‡ºå£èŠ‚ç‚¹
        """
        try:
            exit_nodes = []
            end_nodes = [n for n in sub_nodes if n["type"] == "end"]
            process_nodes = [n for n in sub_nodes if n["type"] not in ["start", "end"]]
            
            logger.debug(f"   ğŸ” åˆ†æå‡ºå£èŠ‚ç‚¹: {len(end_nodes)} ä¸ªendèŠ‚ç‚¹, {len(sub_connections)} ä¸ªè¿æ¥")
            
            # æƒ…å†µ1: æœ‰è¿æ¥æ•°æ®ä¸”æœ‰endèŠ‚ç‚¹
            if sub_connections and end_nodes:
                logger.debug(f"   ğŸ“¡ æƒ…å†µ1: åŸºäºendèŠ‚ç‚¹è¿æ¥åˆ†æ")
                for end_node in end_nodes:
                    for conn in sub_connections:
                        if (str(conn["to_node_id"]) == str(end_node["node_id"]) and
                            str(conn["from_node_id"]) in node_id_mapping):
                            source_node_id = node_id_mapping[str(conn["from_node_id"])]
                            if source_node_id not in exit_nodes:
                                exit_nodes.append(source_node_id)
                                logger.debug(f"     âœ… æ‰¾åˆ°endè¿æ¥çš„å‡ºå£èŠ‚ç‚¹")
                                
            # æƒ…å†µ2: æœ‰è¿æ¥æ•°æ®ä½†æ²¡æœ‰endèŠ‚ç‚¹
            elif sub_connections and not end_nodes:
                logger.debug(f"   ğŸ“¡ æƒ…å†µ2: åŸºäºåç»­èŠ‚ç‚¹åˆ†æ")
                for node_id, mapped_id in node_id_mapping.items():
                    has_successor = False
                    for conn in sub_connections:
                        if (str(conn["from_node_id"]) == node_id and
                            str(conn["to_node_id"]) in node_id_mapping):
                            has_successor = True
                            break
                    
                    if not has_successor:
                        exit_nodes.append(mapped_id)
                        logger.debug(f"     âœ… æ‰¾åˆ°æ— åç»­çš„å‡ºå£èŠ‚ç‚¹")
                        
            # æƒ…å†µ3: æ²¡æœ‰è¿æ¥æ•°æ® - å¯å‘å¼æ–¹æ³•
            else:
                logger.warning(f"   âš ï¸ æƒ…å†µ3: æ— è¿æ¥æ•°æ®ï¼Œä½¿ç”¨å¯å‘å¼å‡ºå£èŠ‚ç‚¹è¯†åˆ«")
                
                # å¯å‘å¼ç­–ç•¥1: å¦‚æœæœ‰endèŠ‚ç‚¹ï¼Œé€‰æ‹©æœ€åä¸€ä¸ªå¤„ç†èŠ‚ç‚¹
                if end_nodes and process_nodes:
                    logger.debug(f"     ğŸ¯ å¯å‘å¼ç­–ç•¥1: é€‰æ‹©endèŠ‚ç‚¹å‰çš„å¤„ç†èŠ‚ç‚¹")
                    # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ™šçš„å¤„ç†èŠ‚ç‚¹ä½œä¸ºå‡ºå£
                    sorted_process_nodes = sorted(process_nodes, key=lambda x: x.get('created_at', ''), reverse=True)
                    if sorted_process_nodes:
                        last_process_node_id = str(sorted_process_nodes[0]["node_id"])
                        if last_process_node_id in node_id_mapping:
                            exit_nodes.append(node_id_mapping[last_process_node_id])
                            logger.debug(f"     âœ… å¯å‘å¼é€‰æ‹©: {sorted_process_nodes[0]['name']}")
                
                # å¯å‘å¼ç­–ç•¥2: æ²¡æœ‰endèŠ‚ç‚¹ï¼Œé€‰æ‹©æœ€åä¸€ä¸ªå¤„ç†èŠ‚ç‚¹
                elif process_nodes:
                    logger.debug(f"     ğŸ¯ å¯å‘å¼ç­–ç•¥2: é€‰æ‹©æœ€åä¸€ä¸ªå¤„ç†èŠ‚ç‚¹")
                    # æŒ‰èŠ‚ç‚¹ä½ç½®æˆ–åˆ›å»ºæ—¶é—´é€‰æ‹©æœ€åçš„èŠ‚ç‚¹
                    sorted_nodes = sorted(process_nodes, key=lambda x: (
                        x.get('position_x', 0), 
                        x.get('position_y', 0),
                        x.get('created_at', '')
                    ), reverse=True)
                    
                    if sorted_nodes:
                        last_node_id = str(sorted_nodes[0]["node_id"])
                        if last_node_id in node_id_mapping:
                            exit_nodes.append(node_id_mapping[last_node_id])
                            logger.debug(f"     âœ… å¯å‘å¼é€‰æ‹©: {sorted_nodes[0]['name']}")
                
                # å¯å‘å¼ç­–ç•¥3: å…œåº•æ–¹æ¡ˆ - é€‰æ‹©æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ä¸­çš„æœ€åä¸€ä¸ª
                if not exit_nodes and node_id_mapping:
                    logger.debug(f"     ğŸ¯ å¯å‘å¼ç­–ç•¥3: å…œåº•é€‰æ‹©æœ€åä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹")
                    # å¦‚æœæœ‰å¤šä¸ªèŠ‚ç‚¹ï¼Œé€‰æ‹©æœ€åä¸€ä¸ªï¼›å¦åˆ™é€‰æ‹©å”¯ä¸€çš„èŠ‚ç‚¹
                    if len(node_id_mapping) > 1:
                        last_mapped_id = list(node_id_mapping.values())[-1]
                    else:
                        last_mapped_id = next(iter(node_id_mapping.values()))
                    exit_nodes.append(last_mapped_id)
                    logger.debug(f"     âœ… å…œåº•é€‰æ‹©å®Œæˆ")
            
            logger.info(f"   ğŸ¯ å‡ºå£èŠ‚ç‚¹è¯†åˆ«å®Œæˆ: {len(exit_nodes)} ä¸ª")
            return exit_nodes
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾å‡ºå£èŠ‚ç‚¹å¤±è´¥: {e}")
            # å…œåº•ç­–ç•¥ï¼šå¦‚æœæœ‰æ˜ å°„èŠ‚ç‚¹ï¼Œè‡³å°‘è¿”å›ä¸€ä¸ª
            if node_id_mapping:
                logger.warning(f"   ğŸš‘ å¯ç”¨å…œåº•å‡ºå£èŠ‚ç‚¹ç­–ç•¥")
                # é€‰æ‹©æœ€åä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºå‡ºå£
                if len(node_id_mapping) > 1:
                    return [list(node_id_mapping.values())[-1]]
                else:
                    return [next(iter(node_id_mapping.values()))]
            return []
    
    async def _rebuild_intelligent_connections(
        self,
        parent_connections: List[Dict[str, Any]],
        parent_node_mapping: Dict[str, uuid.UUID],
        replacement_mapping: Dict[str, Dict[str, List[uuid.UUID]]],
        parent_nodes: List[Dict[str, Any]]
    ) -> int:
        """
        æ™ºèƒ½é‡å»ºè¿æ¥å…³ç³»ï¼Œç¡®ä¿æ›¿æ¢åçš„å­å·¥ä½œæµæ­£ç¡®è¿æ¥åˆ°çˆ¶å·¥ä½œæµï¼ˆæ”¹è¿›ç‰ˆï¼Œæ”¯æŒå¤‡ç”¨ç­–ç•¥ï¼‰
        """
        try:
            bridge_connections = 0
            
            logger.info(f"ğŸ”— [CONNECTION-REBUILD] å¼€å§‹é‡å»ºè¿æ¥å…³ç³»:")
            logger.info(f"   - çˆ¶è¿æ¥æ•°: {len(parent_connections)}")
            logger.info(f"   - æ›¿æ¢æ“ä½œæ•°: {len(replacement_mapping)}")
            logger.info(f"   - çˆ¶èŠ‚ç‚¹æ˜ å°„æ•°: {len(parent_node_mapping)}")
            
            # æƒ…å†µ1: æœ‰çˆ¶è¿æ¥æ•°æ® - ä½¿ç”¨åŸå§‹é€»è¾‘
            if parent_connections:
                logger.info(f"ğŸ“¡ [CONNECTION-REBUILD-CASE1] åŸºäºç°æœ‰çˆ¶è¿æ¥é‡å»º")
                
                for i, parent_conn in enumerate(parent_connections):
                    from_node_id = str(parent_conn["from_node_id"])
                    to_node_id = str(parent_conn["to_node_id"])
                    
                    logger.info(f"   ğŸ”— å¤„ç†çˆ¶è¿æ¥ {i+1}: {from_node_id} -> {to_node_id}")
                    
                    # æ™ºèƒ½è§£æè¿æ¥ç«¯ç‚¹
                    new_from_nodes = self._intelligent_resolve_connection_endpoint(
                        from_node_id, parent_node_mapping, replacement_mapping, 
                        parent_nodes, "output"
                    )
                    new_to_nodes = self._intelligent_resolve_connection_endpoint(
                        to_node_id, parent_node_mapping, replacement_mapping, 
                        parent_nodes, "input"
                    )
                    
                    logger.info(f"     - æ™ºèƒ½è§£æç»“æœ: from={len(new_from_nodes)}ä¸ªèŠ‚ç‚¹, to={len(new_to_nodes)}ä¸ªèŠ‚ç‚¹")
                    
                    # åˆ›å»ºæ¡¥æ¥è¿æ¥
                    for from_node in new_from_nodes:
                        for to_node in new_to_nodes:
                            try:
                                await self._create_node_connection(
                                    from_node, to_node, parent_conn["connection_type"]
                                )
                                bridge_connections += 1
                                logger.info(f"     âœ… æ¡¥æ¥è¿æ¥åˆ›å»ºæˆåŠŸ: {from_node} -> {to_node}")
                            except Exception as e:
                                logger.error(f"     âŒ æ¡¥æ¥è¿æ¥åˆ›å»ºå¤±è´¥: {e}")
            
            # æƒ…å†µ2: æ²¡æœ‰çˆ¶è¿æ¥æ•°æ® - å¯ç”¨å¤‡ç”¨è¿æ¥ç­–ç•¥
            else:
                logger.warning(f"âš ï¸ [CONNECTION-REBUILD-CASE2] æ²¡æœ‰çˆ¶è¿æ¥æ•°æ®ï¼Œå¯ç”¨å¤‡ç”¨è¿æ¥ç­–ç•¥")
                bridge_connections += await self._create_fallback_connections(
                    parent_node_mapping, replacement_mapping, parent_nodes
                )
            
            logger.info(f"âœ… [CONNECTION-REBUILD-SUCCESS] è¿æ¥é‡å»ºå®Œæˆ: {bridge_connections} ä¸ªè¿æ¥")
            return bridge_connections
            
        except Exception as e:
            logger.error(f"âŒ [CONNECTION-REBUILD-EXCEPTION] æ™ºèƒ½é‡å»ºè¿æ¥å¼‚å¸¸: {e}")
            return 0
    
    def _intelligent_resolve_connection_endpoint(
        self,
        original_node_id: str,
        parent_node_mapping: Dict[str, uuid.UUID],
        replacement_mapping: Dict[str, Dict[str, List[uuid.UUID]]],
        parent_nodes: List[Dict[str, Any]],
        direction: str
    ) -> List[uuid.UUID]:
        """
        æ™ºèƒ½è§£æè¿æ¥ç«¯ç‚¹ï¼Œæ­£ç¡®å¤„ç†èŠ‚ç‚¹æ›¿æ¢çš„è¿æ¥å…³ç³»
        """
        try:
            # é¦–å…ˆæ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦è¢«ä¿ç•™ï¼ˆæœªè¢«æ›¿æ¢ï¼‰
            if original_node_id in parent_node_mapping:
                logger.debug(f"     ğŸ“ èŠ‚ç‚¹ {original_node_id} è¢«ä¿ç•™ï¼Œç›´æ¥æ˜ å°„")
                return [parent_node_mapping[original_node_id]]
            
            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦è¢«æ›¿æ¢äº†
            # éœ€è¦æ‰¾åˆ°åŸnode_idå¯¹åº”çš„node_base_id
            original_node_base_id = None
            for parent_node in parent_nodes:
                if str(parent_node["node_id"]) == original_node_id:
                    original_node_base_id = str(parent_node["node_base_id"])
                    break
            
            if not original_node_base_id:
                logger.warning(f"     âš ï¸ æ— æ³•æ‰¾åˆ°èŠ‚ç‚¹ {original_node_id} çš„base_id")
                return []
            
            # æ£€æŸ¥è¿™ä¸ªbase_idæ˜¯å¦åœ¨æ›¿æ¢æ˜ å°„ä¸­
            if original_node_base_id in replacement_mapping:
                replacement = replacement_mapping[original_node_base_id]
                
                if direction == "output":
                    # è¾“å‡ºè¿æ¥ï¼šä½¿ç”¨å­å·¥ä½œæµçš„å‡ºå£èŠ‚ç‚¹
                    logger.debug(f"     ğŸš€ èŠ‚ç‚¹ {original_node_id} è¢«æ›¿æ¢ï¼Œä½¿ç”¨å‡ºå£èŠ‚ç‚¹ ({len(replacement['exit_nodes'])} ä¸ª)")
                    return replacement["exit_nodes"]
                else:
                    # è¾“å…¥è¿æ¥ï¼šä½¿ç”¨å­å·¥ä½œæµçš„å…¥å£èŠ‚ç‚¹
                    logger.debug(f"     ğŸ“¥ èŠ‚ç‚¹ {original_node_id} è¢«æ›¿æ¢ï¼Œä½¿ç”¨å…¥å£èŠ‚ç‚¹ ({len(replacement['entry_nodes'])} ä¸ª)")
                    return replacement["entry_nodes"]
            
            logger.warning(f"     âš ï¸ æ— æ³•è§£æèŠ‚ç‚¹ {original_node_id} çš„è¿æ¥ç«¯ç‚¹")
            return []
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½è§£æè¿æ¥ç«¯ç‚¹å¤±è´¥: {original_node_id}, {e}")
            return []
    
    async def _create_node_connection(
        self,
        from_node_id: uuid.UUID,
        to_node_id: uuid.UUID,
        connection_type: str
    ):
        """åˆ›å»ºèŠ‚ç‚¹è¿æ¥"""
        try:
            # ä½¿ç”¨MySQLæ ¼å¼çš„å‚æ•°å ä½ç¬¦
            connection_query = """
            INSERT INTO node_connection (
                from_node_id, to_node_id, connection_type, workflow_id, created_at
            )
            SELECT %s, %s, %s, 
                   (SELECT workflow_id FROM node WHERE node_id = %s LIMIT 1),
                   %s
            """
            
            await self.db.execute(
                connection_query,
                str(from_node_id), str(to_node_id), connection_type, str(from_node_id), now_utc()
            )
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºèŠ‚ç‚¹è¿æ¥å¤±è´¥: {e}")
            raise
    
    async def _create_fallback_connections(
        self,
        parent_node_mapping: Dict[str, uuid.UUID],
        replacement_mapping: Dict[str, Dict[str, List[uuid.UUID]]],
        parent_nodes: List[Dict[str, Any]]
    ) -> int:
        """
        åˆ›å»ºå¤‡ç”¨è¿æ¥ç­–ç•¥ - å½“çˆ¶å·¥ä½œæµæ²¡æœ‰è¿æ¥æ•°æ®æ—¶ä½¿ç”¨
        
        å¤‡ç”¨ç­–ç•¥ï¼š
        1. åŸºäºèŠ‚ç‚¹ç±»å‹å’Œä½ç½®åˆ›å»ºåŸºæœ¬çš„çº¿æ€§è¿æ¥
        2. ç¡®ä¿start -> processor -> endçš„åŸºæœ¬æµç¨‹
        3. æ­£ç¡®æ•´åˆæ›¿æ¢çš„å­å·¥ä½œæµèŠ‚ç‚¹
        """
        try:
            logger.info(f"ğŸš‘ [FALLBACK-CONNECTION] å¯åŠ¨å¤‡ç”¨è¿æ¥ç­–ç•¥")
            fallback_connections = 0
            
            # è·å–æ‰€æœ‰èŠ‚ç‚¹å¹¶æŒ‰ç±»å‹åˆ†ç±»
            all_mapped_nodes = list(parent_node_mapping.values())
            start_nodes = []
            end_nodes = []
            processor_nodes = []
            
            # åˆ†æçˆ¶èŠ‚ç‚¹ç±»å‹
            for parent_node in parent_nodes:
                if str(parent_node["node_id"]) in parent_node_mapping:
                    mapped_id = parent_node_mapping[str(parent_node["node_id"])]
                    node_type = parent_node["type"]
                    
                    if node_type == "start":
                        start_nodes.append((mapped_id, parent_node))
                    elif node_type == "end":
                        end_nodes.append((mapped_id, parent_node))
                    elif node_type == "processor":
                        processor_nodes.append((mapped_id, parent_node))
            
            logger.info(f"   ğŸ“Š èŠ‚ç‚¹åˆ†ç±»: {len(start_nodes)} ä¸ªstart, {len(processor_nodes)} ä¸ªprocessor, {len(end_nodes)} ä¸ªend")
            
            # è·å–æ›¿æ¢èŠ‚ç‚¹ï¼ˆå­å·¥ä½œæµèŠ‚ç‚¹ï¼‰
            all_replacement_nodes = []
            for replacement_info in replacement_mapping.values():
                all_replacement_nodes.extend(replacement_info["all_process_nodes"])
            
            logger.info(f"   ğŸ”„ æ›¿æ¢èŠ‚ç‚¹: {len(all_replacement_nodes)} ä¸ª")
            
            # ç­–ç•¥1: åˆ›å»ºåŸºæœ¬çš„çº¿æ€§è¿æ¥
            logger.info(f"   ğŸ”— ç­–ç•¥1: åˆ›å»ºåŸºæœ¬çº¿æ€§è¿æ¥")
            
            # åˆå¹¶æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬åŸæœ‰çš„å’Œæ›¿æ¢çš„ï¼‰
            all_process_nodes = [node_id for node_id, _ in processor_nodes] + all_replacement_nodes
            
            # æŒ‰åˆ›å»ºé¡ºåºæˆ–ä½ç½®æ’åºèŠ‚ç‚¹
            try:
                # å°è¯•æŒ‰ä½ç½®æ’åº
                def get_node_position(node_id):
                    for node_id_str, mapped_id in parent_node_mapping.items():
                        if mapped_id == node_id:
                            for parent_node in parent_nodes:
                                if str(parent_node["node_id"]) == node_id_str:
                                    return (
                                        parent_node.get("position_x", 0),
                                        parent_node.get("position_y", 0),
                                        parent_node.get("created_at", "")
                                    )
                    return (0, 0, "")
                
                all_process_nodes.sort(key=get_node_position)
                logger.debug(f"     âœ… èŠ‚ç‚¹æŒ‰ä½ç½®æ’åºå®Œæˆ")
                
            except Exception as e:
                logger.warning(f"     âš ï¸ èŠ‚ç‚¹æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåº: {e}")
            
            # åˆ›å»ºè¿æ¥åºåˆ—
            connection_sequence = []
            
            # start -> first process node
            if start_nodes and all_process_nodes:
                start_id = start_nodes[0][0]
                first_process_id = all_process_nodes[0]
                connection_sequence.append((start_id, first_process_id, "normal"))
                logger.debug(f"     ğŸ“Œ start -> first_process: {start_id} -> {first_process_id}")
            
            # process node -> process node (linear chain)
            for i in range(len(all_process_nodes) - 1):
                from_id = all_process_nodes[i]
                to_id = all_process_nodes[i + 1]
                connection_sequence.append((from_id, to_id, "normal"))
                logger.debug(f"     ğŸ“Œ process chain: {from_id} -> {to_id}")
            
            # last process node -> end
            if all_process_nodes and end_nodes:
                last_process_id = all_process_nodes[-1]
                end_id = end_nodes[0][0]
                connection_sequence.append((last_process_id, end_id, "normal"))
                logger.debug(f"     ğŸ“Œ last_process -> end: {last_process_id} -> {end_id}")
            
            # ç­–ç•¥2: åˆ›å»ºæ›¿æ¢èŠ‚ç‚¹çš„ç‰¹æ®Šè¿æ¥
            logger.info(f"   ğŸ”— ç­–ç•¥2: å¤„ç†æ›¿æ¢èŠ‚ç‚¹çš„å…¥å£/å‡ºå£è¿æ¥")
            
            for target_node_id, replacement_info in replacement_mapping.items():
                entry_nodes = replacement_info["entry_nodes"]
                exit_nodes = replacement_info["exit_nodes"]
                
                logger.debug(f"     ğŸ¯ å¤„ç†æ›¿æ¢èŠ‚ç‚¹ {target_node_id}: {len(entry_nodes)} ä¸ªå…¥å£, {len(exit_nodes)} ä¸ªå‡ºå£")
                
                # å¦‚æœæ›¿æ¢èŠ‚ç‚¹æœ‰å¤šä¸ªå…¥å£/å‡ºå£ï¼Œåˆ›å»ºé¢å¤–çš„å†…éƒ¨è¿æ¥
                if len(entry_nodes) > 1 or len(exit_nodes) > 1:
                    # å°†æ‰€æœ‰å…¥å£èŠ‚ç‚¹è¿æ¥åˆ°ç¬¬ä¸€ä¸ªå…¥å£èŠ‚ç‚¹ï¼ˆèšåˆï¼‰
                    if len(entry_nodes) > 1:
                        main_entry = entry_nodes[0]
                        for i in range(1, len(entry_nodes)):
                            connection_sequence.append((entry_nodes[i], main_entry, "aggregation"))
                            logger.debug(f"     ğŸ“Œ å…¥å£èšåˆ: {entry_nodes[i]} -> {main_entry}")
                    
                    # å°†æœ€åä¸€ä¸ªå‡ºå£èŠ‚ç‚¹è¿æ¥åˆ°æ‰€æœ‰å‡ºå£èŠ‚ç‚¹ï¼ˆåˆ†å‘ï¼‰
                    if len(exit_nodes) > 1:
                        main_exit = exit_nodes[-1]
                        for i in range(len(exit_nodes) - 1):
                            connection_sequence.append((main_exit, exit_nodes[i], "distribution"))
                            logger.debug(f"     ğŸ“Œ å‡ºå£åˆ†å‘: {main_exit} -> {exit_nodes[i]}")
            
            # æ‰§è¡Œè¿æ¥åˆ›å»º
            logger.info(f"   ğŸ”§ æ‰§è¡Œè¿æ¥åˆ›å»º: {len(connection_sequence)} ä¸ªè¿æ¥")
            
            for from_id, to_id, conn_type in connection_sequence:
                try:
                    await self._create_node_connection(from_id, to_id, conn_type)
                    fallback_connections += 1
                    logger.debug(f"     âœ… å¤‡ç”¨è¿æ¥åˆ›å»ºæˆåŠŸ: {from_id} -> {to_id} ({conn_type})")
                except Exception as e:
                    logger.error(f"     âŒ å¤‡ç”¨è¿æ¥åˆ›å»ºå¤±è´¥: {from_id} -> {to_id}, {e}")
                    # ç»§ç»­åˆ›å»ºå…¶ä»–è¿æ¥ï¼Œä¸ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
                    continue
            
            # ç­–ç•¥3: ç¡®ä¿åŸºæœ¬å¯æ‰§è¡Œæ€§
            logger.info(f"   ğŸ”— ç­–ç•¥3: ç¡®ä¿åŸºæœ¬å¯æ‰§è¡Œæ€§æ£€æŸ¥")
            
            if fallback_connections == 0:
                logger.warning(f"     âš ï¸ æ²¡æœ‰åˆ›å»ºä»»ä½•è¿æ¥ï¼Œå°è¯•æœ€åŸºæœ¬çš„è¿æ¥")
                
                # æœ€åŸºæœ¬çš„è¿æ¥ï¼šå¦‚æœæœ‰ä»»ä½•ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œå°±è¿æ¥å®ƒä»¬
                if len(all_mapped_nodes) >= 2:
                    try:
                        await self._create_node_connection(
                            all_mapped_nodes[0], all_mapped_nodes[1], "basic"
                        )
                        fallback_connections += 1
                        logger.info(f"     ğŸš‘ åˆ›å»ºåŸºç¡€è¿æ¥: {all_mapped_nodes[0]} -> {all_mapped_nodes[1]}")
                    except Exception as e:
                        logger.error(f"     âŒ åŸºç¡€è¿æ¥åˆ›å»ºå¤±è´¥: {e}")
            
            logger.info(f"âœ… [FALLBACK-CONNECTION-SUCCESS] å¤‡ç”¨è¿æ¥ç­–ç•¥å®Œæˆ: {fallback_connections} ä¸ªè¿æ¥")
            return fallback_connections
            
        except Exception as e:
            logger.error(f"âŒ [FALLBACK-CONNECTION-EXCEPTION] å¤‡ç”¨è¿æ¥ç­–ç•¥å¤±è´¥: {e}")
            import traceback
            logger.error(f"âŒ [FALLBACK-CONNECTION-EXCEPTION] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return 0