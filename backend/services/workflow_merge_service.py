"""
å·¥ä½œæµåˆå¹¶æœåŠ¡ - Subdivision Tree Merge Service

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»Žæœ€ä½Žå±‚å¼€å§‹é€å±‚åˆå¹¶subdivision
2. å°†çˆ¶èŠ‚ç‚¹ç”¨å­å·¥ä½œæµæ›¿æ¢
3. åŽ»é™¤å­å·¥ä½œæµçš„å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
4. é‡æ–°è¿žæŽ¥ä¸Šä¸‹æ¸¸èŠ‚ç‚¹
5. ç”Ÿæˆæ–°çš„å·¥ä½œæµæ¨¡æ¿
"""

import uuid
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from loguru import logger

from ..repositories.base import BaseRepository
from ..utils.helpers import now_utc
from .workflow_template_tree import WorkflowTemplateTree, WorkflowTemplateNode


@dataclass
class MergeCandidate:
    """åˆå¹¶å€™é€‰é¡¹"""
    subdivision_id: str
    parent_subdivision_id: Optional[str]
    workflow_instance_id: str
    workflow_base_id: str
    node_name: str
    depth: int
    can_merge: bool = True
    merge_reason: str = ""


@dataclass
class MergeOperation:
    """åˆå¹¶æ“ä½œ"""
    target_node_id: str  # è¢«æ›¿æ¢çš„çˆ¶èŠ‚ç‚¹ID
    sub_workflow_id: str  # æ›¿æ¢ç”¨çš„å­å·¥ä½œæµID
    subdivision_id: str  # å¯¹åº”çš„subdivision ID
    depth: int  # åˆå¹¶æ·±åº¦


class WorkflowMergeService:
    """å·¥ä½œæµåˆå¹¶æœåŠ¡"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_merge").db
    
    async def get_merge_candidates(self, workflow_instance_id: uuid.UUID) -> List[MergeCandidate]:
        """
        èŽ·å–å¯åˆå¹¶çš„subdivisionåˆ—è¡¨
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®žä¾‹ID
            
        Returns:
            åˆå¹¶å€™é€‰é¡¹åˆ—è¡¨ï¼ŒæŒ‰æ·±åº¦ä»Žé«˜åˆ°ä½ŽæŽ’åºï¼ˆä»Žå¶å­èŠ‚ç‚¹å¼€å§‹ï¼‰
        """
        try:
            logger.info(f"ðŸ” [åˆå¹¶å€™é€‰] èŽ·å–åˆå¹¶å€™é€‰: {workflow_instance_id}")
            
            # ä½¿ç”¨subdivision tree builderèŽ·å–æ ‘ç»“æž„
            from .workflow_template_connection_service import WorkflowTemplateConnectionService
            connection_service = WorkflowTemplateConnectionService()
            
            subdivisions_data = await connection_service._get_all_subdivisions_simple(workflow_instance_id)
            
            logger.info(f"ðŸ“‹ [åˆå¹¶å€™é€‰] æŸ¥è¯¢åˆ°subdivisionæ•°æ®: {len(subdivisions_data) if subdivisions_data else 0}æ¡")
            
            if not subdivisions_data:
                logger.warning(f"âŒ [åˆå¹¶å€™é€‰å¤±è´¥] æ— subdivisionæ•°æ®: {workflow_instance_id}")
                
                # ðŸ”§ å¢žå¼ºè°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„æ²¡æœ‰subdivisionæ•°æ®
                debug_query = await self.db.fetch_all("""
                    SELECT ts.subdivision_id, ts.sub_workflow_instance_id, 
                           ti.workflow_instance_id, ti.task_title,
                           n.name as node_name
                    FROM task_subdivision ts
                    JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id  
                    JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
                    JOIN node n ON ni.node_id = n.node_id
                    WHERE ti.workflow_instance_id = %s
                    AND ts.is_deleted = FALSE
                    ORDER BY ts.subdivision_created_at DESC
                    LIMIT 10
                """, workflow_instance_id)
                
                logger.warning(f"   ðŸ“Š [è°ƒè¯•æ£€æŸ¥] ç›´æŽ¥æŸ¥è¯¢task_subdivisionç»“æžœ: {len(debug_query)}æ¡")
                for i, row in enumerate(debug_query):
                    logger.warning(f"     {i+1}. subdivision_id: {row['subdivision_id']}")
                    logger.warning(f"        sub_workflow_instance_id: {row['sub_workflow_instance_id']}")
                    logger.warning(f"        node_name: {row['node_name']}")
                    logger.warning(f"        task_title: {row['task_title']}")
                
                if debug_query:
                    logger.error(f"ðŸš¨ [ä¸¥é‡é—®é¢˜] subdivisionæ•°æ®å­˜åœ¨ä½†_get_all_subdivisions_simpleæœªè¿”å›žï¼")
                    logger.error(f"   è¿™è¡¨æ˜ŽWorkflowTemplateConnectionService._get_all_subdivisions_simpleå­˜åœ¨bug")
                else:
                    logger.warning(f"   ç¡®è®¤ï¼šè¯¥å·¥ä½œæµç¡®å®žæ²¡æœ‰subdivisionæ•°æ®")
                
                logger.warning(f"   å¯èƒ½åŽŸå› :")
                logger.warning(f"   1. å·¥ä½œæµå®žä¾‹ä¸å­˜åœ¨")
                logger.warning(f"   2. è¯¥å·¥ä½œæµæ²¡æœ‰è¿›è¡Œä»»ä½•subdivisionæ“ä½œ")
                logger.warning(f"   3. subdivisionæ•°æ®å·²è¢«åˆ é™¤æˆ–æ ‡è®°ä¸ºdeleted")
                logger.warning(f"   å»ºè®®:")
                logger.warning(f"   - æ£€æŸ¥å·¥ä½œæµå®žä¾‹æ˜¯å¦å­˜åœ¨äºŽworkflow_instanceè¡¨")
                logger.warning(f"   - æ£€æŸ¥task_subdivisionè¡¨ä¸­æ˜¯å¦æœ‰ç›¸å…³è®°å½•")
                return []
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºsubdivisionæ•°æ®è¯¦æƒ…
            logger.info(f"ðŸ“Š [åˆå¹¶å€™é€‰] subdivisionæ•°æ®è¯¦æƒ…:")
            for i, sub in enumerate(subdivisions_data[:5]):  # æ˜¾ç¤ºå‰5æ¡
                logger.info(f"  subdivision {i+1}:")
                logger.info(f"    - subdivision_id: {sub.get('subdivision_id')}")
                logger.info(f"    - sub_workflow_instance_id: {sub.get('sub_workflow_instance_id')}")
                logger.info(f"    - sub_workflow_name: {sub.get('sub_workflow_name')}")
                logger.info(f"    - sub_workflow_status: {sub.get('sub_workflow_status')}")
                logger.info(f"    - original_node_name: {sub.get('original_node_name')}")
                logger.info(f"    - depth: {sub.get('depth')}")
            
            if len(subdivisions_data) > 5:
                logger.info(f"    ... è¿˜æœ‰ {len(subdivisions_data) - 5} æ¡subdivisionè®°å½•")
            
            tree = await WorkflowTemplateTree().build_from_subdivisions(subdivisions_data, workflow_instance_id)
            candidates = []
            
            # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹å¹¶æŒ‰æ·±åº¦æŽ’åº
            all_nodes = tree.get_all_nodes()
            logger.info(f"ðŸ“Š [åˆå¹¶å€™é€‰] æ ‘èŠ‚ç‚¹ç»Ÿè®¡: {len(all_nodes)}ä¸ªèŠ‚ç‚¹ï¼Œ{len(tree.roots)}ä¸ªæ ¹èŠ‚ç‚¹")
            
            if len(all_nodes) == 0:
                logger.warning(f"âŒ [åˆå¹¶å€™é€‰å¤±è´¥] subdivisionæ ‘æž„å»ºå¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹")
                logger.warning(f"   å¯èƒ½åŽŸå› :")
                logger.warning(f"   1. subdivisionæ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                logger.warning(f"   2. subdivisionä¹‹é—´çš„å…³ç³»å­˜åœ¨é—®é¢˜")
                logger.warning(f"   3. SubdivisionTreeæž„å»ºç®—æ³•å­˜åœ¨bug")
                return []
            
            # ä»Žæœ€æ·±å±‚å¼€å§‹ï¼ˆå¶å­èŠ‚ç‚¹ä¼˜å…ˆåˆå¹¶ï¼‰
            sorted_nodes = sorted(all_nodes, key=lambda n: n.depth, reverse=True)
            
            logger.info(f"ðŸ” [åˆå¹¶å€™é€‰] å¼€å§‹é€ä¸€æ£€æŸ¥ {len(sorted_nodes)} ä¸ªèŠ‚ç‚¹çš„åˆå¹¶å¯è¡Œæ€§...")
            
            total_candidates = 0
            mergeable_candidates = 0
            
            for node in sorted_nodes:
                total_candidates += 1
                
                logger.info(f"ðŸ” [èŠ‚ç‚¹æ£€æŸ¥ {total_candidates}] æ£€æŸ¥èŠ‚ç‚¹: {node.workflow_name}")
                logger.info(f"   - workflow_base_id: {node.workflow_base_id}")
                logger.info(f"   - workflow_instance_id: {node.workflow_instance_id}")
                logger.info(f"   - workflow_name: {node.workflow_name}")
                logger.info(f"   - status: {node.status}")
                logger.info(f"   - depth: {node.depth}")
                logger.info(f"   - replacement_info: {node.replacement_info}")
                
                # ðŸ”§ ç§»é™¤å¯è¡Œæ€§æ£€æŸ¥ï¼Œç›´æŽ¥è®¤ä¸ºæ‰€æœ‰å·¥ä½œæµæ¨¡æ¿éƒ½å¯ä»¥åˆå¹¶
                can_merge = True
                reason = "åŸºäºŽå·¥ä½œæµæ¨¡æ¿æ ‘ï¼Œç›´æŽ¥å…è®¸åˆå¹¶"
                
                mergeable_candidates += 1
                logger.info(f"   - âœ… [å¯åˆå¹¶] èŠ‚ç‚¹å¯ä»¥åˆå¹¶ (åŸºäºŽå·¥ä½œæµæ¨¡æ¿æ ‘)")
                
                # èŽ·å–æ›¿æ¢ä¿¡æ¯ä½œä¸ºsubdivisionç›¸å…³æ•°æ® - ä½¿ç”¨æ–°çš„source_subdivision
                source_subdivision = getattr(node, 'source_subdivision', {}) or {}
                
                candidate = MergeCandidate(
                    subdivision_id=source_subdivision.get('subdivision_id', f"template_{node.workflow_base_id}"),
                    parent_subdivision_id=node.parent_node.workflow_base_id if node.parent_node else None,
                    workflow_instance_id=node.workflow_instance_id or "",
                    workflow_base_id=node.workflow_base_id,
                    node_name=source_subdivision.get('original_node_name', node.workflow_name),
                    depth=node.depth,
                    can_merge=can_merge,
                    merge_reason=reason
                )
                candidates.append(candidate)
                
                logger.info(f"   - âœ… [å€™é€‰é¡¹] å·²æ·»åŠ åˆ°å€™é€‰åˆ—è¡¨")
            
            logger.info(f"ðŸ“Š [åˆå¹¶å€™é€‰æ€»ç»“] å€™é€‰é¡¹ç»Ÿè®¡:")
            logger.info(f"   - æ€»èŠ‚ç‚¹æ•°: {total_candidates}")
            logger.info(f"   - å¯åˆå¹¶èŠ‚ç‚¹: {mergeable_candidates}")
            logger.info(f"   - ä¸å¯åˆå¹¶èŠ‚ç‚¹: {total_candidates - mergeable_candidates}")
            
            if mergeable_candidates == 0:
                logger.warning(f"âš ï¸ [åˆå¹¶å€™é€‰è­¦å‘Š] æ²¡æœ‰ä»»ä½•å¯åˆå¹¶çš„èŠ‚ç‚¹!")
                logger.warning(f"   å¸¸è§åŽŸå› :")
                logger.warning(f"   1. æ‰€æœ‰å­å·¥ä½œæµçŠ¶æ€ä¸º 'running' æˆ– 'pending' (æ­£åœ¨æ‰§è¡Œä¸­)")
                logger.warning(f"   2. å­å·¥ä½œæµå®žä¾‹ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤")
                logger.warning(f"   3. subdivisionæ•°æ®ä¸å®Œæ•´")
                logger.warning(f"   å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                logger.warning(f"   1. ç­‰å¾…æ­£åœ¨è¿è¡Œçš„å·¥ä½œæµå®Œæˆ")
                logger.warning(f"   2. æ£€æŸ¥workflow_instanceè¡¨ä¸­çš„çŠ¶æ€")
                logger.warning(f"   3. éªŒè¯subdivisionæ•°æ®çš„å®Œæ•´æ€§")
            
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ [åˆå¹¶å€™é€‰å¼‚å¸¸] èŽ·å–åˆå¹¶å€™é€‰å¤±è´¥: {e}")
            logger.error(f"   å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            logger.error(f"   å¯èƒ½å½±å“:")
            logger.error(f"   1. subdivisionæŸ¥è¯¢å¤±è´¥")
            logger.error(f"   2. æ ‘æž„å»ºç®—æ³•å¼‚å¸¸")
            logger.error(f"   3. æ•°æ®åº“è¿žæŽ¥é—®é¢˜")
            raise
    
    async def execute_merge(self, workflow_instance_id: uuid.UUID, 
                          selected_merges: List[str], 
                          creator_id: uuid.UUID) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†å±‚æ¸è¿›å¼å·¥ä½œæµåˆå¹¶
        
        æ–°çš„åˆå¹¶ç­–ç•¥ï¼š
        1. æŒ‰æ·±åº¦å±‚çº§åˆ†ç»„å€™é€‰é¡¹
        2. ä»Žæœ€æ·±å±‚å¼€å§‹ï¼Œé€å±‚å‘ä¸Šåˆå¹¶
        3. æ¯å±‚åˆå¹¶åŽç”Ÿæˆæ–°çš„å·¥ä½œæµç‰ˆæœ¬
        4. ä¸‹ä¸€å±‚åŸºäºŽå‰ä¸€å±‚çš„ç»“æžœç»§ç»­åˆå¹¶
        
        Args:
            workflow_instance_id: ä¸»å·¥ä½œæµå®žä¾‹ID
            selected_merges: é€‰ä¸­çš„subdivision IDåˆ—è¡¨
            creator_id: åˆå¹¶æ“ä½œæ‰§è¡Œè€…ID
            
        Returns:
            åˆå¹¶ç»“æžœä¿¡æ¯
        """
        try:
            logger.info(f"ðŸš€ [åˆ†å±‚åˆå¹¶] å¼€å§‹å·¥ä½œæµåˆå¹¶: {workflow_instance_id}")
            logger.info(f"é€‰ä¸­çš„subdivisions: {selected_merges}")
            logger.info(f"åˆå¹¶æ‰§è¡Œè€…: {creator_id}")
            logger.info(f"ðŸ“Š [è°ƒè¯•] å·¥ä½œæµå®žä¾‹IDç±»åž‹: {type(workflow_instance_id)}, å€¼: {workflow_instance_id}")
            
            # ðŸ”§ å¢žåŠ è°ƒè¯•ï¼šå…ˆæ£€æŸ¥å·¥ä½œæµå®žä¾‹çŠ¶æ€
            workflow_check = await self.db.fetch_one("""
                SELECT wi.workflow_instance_id, wi.status, wi.created_at, 
                       w.name as workflow_name, w.workflow_base_id
                FROM workflow_instance wi
                JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id AND w.is_current_version = TRUE
                WHERE wi.workflow_instance_id = %s
            """, workflow_instance_id)
            logger.info(f"ðŸ“Š [åˆå¹¶è°ƒè¯•] ç›®æ ‡å·¥ä½œæµä¿¡æ¯: {workflow_check}")
            
            if not workflow_check:
                logger.error(f"âŒ [åˆå¹¶å¤±è´¥] å·¥ä½œæµå®žä¾‹ä¸å­˜åœ¨: {workflow_instance_id}")
                return {"success": False, "message": "ç›®æ ‡å·¥ä½œæµå®žä¾‹ä¸å­˜åœ¨"}
            
            # 1. èŽ·å–åˆå¹¶å€™é€‰é¡¹å¹¶ç­›é€‰
            logger.info(f"ðŸ” [åˆå¹¶æ­¥éª¤1] èŽ·å–åˆå¹¶å€™é€‰é¡¹...")
            candidates = await self.get_merge_candidates(workflow_instance_id)
            logger.info(f"ðŸ“‹ èŽ·å–åˆ° {len(candidates)} ä¸ªå€™é€‰é¡¹")
            
            # è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªå€™é€‰é¡¹
            for i, candidate in enumerate(candidates):
                logger.info(f"  å€™é€‰é¡¹ {i+1}:")
                logger.info(f"    - subdivision_id: {candidate.subdivision_id}")
                logger.info(f"    - workflow_instance_id: {candidate.workflow_instance_id}")
                logger.info(f"    - node_name: {candidate.node_name}")
                logger.info(f"    - can_merge: {candidate.can_merge}")
                logger.info(f"    - reason: {candidate.merge_reason}")
            
            # æ”¯æŒé€šè¿‡subdivision_idæˆ–workflow_instance_idåŒ¹é…
            selected_candidates = []
            logger.info(f"ðŸ” [åˆå¹¶æ­¥éª¤2] åŒ¹é…é€‰ä¸­é¡¹: {selected_merges}")
            
            for c in candidates:
                subdivision_match = c.subdivision_id in selected_merges
                workflow_match = c.workflow_instance_id in selected_merges
                
                if subdivision_match or workflow_match:
                    selected_candidates.append(c)
                    match_type = "subdivision_id" if subdivision_match else "workflow_instance_id"
                    logger.info(f"  âœ… åŒ¹é…æˆåŠŸ ({match_type}): {c.node_name}")
                else:
                    logger.info(f"  âŒ æœªåŒ¹é…: {c.node_name} (subdivision: {c.subdivision_id}, workflow: {c.workflow_instance_id})")
            
            if not selected_candidates:
                logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å€™é€‰é¡¹ï¼")
                logger.warning(f"   é€‰ä¸­é¡¹: {selected_merges}")
                logger.warning(f"   å¯ç”¨å€™é€‰é¡¹:")
                for c in candidates:
                    logger.warning(f"     - subdivision_id: {c.subdivision_id}")
                    logger.warning(f"     - workflow_instance_id: {c.workflow_instance_id}")
                return {"success": False, "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆå¹¶å€™é€‰"}
            
            # è¿‡æ»¤å‡ºçœŸæ­£å¯åˆå¹¶çš„å€™é€‰é¡¹
            mergeable_candidates = [c for c in selected_candidates if c.can_merge]
            logger.info(f"ðŸ“Š [åˆå¹¶æ­¥éª¤3] å¯åˆå¹¶å€™é€‰é¡¹: {len(mergeable_candidates)} / {len(selected_candidates)}")
            
            if not mergeable_candidates:
                logger.warning(f"âš ï¸ é€‰ä¸­çš„å€™é€‰é¡¹éƒ½ä¸å¯åˆå¹¶ï¼")
                for c in selected_candidates:
                    logger.warning(f"   - {c.node_name}: {c.merge_reason}")
                return {"success": False, "message": "é€‰ä¸­çš„å€™é€‰é¡¹éƒ½ä¸å¯åˆå¹¶"}
            
            # 2. æŒ‰æ·±åº¦åˆ†ç»„å€™é€‰é¡¹
            candidates_by_depth = {}
            for candidate in mergeable_candidates:
                depth = candidate.depth
                if depth not in candidates_by_depth:
                    candidates_by_depth[depth] = []
                candidates_by_depth[depth].append(candidate)
            
            logger.info(f"ðŸ“Š [åˆ†å±‚åˆå¹¶] å€™é€‰é¡¹åˆ†ç»„:")
            for depth, cands in candidates_by_depth.items():
                names = [c.node_name for c in cands]
                logger.info(f"   æ·±åº¦ {depth}: {len(cands)} ä¸ªå€™é€‰é¡¹ - {names}")
            
            # 3. èŽ·å–åˆå§‹å·¥ä½œæµä¿¡æ¯
            initial_workflow_base_id = await self._get_initial_workflow_base_id(workflow_instance_id)
            if not initial_workflow_base_id:
                return {"success": False, "message": "æ— æ³•èŽ·å–åˆå§‹å·¥ä½œæµåŸºç¡€ID"}
            
            # 4. åˆ†å±‚æ¸è¿›å¼åˆå¹¶
            current_workflow_base_id = initial_workflow_base_id
            layer_results = []
            total_merged = 0
            
            # ä»Žæœ€æ·±å±‚å¼€å§‹å‘ä¸Šé€å±‚åˆå¹¶
            for depth in sorted(candidates_by_depth.keys(), reverse=True):
                depth_candidates = candidates_by_depth[depth]
                
                logger.info(f"ðŸ”„ [ç¬¬{len(layer_results)+1}å±‚] åˆå¹¶æ·±åº¦ {depth}: {len(depth_candidates)} ä¸ªå€™é€‰é¡¹")
                
                # æ‰§è¡Œå•å±‚åˆå¹¶
                layer_result = await self._merge_single_depth_layer(
                    current_workflow_base_id, depth_candidates, creator_id, depth
                )
                
                if layer_result['success']:
                    layer_results.append(layer_result)
                    current_workflow_base_id = layer_result['new_workflow_base_id']
                    total_merged += layer_result['merged_count']
                    
                    logger.info(f"   âœ… [ç¬¬{len(layer_results)}å±‚] åˆå¹¶æˆåŠŸ: åˆå¹¶äº† {layer_result['merged_count']} ä¸ªsubdivision")
                    logger.info(f"   ðŸ“‹ æ–°å·¥ä½œæµåŸºç¡€ID: {current_workflow_base_id}")
                else:
                    logger.error(f"   âŒ [ç¬¬{len(layer_results)+1}å±‚] åˆå¹¶å¤±è´¥: {layer_result['error']}")
                    return {
                        "success": False,
                        "message": f"ç¬¬{len(layer_results)+1}å±‚åˆå¹¶å¤±è´¥",
                        "error": layer_result['error'],
                        "completed_layers": len(layer_results)
                    }
            
            # 5. è¿”å›žåˆ†å±‚åˆå¹¶ç»“æžœ
            logger.info(f"âœ… [åˆ†å±‚åˆå¹¶] å…¨éƒ¨å®Œæˆ: å…± {len(layer_results)} å±‚ï¼Œåˆå¹¶äº† {total_merged} ä¸ªsubdivision")
            
            return {
                "success": True,
                "merge_type": "progressive_layered",
                "final_workflow_base_id": current_workflow_base_id,
                "initial_workflow_base_id": initial_workflow_base_id,
                "total_layers": len(layer_results),
                "total_merged": total_merged,
                "layer_results": layer_results,
                "summary": {
                    "completed_layers": len(layer_results),
                    "total_candidates": len(mergeable_candidates),
                    "successfully_merged": total_merged
                }
            }
                
        except Exception as e:
            logger.error(f"âŒ [åˆ†å±‚åˆå¹¶] å·¥ä½œæµåˆå¹¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    
    async def _check_merge_feasibility(self, node: WorkflowTemplateNode) -> Tuple[bool, str]:
        """æ£€æŸ¥å·¥ä½œæµæ¨¡æ¿èŠ‚ç‚¹æ˜¯å¦å¯ä»¥åˆå¹¶ - åŸºäºŽå·¥ä½œæµæ¨¡æ¿æ ‘"""
        try:
            logger.info(f"ðŸ” [å·¥ä½œæµæ¨¡æ¿å¯è¡Œæ€§æ£€æŸ¥] æ£€æŸ¥åˆå¹¶å¯è¡Œæ€§: {node.workflow_name}")
            logger.info(f"   - workflow_base_id: {node.workflow_base_id}")
            logger.info(f"   - workflow_instance_id: {node.workflow_instance_id}")
            logger.info(f"   - status: {node.status}")
            
            # åŸºæœ¬æ£€æŸ¥ï¼šå¿…é¡»æœ‰å­å·¥ä½œæµå®žä¾‹ID
            if not node.workflow_instance_id:
                logger.warning(f"   âŒ [å·¥ä½œæµæ¨¡æ¿æ£€æŸ¥] ç¼ºå°‘å­å·¥ä½œæµå®žä¾‹ID")
                return False, "ç¼ºå°‘å­å·¥ä½œæµå®žä¾‹ID"
            
            # åŸºæœ¬æ£€æŸ¥ï¼šå¿…é¡»æœ‰workflow_base_id
            if not node.workflow_base_id:
                logger.warning(f"   âŒ [å·¥ä½œæµæ¨¡æ¿æ£€æŸ¥] ç¼ºå°‘workflow_base_id")
                return False, "ç¼ºå°‘workflow_base_id"
            
            # ðŸ”§ ç®€åŒ–é€»è¾‘ï¼šç›´æŽ¥åŸºäºŽå·¥ä½œæµæ¨¡æ¿æ ‘æ•°æ®è¿›è¡Œåˆå¹¶ï¼Œä¸æ£€æŸ¥å…·ä½“çŠ¶æ€
            # åªè¦å·¥ä½œæµæ¨¡æ¿æ•°æ®å®Œæ•´ï¼Œå°±è®¤ä¸ºå¯ä»¥åˆå¹¶
            logger.info(f"   âœ… [å·¥ä½œæµæ¨¡æ¿æ£€æŸ¥] å¯ä»¥åˆå¹¶ - åŸºäºŽå·¥ä½œæµæ¨¡æ¿æ ‘æ•°æ®")
            return True, "åŸºäºŽå·¥ä½œæµæ¨¡æ¿æ ‘æ•°æ®ï¼Œå¯ä»¥åˆå¹¶"
            
        except Exception as e:
            logger.error(f"âŒ [ç®€åŒ–å¯è¡Œæ€§æ£€æŸ¥] æ£€æŸ¥å¤±è´¥: {e}")
            return False, f"æ£€æŸ¥å¤±è´¥: {str(e)}"
    
    async def _get_initial_workflow_base_id(self, workflow_instance_id: uuid.UUID) -> Optional[str]:
        """èŽ·å–åˆå§‹å·¥ä½œæµåŸºç¡€ID"""
        try:
            result = await self.db.fetch_one("""
                SELECT workflow_base_id FROM workflow_instance 
                WHERE workflow_instance_id = %s
            """, workflow_instance_id)
            
            if result:
                return str(result['workflow_base_id'])
            return None
            
        except Exception as e:
            logger.error(f"èŽ·å–åˆå§‹å·¥ä½œæµåŸºç¡€IDå¤±è´¥: {e}")
            return None
    
    async def _merge_single_depth_layer(self, current_workflow_base_id: str, 
                                       depth_candidates: List[MergeCandidate], 
                                       creator_id: uuid.UUID, depth: int) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•å±‚æ·±åº¦çš„åˆå¹¶
        
        Args:
            current_workflow_base_id: å½“å‰å·¥ä½œæµåŸºç¡€ID
            depth_candidates: å½“å‰æ·±åº¦çš„å€™é€‰é¡¹åˆ—è¡¨
            creator_id: åˆ›å»ºè€…ID
            depth: å½“å‰æ·±åº¦
            
        Returns:
            å•å±‚åˆå¹¶ç»“æžœ
        """
        try:
            logger.info(f"ðŸ”§ [å•å±‚åˆå¹¶] å¼€å§‹åˆå¹¶æ·±åº¦ {depth}: {len(depth_candidates)} ä¸ªå€™é€‰é¡¹")
            
            # 1. ç”Ÿæˆæ–°çš„å·¥ä½œæµç‰ˆæœ¬
            new_workflow_base_id = uuid.uuid4()
            
            # 2. èŽ·å–å½“å‰å·¥ä½œæµçš„workflow_id
            current_workflow_id = await self._get_current_workflow_id_by_base(current_workflow_base_id)
            if not current_workflow_id:
                return {"success": False, "error": f"æ— æ³•æ‰¾åˆ°å½“å‰å·¥ä½œæµ: {current_workflow_base_id}"}
            
            # 3. åˆ›å»ºæ–°çš„å·¥ä½œæµè®°å½•
            new_workflow_info = await self._create_layered_workflow_record(
                current_workflow_base_id, new_workflow_base_id, depth, len(depth_candidates), creator_id
            )
            new_workflow_id = new_workflow_info['workflow_id']
            
            # 4. æ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢åˆå¹¶
            merge_stats = await self._execute_layer_node_replacement(
                current_workflow_id, new_workflow_id, new_workflow_base_id, depth_candidates
            )
            
            logger.info(f"âœ… [å•å±‚åˆå¹¶] æ·±åº¦ {depth} åˆå¹¶å®Œæˆ:")
            logger.info(f"   - æ–°å·¥ä½œæµ: {new_workflow_info['name']}")
            logger.info(f"   - åˆå¹¶èŠ‚ç‚¹: {merge_stats.get('nodes_replaced', 0)}")
            logger.info(f"   - é‡å»ºè¿žæŽ¥: {merge_stats.get('connections_count', 0)}")
            
            return {
                "success": True,
                "depth": depth,
                "new_workflow_base_id": str(new_workflow_base_id),
                "new_workflow_id": str(new_workflow_id),
                "merged_count": len(depth_candidates),
                "workflow_info": new_workflow_info,
                "merge_stats": merge_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ [å•å±‚åˆå¹¶] æ·±åº¦ {depth} åˆå¹¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _get_current_workflow_id_by_base(self, workflow_base_id: str) -> Optional[str]:
        """æ ¹æ®workflow_base_idèŽ·å–å½“å‰ç‰ˆæœ¬çš„workflow_id"""
        try:
            result = await self.db.fetch_one("""
                SELECT workflow_id FROM workflow 
                WHERE workflow_base_id = %s AND is_current_version = TRUE
            """, workflow_base_id)
            
            return result['workflow_id'] if result else None
            
        except Exception as e:
            logger.error(f"èŽ·å–å½“å‰å·¥ä½œæµIDå¤±è´¥: {e}")
            return None
    
    async def _create_layered_workflow_record(self, parent_workflow_base_id: str,
                                            new_workflow_base_id: uuid.UUID,
                                            depth: int, merge_count: int,
                                            creator_id: uuid.UUID) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†å±‚åˆå¹¶çš„å·¥ä½œæµè®°å½•"""
        try:
            # èŽ·å–çˆ¶å·¥ä½œæµåç§°
            parent_workflow = await self.db.fetch_one("""
                SELECT name FROM workflow 
                WHERE workflow_base_id = %s AND is_current_version = TRUE
            """, parent_workflow_base_id)
            
            parent_name = parent_workflow['name'] if parent_workflow else "Unknown_Workflow"
            
            # ç”Ÿæˆåˆ†å±‚åˆå¹¶çš„å·¥ä½œæµåç§°
            new_workflow_id = uuid.uuid4()
            merged_name = f"{parent_name}_åˆå¹¶_æ·±åº¦{depth}_{merge_count}é¡¹"
            merged_description = f"åˆ†å±‚åˆå¹¶æ·±åº¦{depth}çš„{merge_count}ä¸ªsubdivisionï¼ŒåŸºäºŽ{parent_name}"
            
            await self.db.execute("""
                INSERT INTO workflow (
                    workflow_id, workflow_base_id, name, description, 
                    creator_id, is_current_version, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, new_workflow_id, new_workflow_base_id, merged_name, merged_description,
                 creator_id, True, now_utc())
            
            logger.info(f"âœ… [å·¥ä½œæµè®°å½•] åˆ›å»ºåˆ†å±‚åˆå¹¶å·¥ä½œæµ: {merged_name}")
            
            return {
                "workflow_id": str(new_workflow_id),
                "workflow_base_id": str(new_workflow_base_id),
                "name": merged_name,
                "description": merged_description
            }
            
        except Exception as e:
            logger.error(f"âŒ [å·¥ä½œæµè®°å½•] åˆ›å»ºåˆ†å±‚åˆå¹¶å·¥ä½œæµå¤±è´¥: {e}")
            raise
    
    async def _execute_layer_node_replacement(self, parent_workflow_id: str,
                                            new_workflow_id: uuid.UUID, new_workflow_base_id: uuid.UUID,
                                            depth_candidates: List[MergeCandidate]) -> Dict[str, Any]:
        """æ‰§è¡Œå•å±‚çš„èŠ‚ç‚¹æ›¿æ¢åˆå¹¶"""
        try:
            # 1. æ”¶é›†å½“å‰å±‚éœ€è¦æ›¿æ¢çš„subdivisionèŠ‚ç‚¹ID
            subdivision_node_ids = set()
            subdivision_mapping = {}  # node_id -> candidate
            
            for candidate in depth_candidates:
                # èŽ·å–subdivisionå¯¹åº”çš„åŽŸå§‹èŠ‚ç‚¹ä¿¡æ¯
                original_node_info = await self._get_original_node_info(candidate.subdivision_id)
                if original_node_info:
                    # ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨node_idè€Œä¸æ˜¯task_instance_idæ¥æŽ’é™¤èŠ‚ç‚¹
                    node_id = original_node_info['node_id']
                    subdivision_node_ids.add(node_id)
                    subdivision_mapping[node_id] = {
                        'candidate': candidate,
                        'original_node': original_node_info
                    }
                    logger.info(f"   ðŸ”§ å°†æŽ’é™¤èŠ‚ç‚¹: {original_node_info['name']} (node_id: {node_id})")
            
            logger.info(f"ðŸ”„ [èŠ‚ç‚¹æ›¿æ¢] å°†æ›¿æ¢ {len(subdivision_node_ids)} ä¸ªsubdivisionèŠ‚ç‚¹")
            logger.info(f"   ðŸ“‹ æŽ’é™¤çš„node_idåˆ—è¡¨: {list(subdivision_node_ids)}")
            
            # 2. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™èŠ‚ç‚¹ï¼ˆæŽ’é™¤è¢«subdivisionçš„èŠ‚ç‚¹ï¼‰
            node_id_mapping = await self._copy_preserved_nodes(
                parent_workflow_id, new_workflow_id, new_workflow_base_id, subdivision_node_ids
            )
            
            # 3. ä¸ºæ¯ä¸ªsubdivisionæ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢
            replacement_stats = await self._replace_subdivision_nodes_layered(
                new_workflow_id, new_workflow_base_id, subdivision_mapping, node_id_mapping
            )
            
            # 4. é‡å»ºæ‰€æœ‰è¿žæŽ¥
            connection_stats = await self._rebuild_layer_connections(
                parent_workflow_id, new_workflow_id, subdivision_mapping, node_id_mapping
            )
            
            logger.info(f"âœ… [èŠ‚ç‚¹æ›¿æ¢] å®Œæˆ: æ›¿æ¢{replacement_stats['nodes_replaced']}èŠ‚ç‚¹, é‡å»º{connection_stats['connections_count']}è¿žæŽ¥")
            
            return {
                **replacement_stats,
                **connection_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹æ›¿æ¢] æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def _replace_subdivision_nodes_layered(self, new_workflow_id: uuid.UUID, new_workflow_base_id: uuid.UUID,
                                               subdivision_mapping: Dict[str, Dict], 
                                               node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """åˆ†å±‚åˆå¹¶ï¼šç”¨å­å·¥ä½œæµèŠ‚ç‚¹æ›¿æ¢subdivisionèŠ‚ç‚¹"""
        replaced_nodes = 0
        
        # éåŽ†æ¯ä¸ªsubdivisionï¼Œå¤åˆ¶å…¶å­å·¥ä½œæµçš„ä¸šåŠ¡èŠ‚ç‚¹
        for original_node_id, mapping_info in subdivision_mapping.items():
            candidate = mapping_info['candidate']
            logger.info(f"ðŸ”„ [èŠ‚ç‚¹æ›¿æ¢] å¤„ç†subdivision: {candidate.node_name}")
            
            # èŽ·å–å­å·¥ä½œæµç»“æž„ï¼ˆé‡æ–°åˆ†æžä»¥ç¡®ä¿æ•°æ®å®Œæ•´ï¼‰
            original_node_info = mapping_info['original_node']
            workflow_structure = await self._analyze_subworkflow_structure(
                candidate.workflow_instance_id,
                original_node_info['position_x'],
                original_node_info['position_y']
            )
            
            # å¤åˆ¶å­å·¥ä½œæµçš„ä¸šåŠ¡èŠ‚ç‚¹
            for node in workflow_structure['business_nodes']:
                new_node_id = uuid.uuid4()
                new_node_base_id = uuid.uuid4()
                
                # ðŸ”§ ä¿®å¤ï¼šå°†æ–°èŠ‚ç‚¹åŠ å…¥æ˜ å°„ï¼Œä¾›è¿žæŽ¥é‡å»ºä½¿ç”¨
                node_id_mapping[node['node_id']] = new_node_id
                
                logger.info(f"   ðŸ“„ å¤åˆ¶ä¸šåŠ¡èŠ‚ç‚¹: {node['name']} -> æ–°ID: {new_node_id}")
                
                await self.db.execute("""
                    INSERT INTO node (
                        node_id, node_base_id, workflow_id, workflow_base_id,
                        name, type, task_description, position_x, position_y,
                        version, is_current_version, created_at, is_deleted
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                     node['name'], node['type'], node['task_description'], 
                     node['position_x'], node['position_y'], 1, True, now_utc(), False)
                
                replaced_nodes += 1
        
        logger.info(f"âœ… [åˆ†å±‚åˆå¹¶] æ›¿æ¢äº† {replaced_nodes} ä¸ªèŠ‚ç‚¹")
        return {"nodes_replaced": replaced_nodes}
    
    async def _rebuild_layer_connections(self, parent_workflow_id: str, new_workflow_id: uuid.UUID,
                                       subdivision_mapping: Dict[str, Dict],
                                       node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """é‡å»ºåˆ†å±‚åˆå¹¶çš„è¿žæŽ¥"""
        # èŽ·å–çˆ¶å·¥ä½œæµçš„æ‰€æœ‰è¿žæŽ¥
        parent_connections = await self.db.fetch_all("""
            SELECT from_node_id, to_node_id, connection_type, condition_config
            FROM node_connection 
            WHERE workflow_id = %s
        """, parent_workflow_id)
        
        parent_connections_copied = 0
        subworkflow_connections_copied = 0
        cross_boundary_connections_created = 0
        
        subdivision_node_ids = set(subdivision_mapping.keys())
        
        # 1. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™è¿žæŽ¥ï¼ˆä¸æ¶‰åŠsubdivisionèŠ‚ç‚¹ï¼‰
        for conn in parent_connections:
            from_id, to_id = conn['from_node_id'], conn['to_node_id']
            
            if from_id in subdivision_node_ids or to_id in subdivision_node_ids:
                continue
            
            if from_id in node_id_mapping and to_id in node_id_mapping:
                await self.db.execute("""
                    INSERT INTO node_connection (
                        from_node_id, to_node_id, workflow_id,
                        connection_type, condition_config, created_at
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                """, node_id_mapping[from_id], node_id_mapping[to_id],
                     new_workflow_id, conn.get('connection_type', 'normal'),
                     conn.get('condition_config'), now_utc())
                parent_connections_copied += 1
        
        # 2. å¤åˆ¶æ¯ä¸ªå­å·¥ä½œæµçš„å†…éƒ¨è¿žæŽ¥å¹¶é‡å»ºè·¨è¾¹ç•Œè¿žæŽ¥
        for original_node_id, mapping_info in subdivision_mapping.items():
            candidate = mapping_info['candidate']
            
            sub_workflow_data = await self._get_subworkflow_data(candidate.workflow_base_id)
            if not sub_workflow_data:
                continue
                
            original_node_info = mapping_info['original_node']
            workflow_structure = await self._analyze_subworkflow_structure(
                candidate.workflow_instance_id,  # ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨å€™é€‰é¡¹çš„workflow_instance_id
                original_node_info['position_x'],
                original_node_info['position_y']
            )
            
            # å¤åˆ¶å­å·¥ä½œæµå†…éƒ¨è¿žæŽ¥
            logger.info(f"   ðŸ”„ å¼€å§‹å¤åˆ¶å­å·¥ä½œæµå†…éƒ¨è¿žæŽ¥: {len(workflow_structure['business_connections'])}ä¸ª")
            for conn in workflow_structure['business_connections']:
                from_id, to_id = conn['from_node_id'], conn['to_node_id']
                logger.info(f"      æ£€æŸ¥è¿žæŽ¥: {from_id} -> {to_id}")
                logger.info(f"      from_idåœ¨æ˜ å°„ä¸­: {from_id in node_id_mapping}")
                logger.info(f"      to_idåœ¨æ˜ å°„ä¸­: {to_id in node_id_mapping}")
                
                if from_id in node_id_mapping and to_id in node_id_mapping:
                    await self.db.execute("""
                        INSERT INTO node_connection (
                            from_node_id, to_node_id, workflow_id,
                            connection_type, condition_config, created_at
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                    """, node_id_mapping[from_id], node_id_mapping[to_id],
                         new_workflow_id, conn.get('connection_type', 'normal'),
                         conn.get('condition_config'), now_utc())
                    subworkflow_connections_copied += 1
                    logger.info(f"      âœ… æˆåŠŸå¤åˆ¶è¿žæŽ¥: {node_id_mapping[from_id]} -> {node_id_mapping[to_id]}")
                else:
                    logger.warning(f"      âŒ è·³è¿‡è¿žæŽ¥ï¼ˆèŠ‚ç‚¹æœªåœ¨æ˜ å°„ä¸­ï¼‰: {from_id} -> {to_id}")
            
            # é‡å»ºè·¨è¾¹ç•Œè¿žæŽ¥
            entry_points = workflow_structure['entry_points']
            exit_points = workflow_structure['exit_points']
            
            # ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„node_idè¿›è¡Œè¿žæŽ¥é‡å»º
            subdivision_node_id = original_node_info['node_id']  # ä½¿ç”¨node_idè€Œä¸æ˜¯task_instance_id
            
            # é‡å»ºä¸Šæ¸¸è¿žæŽ¥
            for conn in parent_connections:
                if conn['to_node_id'] == subdivision_node_id:
                    from_id = conn['from_node_id']
                    if from_id in node_id_mapping:
                        for entry_point in entry_points:
                            if entry_point['node_id'] in node_id_mapping:
                                await self.db.execute("""
                                    INSERT INTO node_connection (
                                        from_node_id, to_node_id, workflow_id,
                                        connection_type, condition_config, created_at
                                    ) VALUES (%s, %s, %s, %s, %s, %s)
                                """, node_id_mapping[from_id],
                                     node_id_mapping[entry_point['node_id']],
                                     new_workflow_id, conn.get('connection_type', 'normal'),
                                     conn.get('condition_config'), now_utc())
                                cross_boundary_connections_created += 1
                                logger.info(f"   ðŸ”— ä¸Šæ¸¸è¿žæŽ¥: {from_id} -> {entry_point['name']}")
            
            # é‡å»ºä¸‹æ¸¸è¿žæŽ¥
            for conn in parent_connections:
                if conn['from_node_id'] == subdivision_node_id:
                    to_id = conn['to_node_id']
                    if to_id in node_id_mapping:
                        for exit_point in exit_points:
                            if exit_point['node_id'] in node_id_mapping:
                                await self.db.execute("""
                                    INSERT INTO node_connection (
                                        from_node_id, to_node_id, workflow_id,
                                        connection_type, condition_config, created_at
                                    ) VALUES (%s, %s, %s, %s, %s, %s)
                                """, node_id_mapping[exit_point['node_id']],
                                     node_id_mapping[to_id],
                                     new_workflow_id, conn.get('connection_type', 'normal'),
                                     conn.get('condition_config'), now_utc())
                                cross_boundary_connections_created += 1
                                logger.info(f"   ðŸ”— ä¸‹æ¸¸è¿žæŽ¥: {exit_point['name']} -> {to_id}")
        
        logger.info(f"âœ… [åˆ†å±‚åˆå¹¶] è¿žæŽ¥é‡å»ºå®Œæˆ: çˆ¶è¿žæŽ¥{parent_connections_copied}, å­è¿žæŽ¥{subworkflow_connections_copied}, è·¨è¾¹ç•Œ{cross_boundary_connections_created}")
        
        return {
            "parent_connections_copied": parent_connections_copied,
            "subworkflow_connections_copied": subworkflow_connections_copied,
            "cross_boundary_connections_created": cross_boundary_connections_created,
            "connections_count": parent_connections_copied + subworkflow_connections_copied + cross_boundary_connections_created
        }
    
    async def _execute_single_merge(self, candidate: MergeCandidate, 
                                  new_workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªsubdivisionçš„åˆå¹¶ - é‡æž„ç‰ˆæœ¬"""
        try:
            logger.info(f"ðŸ”§ æ‰§è¡Œå•ä¸ªåˆå¹¶: {candidate.node_name}")
            
            # 1. èŽ·å–å­å·¥ä½œæµä¿¡æ¯
            sub_workflow_data = await self._get_subworkflow_data(candidate.workflow_base_id)
            if not sub_workflow_data:
                raise Exception(f"æ‰¾ä¸åˆ°å­å·¥ä½œæµ: {candidate.workflow_base_id}")
            
            # 2. èŽ·å–è¢«æ›¿æ¢çš„subdivisionèŠ‚ç‚¹ä¿¡æ¯
            original_node_info = await self._get_original_node_info(candidate.subdivision_id)
            if not original_node_info:
                raise Exception(f"æ‰¾ä¸åˆ°åŽŸå§‹subdivisionèŠ‚ç‚¹ä¿¡æ¯: {candidate.subdivision_id}")
            
            # 3. åˆ†æžå­å·¥ä½œæµç»“æž„
            workflow_structure = await self._analyze_subworkflow_structure(
                candidate.workflow_instance_id,  # ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨å€™é€‰é¡¹çš„workflow_instance_id
                original_node_info['position_x'],
                original_node_info['position_y']
            )
            
            # 4. åˆ›å»ºåˆå¹¶æ“ä½œè®°å½•
            operation = MergeOperation(
                target_node_id=candidate.subdivision_id,
                sub_workflow_id=candidate.workflow_base_id,
                subdivision_id=candidate.subdivision_id,
                depth=candidate.depth
            )
            
            logger.info(f"âœ… åˆå¹¶å‡†å¤‡å®Œæˆ: {len(workflow_structure['business_nodes'])}ä¸ªä¸šåŠ¡èŠ‚ç‚¹, "
                       f"{len(workflow_structure['entry_points'])}ä¸ªå…¥å£, "
                       f"{len(workflow_structure['exit_points'])}ä¸ªå‡ºå£")
            
            return {
                "success": True,
                "operation": operation,
                "original_node": original_node_info,
                "workflow_structure": workflow_structure,
                "candidate": candidate
            }
            
        except Exception as e:
            logger.error(f"âŒ å•ä¸ªåˆå¹¶æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _get_subworkflow_data(self, workflow_base_id: str) -> Optional[Dict[str, Any]]:
        """èŽ·å–å­å·¥ä½œæµåŸºæœ¬ä¿¡æ¯"""
        return await self.db.fetch_one("""
            SELECT workflow_id, name, description 
            FROM workflow 
            WHERE workflow_base_id = %s 
            AND is_current_version = TRUE
        """, workflow_base_id)
    
    async def _get_original_node_info(self, subdivision_id: str) -> Optional[Dict[str, Any]]:
        """èŽ·å–è¢«subdivisionçš„åŽŸå§‹èŠ‚ç‚¹ä¿¡æ¯"""
        logger.info(f"ðŸ” æŸ¥æ‰¾subdivisionåŽŸå§‹èŠ‚ç‚¹ä¿¡æ¯: {subdivision_id}")
        
        # é¦–å…ˆå°è¯•é€šè¿‡ä¸åŒçš„æŸ¥è¯¢æ–¹å¼æ‰¾åˆ°subdivisionè®°å½•
        subdivision_record = None
        
        # æ–¹æ³•1: ç›´æŽ¥åŒ¹é…UUIDå­—ç¬¦ä¸²
        subdivision_record = await self.db.fetch_one("""
            SELECT subdivision_id, original_task_id, created_at 
            FROM task_subdivision 
            WHERE CAST(subdivision_id AS CHAR) = %s
        """, subdivision_id)
        
        if not subdivision_record:
            # æ–¹æ³•2: å°è¯•UUIDè½¬æ¢
            try:
                import uuid as uuid_lib
                subdivision_uuid = uuid_lib.UUID(subdivision_id)
                subdivision_record = await self.db.fetch_one("""
                    SELECT subdivision_id, original_task_id, created_at 
                    FROM task_subdivision 
                    WHERE subdivision_id = %s
                """, subdivision_uuid)
            except ValueError:
                logger.info(f"   subdivision_idä¸æ˜¯æœ‰æ•ˆçš„UUIDæ ¼å¼: {subdivision_id}")
        
        if not subdivision_record:
            # æ–¹æ³•3: æ¨¡ç³ŠåŒ¹é…ï¼ˆç”¨äºŽè°ƒè¯•ï¼‰
            subdivision_record = await self.db.fetch_one("""
                SELECT subdivision_id, original_task_id, created_at 
                FROM task_subdivision 
                WHERE CAST(subdivision_id AS CHAR) LIKE %s
            """, f"%{subdivision_id}%")
        
        logger.info(f"   subdivisionè®°å½•: {subdivision_record}")
        
        if not subdivision_record:
            logger.warning(f"   âŒ åœ¨task_subdivisionè¡¨ä¸­æ‰¾ä¸åˆ°subdivision: {subdivision_id}")
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºtask_subdivisionè¡¨ä¸­çš„æ‰€æœ‰è®°å½•
            all_subdivisions = await self.db.fetch_all("""
                SELECT CAST(subdivision_id AS CHAR) as subdivision_id_str, 
                       CAST(original_task_id AS CHAR) as original_task_id_str,
                       created_at 
                FROM task_subdivision 
                WHERE is_deleted = FALSE
                ORDER BY created_at DESC
                LIMIT 10
            """)
            logger.info(f"   è°ƒè¯•ï¼šæœ€è¿‘çš„10ä¸ªsubdivisionè®°å½•:")
            for sub in all_subdivisions:
                logger.info(f"     - {sub['subdivision_id_str']} -> {sub['original_task_id_str']}")
            
            return None
            
        # èŽ·å–original_task_id
        original_task_id = subdivision_record['original_task_id']
        logger.info(f"   åŽŸå§‹ä»»åŠ¡ID: {original_task_id}")
        
        # æŸ¥æ‰¾å¯¹åº”çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œé€šè¿‡task_instance -> node_instance -> nodeè·¯å¾„
        node_info = await self.db.fetch_one("""
            SELECT 
                ti.task_instance_id,
                ni.node_instance_id,
                n.node_id, n.position_x, n.position_y, n.name, n.type, n.task_description,
                n.workflow_id, w.name as workflow_name
            FROM task_instance ti
            JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id  
            JOIN node n ON ni.node_id = n.node_id
            JOIN workflow w ON n.workflow_id = w.workflow_id
            WHERE ti.task_instance_id = %s
        """, original_task_id)
        
        logger.info(f"   èŠ‚ç‚¹ä¿¡æ¯: {node_info}")
        
        if node_info:
            # åˆå¹¶ä¿¡æ¯
            result = {
                'original_task_id': original_task_id,
                'node_id': node_info['node_id'],  # ðŸ”§ æ·»åŠ node_idç”¨äºŽè¿žæŽ¥é‡å»º
                'position_x': node_info['position_x'],
                'position_y': node_info['position_y'],
                'name': node_info['name'],
                'type': node_info['type'],
                'task_description': node_info['task_description'],
                'workflow_id': node_info['workflow_id'],
                'workflow_name': node_info['workflow_name']
            }
            logger.info(f"   âœ… æˆåŠŸæ‰¾åˆ°åŽŸå§‹èŠ‚ç‚¹ä¿¡æ¯")
            return result
        else:
            logger.warning(f"   âŒ æ‰¾ä¸åˆ°èŠ‚ç‚¹ä¿¡æ¯: {original_task_id}")
            return None
    
    async def _analyze_subworkflow_structure(self, candidate_workflow_instance_id: str, 
                                           center_x: int, center_y: int) -> Dict[str, Any]:
        """
        åˆ†æžå­å·¥ä½œæµç»“æž„ï¼Œè¯†åˆ«å…¥å£ã€å‡ºå£å’Œä¸šåŠ¡èŠ‚ç‚¹
        
        ä¿®å¤ï¼šä»Žå·¥ä½œæµå®žä¾‹ä¸­èŽ·å–å®žé™…æ‰§è¡Œçš„èŠ‚ç‚¹æ•°æ®ï¼Œè€Œä¸æ˜¯ä»Žæ¨¡æ¿ä¸­èŽ·å–
        """
        try:
            logger.info(f"ðŸ” å¼€å§‹åˆ†æžå­å·¥ä½œæµç»“æž„: {candidate_workflow_instance_id}")
            
            # ðŸ”§ ä¿®å¤ï¼šä»Žå·¥ä½œæµå®žä¾‹ä¸­èŽ·å–å®žé™…èŠ‚ç‚¹ï¼ˆnode_instanceï¼‰ï¼Œè€Œä¸æ˜¯æ¨¡æ¿èŠ‚ç‚¹
            all_nodes = await self.db.fetch_all("""
                SELECT ni.node_instance_id, ni.node_id, n.node_base_id, n.name, n.type, 
                       n.position_x, n.position_y, n.task_description, n.version,
                       ti.status as task_status
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                LEFT JOIN task_instance ti ON ni.node_instance_id = ti.node_instance_id
                WHERE ni.workflow_instance_id = %s AND ni.is_deleted = FALSE 
                ORDER BY n.position_x, n.position_y
            """, candidate_workflow_instance_id)
            
            logger.info(f"   ðŸ“‹ å­å·¥ä½œæµå®žä¾‹æ€»èŠ‚ç‚¹æ•°: {len(all_nodes)}")
            
            # ðŸ”§ ä¿®å¤ï¼šå¤„ç†ç©ºç‰ˆæœ¬é—®é¢˜ï¼Œå½“å‰ç‰ˆæœ¬ä¸ºç©ºæ—¶å›žé€€åˆ°æœ‰æ•°æ®çš„ç‰ˆæœ¬
            actual_workflow_info = await self.db.fetch_one("""
                SELECT DISTINCT n.workflow_id 
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.workflow_instance_id = %s 
                LIMIT 1
            """, candidate_workflow_instance_id)
            
            if actual_workflow_info:
                sub_workflow_id = actual_workflow_info['workflow_id']
                logger.info(f"   ðŸ” å®žé™…å­å·¥ä½œæµæ¨¡æ¿ID: {sub_workflow_id}")
            else:
                # å¦‚æžœé€šè¿‡èŠ‚ç‚¹å®žä¾‹æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡å·¥ä½œæµåŸºç¡€IDæ‰¾åˆ°æœ‰æ•°æ®çš„ç‰ˆæœ¬
                logger.warning(f"   âš ï¸ æ— æ³•é€šè¿‡èŠ‚ç‚¹å®žä¾‹æ‰¾åˆ°å·¥ä½œæµæ¨¡æ¿ï¼Œå°è¯•æŸ¥æ‰¾æœ‰æ•°æ®çš„ç‰ˆæœ¬")
                
                workflow_instance_info = await self.db.fetch_one("""
                    SELECT workflow_base_id FROM workflow_instance 
                    WHERE workflow_instance_id = %s
                """, candidate_workflow_instance_id)
                
                if workflow_instance_info:
                    base_id = workflow_instance_info['workflow_base_id']
                    # æŸ¥æ‰¾è¯¥åŸºç¡€IDä¸‹æœ‰èŠ‚ç‚¹æ•°æ®çš„ç‰ˆæœ¬
                    workflow_with_data = await self.db.fetch_one("""
                        SELECT DISTINCT w.workflow_id, w.version, COUNT(n.node_id) as node_count
                        FROM workflow w
                        JOIN node n ON w.workflow_id = n.workflow_id
                        WHERE w.workflow_base_id = %s AND w.is_deleted = FALSE
                        GROUP BY w.workflow_id, w.version
                        HAVING node_count > 0
                        ORDER BY w.version DESC
                        LIMIT 1
                    """, base_id)
                    
                    if workflow_with_data:
                        sub_workflow_id = workflow_with_data['workflow_id']
                        logger.info(f"   ðŸ”§ æ‰¾åˆ°æœ‰æ•°æ®çš„ç‰ˆæœ¬ {workflow_with_data['version']}: {sub_workflow_id}")
                    else:
                        logger.error(f"   âŒ æ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•°æ®çš„å·¥ä½œæµç‰ˆæœ¬: {base_id}")
                        sub_workflow_id = None
                else:
                    logger.error(f"   âŒ æ‰¾ä¸åˆ°å·¥ä½œæµå®žä¾‹ä¿¡æ¯: {candidate_workflow_instance_id}")
                    sub_workflow_id = None
            
            if sub_workflow_id:
                # ç›´æŽ¥ä»Žå®žé™…æ¨¡æ¿è¿žæŽ¥è¡¨èŽ·å–è¿žæŽ¥ä¿¡æ¯
                all_connections = await self.db.fetch_all("""
                    SELECT 
                        nc.from_node_id,
                        nc.to_node_id,
                        from_n.name as from_node_name,
                        to_n.name as to_node_name,
                        from_n.type as from_node_type,
                        to_n.type as to_node_type,
                        nc.connection_type,
                        nc.condition_config
                    FROM node_connection nc
                    JOIN node from_n ON nc.from_node_id = from_n.node_id
                    JOIN node to_n ON nc.to_node_id = to_n.node_id
                    WHERE nc.workflow_id = %s
                """, sub_workflow_id)
            else:
                all_connections = []
            
            logger.info(f"   ðŸ”— å­å·¥ä½œæµå®žä¾‹æ€»è¿žæŽ¥æ•°: {len(all_connections)}")
            
            # æŒ‰ç±»åž‹åˆ†ç±»èŠ‚ç‚¹
            start_nodes = [n for n in all_nodes if n['type'] == 'start']
            end_nodes = [n for n in all_nodes if n['type'] == 'end']
            business_nodes = [n for n in all_nodes if n['type'] not in ('start', 'end')]
            
            logger.info(f"   ðŸ“Š èŠ‚ç‚¹åˆ†ç±»: {len(start_nodes)}ä¸ªå¼€å§‹, {len(business_nodes)}ä¸ªä¸šåŠ¡, {len(end_nodes)}ä¸ªç»“æŸ")
            
            if not business_nodes:
                logger.warning(f"   âš ï¸ å­å·¥ä½œæµå®žä¾‹æ²¡æœ‰ä¸šåŠ¡èŠ‚ç‚¹")
                return {
                    "business_nodes": [],
                    "entry_points": [],
                    "exit_points": [],
                    "business_connections": [],
                    "start_to_entry_connections": [],
                    "exit_to_end_connections": [],
                    "analysis_stats": {
                        "total_nodes": len(all_nodes),
                        "business_nodes": 0,
                        "start_nodes": len(start_nodes),
                        "end_nodes": len(end_nodes),
                        "total_connections": len(all_connections)
                    }
                }
            
            # æž„å»ºè¿žæŽ¥å›¾ - åŸºäºŽèŠ‚ç‚¹æ¨¡æ¿ID
            outgoing = {}  # from_node_id -> [connection_info, ...]
            incoming = {}  # to_node_id -> [connection_info, ...]
            
            for conn in all_connections:
                from_id, to_id = conn['from_node_id'], conn['to_node_id']
                outgoing.setdefault(from_id, []).append(conn)
                incoming.setdefault(to_id, []).append(conn)
            
            # è¯†åˆ«å…¥å£èŠ‚ç‚¹ï¼šä»ŽstartèŠ‚ç‚¹ç›´æŽ¥æˆ–é—´æŽ¥å¯è¾¾çš„ä¸šåŠ¡èŠ‚ç‚¹
            entry_points = self._find_entry_points_enhanced(start_nodes, business_nodes, outgoing, incoming)
            logger.info(f"   ðŸ“¥ è¯†åˆ«å‡º {len(entry_points)} ä¸ªå…¥å£èŠ‚ç‚¹: {[n['name'] for n in entry_points]}")
            
            # è¯†åˆ«å‡ºå£èŠ‚ç‚¹ï¼šå¯ä»¥åˆ°è¾¾endèŠ‚ç‚¹çš„ä¸šåŠ¡èŠ‚ç‚¹
            exit_points = self._find_exit_points_enhanced(end_nodes, business_nodes, incoming, outgoing)
            logger.info(f"   ðŸ“¤ è¯†åˆ«å‡º {len(exit_points)} ä¸ªå‡ºå£èŠ‚ç‚¹: {[n['name'] for n in exit_points]}")
            
            # è®¡ç®—èŠ‚ç‚¹ä½ç½®åç§»ï¼ˆç›¸å¯¹äºŽåŽŸsubdivisionèŠ‚ç‚¹ä½ç½®ï¼‰
            positioned_nodes = self._calculate_node_positions(
                business_nodes, center_x, center_y
            )
            
            # åˆ†ç±»è¿žæŽ¥
            business_connections = []
            start_to_entry_connections = []
            exit_to_end_connections = []
            
            business_node_ids = {n['node_id'] for n in business_nodes}
            start_node_ids = {n['node_id'] for n in start_nodes}
            end_node_ids = {n['node_id'] for n in end_nodes}
            entry_point_ids = {n['node_id'] for n in entry_points}
            exit_point_ids = {n['node_id'] for n in exit_points}
            
            # ðŸ”§ ä¿®å¤ï¼šç®€åŒ–è¿žæŽ¥åˆ†ç±»é€»è¾‘ï¼Œç›´æŽ¥ä½¿ç”¨æ¨¡æ¿è¿žæŽ¥æ•°æ®
            for conn in all_connections:
                from_node_id = conn['from_node_id']
                to_node_id = conn['to_node_id']
                
                # åˆ›å»ºæ ‡å‡†åŒ–çš„è¿žæŽ¥å¯¹è±¡
                normalized_conn = {
                    'from_node_id': from_node_id,
                    'to_node_id': to_node_id,
                    'connection_type': conn.get('connection_type', 'normal'),
                    'condition_config': conn.get('condition_config')
                }
                
                # ä¸šåŠ¡èŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥
                if from_node_id in business_node_ids and to_node_id in business_node_ids:
                    business_connections.append(normalized_conn)
                    logger.info(f"      ðŸ“‹ ä¸šåŠ¡è¿žæŽ¥: {conn['from_node_name']} -> {conn['to_node_name']}")
                # start -> entry çš„è¿žæŽ¥
                elif from_node_id in start_node_ids and to_node_id in entry_point_ids:
                    start_to_entry_connections.append(normalized_conn)
                    logger.info(f"      ðŸ“‹ å¯åŠ¨è¿žæŽ¥: {conn['from_node_name']} -> {conn['to_node_name']}")
                # exit -> end çš„è¿žæŽ¥
                elif from_node_id in exit_point_ids and to_node_id in end_node_ids:
                    exit_to_end_connections.append(normalized_conn)
                    logger.info(f"      ðŸ“‹ ç»“æŸè¿žæŽ¥: {conn['from_node_name']} -> {conn['to_node_name']}")
            
            logger.info(f"   ðŸ”— è¿žæŽ¥åˆ†ç±»:")
            logger.info(f"      - ä¸šåŠ¡è¿žæŽ¥: {len(business_connections)}ä¸ª")
            logger.info(f"      - start->entryè¿žæŽ¥: {len(start_to_entry_connections)}ä¸ª")
            logger.info(f"      - exit->endè¿žæŽ¥: {len(exit_to_end_connections)}ä¸ª")
            
            analysis_stats = {
                "total_nodes": len(all_nodes),
                "business_nodes": len(business_nodes),
                "start_nodes": len(start_nodes),
                "end_nodes": len(end_nodes),
                "entry_points": len(entry_points),
                "exit_points": len(exit_points),
                "total_connections": len(all_connections),
                "business_connections": len(business_connections),
                "boundary_connections": len(start_to_entry_connections) + len(exit_to_end_connections)
            }
            
            logger.info(f"âœ… å­å·¥ä½œæµç»“æž„åˆ†æžå®Œæˆ: {analysis_stats}")
            
            return {
                "business_nodes": positioned_nodes,
                "entry_points": entry_points,
                "exit_points": exit_points,
                "business_connections": business_connections,
                "start_to_entry_connections": start_to_entry_connections,
                "exit_to_end_connections": exit_to_end_connections,
                "analysis_stats": analysis_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æžå­å·¥ä½œæµç»“æž„å¤±è´¥: {e}")
            raise
    
    def _find_entry_points_enhanced(self, start_nodes: List[Dict], business_nodes: List[Dict], 
                                   outgoing: Dict, incoming: Dict = None) -> List[Dict]:
        """å¢žå¼ºç‰ˆå…¥å£ç‚¹æŸ¥æ‰¾ - æ”¯æŒå¤æ‚çš„å…¥å£æ¨¡å¼"""
        entry_points = []
        start_node_ids = {n['node_id'] for n in start_nodes}
        business_node_ids = {n['node_id'] for n in business_nodes}
        
        # æ–¹æ³•1: ç›´æŽ¥ä»ŽstartèŠ‚ç‚¹è¿žæŽ¥çš„ä¸šåŠ¡èŠ‚ç‚¹
        for start_id in start_node_ids:
            if start_id in outgoing:
                for conn in outgoing[start_id]:
                    to_id = conn['to_node_id']
                    if to_id in business_node_ids:
                        entry_node = next(n for n in business_nodes if n['node_id'] == to_id)
                        if entry_node not in entry_points:
                            entry_points.append(entry_node)
                            logger.info(f"      æ‰¾åˆ°ç›´æŽ¥å…¥å£ç‚¹: {entry_node['name']} (from start)")
        
        # æ–¹æ³•2: å¦‚æžœæ²¡æœ‰ç›´æŽ¥è¿žæŽ¥ï¼Œæ‰¾æ²¡æœ‰ä¸šåŠ¡å‰é©±çš„ä¸šåŠ¡èŠ‚ç‚¹
        if not entry_points and incoming:
            for node in business_nodes:
                node_id = node['node_id']
                has_business_predecessor = False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¥è‡ªå…¶ä»–ä¸šåŠ¡èŠ‚ç‚¹çš„è¿žæŽ¥
                if node_id in incoming:
                    for conn in incoming.get(node_id, []):
                        if conn['from_node_id'] in business_node_ids:
                            has_business_predecessor = True
                            break
                
                if not has_business_predecessor:
                    entry_points.append(node)
                    logger.info(f"      æ‰¾åˆ°é—´æŽ¥å…¥å£ç‚¹: {node['name']} (no business predecessor)")
        
        # æ–¹æ³•3: å¦‚æžœè¿˜æ˜¯æ²¡æœ‰ï¼Œé€‰æ‹©ä½ç½®æœ€å‰çš„èŠ‚ç‚¹ä½œä¸ºå…¥å£ç‚¹
        if not entry_points and business_nodes:
            entry_points = [business_nodes[0]]
            logger.info(f"      ä½¿ç”¨é»˜è®¤å…¥å£ç‚¹: {business_nodes[0]['name']} (first node)")
            
        return entry_points
    
    def _find_exit_points_enhanced(self, end_nodes: List[Dict], business_nodes: List[Dict],
                                 incoming: Dict, outgoing: Dict = None) -> List[Dict]:
        """å¢žå¼ºç‰ˆå‡ºå£ç‚¹æŸ¥æ‰¾ - æ”¯æŒå¤æ‚çš„å‡ºå£æ¨¡å¼"""
        exit_points = []
        end_node_ids = {n['node_id'] for n in end_nodes}
        business_node_ids = {n['node_id'] for n in business_nodes}
        
        # æ–¹æ³•1: ç›´æŽ¥è¿žæŽ¥åˆ°endèŠ‚ç‚¹çš„ä¸šåŠ¡èŠ‚ç‚¹
        for end_id in end_node_ids:
            if end_id in incoming:
                for conn in incoming[end_id]:
                    from_id = conn['from_node_id']
                    if from_id in business_node_ids:
                        exit_node = next(n for n in business_nodes if n['node_id'] == from_id)
                        if exit_node not in exit_points:
                            exit_points.append(exit_node)
                            logger.info(f"      æ‰¾åˆ°ç›´æŽ¥å‡ºå£ç‚¹: {exit_node['name']} (to end)")
        
        # æ–¹æ³•2: å¦‚æžœæ²¡æœ‰ç›´æŽ¥è¿žæŽ¥ï¼Œæ‰¾æ²¡æœ‰ä¸šåŠ¡åŽç»§çš„ä¸šåŠ¡èŠ‚ç‚¹
        if not exit_points and outgoing:
            for node in business_nodes:
                node_id = node['node_id']
                has_business_successor = False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ°å…¶ä»–ä¸šåŠ¡èŠ‚ç‚¹çš„è¿žæŽ¥
                for conn in outgoing.get(node_id, []):
                    if conn['to_node_id'] in business_node_ids:
                        has_business_successor = True
                        break
                
                if not has_business_successor:
                    exit_points.append(node)
                    logger.info(f"      æ‰¾åˆ°é—´æŽ¥å‡ºå£ç‚¹: {node['name']} (no business successor)")
        
        # æ–¹æ³•3: å¦‚æžœè¿˜æ˜¯æ²¡æœ‰ï¼Œé€‰æ‹©ä½ç½®æœ€åŽçš„èŠ‚ç‚¹ä½œä¸ºå‡ºå£ç‚¹
        if not exit_points and business_nodes:
            exit_points = [business_nodes[-1]]
            logger.info(f"      ä½¿ç”¨é»˜è®¤å‡ºå£ç‚¹: {business_nodes[-1]['name']} (last node)")
            
        return exit_points
    
    def _find_entry_points(self, start_nodes: List[Dict], business_nodes: List[Dict], 
                          outgoing: Dict) -> List[Dict]:
        """æ‰¾åˆ°å­å·¥ä½œæµçš„å…¥å£èŠ‚ç‚¹"""
        entry_points = []
        start_node_ids = {n['node_id'] for n in start_nodes}
        business_node_ids = {n['node_id'] for n in business_nodes}
        
        for start_id in start_node_ids:
            if start_id in outgoing:
                for conn in outgoing[start_id]:
                    to_id = conn['to_node_id']
                    if to_id in business_node_ids:
                        # æ‰¾åˆ°å¯¹åº”çš„ä¸šåŠ¡èŠ‚ç‚¹
                        entry_node = next(n for n in business_nodes if n['node_id'] == to_id)
                        if entry_node not in entry_points:
                            entry_points.append(entry_node)
        
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°æ˜Žç¡®çš„å…¥å£ç‚¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªä¸šåŠ¡èŠ‚ç‚¹
        if not entry_points and business_nodes:
            entry_points = [business_nodes[0]]
            
        return entry_points
    
    def _find_exit_points(self, end_nodes: List[Dict], business_nodes: List[Dict],
                         incoming: Dict) -> List[Dict]:
        """æ‰¾åˆ°å­å·¥ä½œæµçš„å‡ºå£èŠ‚ç‚¹"""
        exit_points = []
        end_node_ids = {n['node_id'] for n in end_nodes}
        business_node_ids = {n['node_id'] for n in business_nodes}
        
        for end_id in end_node_ids:
            if end_id in incoming:
                for conn in incoming[end_id]:
                    from_id = conn['from_node_id']
                    if from_id in business_node_ids:
                        # æ‰¾åˆ°å¯¹åº”çš„ä¸šåŠ¡èŠ‚ç‚¹
                        exit_node = next(n for n in business_nodes if n['node_id'] == from_id)
                        if exit_node not in exit_points:
                            exit_points.append(exit_node)
        
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°æ˜Žç¡®çš„å‡ºå£ç‚¹ï¼Œé€‰æ‹©æœ€åŽä¸€ä¸ªä¸šåŠ¡èŠ‚ç‚¹
        if not exit_points and business_nodes:
            exit_points = [business_nodes[-1]]
            
        return exit_points
    
    def _calculate_node_positions(self, nodes: List[Dict], center_x: int, center_y: int) -> List[Dict]:
        """è®¡ç®—èŠ‚ç‚¹åœ¨åˆå¹¶åŽå·¥ä½œæµä¸­çš„ä½ç½®"""
        if not nodes:
            return []
        
        # è®¡ç®—åŽŸå§‹èŠ‚ç‚¹çš„è¾¹ç•Œæ¡†
        min_x = min(n['position_x'] for n in nodes)
        max_x = max(n['position_x'] for n in nodes)
        min_y = min(n['position_y'] for n in nodes)
        max_y = max(n['position_y'] for n in nodes)
        
        # è®¡ç®—åç§»é‡ï¼Œä½¿å­å·¥ä½œæµå±…ä¸­äºŽåŽŸsubdivisionèŠ‚ç‚¹ä½ç½®
        offset_x = center_x - (min_x + max_x) // 2
        offset_y = center_y - (min_y + max_y) // 2
        
        # åº”ç”¨åç§»
        positioned_nodes = []
        for node in nodes:
            positioned_node = node.copy()
            positioned_node['position_x'] = node['position_x'] + offset_x
            positioned_node['position_y'] = node['position_y'] + offset_y
            positioned_nodes.append(positioned_node)
        
        return positioned_nodes
    
    async def _finalize_merged_workflow(self, original_workflow_id: uuid.UUID, 
                                      new_workflow_base_id: uuid.UUID, 
                                      merge_operations: List[MergeOperation],
                                      creator_id: uuid.UUID,
                                      merge_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å®Œæˆåˆå¹¶åŽçš„å·¥ä½œæµç”Ÿæˆ - é‡æž„ç‰ˆæœ¬"""
        try:
            logger.info(f"ðŸŽ¯ å¼€å§‹ç”Ÿæˆåˆå¹¶å·¥ä½œæµ: {new_workflow_base_id}")
            
            # 1. åˆ›å»ºæ–°çš„å·¥ä½œæµè®°å½•
            workflow_info = await self._create_merged_workflow_record(
                original_workflow_id, new_workflow_base_id, len(merge_operations), creator_id
            )
            new_workflow_id = workflow_info['workflow_id']
            
            # 2. èŽ·å–çˆ¶å·¥ä½œæµä¿¡æ¯
            parent_workflow_id = await self._get_parent_workflow_id(original_workflow_id)
            if not parent_workflow_id:
                raise Exception("æ— æ³•èŽ·å–çˆ¶å·¥ä½œæµä¿¡æ¯")
            
            # 3. æ‰§è¡ŒçœŸæ­£çš„èŠ‚ç‚¹æ›¿æ¢åˆå¹¶
            merge_stats = await self._execute_node_replacement_merge(
                parent_workflow_id, new_workflow_id, new_workflow_base_id, merge_results
            )
            
            logger.info(f"âœ… åˆå¹¶å·¥ä½œæµç”Ÿæˆå®Œæˆ: {workflow_info['name']}")
            
            return {
                **workflow_info,
                **merge_stats,
                "merge_operations_count": len(merge_operations)
            }
            
        except Exception as e:
            logger.error(f"âŒ å®Œæˆåˆå¹¶å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _create_merged_workflow_record(self, original_workflow_id: uuid.UUID,
                                           new_workflow_base_id: uuid.UUID,
                                           merge_count: int,
                                           creator_id: uuid.UUID) -> Dict[str, Any]:
        """åˆ›å»ºåˆå¹¶å·¥ä½œæµè®°å½•"""
        # èŽ·å–çˆ¶å·¥ä½œæµåç§°
        parent_workflow = await self.db.fetch_one("""
            SELECT w.name, w.workflow_base_id 
            FROM workflow_instance wi
            JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id
            WHERE wi.workflow_instance_id = %s 
            AND w.is_current_version = TRUE
        """, original_workflow_id)
        
        parent_name = parent_workflow['name'] if parent_workflow else "Unknown_Workflow"
        
        # ç”Ÿæˆåˆå¹¶åºå·
        existing_merges = await self.db.fetch_all("""
            SELECT name FROM workflow 
            WHERE name LIKE %s AND is_deleted = FALSE
            ORDER BY created_at
        """, f"{parent_name}_åˆå¹¶_%")
        
        merge_number = len(existing_merges) + 1
        new_workflow_id = uuid.uuid4()
        merged_name = f"{parent_name}_åˆå¹¶_{merge_number}"
        merged_description = f"åˆå¹¶äº†{merge_count}ä¸ªsubdivisionçš„å·¥ä½œæµï¼ŒåŸºäºŽ{parent_name}"
        
        await self.db.execute("""
            INSERT INTO workflow (
                workflow_id, workflow_base_id, name, description, 
                creator_id, is_current_version, created_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s)
        """, new_workflow_id, new_workflow_base_id, merged_name, merged_description,
             creator_id, True, now_utc())
        
        logger.info(f"âœ… åˆ›å»ºåˆå¹¶å·¥ä½œæµè®°å½•: {merged_name}")
        
        return {
            "workflow_id": str(new_workflow_id),
            "workflow_base_id": str(new_workflow_base_id),
            "name": merged_name,
            "description": merged_description
        }
    
    async def _get_parent_workflow_id(self, original_workflow_id: uuid.UUID) -> Optional[str]:
        """èŽ·å–çˆ¶å·¥ä½œæµID"""
        result = await self.db.fetch_one("""
            SELECT w.workflow_id FROM workflow_instance wi
            JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id
            WHERE wi.workflow_instance_id = %s 
            AND w.is_current_version = TRUE
        """, original_workflow_id)
        
        return result['workflow_id'] if result else None
    
    async def _execute_node_replacement_merge(self, parent_workflow_id: str, 
                                            new_workflow_id: uuid.UUID,
                                            new_workflow_base_id: uuid.UUID,
                                            merge_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰§è¡ŒçœŸæ­£çš„èŠ‚ç‚¹æ›¿æ¢åˆå¹¶"""
        
        # 1. æ”¶é›†æ‰€æœ‰subdivisionä¿¡æ¯å’Œè¢«æ›¿æ¢çš„èŠ‚ç‚¹ID
        subdivision_mapping = {}  # original_node_id -> merge_result
        subdivision_node_ids = set()
        
        for result in merge_results:
            if result.get("success"):
                original_node = result['original_node']
                original_node_id = original_node['original_task_id']
                subdivision_node_ids.add(original_node_id)
                subdivision_mapping[original_node_id] = result
        
        logger.info(f"ðŸ”„ å°†æ›¿æ¢ {len(subdivision_node_ids)} ä¸ªsubdivisionèŠ‚ç‚¹")
        
        # 2. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™èŠ‚ç‚¹ï¼ˆæŽ’é™¤è¢«subdivisionçš„èŠ‚ç‚¹ï¼‰
        node_id_mapping = await self._copy_preserved_nodes(
            parent_workflow_id, new_workflow_id, new_workflow_base_id, subdivision_node_ids
        )
        
        # 3. ä¸ºæ¯ä¸ªsubdivisionæ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢
        replacement_stats = await self._replace_subdivision_nodes(
            parent_workflow_id, new_workflow_id, new_workflow_base_id,
            subdivision_mapping, node_id_mapping
        )
        
        # 4. é‡å»ºæ‰€æœ‰è¿žæŽ¥
        connection_stats = await self._rebuild_all_connections(
            parent_workflow_id, new_workflow_id, subdivision_mapping, node_id_mapping
        )
        
        return {
            **replacement_stats,
            **connection_stats
        }
    
    async def _copy_preserved_nodes(self, parent_workflow_id: str, new_workflow_id: uuid.UUID,
                                  new_workflow_base_id: uuid.UUID, 
                                  subdivision_node_ids: set) -> Dict[str, uuid.UUID]:
        """å¤åˆ¶çˆ¶å·¥ä½œæµä¸­éœ€è¦ä¿ç•™çš„èŠ‚ç‚¹"""
        node_id_mapping = {}
        
        # æŸ¥è¯¢ä¿ç•™çš„èŠ‚ç‚¹
        if subdivision_node_ids:
            placeholders = ','.join(['%s'] * len(subdivision_node_ids))
            parent_nodes = await self.db.fetch_all(f"""
                SELECT node_id, node_base_id, name, type, task_description, 
                       position_x, position_y, version
                FROM node 
                WHERE workflow_id = %s AND is_deleted = FALSE
                AND node_id NOT IN ({placeholders})
            """, parent_workflow_id, *subdivision_node_ids)
        else:
            parent_nodes = await self.db.fetch_all("""
                SELECT node_id, node_base_id, name, type, task_description, 
                       position_x, position_y, version
                FROM node 
                WHERE workflow_id = %s AND is_deleted = FALSE
            """, parent_workflow_id)
        
        # å¤åˆ¶èŠ‚ç‚¹
        for node in parent_nodes:
            new_node_id = uuid.uuid4()
            new_node_base_id = uuid.uuid4()
            node_id_mapping[node['node_id']] = new_node_id
            
            await self.db.execute("""
                INSERT INTO node (
                    node_id, node_base_id, workflow_id, workflow_base_id,
                    name, type, task_description, position_x, position_y,
                    version, is_current_version, created_at, is_deleted
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                 node['name'], node['type'], node['task_description'], 
                 node['position_x'], node['position_y'], 1, True, now_utc(), False)
        
        logger.info(f"âœ… å¤åˆ¶äº† {len(parent_nodes)} ä¸ªçˆ¶å·¥ä½œæµä¿ç•™èŠ‚ç‚¹")
        return node_id_mapping
    
    async def _replace_subdivision_nodes(self, parent_workflow_id: str,
                                       new_workflow_id: uuid.UUID, new_workflow_base_id: uuid.UUID,
                                       subdivision_mapping: Dict[str, Dict],
                                       node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """ç”¨å­å·¥ä½œæµèŠ‚ç‚¹æ›¿æ¢subdivisionèŠ‚ç‚¹"""
        replaced_nodes = 0
        
        for original_node_id, result in subdivision_mapping.items():
            workflow_structure = result['workflow_structure']
            
            # å¤åˆ¶å­å·¥ä½œæµçš„ä¸šåŠ¡èŠ‚ç‚¹
            for node in workflow_structure['business_nodes']:
                new_node_id = uuid.uuid4()
                new_node_base_id = uuid.uuid4()
                node_id_mapping[node['node_id']] = new_node_id
                
                await self.db.execute("""
                    INSERT INTO node (
                        node_id, node_base_id, workflow_id, workflow_base_id,
                        name, type, task_description, position_x, position_y,
                        version, is_current_version, created_at, is_deleted
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                     node['name'], node['type'], node['task_description'], 
                     node['position_x'], node['position_y'], 1, True, now_utc(), False)
                
                replaced_nodes += 1
        
        logger.info(f"âœ… æ›¿æ¢äº† {replaced_nodes} ä¸ªsubdivisionèŠ‚ç‚¹ä¸ºå­å·¥ä½œæµä¸šåŠ¡èŠ‚ç‚¹")
        return {"nodes_replaced": replaced_nodes}
    
    async def _rebuild_all_connections(self, parent_workflow_id: str, new_workflow_id: uuid.UUID,
                                     subdivision_mapping: Dict[str, Dict],
                                     node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """é‡å»ºæ‰€æœ‰è¿žæŽ¥ - æ”¹è¿›ç‰ˆæœ¬"""
        # èŽ·å–çˆ¶å·¥ä½œæµçš„æ‰€æœ‰è¿žæŽ¥
        parent_connections = await self.db.fetch_all("""
            SELECT from_node_id, to_node_id, connection_type, condition_config
            FROM node_connection 
            WHERE workflow_id = %s
        """, parent_workflow_id)
        
        parent_connections_copied = 0
        subworkflow_connections_copied = 0
        cross_boundary_connections_created = 0
        
        subdivision_node_ids = set(subdivision_mapping.keys())
        
        # 1. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™è¿žæŽ¥ï¼ˆä¸æ¶‰åŠsubdivisionèŠ‚ç‚¹ï¼‰
        for conn in parent_connections:
            from_id, to_id = conn['from_node_id'], conn['to_node_id']
            
            # è·³è¿‡æ¶‰åŠsubdivisionèŠ‚ç‚¹çš„è¿žæŽ¥
            if from_id in subdivision_node_ids or to_id in subdivision_node_ids:
                continue
            
            # å¤åˆ¶è¿žæŽ¥
            if from_id in node_id_mapping and to_id in node_id_mapping:
                await self.db.execute("""
                    INSERT INTO node_connection (
                        from_node_id, to_node_id, workflow_id,
                        connection_type, condition_config, created_at
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                """, node_id_mapping[from_id], node_id_mapping[to_id],
                     new_workflow_id, conn.get('connection_type', 'normal'),
                     conn.get('condition_config'), now_utc())
                parent_connections_copied += 1
        
        # 2. å¤åˆ¶æ¯ä¸ªå­å·¥ä½œæµçš„å†…éƒ¨è¿žæŽ¥
        for original_node_id, result in subdivision_mapping.items():
            workflow_structure = result['workflow_structure']
            
            # å¤åˆ¶å­å·¥ä½œæµå†…éƒ¨è¿žæŽ¥
            for conn in workflow_structure['business_connections']:
                from_id, to_id = conn['from_node_id'], conn['to_node_id']
                if from_id in node_id_mapping and to_id in node_id_mapping:
                    await self.db.execute("""
                        INSERT INTO node_connection (
                            from_node_id, to_node_id, workflow_id,
                            connection_type, condition_config, created_at
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                    """, node_id_mapping[from_id], node_id_mapping[to_id],
                         new_workflow_id, conn.get('connection_type', 'normal'),
                         conn.get('condition_config'), now_utc())
                    subworkflow_connections_copied += 1
        
        # 3. é‡å»ºè·¨è¾¹ç•Œè¿žæŽ¥ï¼ˆsubdivisionèŠ‚ç‚¹çš„ä¸Šä¸‹æ¸¸è¿žæŽ¥ï¼‰
        cross_boundary_connections_created = await self._rebuild_cross_boundary_connections(
            parent_connections, subdivision_mapping, node_id_mapping, new_workflow_id
        )
        
        logger.info(f"âœ… è¿žæŽ¥é‡å»ºå®Œæˆ:")
        logger.info(f"   - çˆ¶å·¥ä½œæµè¿žæŽ¥: {parent_connections_copied}")
        logger.info(f"   - å­å·¥ä½œæµå†…éƒ¨è¿žæŽ¥: {subworkflow_connections_copied}")
        logger.info(f"   - è·¨è¾¹ç•Œè¿žæŽ¥: {cross_boundary_connections_created}")
        
        return {
            "parent_connections_copied": parent_connections_copied,
            "subworkflow_connections_copied": subworkflow_connections_copied,
            "cross_boundary_connections_created": cross_boundary_connections_created,
            "connections_count": parent_connections_copied + subworkflow_connections_copied + cross_boundary_connections_created
        }
    
    async def _rebuild_cross_boundary_connections(self, parent_connections: List[Dict],
                                                subdivision_mapping: Dict[str, Dict],
                                                node_id_mapping: Dict[str, uuid.UUID],
                                                new_workflow_id: uuid.UUID) -> int:
        """é‡å»ºè·¨è¾¹ç•Œè¿žæŽ¥ - æ”¹è¿›çš„è¿žæŽ¥ç®—æ³•"""
        connections_created = 0
        
        for original_node_id, result in subdivision_mapping.items():
            workflow_structure = result['workflow_structure']
            entry_points = workflow_structure['entry_points']
            exit_points = workflow_structure['exit_points']
            
            # é‡å»ºä¸Šæ¸¸è¿žæŽ¥ï¼šæ‰¾åˆ°æ‰€æœ‰æŒ‡å‘åŽŸsubdivisionèŠ‚ç‚¹çš„è¿žæŽ¥
            for conn in parent_connections:
                if conn['to_node_id'] == original_node_id:
                    from_id = conn['from_node_id']
                    if from_id in node_id_mapping:
                        # è¿žæŽ¥åˆ°æ‰€æœ‰å…¥å£ç‚¹
                        for entry_point in entry_points:
                            if entry_point['node_id'] in node_id_mapping:
                                await self.db.execute("""
                                    INSERT INTO node_connection (
                                        from_node_id, to_node_id, workflow_id,
                                        connection_type, condition_config, created_at
                                    ) VALUES (%s, %s, %s, %s, %s, %s)
                                """, node_id_mapping[from_id],
                                     node_id_mapping[entry_point['node_id']],
                                     new_workflow_id, conn.get('connection_type', 'normal'),
                                     conn.get('condition_config'), now_utc())
                                connections_created += 1
                                logger.info(f"   ðŸ”— ä¸Šæ¸¸è¿žæŽ¥: {from_id} -> {entry_point['name']}")
            
            # é‡å»ºä¸‹æ¸¸è¿žæŽ¥ï¼šæ‰¾åˆ°æ‰€æœ‰ä»ŽåŽŸsubdivisionèŠ‚ç‚¹å‡ºå‘çš„è¿žæŽ¥
            for conn in parent_connections:
                if conn['from_node_id'] == original_node_id:
                    to_id = conn['to_node_id']
                    if to_id in node_id_mapping:
                        # ä»Žæ‰€æœ‰å‡ºå£ç‚¹è¿žæŽ¥
                        for exit_point in exit_points:
                            if exit_point['node_id'] in node_id_mapping:
                                await self.db.execute("""
                                    INSERT INTO node_connection (
                                        from_node_id, to_node_id, workflow_id,
                                        connection_type, condition_config, created_at
                                    ) VALUES (%s, %s, %s, %s, %s, %s)
                                """, node_id_mapping[exit_point['node_id']],
                                     node_id_mapping[to_id],
                                     new_workflow_id, conn.get('connection_type', 'normal'),
                                     conn.get('condition_config'), now_utc())
                                connections_created += 1
                                logger.info(f"   ðŸ”— ä¸‹æ¸¸è¿žæŽ¥: {exit_point['name']} -> {to_id}")
        
        return connections_created
    
    async def _validate_merge_consistency(self, merge_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """éªŒè¯åˆå¹¶æ“ä½œçš„æ•°æ®ä¸€è‡´æ€§"""
        try:
            validation_results = {
                "valid": True,
                "errors": [],
                "warnings": [],
                "stats": {
                    "total_subdivisions": len(merge_results),
                    "valid_subdivisions": 0,
                    "total_business_nodes": 0,
                    "total_connections": 0
                }
            }
            
            for result in merge_results:
                if not result.get("success"):
                    validation_results["errors"].append(f"åˆå¹¶å¤±è´¥çš„subdivision: {result.get('error', 'Unknown')}")
                    continue
                
                validation_results["stats"]["valid_subdivisions"] += 1
                
                # éªŒè¯å·¥ä½œæµç»“æž„
                workflow_structure = result.get('workflow_structure', {})
                business_nodes = workflow_structure.get('business_nodes', [])
                business_connections = workflow_structure.get('business_connections', [])
                entry_points = workflow_structure.get('entry_points', [])
                exit_points = workflow_structure.get('exit_points', [])
                
                validation_results["stats"]["total_business_nodes"] += len(business_nodes)
                validation_results["stats"]["total_connections"] += len(business_connections)
                
                # æ£€æŸ¥å…¥å£å‡ºå£ç‚¹
                if not entry_points:
                    validation_results["warnings"].append(f"Subdivision {result['candidate'].node_name} æ²¡æœ‰è¯†åˆ«å‡ºå…¥å£ç‚¹")
                
                if not exit_points:
                    validation_results["warnings"].append(f"Subdivision {result['candidate'].node_name} æ²¡æœ‰è¯†åˆ«å‡ºå‡ºå£ç‚¹")
                
                # æ£€æŸ¥èŠ‚ç‚¹IDå”¯ä¸€æ€§
                node_ids = [n['node_id'] for n in business_nodes]
                if len(node_ids) != len(set(node_ids)):
                    validation_results["errors"].append(f"Subdivision {result['candidate'].node_name} å­˜åœ¨é‡å¤çš„èŠ‚ç‚¹ID")
                    validation_results["valid"] = False
                
                # æ£€æŸ¥è¿žæŽ¥çš„èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
                for conn in business_connections:
                    from_id, to_id = conn['from_node_id'], conn['to_node_id']
                    if from_id not in node_ids or to_id not in node_ids:
                        validation_results["errors"].append(f"è¿žæŽ¥å¼•ç”¨äº†ä¸å­˜åœ¨çš„èŠ‚ç‚¹: {from_id} -> {to_id}")
                        validation_results["valid"] = False
            
            if validation_results["errors"]:
                validation_results["valid"] = False
                
            logger.info(f"ðŸ” åˆå¹¶ä¸€è‡´æ€§éªŒè¯å®Œæˆ:")
            logger.info(f"   - æœ‰æ•ˆ: {validation_results['valid']}")
            logger.info(f"   - é”™è¯¯: {len(validation_results['errors'])}ä¸ª")
            logger.info(f"   - è­¦å‘Š: {len(validation_results['warnings'])}ä¸ª")
            logger.info(f"   - ç»Ÿè®¡: {validation_results['stats']}")
            
            return validation_results
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")
            return {
                "valid": False,
                "errors": [f"éªŒè¯è¿‡ç¨‹å¤±è´¥: {str(e)}"],
                "warnings": [],
                "stats": {}
            }
