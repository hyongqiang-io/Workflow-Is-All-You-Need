"""
å·¥ä½œæµåˆå¹¶æœåŠ¡ - Workflow Template Tree Based Merge Service

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å®Œå…¨åŸºäºWorkflowTemplateTreeè¿›è¡Œåˆå¹¶
2. æ”¯æŒé€’å½’åˆå¹¶ï¼šä»å­èŠ‚ç‚¹æ²¿ç€è·¯å¾„ä¸€è·¯åˆå¹¶åˆ°æ ¹èŠ‚ç‚¹
3. é¿å…ç›´æ¥æŸ¥è¯¢subdivisionè¡¨ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
4. ç”Ÿæˆæ–°çš„å·¥ä½œæµæ¨¡æ¿
"""

import uuid
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from loguru import logger

from ..repositories.base import BaseRepository
from ..utils.helpers import now_utc
from .workflow_template_tree import WorkflowTemplateTree, WorkflowTemplateNode


@dataclass
class MergeCandidate:
    """åˆå¹¶å€™é€‰é¡¹"""
    subdivision_id: str
    parent_subdivision_id: Optional[str]
    workflow_instance_id: str
    workflow_base_id: str
    node_name: str
    depth: int
    can_merge: bool = True
    merge_reason: str = ""


@dataclass
class MergeOperation:
    """åˆå¹¶æ“ä½œ"""
    target_node_id: str  # è¢«æ›¿æ¢çš„çˆ¶èŠ‚ç‚¹ID
    sub_workflow_id: str  # æ›¿æ¢ç”¨çš„å­å·¥ä½œæµID
    subdivision_id: str  # å¯¹åº”çš„subdivision ID
    depth: int  # åˆå¹¶æ·±åº¦


class WorkflowMergeService:
    """å·¥ä½œæµåˆå¹¶æœåŠ¡"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_merge").db
    
    async def get_merge_candidates(self, workflow_instance_id: uuid.UUID) -> List[MergeCandidate]:
        """åŸºäºworkflow_template_treeè·å–åˆå¹¶å€™é€‰é¡¹ - é‡æ„ç‰ˆæœ¬ï¼Œå®Œå…¨åŸºäºæ ‘æ•°æ®"""
        try:
            logger.info(f"ğŸ” è·å–åˆå¹¶å€™é€‰: {workflow_instance_id}")
            
            # ç›´æ¥æ„å»ºå·¥ä½œæµæ¨¡æ¿æ ‘
            from .workflow_template_connection_service import WorkflowTemplateConnectionService
            connection_service = WorkflowTemplateConnectionService()
            subdivisions_data = await connection_service._get_all_subdivisions_simple(workflow_instance_id)
            
            if not subdivisions_data:
                logger.info("æ— subdivisionæ•°æ®")
                return []
                
            tree = await WorkflowTemplateTree().build_from_subdivisions(subdivisions_data, workflow_instance_id)
            
            # ğŸ”§ å®Œå…¨åŸºäºtreeè·å–å€™é€‰é¡¹ï¼Œä¸å†æŸ¥è¯¢subdivisionè¡¨
            tree_candidates = tree.get_merge_candidates_with_tree_data()
            
            # è½¬æ¢ä¸ºMergeCandidateå¯¹è±¡
            candidates = []
            for tree_candidate in tree_candidates:
                candidate = MergeCandidate(
                    subdivision_id=tree_candidate['subdivision_id'],
                    parent_subdivision_id=tree_candidate.get('parent_subdivision_id'),
                    workflow_instance_id=tree_candidate['workflow_instance_id'],
                    workflow_base_id=tree_candidate['workflow_base_id'],
                    node_name=tree_candidate['node_name'],
                    depth=tree_candidate['depth'],
                    can_merge=tree_candidate['can_merge'],
                    merge_reason=tree_candidate['merge_reason']
                )
                candidates.append(candidate)
                
                logger.info(f"   ğŸ“‹ å€™é€‰é¡¹: {candidate.node_name} (æ·±åº¦: {candidate.depth})")
            
            logger.info(f"è·å¾— {len(candidates)} ä¸ªå€™é€‰é¡¹")
            return candidates
            
        except Exception as e:
            logger.error(f"è·å–åˆå¹¶å€™é€‰å¤±è´¥: {e}")
            raise
    
    async def execute_merge(self, workflow_instance_id: uuid.UUID, 
                          selected_merges: List[str], 
                          creator_id: uuid.UUID, 
                          recursive: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµåˆå¹¶ - é‡æ„ç‰ˆæœ¬ï¼Œå®Œå…¨åŸºäºworkflow_template_treeï¼Œé»˜è®¤å¯ç”¨é€’å½’åˆå¹¶"""
        try:
            logger.info(f"ğŸš€ [é€’å½’åˆå¹¶] å¼€å§‹å·¥ä½œæµåˆå¹¶")
            logger.info(f"   - workflow_instance_id: {workflow_instance_id}")
            logger.info(f"   - selected_merges: {selected_merges}")
            logger.info(f"   - creator_id: {creator_id}")
            logger.info(f"   - recursive: {recursive} (é»˜è®¤å¯ç”¨)")
            
            # æ„å»ºå·¥ä½œæµæ¨¡æ¿æ ‘
            logger.info(f"ğŸŒ³ [æ„å»ºæ ‘] å¼€å§‹æ„å»ºå·¥ä½œæµæ¨¡æ¿æ ‘...")
            from .workflow_template_connection_service import WorkflowTemplateConnectionService
            connection_service = WorkflowTemplateConnectionService()
            subdivisions_data = await connection_service._get_all_subdivisions_simple(workflow_instance_id)
            
            logger.info(f"ğŸ“Š [subdivisionæ•°æ®] è·å¾— {len(subdivisions_data) if subdivisions_data else 0} ä¸ªsubdivisionè®°å½•")
            
            if not subdivisions_data:
                logger.warning(f"âš ï¸ [subdivisionæ•°æ®] æ²¡æœ‰subdivisionæ•°æ®")
                return {"success": False, "message": "æ²¡æœ‰subdivisionæ•°æ®"}
                
            tree = await WorkflowTemplateTree().build_from_subdivisions(subdivisions_data, workflow_instance_id)
            logger.info(f"âœ… [æ„å»ºæ ‘] å·¥ä½œæµæ¨¡æ¿æ ‘æ„å»ºå®Œæˆ")
            
            # ğŸ”§ åŸºäºtreeè®¡ç®—åˆå¹¶å€™é€‰é¡¹ï¼ˆæ™ºèƒ½é€’å½’è·¯å¾„è®¡ç®—ï¼‰
            logger.info(f"ğŸ”„ [æ™ºèƒ½é€’å½’] è®¡ç®—é€’å½’åˆå¹¶è·¯å¾„...")
            
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†å‰ç«¯ä¼ é€’çš„template_å‰ç¼€
            cleaned_selected_merges = []
            for merge_id in selected_merges:
                if merge_id.startswith('template_'):
                    # ç§»é™¤template_å‰ç¼€
                    cleaned_id = merge_id.replace('template_', '')
                    cleaned_selected_merges.append(cleaned_id)
                    logger.info(f"   ğŸ”§ æ¸…ç†èŠ‚ç‚¹ID: {merge_id} -> {cleaned_id}")
                else:
                    cleaned_selected_merges.append(merge_id)
            
            tree_candidates = tree.calculate_recursive_merge_path(cleaned_selected_merges)
            
            logger.info(f"ğŸ“Š [åˆå¹¶è·¯å¾„] é€’å½’åˆå¹¶è·¯å¾„åŒ…å« {len(tree_candidates)} ä¸ªèŠ‚ç‚¹")
            if not tree_candidates:
                logger.warning(f"âš ï¸ [åˆå¹¶è·¯å¾„] æœªæ‰¾åˆ°åŒ¹é…çš„å€™é€‰é¡¹")
                return {"success": False, "message": "æœªæ‰¾åˆ°åŒ¹é…çš„å€™é€‰é¡¹"}
            
            for i, candidate in enumerate(tree_candidates):
                logger.info(f"   è·¯å¾„èŠ‚ç‚¹ {i+1}: {candidate['node_name']} (æ·±åº¦: {candidate['depth']})")
                
            # è·å–åˆå§‹å·¥ä½œæµåŸºç¡€ID
            initial_workflow_base_id = await self._get_workflow_base_id(workflow_instance_id)
            if not initial_workflow_base_id:
                return {"success": False, "message": "æ— æ³•è·å–å·¥ä½œæµåŸºç¡€ID"}
            
            logger.info(f"ğŸ“‹ [åˆå§‹å·¥ä½œæµ] åˆå§‹å·¥ä½œæµåŸºç¡€ID: {initial_workflow_base_id}")
            
            # ğŸ”§ æ”¹ä¸ºçœŸæ­£çš„é€’å½’åˆå¹¶ï¼šä¸€æ¬¡æ€§æ›¿æ¢æ‰€æœ‰é€‰ä¸­èŠ‚ç‚¹åˆ°ä¸€ä¸ªæ–°æ¨¡æ¿
            logger.info(f"ğŸ”„ [çœŸé€’å½’åˆå¹¶] å¼€å§‹çœŸæ­£çš„é€’å½’åˆå¹¶...")
            logger.info(f"ğŸ“Š [åˆå¹¶å€™é€‰] å°†æŠŠ {len(tree_candidates)} ä¸ªèŠ‚ç‚¹é€’å½’åˆå¹¶åˆ°ä¸€ä¸ªæ–°æ¨¡æ¿ä¸­")
            
            # ç”Ÿæˆå•ä¸€çš„æ–°å·¥ä½œæµæ¨¡æ¿
            new_workflow_base_id = uuid.uuid4()
            logger.info(f"ğŸ†• [æ–°æ¨¡æ¿] ç”Ÿæˆç»Ÿä¸€çš„æ–°å·¥ä½œæµåŸºç¡€ID: {new_workflow_base_id}")
            
            # è·å–å½“å‰å·¥ä½œæµçš„æœ€ä½³ç‰ˆæœ¬ID
            current_workflow_id = await self._get_best_workflow_id_by_base(initial_workflow_base_id)
            if not current_workflow_id:
                return {"success": False, "error": f"æ— æ³•æ‰¾åˆ°å½“å‰å·¥ä½œæµ: {initial_workflow_base_id}"}
                
            logger.info(f"ğŸ“‹ [æºå·¥ä½œæµ] å½“å‰å·¥ä½œæµID: {current_workflow_id}")
            
            # åˆ›å»ºæ–°çš„ç»Ÿä¸€å·¥ä½œæµè®°å½•
            unified_workflow_info = await self._create_unified_recursive_workflow_record(
                initial_workflow_base_id, new_workflow_base_id, len(tree_candidates), creator_id
            )
            new_workflow_id = unified_workflow_info['workflow_id']
            logger.info(f"âœ… [æ–°æ¨¡æ¿è®°å½•] åˆ›å»ºå®Œæˆ: {unified_workflow_info['name']} (ID: {new_workflow_id})")
            
            # æ‰§è¡Œç»Ÿä¸€çš„é€’å½’èŠ‚ç‚¹æ›¿æ¢åˆå¹¶
            logger.info(f"ğŸ”„ [å¼€å§‹é€’å½’æ›¿æ¢] æ‰§è¡Œç»Ÿä¸€çš„é€’å½’èŠ‚ç‚¹æ›¿æ¢åˆå¹¶")
            merge_stats = await self._execute_unified_recursive_node_replacement(
                current_workflow_id, new_workflow_id, new_workflow_base_id, tree_candidates
            )
            
            total_merged = len(tree_candidates)
            
            logger.info(f"ğŸ‰ [çœŸé€’å½’åˆå¹¶] åˆå¹¶æµç¨‹å®Œæˆ!")
            logger.info(f"   - åˆå§‹å·¥ä½œæµåŸºç¡€ID: {initial_workflow_base_id}")
            logger.info(f"   - æœ€ç»ˆå·¥ä½œæµåŸºç¡€ID: {new_workflow_base_id}")
            logger.info(f"   - æ€»åˆå¹¶èŠ‚ç‚¹æ•°: {total_merged}")
            logger.info(f"   - åˆå¹¶æ¨¡å¼: çœŸæ­£é€’å½’ï¼ˆå•ä¸€æ¨¡æ¿ï¼‰")
            
            return {
                "success": True,
                "final_workflow_base_id": str(new_workflow_base_id),
                "total_merged": total_merged,
                "merge_layers": 1,  # åªæœ‰ä¸€ä¸ªå±‚çº§ï¼Œå› ä¸ºæ˜¯ç»Ÿä¸€åˆå¹¶
                "workflow_info": unified_workflow_info,
                "merge_stats": merge_stats,
                "message": f"é€’å½’åˆå¹¶å®Œæˆï¼Œå¤„ç†äº†{total_merged}ä¸ªèŠ‚ç‚¹åˆ°ç»Ÿä¸€æ¨¡æ¿"
            }
                
        except Exception as e:
            logger.error(f"âŒ [é€’å½’åˆå¹¶] å·¥ä½œæµåˆå¹¶å¤±è´¥: {e}")
            import traceback
            logger.error(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {"success": False, "error": str(e)}

    async def _find_parent_workflow_base_id(self, current_candidate: Dict[str, Any]) -> Optional[str]:
        """
        æ ¹æ®æ ‘ç»“æ„æ‰¾åˆ°å½“å‰å€™é€‰é¡¹çš„çˆ¶å·¥ä½œæµåŸºç¡€ID
        
        Args:
            current_candidate: å½“å‰å€™é€‰é¡¹
            
        Returns:
            çˆ¶å·¥ä½œæµçš„åŸºç¡€IDï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
        """
        try:
            logger.info(f"ğŸ” [çˆ¶å·¥ä½œæµæŸ¥æ‰¾] æŸ¥æ‰¾å€™é€‰é¡¹çš„çˆ¶å·¥ä½œæµ: {current_candidate['node_name']}")
            logger.info(f"   - å½“å‰å€™é€‰é¡¹æ·±åº¦: {current_candidate['depth']}")
            logger.info(f"   - subdivision_id: {current_candidate['subdivision_id']}")
            
            # ä»treeä¸­æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
            current_tree_node = current_candidate.get('tree_node')
            if not current_tree_node:
                logger.warning(f"   âš ï¸ å½“å‰å€™é€‰é¡¹æ²¡æœ‰tree_nodeå¼•ç”¨")
                return None
            
            # è·å–çˆ¶èŠ‚ç‚¹
            parent_tree_node = current_tree_node.parent_node
            if not parent_tree_node:
                logger.info(f"   ğŸ“‹ å½“å‰èŠ‚ç‚¹æ˜¯æ ¹èŠ‚ç‚¹ï¼Œæ— çˆ¶å·¥ä½œæµ")
                return None
            
            logger.info(f"   ğŸ“‹ æ‰¾åˆ°çˆ¶èŠ‚ç‚¹: {parent_tree_node.workflow_name}")
            logger.info(f"   ğŸ“‹ çˆ¶èŠ‚ç‚¹workflow_base_id: {parent_tree_node.workflow_base_id}")
            logger.info(f"   ğŸ“‹ çˆ¶èŠ‚ç‚¹workflow_instance_id: {parent_tree_node.workflow_instance_id}")
            
            # è·å–çˆ¶å·¥ä½œæµçš„å½“å‰ç‰ˆæœ¬workflow_id
            if parent_tree_node.workflow_instance_id:
                parent_workflow_base_id = await self._get_workflow_base_id_from_instance(
                    parent_tree_node.workflow_instance_id
                )
                
                if parent_workflow_base_id:
                    logger.info(f"   âœ… æ‰¾åˆ°çˆ¶å·¥ä½œæµåŸºç¡€ID: {parent_workflow_base_id}")
                    return parent_workflow_base_id
            
            # å¦‚æœé€šè¿‡workflow_instance_idæ‰¾ä¸åˆ°ï¼Œç›´æ¥ä½¿ç”¨workflow_base_id
            parent_workflow_base_id = parent_tree_node.workflow_base_id
            logger.info(f"   ğŸ”§ ä½¿ç”¨çˆ¶èŠ‚ç‚¹çš„workflow_base_id: {parent_workflow_base_id}")
            
            return parent_workflow_base_id
            
        except Exception as e:
            logger.error(f"âŒ [çˆ¶å·¥ä½œæµæŸ¥æ‰¾] æŸ¥æ‰¾å¤±è´¥: {e}")
            return None
    
    async def _get_workflow_base_id_from_instance(self, workflow_instance_id: str) -> Optional[str]:
        """æ ¹æ®å·¥ä½œæµå®ä¾‹IDè·å–å¯¹åº”çš„å·¥ä½œæµåŸºç¡€ID"""
        try:
            result = await self.db.fetch_one("""
                SELECT workflow_base_id FROM workflow_instance 
                WHERE workflow_instance_id = %s
            """, workflow_instance_id)
            
            return str(result['workflow_base_id']) if result else None
            
        except Exception as e:
            logger.error(f"æ ¹æ®å®ä¾‹IDè·å–åŸºç¡€IDå¤±è´¥: {e}")
            return None
        
    async def _get_workflow_base_id(self, workflow_instance_id: uuid.UUID) -> Optional[str]:
        """è·å–å·¥ä½œæµåŸºç¡€ID"""
        result = await self.db.fetch_one("""
            SELECT workflow_base_id FROM workflow_instance 
            WHERE workflow_instance_id = %s
        """, workflow_instance_id)
        return str(result['workflow_base_id']) if result else None
    
    async def _merge_depth_layer_tree_based(self, current_workflow_base_id: str, 
                                          depth_candidates: List[Dict[str, Any]], 
                                          creator_id: uuid.UUID, depth: int) -> Dict[str, Any]:
        """åŸºäºtreeæ•°æ®çš„åˆ†å±‚åˆå¹¶ - ä¸å†æŸ¥è¯¢subdivisionè¡¨"""
        try:
            logger.info(f"ğŸ”§ [Treeåˆå¹¶] å¼€å§‹åˆå¹¶æ·±åº¦ {depth}: {len(depth_candidates)} ä¸ªå€™é€‰é¡¹")
            
            # ğŸ” è°ƒè¯•ï¼šåˆ†æå½“å‰åˆå¹¶çŠ¶æ€
            logger.info(f"ğŸ“Š [åˆå¹¶çŠ¶æ€] å½“å‰å·¥ä½œæµåŸºç¡€ID: {current_workflow_base_id}")
            logger.info(f"ğŸ“Š [åˆå¹¶çŠ¶æ€] åˆ›å»ºè€…ID: {creator_id}")
            logger.info(f"ğŸ“Š [åˆå¹¶çŠ¶æ€] åˆå¹¶æ·±åº¦: {depth}")
            
            # ğŸ”§ æ–°å¢ï¼šåˆ†æçˆ¶å·¥ä½œæµçš„ç‰ˆæœ¬æƒ…å†µ
            await self._debug_workflow_versions(current_workflow_base_id, f"åˆå¹¶æ·±åº¦{depth}å‰")
            
            for i, candidate in enumerate(depth_candidates):
                logger.info(f"   å€™é€‰é¡¹ {i+1}: {candidate.get('node_name', 'Unknown')}")
                logger.info(f"     - subdivision_id: {candidate.get('subdivision_id')}")
                logger.info(f"     - workflow_instance_id: {candidate.get('workflow_instance_id')}")
                logger.info(f"     - workflow_base_id: {candidate.get('workflow_base_id')}")
                logger.info(f"     - depth: {candidate.get('depth')}")
            
            # 1. ç”Ÿæˆæ–°çš„å·¥ä½œæµç‰ˆæœ¬
            new_workflow_base_id = uuid.uuid4()
            logger.info(f"ğŸ†• [æ–°å·¥ä½œæµ] ç”Ÿæˆæ–°å·¥ä½œæµåŸºç¡€ID: {new_workflow_base_id}")
            
            # 2. è·å–å½“å‰å·¥ä½œæµçš„workflow_id - ğŸ”§ æ™ºèƒ½ç‰ˆæœ¬é€‰æ‹©
            current_workflow_id = await self._get_best_workflow_id_by_base(current_workflow_base_id)
            if not current_workflow_id:
                return {"success": False, "error": f"æ— æ³•æ‰¾åˆ°å½“å‰å·¥ä½œæµ: {current_workflow_base_id}"}
                
            logger.info(f"ğŸ“‹ [å½“å‰å·¥ä½œæµ] å½“å‰å·¥ä½œæµID: {current_workflow_id}")
            
            # ğŸ” è°ƒè¯•ï¼šåˆ†æå½“å‰çˆ¶å·¥ä½œæµçŠ¶æ€
            await self._debug_current_workflow_state(current_workflow_id, "åˆå¹¶å‰")
            
            # 3. åˆ›å»ºæ–°çš„å·¥ä½œæµè®°å½•
            new_workflow_info = await self._create_layered_workflow_record(
                current_workflow_base_id, new_workflow_base_id, depth, len(depth_candidates), creator_id
            )
            new_workflow_id = new_workflow_info['workflow_id']
            logger.info(f"âœ… [æ–°å·¥ä½œæµè®°å½•] åˆ›å»ºå®Œæˆ: {new_workflow_info['name']} (ID: {new_workflow_id})")
            
            # 4. æ‰§è¡ŒåŸºäºtreeçš„èŠ‚ç‚¹æ›¿æ¢åˆå¹¶
            logger.info(f"ğŸ”„ [å¼€å§‹èŠ‚ç‚¹æ›¿æ¢] æ‰§è¡ŒåŸºäºtreeçš„èŠ‚ç‚¹æ›¿æ¢åˆå¹¶")
            merge_stats = await self._execute_tree_based_node_replacement(
                current_workflow_id, new_workflow_id, new_workflow_base_id, depth_candidates
            )
            
            # ğŸ” è°ƒè¯•ï¼šåˆ†æåˆå¹¶åçš„æ–°å·¥ä½œæµçŠ¶æ€
            await self._debug_current_workflow_state(new_workflow_id, "åˆå¹¶å")
            
            logger.info(f"âœ… [Treeåˆå¹¶] æ·±åº¦ {depth} åˆå¹¶å®Œæˆ:")
            logger.info(f"   - æ–°å·¥ä½œæµ: {new_workflow_info['name']}")
            logger.info(f"   - åˆå¹¶èŠ‚ç‚¹: {merge_stats.get('nodes_replaced', 0)}")
            logger.info(f"   - é‡å»ºè¿æ¥: {merge_stats.get('connections_count', 0)}")
            
            # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºå·¥ä½œæµåˆ‡æ¢è¿‡ç¨‹
            logger.info(f"ğŸ”„ [å·¥ä½œæµåˆ‡æ¢] ä» {current_workflow_base_id} åˆ‡æ¢åˆ° {new_workflow_base_id}")
            logger.info(f"   - åŸå·¥ä½œæµID: {current_workflow_id}")
            logger.info(f"   - æ–°å·¥ä½œæµID: {new_workflow_id}")
            logger.info(f"   - æ–°å·¥ä½œæµåŸºç¡€IDå°†ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥")
            
            return {
                "success": True,
                "depth": depth,
                "new_workflow_base_id": str(new_workflow_base_id),
                "new_workflow_id": str(new_workflow_id),
                "merged_count": len(depth_candidates),
                "workflow_info": new_workflow_info,
                "merge_stats": merge_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ [Treeåˆå¹¶] æ·±åº¦ {depth} åˆå¹¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_tree_based_node_replacement(self, parent_workflow_id: str,
                                                 new_workflow_id: uuid.UUID, new_workflow_base_id: uuid.UUID,
                                                 tree_candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºäºtreeæ•°æ®æ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢åˆå¹¶ - æ··åˆæ–¹æ³•ï¼šä¼˜å…ˆä½¿ç”¨treeæ•°æ®ï¼Œå¿…è¦æ—¶æŸ¥è¯¢subdivisionè¡¨"""
        try:
            # 1. æ”¶é›†éœ€è¦æ›¿æ¢çš„èŠ‚ç‚¹IDï¼ˆä¼˜å…ˆä½¿ç”¨treeæ•°æ®ï¼Œå¿…è¦æ—¶æŸ¥è¯¢subdivisionï¼‰
            nodes_to_replace = set()
            tree_mapping = {}  # original_node_id -> tree_candidate
            
            logger.info(f"ğŸ” [æ··åˆæ–¹æ³•] åˆ†æ {len(tree_candidates)} ä¸ªtreeå€™é€‰é¡¹:")
            for candidate in tree_candidates:
                original_node_id = candidate.get('original_node_id')
                subdivision_id = candidate.get('subdivision_id')
                node_name = candidate.get('node_name', 'Unknown')
                
                logger.info(f"     å€™é€‰é¡¹: {node_name}")
                logger.info(f"       - original_node_id (from tree): {original_node_id}")
                logger.info(f"       - subdivision_id: {subdivision_id}")
                
                # å¦‚æœtreeä¸­æœ‰original_node_idï¼Œç›´æ¥ä½¿ç”¨
                if original_node_id:
                    nodes_to_replace.add(original_node_id)
                    tree_mapping[original_node_id] = candidate
                    logger.info(f"   ğŸ”§ å°†æ›¿æ¢èŠ‚ç‚¹ (æ¥è‡ªtree): {node_name} (node_id: {original_node_id})")
                # å¦åˆ™ï¼Œå›é€€åˆ°subdivisionæŸ¥è¯¢
                elif subdivision_id:
                    logger.info(f"   ğŸ” treeæ•°æ®ä¸å®Œæ•´ï¼ŒæŸ¥è¯¢subdivision: {subdivision_id}")
                    original_node_info = await self._get_original_node_info(subdivision_id)
                    if original_node_info:
                        actual_node_id = original_node_info['node_id']
                        nodes_to_replace.add(actual_node_id)
                        # å°†subdivisionæŸ¥è¯¢ç»“æœè¡¥å……åˆ°candidateä¸­
                        enhanced_candidate = candidate.copy()
                        enhanced_candidate.update({
                            'original_node_id': actual_node_id,
                            'original_task_id': original_node_info.get('original_task_id'),
                            'original_node_position': {
                                'x': original_node_info.get('position_x', 0),
                                'y': original_node_info.get('position_y', 0)
                            },
                            'original_node_info': original_node_info  # å®Œæ•´ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
                        })
                        tree_mapping[actual_node_id] = enhanced_candidate
                        logger.info(f"   ğŸ”§ å°†æ›¿æ¢èŠ‚ç‚¹ (æ¥è‡ªsubdivision): {original_node_info['name']} (node_id: {actual_node_id})")
                    else:
                        logger.warning(f"   âŒ æ— æ³•è·å–subdivisionçš„åŸå§‹èŠ‚ç‚¹ä¿¡æ¯: {subdivision_id}")
                else:
                    logger.warning(f"   âš ï¸ å€™é€‰é¡¹ç¼ºå°‘è¯†åˆ«ä¿¡æ¯: {node_name}")
            
            logger.info(f"ğŸ”„ [æ··åˆæ–¹æ³•] å°†æ›¿æ¢ {len(nodes_to_replace)} ä¸ªèŠ‚ç‚¹")
            
            # 2. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™èŠ‚ç‚¹ï¼ˆæ’é™¤è¦æ›¿æ¢çš„èŠ‚ç‚¹ï¼‰
            node_id_mapping = await self._copy_preserved_nodes_simple(
                parent_workflow_id, new_workflow_id, new_workflow_base_id, nodes_to_replace
            )
            
            # 3. åŸºäºtreeæ•°æ®æ‰§è¡ŒèŠ‚ç‚¹æ›¿æ¢
            replacement_stats = await self._replace_nodes_from_tree_data(
                new_workflow_id, new_workflow_base_id, tree_mapping, node_id_mapping
            )
            
            # 4. é‡å»ºè¿æ¥
            connection_stats = await self._rebuild_connections_from_tree_data(
                parent_workflow_id, new_workflow_id, tree_mapping, node_id_mapping
            )
            
            logger.info(f"âœ… [æ··åˆæ–¹æ³•] å®Œæˆ: æ›¿æ¢{replacement_stats['nodes_replaced']}èŠ‚ç‚¹, é‡å»º{connection_stats['connections_count']}è¿æ¥")
            
            return {
                **replacement_stats,
                **connection_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ [æ··åˆæ–¹æ³•] æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def _copy_preserved_nodes_simple(self, parent_workflow_id: str, new_workflow_id: uuid.UUID,
                                          new_workflow_base_id: uuid.UUID, 
                                          nodes_to_replace: set) -> Dict[str, uuid.UUID]:
        """å¤åˆ¶çˆ¶å·¥ä½œæµä¸­éœ€è¦ä¿ç•™çš„èŠ‚ç‚¹ - ä¿®å¤ç‰ˆæœ¬"""
        node_id_mapping = {}
        
        # ğŸ”§ è°ƒè¯•ï¼šéªŒè¯parent_workflow_idæ˜¯å¦æœ‰æ•ˆ
        logger.info(f"ğŸ” [èŠ‚ç‚¹å¤åˆ¶] å¼€å§‹å¤åˆ¶çˆ¶å·¥ä½œæµèŠ‚ç‚¹:")
        logger.info(f"   - çˆ¶å·¥ä½œæµID: {parent_workflow_id}")
        logger.info(f"   - æ–°å·¥ä½œæµID: {new_workflow_id}")
        logger.info(f"   - æ–°å·¥ä½œæµåŸºç¡€ID: {new_workflow_base_id}")
        
        # ğŸ”§ ä¿®å¤ï¼šå…ˆéªŒè¯çˆ¶å·¥ä½œæµæ˜¯å¦å­˜åœ¨
        parent_workflow_check = await self.db.fetch_one("""
            SELECT workflow_id, name, workflow_base_id, is_current_version
            FROM workflow 
            WHERE workflow_id = %s
        """, parent_workflow_id)
        
        if not parent_workflow_check:
            logger.error(f"âŒ [èŠ‚ç‚¹å¤åˆ¶] çˆ¶å·¥ä½œæµä¸å­˜åœ¨: {parent_workflow_id}")
            return node_id_mapping
            
        logger.info(f"âœ… [çˆ¶å·¥ä½œæµéªŒè¯] æ‰¾åˆ°çˆ¶å·¥ä½œæµ: {parent_workflow_check['name']}")
        logger.info(f"   - å·¥ä½œæµåŸºç¡€ID: {parent_workflow_check['workflow_base_id']}")
        logger.info(f"   - æ˜¯å¦å½“å‰ç‰ˆæœ¬: {parent_workflow_check['is_current_version']}")
        
        # ğŸ”§ è°ƒè¯•ï¼šå…ˆæŸ¥è¯¢æ‰€æœ‰çˆ¶å·¥ä½œæµèŠ‚ç‚¹
        all_parent_nodes = await self.db.fetch_all("""
            SELECT node_id, name, type, workflow_id, is_deleted
            FROM node 
            WHERE workflow_id = %s
        """, parent_workflow_id)
        
        logger.info(f"ğŸ” [çˆ¶å·¥ä½œæµèŠ‚ç‚¹] æ€»èŠ‚ç‚¹æ•°: {len(all_parent_nodes)}")
        active_nodes = [n for n in all_parent_nodes if not n['is_deleted']]
        logger.info(f"ğŸ” [çˆ¶å·¥ä½œæµèŠ‚ç‚¹] æ´»è·ƒèŠ‚ç‚¹æ•°: {len(active_nodes)}")
        
        for node in all_parent_nodes:
            status = "å·²åˆ é™¤" if node['is_deleted'] else "æ´»è·ƒ"
            logger.info(f"     - {node['name']} ({node['type']}) ID: {node['node_id'][:8]}... çŠ¶æ€: {status}")
        
        logger.info(f"ğŸ” [å¾…æ›¿æ¢èŠ‚ç‚¹] éœ€è¦æ›¿æ¢çš„èŠ‚ç‚¹IDé›†åˆ: {[str(nid)[:8] + '...' for nid in nodes_to_replace]}")
        
        # ğŸ”§ ä¿®å¤ï¼šåˆ†ä¸¤æ­¥æŸ¥è¯¢ï¼Œæ›´æ¸…æ™°åœ°å¤„ç†è¿‡æ»¤é€»è¾‘
        if nodes_to_replace:
            # æ„å»ºNOT INå­å¥ï¼Œæ³¨æ„UUIDç±»å‹è½¬æ¢
            node_ids_list = list(nodes_to_replace)
            placeholders = ','.join(['%s'] * len(node_ids_list))
            
            logger.info(f"ğŸ” [æŸ¥è¯¢å‚æ•°] æ‰§è¡ŒNOT INæŸ¥è¯¢ï¼Œæ’é™¤ {len(node_ids_list)} ä¸ªèŠ‚ç‚¹")
            
            parent_nodes = await self.db.fetch_all(f"""
                SELECT node_id, node_base_id, name, type, task_description, 
                       position_x, position_y, version, is_deleted
                FROM node 
                WHERE workflow_id = %s 
                AND is_deleted = FALSE
                AND node_id NOT IN ({placeholders})
            """, parent_workflow_id, *node_ids_list)
        else:
            logger.info(f"ğŸ” [æŸ¥è¯¢å‚æ•°] æ‰§è¡Œå…¨é‡æŸ¥è¯¢ï¼ˆæ— éœ€æ’é™¤èŠ‚ç‚¹ï¼‰")
            parent_nodes = await self.db.fetch_all("""
                SELECT node_id, node_base_id, name, type, task_description, 
                       position_x, position_y, version, is_deleted
                FROM node 
                WHERE workflow_id = %s AND is_deleted = FALSE
            """, parent_workflow_id)
        
        logger.info(f"ğŸ” [è¿‡æ»¤ç»“æœ] è¿‡æ»¤åä¿ç•™çš„èŠ‚ç‚¹æ•°: {len(parent_nodes)}")
        
        if not parent_nodes:
            logger.warning(f"âš ï¸ [è¿‡æ»¤ç»“æœ] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦ä¿ç•™çš„çˆ¶å·¥ä½œæµèŠ‚ç‚¹!")
            logger.warning(f"   å¯èƒ½åŸå› :")
            logger.warning(f"   1. æ‰€æœ‰çˆ¶å·¥ä½œæµèŠ‚ç‚¹éƒ½è¢«æ ‡è®°ä¸ºæ›¿æ¢")
            logger.warning(f"   2. çˆ¶å·¥ä½œæµä¸­æ²¡æœ‰æ´»è·ƒèŠ‚ç‚¹") 
            logger.warning(f"   3. æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶æœ‰é—®é¢˜")
            
            # ğŸ”§ è¿›ä¸€æ­¥è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨æ›¿æ¢åˆ—è¡¨ä¸­
            if nodes_to_replace and all_parent_nodes:
                all_active_node_ids = {n['node_id'] for n in all_parent_nodes if not n['is_deleted']}
                replace_node_ids = set(nodes_to_replace)
                
                logger.warning(f"   è°ƒè¯•ä¿¡æ¯:")
                logger.warning(f"   - æ´»è·ƒèŠ‚ç‚¹æ€»æ•°: {len(all_active_node_ids)}")
                logger.warning(f"   - å¾…æ›¿æ¢èŠ‚ç‚¹æ•°: {len(replace_node_ids)}")
                logger.warning(f"   - æ´»è·ƒèŠ‚ç‚¹ID: {[str(nid)[:8] + '...' for nid in all_active_node_ids]}")
                logger.warning(f"   - å¾…æ›¿æ¢ID: {[str(nid)[:8] + '...' for nid in replace_node_ids]}")
                
                # æ£€æŸ¥äº¤é›†
                intersection = all_active_node_ids.intersection(replace_node_ids)
                remaining = all_active_node_ids - replace_node_ids
                
                logger.warning(f"   - åŒ¹é…çš„å¾…æ›¿æ¢èŠ‚ç‚¹: {len(intersection)} ä¸ª")
                logger.warning(f"   - åº”è¯¥ä¿ç•™çš„èŠ‚ç‚¹: {len(remaining)} ä¸ª")
                
                if len(intersection) == 0:
                    logger.error(f"âŒ [æ•°æ®ä¸ä¸€è‡´] å¾…æ›¿æ¢èŠ‚ç‚¹IDåœ¨çˆ¶å·¥ä½œæµä¸­ä¸å­˜åœ¨!")
                if len(remaining) == 0:
                    logger.error(f"âŒ [å…¨éƒ¨æ›¿æ¢] æ‰€æœ‰çˆ¶å·¥ä½œæµèŠ‚ç‚¹éƒ½è¢«æ ‡è®°ä¸ºæ›¿æ¢!")
            
            return node_id_mapping
        
        for node in parent_nodes:
            logger.info(f"     âœ… ä¿ç•™: {node['name']} ({node['type']}) ID: {node['node_id'][:8]}...")
        
        # å¤åˆ¶èŠ‚ç‚¹
        for node in parent_nodes:
            new_node_id = uuid.uuid4()
            new_node_base_id = uuid.uuid4()
            node_id_mapping[node['node_id']] = new_node_id
            
            logger.info(f"   ğŸ“„ å¤åˆ¶èŠ‚ç‚¹: {node['name']} -> æ–°ID: {str(new_node_id)[:8]}...")
            
            await self.db.execute("""
                INSERT INTO node (
                    node_id, node_base_id, workflow_id, workflow_base_id,
                    name, type, task_description, position_x, position_y,
                    version, is_current_version, created_at, is_deleted
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                 node['name'], node['type'], node['task_description'], 
                 node['position_x'], node['position_y'], 1, True, now_utc(), False)
        
        logger.info(f"âœ… [èŠ‚ç‚¹å¤åˆ¶] æˆåŠŸå¤åˆ¶äº† {len(parent_nodes)} ä¸ªçˆ¶å·¥ä½œæµä¿ç•™èŠ‚ç‚¹")
        logger.info(f"ğŸ“Š [èŠ‚ç‚¹æ˜ å°„] å»ºç«‹äº† {len(node_id_mapping)} ä¸ªèŠ‚ç‚¹IDæ˜ å°„å…³ç³»")
        
        return node_id_mapping
    
    async def _replace_nodes_from_tree_data(self, new_workflow_id: uuid.UUID, new_workflow_base_id: uuid.UUID,
                                           tree_mapping: Dict[str, Dict], 
                                           node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """åŸºäºtreeæ•°æ®æ›¿æ¢èŠ‚ç‚¹ - ä¸æŸ¥è¯¢subdivisionè¡¨"""
        replaced_nodes = 0
        
        logger.info(f"ğŸ”„ [èŠ‚ç‚¹æ›¿æ¢] å¼€å§‹å¤„ç† {len(tree_mapping)} ä¸ªå¾…æ›¿æ¢çš„èŠ‚ç‚¹")
        
        for tree_candidate in tree_mapping.values():
            logger.info(f"ğŸ”„ [Treeæ›¿æ¢] å¤„ç†èŠ‚ç‚¹: {tree_candidate['node_name']}")
            
            # ç›´æ¥ä»tree_candidateè·å–å­å·¥ä½œæµç»“æ„
            tree_node = tree_candidate.get('tree_node')
            if not tree_node:
                logger.warning(f"âš ï¸ tree_candidateä¸­æ²¡æœ‰tree_nodeå¼•ç”¨")
                continue
            
            # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºå­å·¥ä½œæµä¿¡æ¯
            logger.info(f"   ğŸ“‹ [å­å·¥ä½œæµ] workflow_instance_id: {tree_node.workflow_instance_id}")
            logger.info(f"   ğŸ“‹ [å­å·¥ä½œæµ] workflow_base_id: {tree_node.workflow_base_id}")
            logger.info(f"   ğŸ“‹ [å­å·¥ä½œæµ] workflow_name: {tree_node.workflow_name}")
            logger.info(f"   ğŸ“‹ [å­å·¥ä½œæµ] status: {tree_node.status}")
            
            # ä»treeä¸­è·å–åŸå§‹èŠ‚ç‚¹ä½ç½®ä¿¡æ¯    
            original_position = tree_candidate.get('original_node_position', {})
            center_x = original_position.get('x', 0)
            center_y = original_position.get('y', 0)
            logger.info(f"   ğŸ“ [åŸå§‹ä½ç½®] x: {center_x}, y: {center_y}")
            
            # è·å–å­å·¥ä½œæµç»“æ„
            logger.info(f"   ğŸ” [åˆ†æå­å·¥ä½œæµ] å¼€å§‹åˆ†æå­å·¥ä½œæµç»“æ„...")
            workflow_structure = await self._analyze_subworkflow_structure_from_tree(
                tree_node, center_x, center_y
            )
            
            # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºå­å·¥ä½œæµç»“æ„åˆ†æç»“æœ
            business_nodes = workflow_structure['business_nodes']
            entry_points = workflow_structure['entry_points']
            exit_points = workflow_structure['exit_points']
            business_connections = workflow_structure['business_connections']
            
            logger.info(f"   ğŸ“Š [å­å·¥ä½œæµç»“æ„] ä¸šåŠ¡èŠ‚ç‚¹æ•°: {len(business_nodes)}")
            logger.info(f"   ğŸ“Š [å­å·¥ä½œæµç»“æ„] å…¥å£ç‚¹æ•°: {len(entry_points)}")
            logger.info(f"   ğŸ“Š [å­å·¥ä½œæµç»“æ„] å‡ºå£ç‚¹æ•°: {len(exit_points)}")
            logger.info(f"   ğŸ“Š [å­å·¥ä½œæµç»“æ„] å†…éƒ¨è¿æ¥æ•°: {len(business_connections)}")
            
            for i, node in enumerate(business_nodes):
                logger.info(f"     ä¸šåŠ¡èŠ‚ç‚¹ {i+1}: {node['name']} ({node['type']}) pos:({node['position_x']},{node['position_y']})")
            
            for i, entry in enumerate(entry_points):
                logger.info(f"     å…¥å£ç‚¹ {i+1}: {entry['name']} ({entry['type']})")
                
            for i, exit_point in enumerate(exit_points):
                logger.info(f"     å‡ºå£ç‚¹ {i+1}: {exit_point['name']} ({exit_point['type']})")
            
            # å¤åˆ¶å­å·¥ä½œæµçš„ä¸šåŠ¡èŠ‚ç‚¹
            logger.info(f"   ğŸ“„ [å¤åˆ¶èŠ‚ç‚¹] å¼€å§‹å¤åˆ¶ {len(business_nodes)} ä¸ªä¸šåŠ¡èŠ‚ç‚¹")
            for node in workflow_structure['business_nodes']:
                new_node_id = uuid.uuid4()
                new_node_base_id = uuid.uuid4()
                
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å¤åˆkeyé¿å…æ˜ å°„å†²çª (åŸèŠ‚ç‚¹ID + å­å·¥ä½œæµæ ‡è¯†)
                composite_key = f"{node['node_id']}@{tree_candidate['node_name']}"
                node_id_mapping[composite_key] = new_node_id
                
                # ğŸ”§ ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹Ÿä¿ç•™åŸå§‹æ˜ å°„ï¼Œä½†è¦ç¡®ä¿ä¸è¦†ç›–
                if node['node_id'] not in node_id_mapping:
                    node_id_mapping[node['node_id']] = new_node_id
                else:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œè®°å½•åŸå§‹æ˜ å°„ç”¨äºè¿æ¥é‡å»º
                    logger.info(f"   ğŸ”§ åŸå§‹èŠ‚ç‚¹IDå†²çª: {node['node_id']} å·²å­˜åœ¨æ˜ å°„ï¼Œä½¿ç”¨å¤åˆkey: {composite_key}")
                
                logger.info(f"   ğŸ“„ å¤åˆ¶ä¸šåŠ¡èŠ‚ç‚¹: {node['name']} -> æ–°ID: {new_node_id} (key: {composite_key})")
                
                await self.db.execute("""
                    INSERT INTO node (
                        node_id, node_base_id, workflow_id, workflow_base_id,
                        name, type, task_description, position_x, position_y,
                        version, is_current_version, created_at, is_deleted
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                     node['name'], node['type'], node['task_description'], 
                     node['position_x'], node['position_y'], 1, True, now_utc(), False)
                
                replaced_nodes += 1
        
        logger.info(f"âœ… [Treeæ›¿æ¢] æ›¿æ¢äº† {replaced_nodes} ä¸ªèŠ‚ç‚¹")
        return {"nodes_replaced": replaced_nodes}
    
    async def _analyze_subworkflow_structure_from_tree(self, tree_node: WorkflowTemplateNode, 
                                                     center_x: int, center_y: int) -> Dict[str, Any]:
        """åŸºäºtreeèŠ‚ç‚¹åˆ†æå­å·¥ä½œæµç»“æ„ - é‡ç”¨ç°æœ‰é€»è¾‘"""
        # ç›´æ¥è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•ï¼Œä½¿ç”¨tree_nodeä¸­çš„workflow_instance_id
        return await self._analyze_subworkflow_structure(
            tree_node.workflow_instance_id, center_x, center_y
        )
    
    async def _rebuild_connections_from_tree_data(self, parent_workflow_id: str, new_workflow_id: uuid.UUID,
                                                tree_mapping: Dict[str, Dict],
                                                node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """åŸºäºtreeæ•°æ®é‡å»ºè¿æ¥"""
        logger.info(f"ğŸ”— [è¿æ¥é‡å»º] å¼€å§‹é‡å»ºè¿æ¥")
        logger.info(f"   - çˆ¶å·¥ä½œæµID: {parent_workflow_id}")
        logger.info(f"   - æ–°å·¥ä½œæµID: {new_workflow_id}")
        logger.info(f"   - èŠ‚ç‚¹æ˜ å°„æ•°é‡: {len(node_id_mapping)}")
        logger.info(f"   - å¾…æ›¿æ¢èŠ‚ç‚¹æ•°é‡: {len(tree_mapping)}")
        
        # è·å–çˆ¶å·¥ä½œæµçš„æ‰€æœ‰è¿æ¥
        parent_connections = await self.db.fetch_all("""
            SELECT from_node_id, to_node_id, connection_type, condition_config
            FROM node_connection 
            WHERE workflow_id = %s
        """, parent_workflow_id)
        
        logger.info(f"ğŸ“Š [çˆ¶å·¥ä½œæµè¿æ¥] æ‰¾åˆ° {len(parent_connections)} ä¸ªçˆ¶å·¥ä½œæµè¿æ¥")
        for i, conn in enumerate(parent_connections):
            logger.info(f"   è¿æ¥ {i+1}: {conn['from_node_id']} -> {conn['to_node_id']} (ç±»å‹: {conn.get('connection_type', 'normal')})")
        
        parent_connections_copied = 0
        subworkflow_connections_copied = 0
        cross_boundary_connections_created = 0
        
        replaced_node_ids = set(tree_mapping.keys())
        logger.info(f"ğŸ”§ [æ›¿æ¢èŠ‚ç‚¹] è¢«æ›¿æ¢çš„èŠ‚ç‚¹IDé›†åˆ: {list(replaced_node_ids)}")
        
        # 1. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™è¿æ¥ï¼ˆä¸æ¶‰åŠè¢«æ›¿æ¢èŠ‚ç‚¹çš„è¿æ¥ï¼‰
        logger.info(f"ğŸ“‹ [ä¿ç•™è¿æ¥] å¼€å§‹å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™è¿æ¥...")
        for conn in parent_connections:
            from_id, to_id = conn['from_node_id'], conn['to_node_id']
            
            # è·³è¿‡æ¶‰åŠè¢«æ›¿æ¢èŠ‚ç‚¹çš„è¿æ¥ï¼ˆè¿™äº›è¿æ¥éœ€è¦åœ¨è·¨è¾¹ç•Œè¿æ¥ä¸­é‡å»ºï¼‰
            if from_id in replaced_node_ids or to_id in replaced_node_ids:
                logger.info(f"   è·³è¿‡æ¶‰åŠæ›¿æ¢èŠ‚ç‚¹çš„è¿æ¥: {from_id} -> {to_id}")
                continue
            
            if from_id in node_id_mapping and to_id in node_id_mapping:
                new_from_id = node_id_mapping[from_id]
                new_to_id = node_id_mapping[to_id]
                
                await self.db.execute("""
                    INSERT INTO node_connection (
                        from_node_id, to_node_id, workflow_id,
                        connection_type, condition_config, created_at
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                """, new_from_id, new_to_id,
                     new_workflow_id, conn.get('connection_type', 'normal'),
                     conn.get('condition_config'), now_utc())
                parent_connections_copied += 1
                logger.info(f"   âœ… å¤åˆ¶ä¿ç•™è¿æ¥: {from_id} -> {to_id} æ˜ å°„ä¸º {new_from_id} -> {new_to_id}")
            else:
                logger.info(f"   â­ï¸ è·³è¿‡è¿æ¥ï¼ˆèŠ‚ç‚¹ä¸åœ¨æ˜ å°„ä¸­ï¼‰: {from_id} -> {to_id}")
        
        logger.info(f"ğŸ“Š [ä¿ç•™è¿æ¥] å¤åˆ¶äº† {parent_connections_copied} ä¸ªçˆ¶å·¥ä½œæµä¿ç•™è¿æ¥")
        
        # 2. å¤åˆ¶å­å·¥ä½œæµå†…éƒ¨è¿æ¥å¹¶æ”¶é›†å‡ºå…¥å£ç‚¹æ˜ å°„
        logger.info(f"ğŸ”„ [å­å·¥ä½œæµè¿æ¥] å¼€å§‹å¤„ç† {len(tree_mapping)} ä¸ªå­å·¥ä½œæµçš„å†…éƒ¨è¿æ¥...")
        
        # 3. ğŸ”§ æ–°å¢ï¼šæ”¶é›†æ›¿æ¢èŠ‚ç‚¹çš„å‡ºå…¥å£ç‚¹æ˜ å°„
        logger.info(f"ğŸ”— [æ˜ å°„æ”¶é›†] æ”¶é›†æ›¿æ¢èŠ‚ç‚¹çš„å‡ºå…¥å£ç‚¹æ˜ å°„...")
        replaced_to_exit_mapping = {}  # replaced_node_id -> [exit_point_new_ids]
        replaced_to_entry_mapping = {}  # replaced_node_id -> [entry_point_new_ids]
        for original_node_id, tree_candidate in tree_mapping.items():
            logger.info(f"   å¤„ç†å­å·¥ä½œæµ: {tree_candidate['node_name']}")
            
            tree_node = tree_candidate.get('tree_node')
            if not tree_node:
                logger.warning(f"   âš ï¸ è·³è¿‡ï¼šç¼ºå°‘tree_nodeå¼•ç”¨")
                continue
                
            original_position = tree_candidate.get('original_node_position', {})
            workflow_structure = await self._analyze_subworkflow_structure_from_tree(
                tree_node, original_position.get('x', 0), original_position.get('y', 0)
            )
            
            # å¤åˆ¶å­å·¥ä½œæµå†…éƒ¨è¿æ¥
            business_connections = workflow_structure['business_connections']
            logger.info(f"     ğŸ“‹ [å†…éƒ¨è¿æ¥] å‘ç° {len(business_connections)} ä¸ªå­å·¥ä½œæµå†…éƒ¨è¿æ¥")
            
            # ğŸ”§ è°ƒè¯•ï¼šæ˜¾ç¤ºå­å·¥ä½œæµçš„å®Œæ•´ç»“æ„
            business_nodes = workflow_structure['business_nodes']
            entry_points = workflow_structure['entry_points']
            exit_points = workflow_structure['exit_points']
            
            logger.info(f"     ğŸ“Š [å­å·¥ä½œæµç»“æ„è¯¦æƒ…] {tree_candidate['node_name']}:")
            logger.info(f"       - ä¸šåŠ¡èŠ‚ç‚¹: {len(business_nodes)}ä¸ª")
            for i, node in enumerate(business_nodes):
                logger.info(f"         èŠ‚ç‚¹{i+1}: {node['name']} (ID: {node['node_id'][:8]}...) pos:({node['position_x']},{node['position_y']})")
            
            logger.info(f"       - å…¥å£ç‚¹: {len(entry_points)}ä¸ª")
            for i, entry in enumerate(entry_points):
                logger.info(f"         å…¥å£{i+1}: {entry['name']} (ID: {entry['node_id'][:8]}...)")
                
            logger.info(f"       - å‡ºå£ç‚¹: {len(exit_points)}ä¸ª")
            for i, exit_pt in enumerate(exit_points):
                logger.info(f"         å‡ºå£{i+1}: {exit_pt['name']} (ID: {exit_pt['node_id'][:8]}...)")
            
            logger.info(f"       - å†…éƒ¨è¿æ¥: {len(business_connections)}ä¸ª")
            for i, conn in enumerate(business_connections):
                logger.info(f"         è¿æ¥{i+1}: {conn['from_node_id'][:8]}... -> {conn['to_node_id'][:8]}...")
            
            for conn in business_connections:
                from_id, to_id = conn['from_node_id'], conn['to_node_id']
                
                # ğŸ”§ è°ƒè¯•ï¼šæ£€æŸ¥èŠ‚ç‚¹æ˜ å°„çŠ¶æ€
                from_mapped = from_id in node_id_mapping
                to_mapped = to_id in node_id_mapping
                logger.info(f"     ğŸ” [è¿æ¥æ£€æŸ¥] {from_id[:8]}... -> {to_id[:8]}... (from_mapped:{from_mapped}, to_mapped:{to_mapped})")
                
                if from_mapped and to_mapped:
                    new_from_id = node_id_mapping[from_id]
                    new_to_id = node_id_mapping[to_id]
                    
                    logger.info(f"     âœ… [è¿æ¥å¤åˆ¶] {from_id[:8]}... -> {to_id[:8]}... æ˜ å°„ä¸º {new_from_id} -> {new_to_id}")
                    
                    await self.db.execute("""
                        INSERT INTO node_connection (
                            from_node_id, to_node_id, workflow_id,
                            connection_type, condition_config, created_at
                        ) VALUES (%s, %s, %s, %s, %s, %s)
                    """, new_from_id, new_to_id,
                         new_workflow_id, conn.get('connection_type', 'normal'),
                         conn.get('condition_config'), now_utc())
                    subworkflow_connections_copied += 1
                else:
                    logger.warning(f"     âŒ [è¿æ¥è·³è¿‡] {from_id[:8]}... -> {to_id[:8]}... (from_mapped:{from_mapped}, to_mapped:{to_mapped})")
                    if not from_mapped:
                        logger.warning(f"       ç¼ºå¤±fromèŠ‚ç‚¹æ˜ å°„: {from_id}")
                    if not to_mapped:
                        logger.warning(f"       ç¼ºå¤±toèŠ‚ç‚¹æ˜ å°„: {to_id}")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†é€’å½’å±•å¼€çš„è·¨è¾¹ç•Œè¿æ¥
            logger.info(f"     ğŸ”— [è·¨è¾¹ç•Œè¿æ¥] å¼€å§‹å¤„ç†å€™é€‰é¡¹ {tree_candidate['node_name']} çš„è·¨è¾¹ç•Œè¿æ¥")
            
            # è·å–å½“å‰å€™é€‰é¡¹åœ¨æ–°å·¥ä½œæµä¸­çš„å…¥å£å’Œå‡ºå£èŠ‚ç‚¹
            business_nodes = workflow_structure['business_nodes']
            entry_points = workflow_structure['entry_points'] or (business_nodes[:1] if business_nodes else [])
            exit_points = workflow_structure['exit_points'] or (business_nodes[-1:] if business_nodes else [])
            
            logger.info(f"       - ä¸šåŠ¡èŠ‚ç‚¹æ•°: {len(business_nodes)}")
            logger.info(f"       - å…¥å£ç‚¹æ•°: {len(entry_points)}")
            logger.info(f"       - å‡ºå£ç‚¹æ•°: {len(exit_points)}")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥è¿™ä¸ªå€™é€‰é¡¹æ˜¯å¦è¢«é€’å½’å±•å¼€äº†
            # å¦‚æœå½“å‰å€™é€‰é¡¹çš„ä¸šåŠ¡èŠ‚ç‚¹åœ¨å…¶ä»–å€™é€‰é¡¹ä¸­è¢«è¿›ä¸€æ­¥æ›¿æ¢ï¼Œåˆ™ä½¿ç”¨å±•å¼€åçš„èŠ‚ç‚¹
            final_entry_new_ids = []
            final_exit_new_ids = []
            
            for entry in entry_points:
                original_entry_id = entry['node_id']
                entry_node_name = entry['name']
                
                # æ£€æŸ¥è¿™ä¸ªå…¥å£èŠ‚ç‚¹æ˜¯å¦åœ¨å…¶ä»–å€™é€‰é¡¹ä¸­è¢«é€’å½’å±•å¼€
                expanded_entry_ids = []
                for other_original_node_id, other_candidate in tree_mapping.items():
                    if other_candidate != tree_candidate:
                        # æ£€æŸ¥å…¶ä»–å€™é€‰é¡¹æ˜¯å¦æ˜¯å½“å‰å…¥å£èŠ‚ç‚¹çš„å±•å¼€
                        if other_candidate.get('original_node_id') == original_entry_id:
                            logger.info(f"       ğŸ” [é€’å½’æ£€æµ‹] å…¥å£èŠ‚ç‚¹ {entry_node_name} è¢«é€’å½’å±•å¼€ä¸º {other_candidate['node_name']}")
                            
                            # è·å–å±•å¼€åçš„å­å·¥ä½œæµç»“æ„
                            other_tree_node = other_candidate.get('tree_node')
                            if other_tree_node:
                                other_structure = await self._analyze_subworkflow_structure_from_tree(
                                    other_tree_node, entry['position_x'], entry['position_y']
                                )
                                
                                # ä½¿ç”¨å±•å¼€åçš„å…¥å£èŠ‚ç‚¹
                                for expanded_entry in other_structure['entry_points']:
                                    expanded_entry_id = expanded_entry['node_id']
                                    # ä¼˜å…ˆä½¿ç”¨å¤åˆkey
                                    composite_key = f"{expanded_entry_id}@{other_candidate['node_name']}"
                                    
                                    if composite_key in node_id_mapping:
                                        expanded_entry_ids.append(node_id_mapping[composite_key])
                                        logger.info(f"       âœ… [é€’å½’å…¥å£] æ‰¾åˆ°å±•å¼€å…¥å£: {expanded_entry['name']} -> {node_id_mapping[composite_key]}")
                                    elif expanded_entry_id in node_id_mapping:
                                        expanded_entry_ids.append(node_id_mapping[expanded_entry_id])
                                        logger.info(f"       âœ… [é€’å½’å…¥å£] æ‰¾åˆ°å±•å¼€å…¥å£: {expanded_entry['name']} -> {node_id_mapping[expanded_entry_id]}")
                
                # å¦‚æœæ‰¾åˆ°äº†é€’å½’å±•å¼€çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨å±•å¼€åçš„èŠ‚ç‚¹
                if expanded_entry_ids:
                    final_entry_new_ids.extend(expanded_entry_ids)
                    logger.info(f"       ğŸ”„ [ä½¿ç”¨å±•å¼€å…¥å£] {entry_node_name} -> {len(expanded_entry_ids)}ä¸ªå±•å¼€å…¥å£")
                else:
                    # æ²¡æœ‰é€’å½’å±•å¼€ï¼Œä½¿ç”¨åŸå§‹èŠ‚ç‚¹
                    composite_key = f"{original_entry_id}@{tree_candidate['node_name']}"
                    
                    new_entry_id = None
                    if composite_key in node_id_mapping:
                        new_entry_id = node_id_mapping[composite_key]
                        logger.info(f"       ğŸ“ [åŸå§‹å…¥å£] æ‰¾åˆ°å¤åˆkey: {composite_key} -> {new_entry_id}")
                    elif original_entry_id in node_id_mapping:
                        new_entry_id = node_id_mapping[original_entry_id]
                        logger.info(f"       ğŸ“ [åŸå§‹å…¥å£] æ‰¾åˆ°åŸå§‹key: {original_entry_id} -> {new_entry_id}")
                    
                    if new_entry_id:
                        final_entry_new_ids.append(new_entry_id)
                    else:
                        logger.warning(f"       âŒ [å…¥å£æ˜ å°„] æœªæ‰¾åˆ°æ˜ å°„: {original_entry_id}")
            
            # åŒæ ·å¤„ç†å‡ºå£ç‚¹
            for exit_point in exit_points:
                original_exit_id = exit_point['node_id']
                exit_node_name = exit_point['name']
                
                # æ£€æŸ¥è¿™ä¸ªå‡ºå£èŠ‚ç‚¹æ˜¯å¦åœ¨å…¶ä»–å€™é€‰é¡¹ä¸­è¢«é€’å½’å±•å¼€
                expanded_exit_ids = []
                for other_original_node_id, other_candidate in tree_mapping.items():
                    if other_candidate != tree_candidate:
                        # æ£€æŸ¥å…¶ä»–å€™é€‰é¡¹æ˜¯å¦æ˜¯å½“å‰å‡ºå£èŠ‚ç‚¹çš„å±•å¼€
                        if other_candidate.get('original_node_id') == original_exit_id:
                            logger.info(f"       ğŸ” [é€’å½’æ£€æµ‹] å‡ºå£èŠ‚ç‚¹ {exit_node_name} è¢«é€’å½’å±•å¼€ä¸º {other_candidate['node_name']}")
                            
                            # è·å–å±•å¼€åçš„å­å·¥ä½œæµç»“æ„
                            other_tree_node = other_candidate.get('tree_node')
                            if other_tree_node:
                                other_structure = await self._analyze_subworkflow_structure_from_tree(
                                    other_tree_node, exit_point['position_x'], exit_point['position_y']
                                )
                                
                                # ä½¿ç”¨å±•å¼€åçš„å‡ºå£èŠ‚ç‚¹
                                for expanded_exit in other_structure['exit_points']:
                                    expanded_exit_id = expanded_exit['node_id']
                                    # ä¼˜å…ˆä½¿ç”¨å¤åˆkey
                                    composite_key = f"{expanded_exit_id}@{other_candidate['node_name']}"
                                    
                                    if composite_key in node_id_mapping:
                                        expanded_exit_ids.append(node_id_mapping[composite_key])
                                        logger.info(f"       âœ… [é€’å½’å‡ºå£] æ‰¾åˆ°å±•å¼€å‡ºå£: {expanded_exit['name']} -> {node_id_mapping[composite_key]}")
                                    elif expanded_exit_id in node_id_mapping:
                                        expanded_exit_ids.append(node_id_mapping[expanded_exit_id])
                                        logger.info(f"       âœ… [é€’å½’å‡ºå£] æ‰¾åˆ°å±•å¼€å‡ºå£: {expanded_exit['name']} -> {node_id_mapping[expanded_exit_id]}")
                
                # å¦‚æœæ‰¾åˆ°äº†é€’å½’å±•å¼€çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨å±•å¼€åçš„èŠ‚ç‚¹
                if expanded_exit_ids:
                    final_exit_new_ids.extend(expanded_exit_ids)
                    logger.info(f"       ğŸ”„ [ä½¿ç”¨å±•å¼€å‡ºå£] {exit_node_name} -> {len(expanded_exit_ids)}ä¸ªå±•å¼€å‡ºå£")
                else:
                    # æ²¡æœ‰é€’å½’å±•å¼€ï¼Œä½¿ç”¨åŸå§‹èŠ‚ç‚¹
                    composite_key = f"{original_exit_id}@{tree_candidate['node_name']}"
                    
                    new_exit_id = None
                    if composite_key in node_id_mapping:
                        new_exit_id = node_id_mapping[composite_key]
                        logger.info(f"       ğŸ“ [åŸå§‹å‡ºå£] æ‰¾åˆ°å¤åˆkey: {composite_key} -> {new_exit_id}")
                    elif original_exit_id in node_id_mapping:
                        new_exit_id = node_id_mapping[original_exit_id]
                        logger.info(f"       ğŸ“ [åŸå§‹å‡ºå£] æ‰¾åˆ°åŸå§‹key: {original_exit_id} -> {new_exit_id}")
                    
                    if new_exit_id:
                        final_exit_new_ids.append(new_exit_id)
                    else:
                        logger.warning(f"       âŒ [å‡ºå£æ˜ å°„] æœªæ‰¾åˆ°æ˜ å°„: {original_exit_id}")
            
            logger.info(f"       ğŸ“Š [æœ€ç»ˆæ˜ å°„] å…¥å£èŠ‚ç‚¹: {len(final_entry_new_ids)}ä¸ª, å‡ºå£èŠ‚ç‚¹: {len(final_exit_new_ids)}ä¸ª")
            
            # é‡å»ºä¸Šæ¸¸è¿æ¥ - è¿æ¥åˆ°é€’å½’å±•å¼€åçš„å…¥å£èŠ‚ç‚¹
            upstream_count = 0
            for conn in parent_connections:
                if conn['to_node_id'] == original_node_id:
                    from_id = conn['from_node_id']
                    if from_id in node_id_mapping:
                        new_from_id = node_id_mapping[from_id]
                        
                        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè¿æ¥åˆ°æœ€ç»ˆçš„å…¥å£èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯é€’å½’å±•å¼€åçš„èŠ‚ç‚¹ï¼‰
                        for new_entry_id in final_entry_new_ids:
                            try:
                                await self.db.execute("""
                                    INSERT INTO node_connection (
                                        from_node_id, to_node_id, workflow_id,
                                        connection_type, condition_config, created_at
                                    ) VALUES (%s, %s, %s, %s, %s, %s)
                                """, new_from_id, new_entry_id,
                                     new_workflow_id, conn.get('connection_type', 'normal'),
                                     conn.get('condition_config'), now_utc())
                                cross_boundary_connections_created += 1
                                upstream_count += 1
                                logger.info(f"       âœ… [ä¸Šæ¸¸è¿æ¥] {from_id[:8]}... -> {new_entry_id} (åŸè¿æ¥: {conn['from_node_id'][:8]}... -> {original_node_id[:8]}...)")
                            except Exception as e:
                                logger.error(f"       âŒ [ä¸Šæ¸¸è¿æ¥] åˆ›å»ºå¤±è´¥: {e}")
            
            # é‡å»ºä¸‹æ¸¸è¿æ¥ - è¿æ¥åˆ°é€’å½’å±•å¼€åçš„å‡ºå£èŠ‚ç‚¹
            downstream_count = 0
            for conn in parent_connections:
                if conn['from_node_id'] == original_node_id:
                    to_id = conn['to_node_id']
                    if to_id in node_id_mapping:
                        new_to_id = node_id_mapping[to_id]
                        
                        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä»æœ€ç»ˆçš„å‡ºå£èŠ‚ç‚¹è¿æ¥ï¼ˆå¯èƒ½æ˜¯é€’å½’å±•å¼€åçš„èŠ‚ç‚¹ï¼‰
                        for new_exit_id in final_exit_new_ids:
                            try:
                                await self.db.execute("""
                                    INSERT INTO node_connection (
                                        from_node_id, to_node_id, workflow_id,
                                        connection_type, condition_config, created_at
                                    ) VALUES (%s, %s, %s, %s, %s, %s)
                                """, new_exit_id, new_to_id,
                                     new_workflow_id, conn.get('connection_type', 'normal'),
                                     conn.get('condition_config'), now_utc())
                                cross_boundary_connections_created += 1
                                downstream_count += 1
                                logger.info(f"       âœ… [ä¸‹æ¸¸è¿æ¥] {new_exit_id} -> {to_id[:8]}... (åŸè¿æ¥: {original_node_id[:8]}... -> {conn['to_node_id'][:8]}...)")
                            except Exception as e:
                                logger.error(f"       âŒ [ä¸‹æ¸¸è¿æ¥] åˆ›å»ºå¤±è´¥: {e}")
            
            # å°†å½“å‰å€™é€‰é¡¹çš„å‡ºå…¥å£ç‚¹åŠ å…¥åˆ°å…¨å±€æ˜ å°„ä¸­ï¼Œä¾›æ›¿æ¢èŠ‚ç‚¹é—´è¿æ¥ä½¿ç”¨
            replaced_to_entry_mapping[original_node_id] = final_entry_new_ids
            replaced_to_exit_mapping[original_node_id] = final_exit_new_ids
            
            logger.info(f"     ğŸ“Š [æ˜ å°„æ”¶é›†] {tree_candidate['node_name']}: {len(final_entry_new_ids)}ä¸ªå…¥å£ç‚¹, {len(final_exit_new_ids)}ä¸ªå‡ºå£ç‚¹")
            
            logger.info(f"     ğŸ“Š [è·¨è¾¹ç•Œç»Ÿè®¡] ä¸Šæ¸¸è¿æ¥: {upstream_count}ä¸ª, ä¸‹æ¸¸è¿æ¥: {downstream_count}ä¸ª")
        
        # 4. ğŸ”§ æ–°å¢ï¼šé‡å»ºæ›¿æ¢èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
        logger.info(f"ğŸ”— [æ›¿æ¢è¿æ¥] é‡å»ºæ›¿æ¢èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥...")
        replaced_connections_created = 0
        
        for conn in parent_connections:
            from_id, to_id = conn['from_node_id'], conn['to_node_id']
            
            # åªå¤„ç†æ›¿æ¢èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
            if from_id in replaced_node_ids and to_id in replaced_node_ids:
                logger.info(f"   ğŸ”— [æ›¿æ¢è¿æ¥] å¤„ç†: {from_id} -> {to_id}")
                
                # è·å–fromèŠ‚ç‚¹çš„å‡ºå£ç‚¹å’ŒtoèŠ‚ç‚¹çš„å…¥å£ç‚¹
                from_exit_points = replaced_to_exit_mapping.get(from_id, [])
                to_entry_points = replaced_to_entry_mapping.get(to_id, [])
                
                logger.info(f"     - Fromå‡ºå£ç‚¹æ•°: {len(from_exit_points)}, Toå…¥å£ç‚¹æ•°: {len(to_entry_points)}")
                
                # å»ºç«‹å‡ºå£ç‚¹åˆ°å…¥å£ç‚¹çš„è¿æ¥
                for from_exit_id in from_exit_points:
                    for to_entry_id in to_entry_points:
                        await self.db.execute("""
                            INSERT INTO node_connection (
                                from_node_id, to_node_id, workflow_id,
                                connection_type, condition_config, created_at
                            ) VALUES (%s, %s, %s, %s, %s, %s)
                        """, from_exit_id, to_entry_id,
                             new_workflow_id, conn.get('connection_type', 'normal'),
                             conn.get('condition_config'), now_utc())
                        replaced_connections_created += 1
                        logger.info(f"     âœ… åˆ›å»ºæ›¿æ¢è¿æ¥: {from_exit_id} -> {to_entry_id}")
        
        logger.info(f"âœ… [Treeè¿æ¥] é‡å»ºå®Œæˆ: çˆ¶è¿æ¥{parent_connections_copied}, å­è¿æ¥{subworkflow_connections_copied}, è·¨è¾¹ç•Œ{cross_boundary_connections_created}, æ›¿æ¢è¿æ¥{replaced_connections_created}")
        
        return {
            "parent_connections_copied": parent_connections_copied,
            "subworkflow_connections_copied": subworkflow_connections_copied,
            "cross_boundary_connections_created": cross_boundary_connections_created,
            "replaced_connections_created": replaced_connections_created,
            "connections_count": parent_connections_copied + subworkflow_connections_copied + cross_boundary_connections_created + replaced_connections_created
        }
    
    async def _get_best_workflow_id_by_base(self, workflow_base_id: str) -> Optional[str]:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³çš„workflow_id - ä¼˜å…ˆå½“å‰ç‰ˆæœ¬ï¼Œå¦‚æœå½“å‰ç‰ˆæœ¬ä¸ºç©ºåˆ™é€‰æ‹©æœ‰èŠ‚ç‚¹çš„ç‰ˆæœ¬"""
        try:
            logger.info(f"ğŸ” [æ™ºèƒ½ç‰ˆæœ¬é€‰æ‹©] ä¸ºåŸºç¡€IDé€‰æ‹©æœ€ä½³ç‰ˆæœ¬: {workflow_base_id}")
            
            # é¦–å…ˆå°è¯•è·å–å½“å‰ç‰ˆæœ¬
            current_version = await self.db.fetch_one("""
                SELECT w.workflow_id, w.version, COUNT(n.node_id) as node_count
                FROM workflow w
                LEFT JOIN node n ON w.workflow_id = n.workflow_id AND n.is_deleted = FALSE
                WHERE w.workflow_base_id = %s AND w.is_current_version = TRUE
                GROUP BY w.workflow_id, w.version
            """, workflow_base_id)
            
            if current_version:
                current_node_count = current_version['node_count'] or 0
                logger.info(f"âœ… [å½“å‰ç‰ˆæœ¬] æ‰¾åˆ°å½“å‰ç‰ˆæœ¬: {current_version['workflow_id']}")
                logger.info(f"   - ç‰ˆæœ¬: {current_version['version']}")
                logger.info(f"   - èŠ‚ç‚¹æ•°: {current_node_count}")
                
                if current_node_count > 0:
                    logger.info(f"âœ… [é€‰æ‹©å½“å‰ç‰ˆæœ¬] å½“å‰ç‰ˆæœ¬æœ‰èŠ‚ç‚¹ï¼Œä½¿ç”¨å½“å‰ç‰ˆæœ¬")
                    return current_version['workflow_id']
                else:
                    logger.warning(f"âš ï¸ [å½“å‰ç‰ˆæœ¬ä¸ºç©º] å½“å‰ç‰ˆæœ¬æ— èŠ‚ç‚¹ï¼Œå¯»æ‰¾æœ‰èŠ‚ç‚¹çš„ç‰ˆæœ¬")
            
            # å¦‚æœå½“å‰ç‰ˆæœ¬ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾æœ‰èŠ‚ç‚¹çš„æœ€æ–°ç‰ˆæœ¬
            best_version = await self.db.fetch_one("""
                SELECT w.workflow_id, w.version, w.is_current_version, COUNT(n.node_id) as node_count
                FROM workflow w
                LEFT JOIN node n ON w.workflow_id = n.workflow_id AND n.is_deleted = FALSE
                WHERE w.workflow_base_id = %s AND w.is_deleted = FALSE
                GROUP BY w.workflow_id, w.version, w.is_current_version
                HAVING node_count > 0
                ORDER BY w.version DESC, w.created_at DESC
                LIMIT 1
            """, workflow_base_id)
            
            if best_version:
                best_node_count = best_version['node_count'] or 0
                is_current = "âœ“å½“å‰" if best_version['is_current_version'] else "å†å²"
                logger.info(f"ğŸ”§ [æ‰¾åˆ°æœ‰èŠ‚ç‚¹ç‰ˆæœ¬] {is_current}ç‰ˆæœ¬: {best_version['workflow_id']}")
                logger.info(f"   - ç‰ˆæœ¬: {best_version['version']}")
                logger.info(f"   - èŠ‚ç‚¹æ•°: {best_node_count}")
                logger.info(f"   - å°†ä½¿ç”¨æ­¤ç‰ˆæœ¬è¿›è¡Œåˆå¹¶")
                return best_version['workflow_id']
            
            logger.error(f"âŒ [æ— å¯ç”¨ç‰ˆæœ¬] æ‰¾ä¸åˆ°ä»»ä½•æœ‰èŠ‚ç‚¹çš„å·¥ä½œæµç‰ˆæœ¬")
            return None
            
        except Exception as e:
            logger.error(f"âŒ [æ™ºèƒ½ç‰ˆæœ¬é€‰æ‹©] é€‰æ‹©å¤±è´¥: {e}")
            return None
            
    async def _get_current_workflow_id_by_base(self, workflow_base_id: str) -> Optional[str]:
        """æ ¹æ®workflow_base_idè·å–å½“å‰ç‰ˆæœ¬çš„workflow_id"""
        try:
            result = await self.db.fetch_one("""
                SELECT workflow_id FROM workflow 
                WHERE workflow_base_id = %s AND is_current_version = TRUE
            """, workflow_base_id)
            
            return result['workflow_id'] if result else None
            
        except Exception as e:
            logger.error(f"è·å–å½“å‰å·¥ä½œæµIDå¤±è´¥: {e}")
            return None
    
    async def _debug_workflow_versions(self, workflow_base_id: str, stage: str):
        """è°ƒè¯•ï¼šåˆ†æå·¥ä½œæµçš„æ‰€æœ‰ç‰ˆæœ¬åŠå…¶èŠ‚ç‚¹æƒ…å†µ"""
        try:
            logger.info(f"ğŸ” [ç‰ˆæœ¬åˆ†æ-{stage}] åˆ†æå·¥ä½œæµåŸºç¡€ID: {workflow_base_id}")
            
            # æŸ¥è¯¢è¯¥workflow_base_idä¸‹çš„æ‰€æœ‰ç‰ˆæœ¬
            all_versions = await self.db.fetch_all("""
                SELECT w.workflow_id, w.workflow_base_id, w.name, w.description, w.version,
                       w.creator_id, w.is_current_version, w.created_at, w.is_deleted,
                       COUNT(n.node_id) as node_count
                FROM workflow w
                LEFT JOIN node n ON w.workflow_id = n.workflow_id AND n.is_deleted = FALSE
                WHERE w.workflow_base_id = %s
                GROUP BY w.workflow_id, w.workflow_base_id, w.name, w.description, w.version,
                         w.creator_id, w.is_current_version, w.created_at, w.is_deleted
                ORDER BY w.version DESC, w.created_at DESC
            """, workflow_base_id)
            
            logger.info(f"ğŸ“Š [ç‰ˆæœ¬ç»Ÿè®¡] æ‰¾åˆ° {len(all_versions)} ä¸ªç‰ˆæœ¬:")
            
            current_version = None
            best_version_with_nodes = None
            
            for version in all_versions:
                is_current = "âœ“å½“å‰" if version['is_current_version'] else ""
                is_deleted = "å·²åˆ é™¤" if version['is_deleted'] else "æ´»è·ƒ"
                node_count = version['node_count'] or 0
                
                logger.info(f"   ç‰ˆæœ¬ {version['version']}: {node_count}ä¸ªèŠ‚ç‚¹ {is_current} {is_deleted}")
                logger.info(f"     - å·¥ä½œæµID: {version['workflow_id']}")
                logger.info(f"     - åç§°: {version['name']}")
                logger.info(f"     - åˆ›å»ºæ—¶é—´: {version['created_at']}")
                
                if version['is_current_version']:
                    current_version = version
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰èŠ‚ç‚¹çš„ç‰ˆæœ¬ä½œä¸ºæœ€ä½³å€™é€‰
                if node_count > 0 and not best_version_with_nodes:
                    best_version_with_nodes = version
            
            # åˆ†æç»“æœ
            if current_version:
                current_node_count = current_version['node_count'] or 0
                logger.info(f"âœ… [å½“å‰ç‰ˆæœ¬åˆ†æ] å½“å‰ç‰ˆæœ¬æœ‰ {current_node_count} ä¸ªèŠ‚ç‚¹")
                
                if current_node_count == 0:
                    logger.warning(f"âš ï¸ [ç‰ˆæœ¬é—®é¢˜] å½“å‰ç‰ˆæœ¬ä¸ºç©ºå·¥ä½œæµ!")
                    
                    if best_version_with_nodes:
                        logger.info(f"ğŸ”§ [å»ºè®®] å‘ç°æœ‰èŠ‚ç‚¹çš„ç‰ˆæœ¬:")
                        logger.info(f"   - ç‰ˆæœ¬ {best_version_with_nodes['version']}: {best_version_with_nodes['node_count']}ä¸ªèŠ‚ç‚¹")
                        logger.info(f"   - å·¥ä½œæµID: {best_version_with_nodes['workflow_id']}")
                        logger.info(f"   - å¯ä»¥è€ƒè™‘ä½¿ç”¨æ­¤ç‰ˆæœ¬è¿›è¡Œåˆå¹¶")
                    else:
                        logger.error(f"âŒ [ä¸¥é‡é—®é¢˜] æ‰€æœ‰ç‰ˆæœ¬éƒ½æ²¡æœ‰èŠ‚ç‚¹!")
            else:
                logger.error(f"âŒ [ç‰ˆæœ¬é”™è¯¯] æ‰¾ä¸åˆ°å½“å‰ç‰ˆæœ¬!")
            
            return {
                'current_version': current_version,
                'best_version_with_nodes': best_version_with_nodes,
                'all_versions': all_versions
            }
            
        except Exception as e:
            logger.error(f"âŒ [ç‰ˆæœ¬åˆ†æ] åˆ†æå¤±è´¥: {e}")
            return None
    
    async def _debug_current_workflow_state(self, workflow_id: str, stage: str):
        """è°ƒè¯•ï¼šæ˜¾ç¤ºå½“å‰å·¥ä½œæµçŠ¶æ€"""
        try:
            logger.info(f"ğŸ” [å·¥ä½œæµçŠ¶æ€-{stage}] åˆ†æå·¥ä½œæµ: {workflow_id}")
            
            # æŸ¥è¯¢å·¥ä½œæµåŸºæœ¬ä¿¡æ¯
            workflow_info = await self.db.fetch_one("""
                SELECT w.workflow_id, w.workflow_base_id, w.name, w.description, w.version,
                       w.creator_id, w.is_current_version, w.created_at
                FROM workflow w
                WHERE w.workflow_id = %s
            """, workflow_id)
            
            if workflow_info:
                logger.info(f"   ğŸ“‹ [å·¥ä½œæµä¿¡æ¯]")
                logger.info(f"     - å·¥ä½œæµID: {workflow_info['workflow_id']}")
                logger.info(f"     - å·¥ä½œæµåŸºç¡€ID: {workflow_info['workflow_base_id']}")
                logger.info(f"     - åç§°: {workflow_info['name']}")
                logger.info(f"     - æè¿°: {workflow_info['description']}")
                logger.info(f"     - ç‰ˆæœ¬: {workflow_info['version']}")
                logger.info(f"     - æ˜¯å¦å½“å‰ç‰ˆæœ¬: {workflow_info['is_current_version']}")
                logger.info(f"     - åˆ›å»ºæ—¶é—´: {workflow_info['created_at']}")
            else:
                logger.warning(f"   âŒ æ‰¾ä¸åˆ°å·¥ä½œæµä¿¡æ¯: {workflow_id}")
                return
            
            # æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹
            nodes = await self.db.fetch_all("""
                SELECT n.node_id, n.node_base_id, n.name, n.type, n.task_description,
                       n.position_x, n.position_y, n.version, n.is_current_version,
                       n.created_at, n.is_deleted
                FROM node n
                WHERE n.workflow_id = %s
                ORDER BY n.position_x, n.position_y, n.name
            """, workflow_id)
            
            logger.info(f"   ğŸ“Š [èŠ‚ç‚¹ç»Ÿè®¡] æ€»èŠ‚ç‚¹æ•°: {len(nodes)}")
            
            # æŒ‰ç±»å‹åˆ†ç±»èŠ‚ç‚¹
            nodes_by_type = {}
            for node in nodes:
                node_type = node['type']
                if node_type not in nodes_by_type:
                    nodes_by_type[node_type] = []
                nodes_by_type[node_type].append(node)
            
            for node_type, type_nodes in nodes_by_type.items():
                logger.info(f"     - {node_type}ç±»å‹: {len(type_nodes)}ä¸ª")
            
            # è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹
            logger.info(f"   ğŸ“‹ [èŠ‚ç‚¹è¯¦æƒ…]")
            for i, node in enumerate(nodes):
                status_info = f"v{node['version']}"
                if node['is_current_version']:
                    status_info += " (å½“å‰ç‰ˆæœ¬)"
                if node['is_deleted']:
                    status_info += " (å·²åˆ é™¤)"
                    
                logger.info(f"     èŠ‚ç‚¹ {i+1}: {node['name']} ({node['type']})")
                logger.info(f"       - èŠ‚ç‚¹ID: {node['node_id']}")
                logger.info(f"       - èŠ‚ç‚¹åŸºç¡€ID: {node['node_base_id']}")
                logger.info(f"       - ä½ç½®: ({node['position_x']}, {node['position_y']})")
                logger.info(f"       - çŠ¶æ€: {status_info}")
                if node['task_description']:
                    logger.info(f"       - æè¿°: {node['task_description']}")
            
            # æŸ¥è¯¢è¿æ¥
            connections = await self.db.fetch_all("""
                SELECT nc.from_node_id, nc.to_node_id, nc.connection_type, nc.condition_config,
                       from_n.name as from_node_name, from_n.type as from_node_type,
                       to_n.name as to_node_name, to_n.type as to_node_type
                FROM node_connection nc
                JOIN node from_n ON nc.from_node_id = from_n.node_id
                JOIN node to_n ON nc.to_node_id = to_n.node_id
                WHERE nc.workflow_id = %s
                ORDER BY from_n.name, to_n.name
            """, workflow_id)
            
            logger.info(f"   ğŸ”— [è¿æ¥ç»Ÿè®¡] æ€»è¿æ¥æ•°: {len(connections)}")
            logger.info(f"   ğŸ“‹ [è¿æ¥è¯¦æƒ…]")
            for i, conn in enumerate(connections):
                conn_type = conn.get('connection_type', 'normal')
                condition = conn.get('condition_config', '')
                condition_info = f" (æ¡ä»¶: {condition})" if condition else ""
                
                logger.info(f"     è¿æ¥ {i+1}: {conn['from_node_name']} -> {conn['to_node_name']} (ç±»å‹: {conn_type}){condition_info}")
                logger.info(f"       - ä»èŠ‚ç‚¹: {conn['from_node_id']} ({conn['from_node_type']})")
                logger.info(f"       - åˆ°èŠ‚ç‚¹: {conn['to_node_id']} ({conn['to_node_type']})")
            
            if not connections:
                logger.info(f"     âš ï¸ è¯¥å·¥ä½œæµæš‚æ— è¿æ¥")
                
        except Exception as e:
            logger.error(f"âŒ [å·¥ä½œæµçŠ¶æ€è°ƒè¯•] åˆ†æå¤±è´¥: {e}")
    
    async def _create_layered_workflow_record(self, parent_workflow_base_id: str,
                                            new_workflow_base_id: uuid.UUID,
                                            depth: int, merge_count: int,
                                            creator_id: uuid.UUID) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†å±‚åˆå¹¶çš„å·¥ä½œæµè®°å½•"""
        try:
            # è·å–çˆ¶å·¥ä½œæµåç§°
            parent_workflow = await self.db.fetch_one("""
                SELECT name FROM workflow 
                WHERE workflow_base_id = %s AND is_current_version = TRUE
            """, parent_workflow_base_id)
            
            parent_name = parent_workflow['name'] if parent_workflow else "Unknown_Workflow"
            
            # ç”Ÿæˆåˆ†å±‚åˆå¹¶çš„å·¥ä½œæµåç§°
            new_workflow_id = uuid.uuid4()
            merged_name = f"{parent_name}_åˆå¹¶_æ·±åº¦{depth}_{merge_count}é¡¹"
            merged_description = f"åˆ†å±‚åˆå¹¶æ·±åº¦{depth}çš„{merge_count}ä¸ªsubdivisionï¼ŒåŸºäº{parent_name}"
            
            await self.db.execute("""
                INSERT INTO workflow (
                    workflow_id, workflow_base_id, name, description, 
                    creator_id, is_current_version, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, new_workflow_id, new_workflow_base_id, merged_name, merged_description,
                 creator_id, True, now_utc())
            
            logger.info(f"âœ… [å·¥ä½œæµè®°å½•] åˆ›å»ºåˆ†å±‚åˆå¹¶å·¥ä½œæµ: {merged_name}")
            
            return {
                "workflow_id": str(new_workflow_id),
                "workflow_base_id": str(new_workflow_base_id),
                "name": merged_name,
                "description": merged_description
            }
            
        except Exception as e:
            logger.error(f"âŒ [å·¥ä½œæµè®°å½•] åˆ›å»ºåˆ†å±‚åˆå¹¶å·¥ä½œæµå¤±è´¥: {e}")
            raise
    
    async def _analyze_subworkflow_structure(self, candidate_workflow_instance_id: str, 
                                           center_x: int, center_y: int) -> Dict[str, Any]:
        """
        åˆ†æå­å·¥ä½œæµç»“æ„ï¼Œè¯†åˆ«å…¥å£ã€å‡ºå£å’Œä¸šåŠ¡èŠ‚ç‚¹
        
        ä¿®å¤ï¼šä»å·¥ä½œæµå®ä¾‹ä¸­è·å–å®é™…æ‰§è¡Œçš„èŠ‚ç‚¹æ•°æ®ï¼Œè€Œä¸æ˜¯ä»æ¨¡æ¿ä¸­è·å–
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹åˆ†æå­å·¥ä½œæµç»“æ„: {candidate_workflow_instance_id}")
            
            # ğŸ”§ ä¿®å¤ï¼šä»å·¥ä½œæµå®ä¾‹ä¸­è·å–å®é™…èŠ‚ç‚¹ï¼ˆnode_instanceï¼‰ï¼Œè€Œä¸æ˜¯æ¨¡æ¿èŠ‚ç‚¹
            all_nodes = await self.db.fetch_all("""
                SELECT ni.node_instance_id, ni.node_id, n.node_base_id, n.name, n.type, 
                       n.position_x, n.position_y, n.task_description, n.version,
                       ti.status as task_status
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                LEFT JOIN task_instance ti ON ni.node_instance_id = ti.node_instance_id
                WHERE ni.workflow_instance_id = %s AND ni.is_deleted = FALSE 
                ORDER BY n.position_x, n.position_y
            """, candidate_workflow_instance_id)
            
            logger.info(f"   ğŸ“‹ å­å·¥ä½œæµå®ä¾‹æ€»èŠ‚ç‚¹æ•°: {len(all_nodes)}")
            
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ç©ºç‰ˆæœ¬é—®é¢˜ï¼Œå½“å‰ç‰ˆæœ¬ä¸ºç©ºæ—¶å›é€€åˆ°æœ‰æ•°æ®çš„ç‰ˆæœ¬
            actual_workflow_info = await self.db.fetch_one("""
                SELECT DISTINCT n.workflow_id 
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.workflow_instance_id = %s 
                LIMIT 1
            """, candidate_workflow_instance_id)
            
            if actual_workflow_info:
                sub_workflow_id = actual_workflow_info['workflow_id']
                logger.info(f"   ğŸ” å®é™…å­å·¥ä½œæµæ¨¡æ¿ID: {sub_workflow_id}")
            else:
                # å¦‚æœé€šè¿‡èŠ‚ç‚¹å®ä¾‹æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡å·¥ä½œæµåŸºç¡€IDæ‰¾åˆ°æœ‰æ•°æ®çš„ç‰ˆæœ¬
                logger.warning(f"   âš ï¸ æ— æ³•é€šè¿‡èŠ‚ç‚¹å®ä¾‹æ‰¾åˆ°å·¥ä½œæµæ¨¡æ¿ï¼Œå°è¯•æŸ¥æ‰¾æœ‰æ•°æ®çš„ç‰ˆæœ¬")
                
                workflow_instance_info = await self.db.fetch_one("""
                    SELECT workflow_base_id FROM workflow_instance 
                    WHERE workflow_instance_id = %s
                """, candidate_workflow_instance_id)
                
                if workflow_instance_info:
                    base_id = workflow_instance_info['workflow_base_id']
                    # æŸ¥æ‰¾è¯¥åŸºç¡€IDä¸‹æœ‰èŠ‚ç‚¹æ•°æ®çš„ç‰ˆæœ¬
                    workflow_with_data = await self.db.fetch_one("""
                        SELECT DISTINCT w.workflow_id, w.version, COUNT(n.node_id) as node_count
                        FROM workflow w
                        JOIN node n ON w.workflow_id = n.workflow_id
                        WHERE w.workflow_base_id = %s AND w.is_deleted = FALSE
                        GROUP BY w.workflow_id, w.version
                        HAVING node_count > 0
                        ORDER BY w.version DESC
                        LIMIT 1
                    """, base_id)
                    
                    if workflow_with_data:
                        sub_workflow_id = workflow_with_data['workflow_id']
                        logger.info(f"   ğŸ”§ æ‰¾åˆ°æœ‰æ•°æ®çš„ç‰ˆæœ¬ {workflow_with_data['version']}: {sub_workflow_id}")
                    else:
                        logger.error(f"   âŒ æ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•°æ®çš„å·¥ä½œæµç‰ˆæœ¬: {base_id}")
                        sub_workflow_id = None
                else:
                    logger.error(f"   âŒ æ‰¾ä¸åˆ°å·¥ä½œæµå®ä¾‹ä¿¡æ¯: {candidate_workflow_instance_id}")
                    sub_workflow_id = None
            
            if sub_workflow_id:
                # ğŸ”§ ä¿®å¤ï¼šæ”¹è¿›è¿æ¥æŸ¥è¯¢ï¼Œç¡®ä¿è·å–å­å·¥ä½œæµçš„æ‰€æœ‰è¿æ¥
                all_connections = await self.db.fetch_all("""
                    SELECT 
                        nc.from_node_id,
                        nc.to_node_id,
                        from_n.name as from_node_name,
                        to_n.name as to_node_name,
                        from_n.type as from_node_type,
                        to_n.type as to_node_type,
                        nc.connection_type,
                        nc.condition_config
                    FROM node_connection nc
                    JOIN node from_n ON nc.from_node_id = from_n.node_id
                    JOIN node to_n ON nc.to_node_id = to_n.node_id
                    WHERE nc.workflow_id = %s
                    ORDER BY from_n.position_x, from_n.position_y, to_n.position_x, to_n.position_y
                """, sub_workflow_id)
                
                # ğŸ”§ è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰æ‰¾åˆ°çš„è¿æ¥
                logger.info(f"   ğŸ”— [è¿æ¥è¯¦æƒ…] å­å·¥ä½œæµè¿æ¥åˆ†æ:")
                for i, conn in enumerate(all_connections):
                    logger.info(f"     è¿æ¥{i+1}: {conn['from_node_name']}({conn['from_node_type']}) -> {conn['to_node_name']}({conn['to_node_type']})")
                    logger.info(f"       from_node_id: {conn['from_node_id']}")
                    logger.info(f"       to_node_id: {conn['to_node_id']}")
            else:
                all_connections = []
            
            logger.info(f"   ğŸ”— å­å·¥ä½œæµå®ä¾‹æ€»è¿æ¥æ•°: {len(all_connections)}")
            
            # æŒ‰ç±»å‹åˆ†ç±»èŠ‚ç‚¹
            start_nodes = [n for n in all_nodes if n['type'] == 'start']
            end_nodes = [n for n in all_nodes if n['type'] == 'end']
            business_nodes = [n for n in all_nodes if n['type'] not in ('start', 'end')]
            
            logger.info(f"   ğŸ“Š èŠ‚ç‚¹åˆ†ç±»: {len(start_nodes)}ä¸ªå¼€å§‹, {len(business_nodes)}ä¸ªä¸šåŠ¡, {len(end_nodes)}ä¸ªç»“æŸ")
            
            if not business_nodes:
                logger.warning(f"   âš ï¸ å­å·¥ä½œæµå®ä¾‹æ²¡æœ‰ä¸šåŠ¡èŠ‚ç‚¹")
                return {
                    "business_nodes": [],
                    "entry_points": [],
                    "exit_points": [],
                    "business_connections": [],
                    "start_to_entry_connections": [],
                    "exit_to_end_connections": [],
                    "analysis_stats": {
                        "total_nodes": len(all_nodes),
                        "business_nodes": 0,
                        "start_nodes": len(start_nodes),
                        "end_nodes": len(end_nodes),
                        "total_connections": len(all_connections)
                    }
                }
            
            # æ„å»ºè¿æ¥å›¾ - åŸºäºèŠ‚ç‚¹æ¨¡æ¿ID
            outgoing = {}  # from_node_id -> [connection_info, ...]
            incoming = {}  # to_node_id -> [connection_info, ...]
            
            for conn in all_connections:
                from_id, to_id = conn['from_node_id'], conn['to_node_id']
                outgoing.setdefault(from_id, []).append(conn)
                incoming.setdefault(to_id, []).append(conn)
            
            # è¯†åˆ«å…¥å£èŠ‚ç‚¹ï¼šä»startèŠ‚ç‚¹ç›´æ¥æˆ–é—´æ¥å¯è¾¾çš„ä¸šåŠ¡èŠ‚ç‚¹
            entry_points = self._find_entry_points_enhanced(start_nodes, business_nodes, outgoing, incoming)
            logger.info(f"   ğŸ“¥ è¯†åˆ«å‡º {len(entry_points)} ä¸ªå…¥å£èŠ‚ç‚¹: {[n['name'] for n in entry_points]}")
            
            # è¯†åˆ«å‡ºå£èŠ‚ç‚¹ï¼šå¯ä»¥åˆ°è¾¾endèŠ‚ç‚¹çš„ä¸šåŠ¡èŠ‚ç‚¹
            exit_points = self._find_exit_points_enhanced(end_nodes, business_nodes, incoming, outgoing)
            logger.info(f"   ğŸ“¤ è¯†åˆ«å‡º {len(exit_points)} ä¸ªå‡ºå£èŠ‚ç‚¹: {[n['name'] for n in exit_points]}")
            
            # è®¡ç®—èŠ‚ç‚¹ä½ç½®åç§»ï¼ˆç›¸å¯¹äºåŸsubdivisionèŠ‚ç‚¹ä½ç½®ï¼‰
            positioned_nodes = self._calculate_node_positions(
                business_nodes, center_x, center_y
            )
            
            # åˆ†ç±»è¿æ¥
            business_connections = []
            start_to_entry_connections = []
            exit_to_end_connections = []
            
            business_node_ids = {n['node_id'] for n in business_nodes}
            start_node_ids = {n['node_id'] for n in start_nodes}
            end_node_ids = {n['node_id'] for n in end_nodes}
            entry_point_ids = {n['node_id'] for n in entry_points}
            exit_point_ids = {n['node_id'] for n in exit_points}
            
            # ğŸ”§ ä¿®å¤ï¼šç®€åŒ–è¿æ¥åˆ†ç±»é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ¿è¿æ¥æ•°æ®
            for conn in all_connections:
                from_node_id = conn['from_node_id']
                to_node_id = conn['to_node_id']
                
                # åˆ›å»ºæ ‡å‡†åŒ–çš„è¿æ¥å¯¹è±¡
                normalized_conn = {
                    'from_node_id': from_node_id,
                    'to_node_id': to_node_id,
                    'connection_type': conn.get('connection_type', 'normal'),
                    'condition_config': conn.get('condition_config')
                }
                
                # ä¸šåŠ¡èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
                if from_node_id in business_node_ids and to_node_id in business_node_ids:
                    business_connections.append(normalized_conn)
                    logger.info(f"      ğŸ“‹ ä¸šåŠ¡è¿æ¥: {conn['from_node_name']} -> {conn['to_node_name']}")
                # start -> entry çš„è¿æ¥
                elif from_node_id in start_node_ids and to_node_id in entry_point_ids:
                    start_to_entry_connections.append(normalized_conn)
                    logger.info(f"      ğŸ“‹ å¯åŠ¨è¿æ¥: {conn['from_node_name']} -> {conn['to_node_name']}")
                # exit -> end çš„è¿æ¥
                elif from_node_id in exit_point_ids and to_node_id in end_node_ids:
                    exit_to_end_connections.append(normalized_conn)
                    logger.info(f"      ğŸ“‹ ç»“æŸè¿æ¥: {conn['from_node_name']} -> {conn['to_node_name']}")
            
            logger.info(f"   ğŸ”— è¿æ¥åˆ†ç±»:")
            logger.info(f"      - ä¸šåŠ¡è¿æ¥: {len(business_connections)}ä¸ª")
            logger.info(f"      - start->entryè¿æ¥: {len(start_to_entry_connections)}ä¸ª")
            logger.info(f"      - exit->endè¿æ¥: {len(exit_to_end_connections)}ä¸ª")
            
            analysis_stats = {
                "total_nodes": len(all_nodes),
                "business_nodes": len(business_nodes),
                "start_nodes": len(start_nodes),
                "end_nodes": len(end_nodes),
                "entry_points": len(entry_points),
                "exit_points": len(exit_points),
                "total_connections": len(all_connections),
                "business_connections": len(business_connections),
                "boundary_connections": len(start_to_entry_connections) + len(exit_to_end_connections)
            }
            
            logger.info(f"âœ… å­å·¥ä½œæµç»“æ„åˆ†æå®Œæˆ: {analysis_stats}")
            
            return {
                "business_nodes": positioned_nodes,
                "entry_points": entry_points,
                "exit_points": exit_points,
                "business_connections": business_connections,
                "start_to_entry_connections": start_to_entry_connections,
                "exit_to_end_connections": exit_to_end_connections,
                "analysis_stats": analysis_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå­å·¥ä½œæµç»“æ„å¤±è´¥: {e}")
            raise
    
    def _find_entry_points_enhanced(self, start_nodes: List[Dict], business_nodes: List[Dict], 
                                   outgoing: Dict, incoming: Dict = None) -> List[Dict]:
        """å¢å¼ºç‰ˆå…¥å£ç‚¹æŸ¥æ‰¾ - æ”¯æŒå¤æ‚çš„å…¥å£æ¨¡å¼"""
        entry_points = []
        start_node_ids = {n['node_id'] for n in start_nodes}
        business_node_ids = {n['node_id'] for n in business_nodes}
        
        # æ–¹æ³•1: ç›´æ¥ä»startèŠ‚ç‚¹è¿æ¥çš„ä¸šåŠ¡èŠ‚ç‚¹
        for start_id in start_node_ids:
            if start_id in outgoing:
                for conn in outgoing[start_id]:
                    to_id = conn['to_node_id']
                    if to_id in business_node_ids:
                        entry_node = next(n for n in business_nodes if n['node_id'] == to_id)
                        if entry_node not in entry_points:
                            entry_points.append(entry_node)
                            logger.info(f"      æ‰¾åˆ°ç›´æ¥å…¥å£ç‚¹: {entry_node['name']} (from start)")
        
        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰ç›´æ¥è¿æ¥ï¼Œæ‰¾æ²¡æœ‰ä¸šåŠ¡å‰é©±çš„ä¸šåŠ¡èŠ‚ç‚¹
        if not entry_points and incoming:
            for node in business_nodes:
                node_id = node['node_id']
                has_business_predecessor = False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¥è‡ªå…¶ä»–ä¸šåŠ¡èŠ‚ç‚¹çš„è¿æ¥
                if node_id in incoming:
                    for conn in incoming.get(node_id, []):
                        if conn['from_node_id'] in business_node_ids:
                            has_business_predecessor = True
                            break
                
                if not has_business_predecessor:
                    entry_points.append(node)
                    logger.info(f"      æ‰¾åˆ°é—´æ¥å…¥å£ç‚¹: {node['name']} (no business predecessor)")
        
        # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œé€‰æ‹©ä½ç½®æœ€å‰çš„èŠ‚ç‚¹ä½œä¸ºå…¥å£ç‚¹
        if not entry_points and business_nodes:
            entry_points = [business_nodes[0]]
            logger.info(f"      ä½¿ç”¨é»˜è®¤å…¥å£ç‚¹: {business_nodes[0]['name']} (first node)")
            
        return entry_points
    
    def _find_exit_points_enhanced(self, end_nodes: List[Dict], business_nodes: List[Dict],
                                 incoming: Dict, outgoing: Dict = None) -> List[Dict]:
        """å¢å¼ºç‰ˆå‡ºå£ç‚¹æŸ¥æ‰¾ - æ”¯æŒå¤æ‚çš„å‡ºå£æ¨¡å¼"""
        exit_points = []
        end_node_ids = {n['node_id'] for n in end_nodes}
        business_node_ids = {n['node_id'] for n in business_nodes}
        
        # æ–¹æ³•1: ç›´æ¥è¿æ¥åˆ°endèŠ‚ç‚¹çš„ä¸šåŠ¡èŠ‚ç‚¹
        for end_id in end_node_ids:
            if end_id in incoming:
                for conn in incoming[end_id]:
                    from_id = conn['from_node_id']
                    if from_id in business_node_ids:
                        exit_node = next(n for n in business_nodes if n['node_id'] == from_id)
                        if exit_node not in exit_points:
                            exit_points.append(exit_node)
                            logger.info(f"      æ‰¾åˆ°ç›´æ¥å‡ºå£ç‚¹: {exit_node['name']} (to end)")
        
        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰ç›´æ¥è¿æ¥ï¼Œæ‰¾æ²¡æœ‰ä¸šåŠ¡åç»§çš„ä¸šåŠ¡èŠ‚ç‚¹
        if not exit_points and outgoing:
            for node in business_nodes:
                node_id = node['node_id']
                has_business_successor = False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ°å…¶ä»–ä¸šåŠ¡èŠ‚ç‚¹çš„è¿æ¥
                for conn in outgoing.get(node_id, []):
                    if conn['to_node_id'] in business_node_ids:
                        has_business_successor = True
                        break
                
                if not has_business_successor:
                    exit_points.append(node)
                    logger.info(f"      æ‰¾åˆ°é—´æ¥å‡ºå£ç‚¹: {node['name']} (no business successor)")
        
        # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œé€‰æ‹©ä½ç½®æœ€åçš„èŠ‚ç‚¹ä½œä¸ºå‡ºå£ç‚¹
        if not exit_points and business_nodes:
            exit_points = [business_nodes[-1]]
            logger.info(f"      ä½¿ç”¨é»˜è®¤å‡ºå£ç‚¹: {business_nodes[-1]['name']} (last node)")
            
        return exit_points
    
    def _calculate_node_positions(self, nodes: List[Dict], center_x: int, center_y: int) -> List[Dict]:
        """è®¡ç®—èŠ‚ç‚¹åœ¨åˆå¹¶åå·¥ä½œæµä¸­çš„ä½ç½®"""
        if not nodes:
            return []
        
        # è®¡ç®—åŸå§‹èŠ‚ç‚¹çš„è¾¹ç•Œæ¡†
        min_x = min(n['position_x'] for n in nodes)
        max_x = max(n['position_x'] for n in nodes)
        min_y = min(n['position_y'] for n in nodes)
        max_y = max(n['position_y'] for n in nodes)
        
        # è®¡ç®—åç§»é‡ï¼Œä½¿å­å·¥ä½œæµå±…ä¸­äºåŸsubdivisionèŠ‚ç‚¹ä½ç½®
        offset_x = center_x - (min_x + max_x) // 2
        offset_y = center_y - (min_y + max_y) // 2
        
        # åº”ç”¨åç§»
        positioned_nodes = []
        for node in nodes:
            positioned_node = node.copy()
            positioned_node['position_x'] = node['position_x'] + offset_x
            positioned_node['position_y'] = node['position_y'] + offset_y
            positioned_nodes.append(positioned_node)
        
        return positioned_nodes

    async def _get_original_node_info(self, subdivision_id: str) -> Optional[Dict[str, Any]]:
        """è·å–è¢«subdivisionçš„åŸå§‹èŠ‚ç‚¹ä¿¡æ¯"""
        logger.info(f"ğŸ” æŸ¥æ‰¾subdivisionåŸå§‹èŠ‚ç‚¹ä¿¡æ¯: {subdivision_id}")
        
        # é¦–å…ˆå°è¯•é€šè¿‡ä¸åŒçš„æŸ¥è¯¢æ–¹å¼æ‰¾åˆ°subdivisionè®°å½•
        subdivision_record = None
        
        # æ–¹æ³•1: ç›´æ¥åŒ¹é…UUIDå­—ç¬¦ä¸²
        subdivision_record = await self.db.fetch_one("""
            SELECT subdivision_id, original_task_id, created_at 
            FROM task_subdivision 
            WHERE CAST(subdivision_id AS CHAR) = %s
        """, subdivision_id)
        
        if not subdivision_record:
            # æ–¹æ³•2: å°è¯•UUIDè½¬æ¢
            try:
                import uuid as uuid_lib
                subdivision_uuid = uuid_lib.UUID(subdivision_id)
                subdivision_record = await self.db.fetch_one("""
                    SELECT subdivision_id, original_task_id, created_at 
                    FROM task_subdivision 
                    WHERE subdivision_id = %s
                """, subdivision_uuid)
            except ValueError:
                logger.info(f"   subdivision_idä¸æ˜¯æœ‰æ•ˆçš„UUIDæ ¼å¼: {subdivision_id}")
        
        if not subdivision_record:
            # æ–¹æ³•3: æ¨¡ç³ŠåŒ¹é…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            subdivision_record = await self.db.fetch_one("""
                SELECT subdivision_id, original_task_id, created_at 
                FROM task_subdivision 
                WHERE CAST(subdivision_id AS CHAR) LIKE %s
            """, f"%{subdivision_id}%")
        
        logger.info(f"   subdivisionè®°å½•: {subdivision_record}")
        
        if not subdivision_record:
            logger.warning(f"   âŒ åœ¨task_subdivisionè¡¨ä¸­æ‰¾ä¸åˆ°subdivision: {subdivision_id}")
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºtask_subdivisionè¡¨ä¸­çš„æ‰€æœ‰è®°å½•
            all_subdivisions = await self.db.fetch_all("""
                SELECT CAST(subdivision_id AS CHAR) as subdivision_id_str, 
                       CAST(original_task_id AS CHAR) as original_task_id_str,
                       created_at 
                FROM task_subdivision 
                WHERE is_deleted = FALSE
                ORDER BY created_at DESC
                LIMIT 10
            """)
            logger.info(f"   è°ƒè¯•ï¼šæœ€è¿‘çš„10ä¸ªsubdivisionè®°å½•:")
            for sub in all_subdivisions:
                logger.info(f"     - {sub['subdivision_id_str']} -> {sub['original_task_id_str']}")
            
            return None
            
        # è·å–original_task_id
        original_task_id = subdivision_record['original_task_id']
        logger.info(f"   åŸå§‹ä»»åŠ¡ID: {original_task_id}")
        
        # æŸ¥æ‰¾å¯¹åº”çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œé€šè¿‡task_instance -> node_instance -> nodeè·¯å¾„
        node_info = await self.db.fetch_one("""
            SELECT 
                ti.task_instance_id,
                ni.node_instance_id,
                n.node_id, n.position_x, n.position_y, n.name, n.type, n.task_description,
                n.workflow_id, w.name as workflow_name
            FROM task_instance ti
            JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id  
            JOIN node n ON ni.node_id = n.node_id
            JOIN workflow w ON n.workflow_id = w.workflow_id
            WHERE ti.task_instance_id = %s
        """, original_task_id)
        
        logger.info(f"   èŠ‚ç‚¹ä¿¡æ¯: {node_info}")
        
        if node_info:
            # åˆå¹¶ä¿¡æ¯
            result = {
                'original_task_id': original_task_id,
                'node_id': node_info['node_id'],  # ğŸ”§ æ·»åŠ node_idç”¨äºè¿æ¥é‡å»º
                'position_x': node_info['position_x'],
                'position_y': node_info['position_y'],
                'name': node_info['name'],
                'type': node_info['type'],
                'task_description': node_info['task_description'],
                'workflow_id': node_info['workflow_id'],
                'workflow_name': node_info['workflow_name']
            }
            logger.info(f"   âœ… æˆåŠŸæ‰¾åˆ°åŸå§‹èŠ‚ç‚¹ä¿¡æ¯")
            return result
        else:
            logger.warning(f"   âŒ æ‰¾ä¸åˆ°èŠ‚ç‚¹ä¿¡æ¯: {original_task_id}")
            return None

    async def _create_unified_recursive_workflow_record(self, parent_workflow_base_id: str,
                                                      new_workflow_base_id: uuid.UUID,
                                                      total_candidates: int,
                                                      creator_id: uuid.UUID) -> Dict[str, Any]:
        """åˆ›å»ºç»Ÿä¸€é€’å½’åˆå¹¶çš„å·¥ä½œæµè®°å½•"""
        try:
            # è·å–çˆ¶å·¥ä½œæµåç§°
            parent_workflow = await self.db.fetch_one("""
                SELECT name FROM workflow 
                WHERE workflow_base_id = %s AND is_current_version = TRUE
            """, parent_workflow_base_id)
            
            parent_name = parent_workflow['name'] if parent_workflow else "Unknown_Workflow"
            
            # ç”Ÿæˆé€’å½’åˆå¹¶çš„å·¥ä½œæµåç§°
            new_workflow_id = uuid.uuid4()
            merged_name = f"{parent_name}_é€’å½’åˆå¹¶_{total_candidates}é¡¹"
            merged_description = f"é€’å½’åˆå¹¶{total_candidates}ä¸ªsubdivisionåˆ°ç»Ÿä¸€æ¨¡æ¿ï¼ŒåŸºäº{parent_name}"
            
            await self.db.execute("""
                INSERT INTO workflow (
                    workflow_id, workflow_base_id, name, description, 
                    creator_id, is_current_version, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, new_workflow_id, new_workflow_base_id, merged_name, merged_description,
                 creator_id, True, now_utc())
            
            logger.info(f"âœ… [ç»Ÿä¸€å·¥ä½œæµè®°å½•] åˆ›å»ºé€’å½’åˆå¹¶å·¥ä½œæµ: {merged_name}")
            
            return {
                "workflow_id": str(new_workflow_id),
                "workflow_base_id": str(new_workflow_base_id),
                "name": merged_name,
                "description": merged_description
            }
            
        except Exception as e:
            logger.error(f"âŒ [ç»Ÿä¸€å·¥ä½œæµè®°å½•] åˆ›å»ºé€’å½’åˆå¹¶å·¥ä½œæµå¤±è´¥: {e}")
            raise

    async def _execute_unified_recursive_node_replacement(self, parent_workflow_id: str,
                                                        new_workflow_id: uuid.UUID, 
                                                        new_workflow_base_id: uuid.UUID,
                                                        tree_candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿä¸€çš„é€’å½’èŠ‚ç‚¹æ›¿æ¢åˆå¹¶ - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å€™é€‰é¡¹"""
        try:
            logger.info(f"ğŸ”„ [ç»Ÿä¸€é€’å½’æ›¿æ¢] å¼€å§‹å¤„ç† {len(tree_candidates)} ä¸ªå€™é€‰é¡¹")
            
            # 1. æ”¶é›†éœ€è¦æ›¿æ¢çš„èŠ‚ç‚¹ID
            nodes_to_replace = set()
            tree_mapping = {}  # original_node_id -> tree_candidate
            
            logger.info(f"ğŸ” [å€™é€‰é¡¹åˆ†æ] åˆ†æ {len(tree_candidates)} ä¸ªtreeå€™é€‰é¡¹:")
            for candidate in tree_candidates:
                original_node_id = candidate.get('original_node_id')
                subdivision_id = candidate.get('subdivision_id')
                node_name = candidate.get('node_name', 'Unknown')
                
                logger.info(f"     å€™é€‰é¡¹: {node_name} (æ·±åº¦: {candidate.get('depth', 0)})")
                logger.info(f"       - original_node_id: {original_node_id}")
                logger.info(f"       - subdivision_id: {subdivision_id}")
                
                # å¦‚æœtreeä¸­æœ‰original_node_idï¼Œç›´æ¥ä½¿ç”¨
                if original_node_id:
                    nodes_to_replace.add(original_node_id)
                    tree_mapping[original_node_id] = candidate
                    logger.info(f"   ğŸ”§ å°†æ›¿æ¢èŠ‚ç‚¹: {node_name} (node_id: {original_node_id})")
                # å¦åˆ™ï¼Œå›é€€åˆ°subdivisionæŸ¥è¯¢
                elif subdivision_id:
                    logger.info(f"   ğŸ” treeæ•°æ®ä¸å®Œæ•´ï¼ŒæŸ¥è¯¢subdivision: {subdivision_id}")
                    original_node_info = await self._get_original_node_info(subdivision_id)
                    if original_node_info:
                        actual_node_id = original_node_info['node_id']
                        nodes_to_replace.add(actual_node_id)
                        # å°†subdivisionæŸ¥è¯¢ç»“æœè¡¥å……åˆ°candidateä¸­
                        enhanced_candidate = candidate.copy()
                        enhanced_candidate.update({
                            'original_node_id': actual_node_id,
                            'original_task_id': original_node_info.get('original_task_id'),
                            'original_node_position': {
                                'x': original_node_info.get('position_x', 0),
                                'y': original_node_info.get('position_y', 0)
                            },
                            'original_node_info': original_node_info
                        })
                        tree_mapping[actual_node_id] = enhanced_candidate
                        logger.info(f"   ğŸ”§ å°†æ›¿æ¢èŠ‚ç‚¹: {original_node_info['name']} (node_id: {actual_node_id})")
                    else:
                        logger.warning(f"   âŒ æ— æ³•è·å–subdivisionçš„åŸå§‹èŠ‚ç‚¹ä¿¡æ¯: {subdivision_id}")
                else:
                    logger.warning(f"   âš ï¸ å€™é€‰é¡¹ç¼ºå°‘è¯†åˆ«ä¿¡æ¯: {node_name}")
            
            logger.info(f"ğŸ”„ [ç»Ÿä¸€æ›¿æ¢] å°†æ›¿æ¢ {len(nodes_to_replace)} ä¸ªèŠ‚ç‚¹")
            
            # 2. å¤åˆ¶çˆ¶å·¥ä½œæµçš„ä¿ç•™èŠ‚ç‚¹ï¼ˆæ’é™¤è¦æ›¿æ¢çš„èŠ‚ç‚¹ï¼‰
            node_id_mapping = await self._copy_preserved_nodes_simple(
                parent_workflow_id, new_workflow_id, new_workflow_base_id, nodes_to_replace
            )
            
            # 3. æ‰§è¡Œç»Ÿä¸€çš„é€’å½’èŠ‚ç‚¹æ›¿æ¢
            replacement_stats = await self._replace_nodes_with_recursive_expansion(
                new_workflow_id, new_workflow_base_id, tree_mapping, node_id_mapping
            )
            
            # 4. é‡å»ºè¿æ¥ - ğŸ”§ ç»Ÿä¸€é€’å½’åˆå¹¶çš„è¿æ¥é‡å»º  
            connection_stats = await self._rebuild_unified_recursive_connections(
                parent_workflow_id, new_workflow_id, tree_mapping, node_id_mapping
            )
            
            logger.info(f"âœ… [ç»Ÿä¸€é€’å½’æ›¿æ¢] å®Œæˆ: æ›¿æ¢{replacement_stats['nodes_replaced']}èŠ‚ç‚¹, é‡å»º{connection_stats['connections_count']}è¿æ¥")
            
            return {
                **replacement_stats,
                **connection_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ [ç»Ÿä¸€é€’å½’æ›¿æ¢] æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _replace_nodes_with_recursive_expansion(self, new_workflow_id: uuid.UUID, 
                                                    new_workflow_base_id: uuid.UUID,
                                                    tree_mapping: Dict[str, Dict], 
                                                    node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """é€’å½’å±•å¼€æ›¿æ¢èŠ‚ç‚¹ - ä¿®å¤ç‰ˆï¼šçœŸæ­£ç†è§£é€’å½’æ›¿æ¢çš„å«ä¹‰"""
        replaced_nodes = 0
        
        logger.info(f"ğŸ”„ [çœŸé€’å½’ç†è§£] é‡æ–°ç†è§£é€’å½’æ›¿æ¢")
        logger.info(f"ğŸ“Š [å€™é€‰åˆ†æ] å¾…æ›¿æ¢å€™é€‰é¡¹: {len(tree_mapping)}")
        
        # ğŸ”§ å…³é”®ç†è§£ï¼šæ¯ä¸ªå€™é€‰é¡¹ä»£è¡¨ä¸€ä¸ªsubdivisionï¼Œå³ä¸€ä¸ªèŠ‚ç‚¹è¢«å­å·¥ä½œæµæ›¿æ¢
        # çœŸæ­£çš„é€’å½’æ˜¯ï¼šå¦‚æœå­å·¥ä½œæµä¸­çš„èŠ‚ç‚¹ä¹Ÿè¢«subdivisionï¼Œé‚£ä¹ˆç»§ç»­å±•å¼€
        
        for original_node_id, tree_candidate in tree_mapping.items():
            logger.info(f"ğŸ”„ [å€™é€‰é¡¹å¤„ç†] å¤„ç†: {tree_candidate['node_name']} (æ·±åº¦: {tree_candidate.get('depth', 0)})")
            logger.info(f"   subdivision_id: {tree_candidate.get('subdivision_id')}")
            logger.info(f"   workflow_instance_id: {tree_candidate.get('workflow_instance_id')}")
            
            # è·å–å­å·¥ä½œæµç»“æ„
            tree_node = tree_candidate.get('tree_node')
            if not tree_node:
                logger.warning(f"âš ï¸ tree_candidateä¸­æ²¡æœ‰tree_nodeå¼•ç”¨")
                continue
            
            # ä»treeä¸­è·å–åŸå§‹èŠ‚ç‚¹ä½ç½®ä¿¡æ¯    
            original_position = tree_candidate.get('original_node_position', {})
            center_x = original_position.get('x', 0)
            center_y = original_position.get('y', 0)
            logger.info(f"   ğŸ“ [åŸå§‹ä½ç½®] x: {center_x}, y: {center_y}")
            
            # è·å–å­å·¥ä½œæµçš„åŸºç¡€ç»“æ„
            base_structure = await self._analyze_subworkflow_structure_from_tree(
                tree_node, center_x, center_y
            )
            
            logger.info(f"   ğŸ“Š [å­å·¥ä½œæµåŸºç¡€] ä¸šåŠ¡èŠ‚ç‚¹: {len(base_structure['business_nodes'])}ä¸ª")
            for i, node in enumerate(base_structure['business_nodes']):
                logger.info(f"     ä¸šåŠ¡èŠ‚ç‚¹{i+1}: {node['name']} (ID: {node['node_id'][:8]}...)")
            
            # ğŸ”§ å…³é”®é€’å½’é€»è¾‘ï¼šæ£€æŸ¥å­å·¥ä½œæµçš„ä¸šåŠ¡èŠ‚ç‚¹æ˜¯å¦è¿˜éœ€è¦è¿›ä¸€æ­¥å±•å¼€
            # ä¼ é€’æ‰€æœ‰tree_candidatesä½œä¸ºå‚æ•°ï¼Œè€Œä¸æ˜¯tree_mapping
            all_tree_candidates = list(tree_mapping.values())
            final_business_nodes = await self._recursive_expand_business_nodes(
                base_structure['business_nodes'], all_tree_candidates, tree_candidate['node_name']
            )
            
            logger.info(f"   ğŸ“Š [é€’å½’å±•å¼€å] æœ€ç»ˆä¸šåŠ¡èŠ‚ç‚¹: {len(final_business_nodes)}ä¸ª")
            for i, node in enumerate(final_business_nodes):
                logger.info(f"     æœ€ç»ˆèŠ‚ç‚¹{i+1}: {node['name']} (ID: {node.get('node_id', 'NEW')[:8] if node.get('node_id') else 'NEW'}...)")
            
            # å¤åˆ¶æœ€ç»ˆå±•å¼€çš„ä¸šåŠ¡èŠ‚ç‚¹
            for node in final_business_nodes:
                new_node_id = uuid.uuid4()
                new_node_base_id = uuid.uuid4()
                
                # ğŸ”§ ä½¿ç”¨å¤åˆkeyé¿å…æ˜ å°„å†²çª
                composite_key = f"{node.get('node_id', new_node_id)}@{tree_candidate['node_name']}"
                node_id_mapping[composite_key] = new_node_id
                
                # å‘åå…¼å®¹æ˜ å°„
                if node.get('node_id') and node['node_id'] not in node_id_mapping:
                    node_id_mapping[node['node_id']] = new_node_id
                else:
                    logger.info(f"   ğŸ”§ èŠ‚ç‚¹IDå¤„ç†: ä½¿ç”¨å¤åˆkey: {composite_key}")
                
                logger.info(f"   ğŸ“„ å¤åˆ¶æœ€ç»ˆèŠ‚ç‚¹: {node['name']} -> æ–°ID: {new_node_id}")
                
                await self.db.execute("""
                    INSERT INTO node (
                        node_id, node_base_id, workflow_id, workflow_base_id,
                        name, type, task_description, position_x, position_y,
                        version, is_current_version, created_at, is_deleted
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, new_node_id, new_node_base_id, new_workflow_id, new_workflow_base_id,
                     node['name'], node['type'], node['task_description'], 
                     node['position_x'], node['position_y'], 1, True, now_utc(), False)
                
                replaced_nodes += 1
        
        logger.info(f"âœ… [çœŸé€’å½’å±•å¼€] å®ŒæˆçœŸæ­£çš„é€’å½’å±•å¼€ï¼Œå…±æ›¿æ¢ {replaced_nodes} ä¸ªèŠ‚ç‚¹")
        return {"nodes_replaced": replaced_nodes}
    
    async def _recursive_expand_business_nodes(self, business_nodes: List[Dict], 
                                             all_tree_candidates: List[Dict[str, Any]],
                                             parent_name: str) -> List[Dict]:
        """é€’å½’å±•å¼€ä¸šåŠ¡èŠ‚ç‚¹ - æ­£ç¡®åŒ¹é…å­å·¥ä½œæµèŠ‚ç‚¹ä¸subdivision"""
        logger.info(f"   ğŸ” [é€’å½’å±•å¼€æ£€æŸ¥] æ£€æŸ¥ {len(business_nodes)} ä¸ªä¸šåŠ¡èŠ‚ç‚¹æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥å±•å¼€")
        logger.info(f"   ğŸ“‹ [å€™é€‰é¡¹] å¯ç”¨çš„treeå€™é€‰é¡¹: {len(all_tree_candidates)}ä¸ª")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ„å»ºsubdivision_idåˆ°å­å·¥ä½œæµå®ä¾‹çš„æ˜ å°„
        subdivision_to_instance = {}
        for candidate in all_tree_candidates:
            tree_node = candidate.get('tree_node')
            if tree_node and tree_node.workflow_instance_id:
                subdivision_id = candidate.get('subdivision_id')
                subdivision_to_instance[subdivision_id] = tree_node.workflow_instance_id
                logger.info(f"     æ˜ å°„: subdivision {subdivision_id[:8]}... -> instance {tree_node.workflow_instance_id[:8]}...")
        
        final_nodes = []
        
        for node in business_nodes:
            node_id = node['node_id']
            node_name = node['name']
            logger.info(f"     ğŸ” [èŠ‚ç‚¹æ£€æŸ¥] æ£€æŸ¥èŠ‚ç‚¹: {node_name} (ID: {node_id[:8]}...)")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šé€šè¿‡task_subdivisionè¡¨æŸ¥æ‰¾è¿™ä¸ªèŠ‚ç‚¹æ˜¯å¦è¢«subdivision
            needs_expansion = False
            matching_candidate = None
            
            # æŸ¥æ‰¾ä»¥è¿™ä¸ªèŠ‚ç‚¹ä¸ºåŸå§‹èŠ‚ç‚¹çš„subdivision
            try:
                subdivision_record = await self.db.fetch_one("""
                    SELECT ts.subdivision_id, ts.original_task_id,
                           ti.node_instance_id, ni.node_id as original_node_id
                    FROM task_subdivision ts
                    JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
                    JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
                    WHERE ni.node_id = %s AND ts.is_deleted = FALSE
                """, node_id)
                
                if subdivision_record:
                    subdivision_id = str(subdivision_record['subdivision_id'])
                    logger.info(f"       ğŸ” [å‘ç°subdivision] èŠ‚ç‚¹ {node_name} æœ‰subdivision: {subdivision_id[:8]}...")
                    
                    # åœ¨treeå€™é€‰é¡¹ä¸­æ‰¾åˆ°å¯¹åº”çš„candidate
                    for candidate in all_tree_candidates:
                        if candidate.get('subdivision_id') == subdivision_id:
                            logger.info(f"       ğŸ”„ [å‘ç°åŒ¹é…] èŠ‚ç‚¹ {node_name} éœ€è¦å±•å¼€ä¸º {candidate['node_name']}")
                            needs_expansion = True
                            matching_candidate = candidate
                            break
                
            except Exception as e:
                logger.warning(f"       âš ï¸ [æŸ¥è¯¢å¤±è´¥] æŸ¥æ‰¾subdivisionå¤±è´¥: {e}")
            
            if needs_expansion and matching_candidate:
                logger.info(f"       ğŸ”§ [é€’å½’å±•å¼€] å±•å¼€èŠ‚ç‚¹ {node_name} -> {matching_candidate['node_name']}")
                
                # è·å–åŒ¹é…å€™é€‰é¡¹çš„å­å·¥ä½œæµç»“æ„
                tree_node = matching_candidate.get('tree_node')
                if tree_node:
                    # é€’å½’è·å–å­å·¥ä½œæµç»“æ„
                    sub_structure = await self._analyze_subworkflow_structure_from_tree(
                        tree_node, node['position_x'], node['position_y']
                    )
                    
                    # é€’å½’å±•å¼€å­å·¥ä½œæµçš„ä¸šåŠ¡èŠ‚ç‚¹
                    expanded_sub_nodes = await self._recursive_expand_business_nodes(
                        sub_structure['business_nodes'], all_tree_candidates, f"{parent_name}->{matching_candidate['node_name']}"
                    )
                    
                    logger.info(f"       ğŸ“‹ [å±•å¼€ç»“æœ] {node_name} å±•å¼€ä¸º {len(expanded_sub_nodes)} ä¸ªèŠ‚ç‚¹")
                    final_nodes.extend(expanded_sub_nodes)
                else:
                    logger.warning(f"       âš ï¸ [å±•å¼€å¤±è´¥] åŒ¹é…å€™é€‰é¡¹ç¼ºå°‘tree_node")
                    final_nodes.append(node)
            else:
                logger.info(f"       âœ… [ä¿æŒä¸å˜] èŠ‚ç‚¹ {node_name} æ— éœ€å±•å¼€")
                final_nodes.append(node)
        
        logger.info(f"   ğŸ“Š [é€’å½’å±•å¼€ç»“æœ] åŸå§‹ {len(business_nodes)} ä¸ªèŠ‚ç‚¹ -> æœ€ç»ˆ {len(final_nodes)} ä¸ªèŠ‚ç‚¹")
        return final_nodes

    async def _fully_recursive_analyze_subworkflow(self, tree_node: WorkflowTemplateNode, 
                                                  center_x: int, center_y: int,
                                                  tree_mapping: Dict[str, Dict]) -> Dict[str, Any]:
        """å®Œå…¨é€’å½’åˆ†æå­å·¥ä½œæµç»“æ„ - ç®€åŒ–çš„çœŸæ­£é€’å½’å±•å¼€"""
        try:
            logger.info(f"ğŸ” [å®Œå…¨é€’å½’åˆ†æ] åˆ†æèŠ‚ç‚¹: {tree_node.workflow_name}")
            
            # ğŸ”§ ç®€åŒ–ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨åŸºç¡€åˆ†æï¼ŒçœŸæ­£çš„é€’å½’å±•å¼€ç”±ä¸Šå±‚ç»Ÿä¸€å¤„ç†
            # è¿™æ ·é¿å…å¤æ‚çš„åµŒå¥—é€’å½’é€»è¾‘ï¼Œè®©æ¯ä¸ªå­å·¥ä½œæµåªå¤„ç†è‡ªå·±çš„ç›´æ¥ç»“æ„
            base_structure = await self._analyze_subworkflow_structure_from_tree(
                tree_node, center_x, center_y
            )
            
            logger.info(f"   ğŸ“Š [åŸºç¡€ç»“æ„] ä¸šåŠ¡èŠ‚ç‚¹æ•°: {len(base_structure['business_nodes'])}")
            logger.info(f"      - å…¥å£ç‚¹: {len(base_structure['entry_points'])}ä¸ª")  
            logger.info(f"      - å‡ºå£ç‚¹: {len(base_structure['exit_points'])}ä¸ª")
            logger.info(f"      - ä¸šåŠ¡è¿æ¥: {len(base_structure['business_connections'])}ä¸ª")
            
            return base_structure
            
        except Exception as e:
            logger.error(f"âŒ [å®Œå…¨é€’å½’åˆ†æ] åˆ†æå¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›åŸºç¡€ç»“æ„
            return await self._analyze_subworkflow_structure_from_tree(tree_node, center_x, center_y)

    async def _rebuild_unified_recursive_connections(self, parent_workflow_id: str, new_workflow_id: uuid.UUID,
                                                   tree_mapping: Dict[str, Dict],
                                                   node_id_mapping: Dict[str, uuid.UUID]) -> Dict[str, int]:
        """ç»Ÿä¸€é€’å½’åˆå¹¶çš„è¿æ¥é‡å»º - ç®€åŒ–ç‰ˆæœ¬ï¼Œé‡ç”¨ç°æœ‰é€»è¾‘"""
        logger.info(f"ğŸ”— [ç»Ÿä¸€é€’å½’è¿æ¥] å¼€å§‹é‡å»ºè¿æ¥")
        logger.info(f"   - çˆ¶å·¥ä½œæµID: {parent_workflow_id}")
        logger.info(f"   - æ–°å·¥ä½œæµID: {new_workflow_id}")
        logger.info(f"   - èŠ‚ç‚¹æ˜ å°„æ•°é‡: {len(node_id_mapping)}")
        logger.info(f"   - é€’å½’å€™é€‰é¡¹æ•°é‡: {len(tree_mapping)}")
        
        # ğŸ”§ é‡ç”¨ç°æœ‰çš„è¿æ¥é‡å»ºé€»è¾‘
        # ç»Ÿä¸€é€’å½’åˆå¹¶çš„è¿æ¥å¤„ç†æœ¬è´¨ä¸Šå’Œåˆ†å±‚åˆå¹¶ç›¸åŒï¼Œåªæ˜¯è§„æ¨¡æ›´å¤§
        return await self._rebuild_connections_from_tree_data(
            parent_workflow_id, new_workflow_id, tree_mapping, node_id_mapping
        )

    async def _get_child_subdivisions(self, workflow_instance_id: str) -> List[Dict[str, Any]]:
        """è·å–å·¥ä½œæµå®ä¾‹çš„å­subdivision"""
        try:
            child_subdivisions = await self.db.fetch_all("""
                SELECT subdivision_id, sub_workflow_instance_id, sub_workflow_base_id,
                       task_title, subdivision_name, sub_workflow_status
                FROM workflow_subdivisions 
                WHERE root_workflow_instance_id = %s AND is_deleted = FALSE
            """, workflow_instance_id)
            
            return child_subdivisions if child_subdivisions else []
            
        except Exception as e:
            logger.error(f"è·å–å­subdivisionå¤±è´¥: {e}")
            return []

    async def _recursive_expand_subworkflow(self, base_structure: Dict[str, Any], 
                                          child_subdivisions: List[Dict[str, Any]],
                                          center_x: int, center_y: int) -> Dict[str, Any]:
        """é€’å½’å±•å¼€å­å·¥ä½œæµç»“æ„"""
        logger.info(f"ğŸ”„ [é€’å½’å±•å¼€å­å·¥ä½œæµ] å¤„ç† {len(child_subdivisions)} ä¸ªå­subdivision")
        
        # è¿™é‡Œåº”è¯¥é€’å½’å¤„ç†æ¯ä¸ªchild_subdivision
        # ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶ç›´æ¥è¿”å›åŸºç¡€ç»“æ„
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥é€’å½’è°ƒç”¨ç±»ä¼¼çš„å±•å¼€é€»è¾‘
        
        return base_structure