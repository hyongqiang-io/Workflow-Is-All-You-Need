"""
å·¥ä½œæµç›‘æ§æœåŠ¡
Workflow Monitor Service
"""

import uuid
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from loguru import logger

from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
from ..repositories.instance.node_instance_repository import NodeInstanceRepository
from ..repositories.instance.task_instance_repository import TaskInstanceRepository
from .workflow_execution_context import get_context_manager
from .execution_service import execution_engine


class WorkflowMonitorService:
    """å·¥ä½œæµç›‘æ§æœåŠ¡ - å®šæœŸæ‰«æå’Œä¿®å¤åœæ»çš„å·¥ä½œæµå®ä¾‹"""
    
    def __init__(self):
        self.workflow_repo = WorkflowInstanceRepository()
        self.node_repo = NodeInstanceRepository()
        self.task_repo = TaskInstanceRepository()
        self.context_manager = get_context_manager()
        
        # ç›‘æ§é…ç½®
        self.scan_interval = 300  # 5åˆ†é’Ÿæ‰«æä¸€æ¬¡
        self.stale_threshold_hours = 2  # è¶…è¿‡2å°æ—¶æœªæ›´æ–°è§†ä¸ºåœæ»
        self.max_recovery_attempts = 3  # æœ€å¤§æ¢å¤å°è¯•æ¬¡æ•°
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.monitor_task = None
        self.recovery_stats = {
            'total_scanned': 0,
            'stale_workflows_found': 0,
            'recovery_attempts': 0,
            'successful_recoveries': 0,
            'failed_recoveries': 0,
            'last_scan_time': None
        }
        
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§æœåŠ¡"""
        if self.is_running:
            logger.warning("å·¥ä½œæµç›‘æ§æœåŠ¡å·²åœ¨è¿è¡Œä¸­")
            return
            
        logger.info("ğŸ” å¯åŠ¨å·¥ä½œæµåœæ»ç›‘æ§æœåŠ¡")
        logger.info(f"   - æ‰«æé—´éš”: {self.scan_interval}ç§’")
        logger.info(f"   - åœæ»é˜ˆå€¼: {self.stale_threshold_hours}å°æ—¶")
        logger.info(f"   - æœ€å¤§é‡è¯•: {self.max_recovery_attempts}æ¬¡")
        
        self.is_running = True
        self.monitor_task = asyncio.create_task(self._monitoring_loop())
        
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§æœåŠ¡"""
        if not self.is_running:
            return
            
        logger.info("ğŸ›‘ åœæ­¢å·¥ä½œæµç›‘æ§æœåŠ¡")
        self.is_running = False
        
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                await self.monitor_task
            except asyncio.CancelledError:
                pass
                
    async def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        logger.info("ğŸ”„ å·¥ä½œæµç›‘æ§å¾ªç¯å·²å¯åŠ¨")
        
        while self.is_running:
            try:
                await self._scan_and_recover_stale_workflows()
                await asyncio.sleep(self.scan_interval)
            except asyncio.CancelledError:
                logger.info("ç›‘æ§å¾ªç¯è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(self.scan_interval)  # å‡ºé”™åä»ç»§ç»­ç›‘æ§
                
    async def _scan_and_recover_stale_workflows(self):
        """æ‰«æå¹¶æ¢å¤åœæ»çš„å·¥ä½œæµ"""
        try:
            logger.debug("ğŸ” å¼€å§‹æ‰«æåœæ»çš„å·¥ä½œæµå®ä¾‹...")
            
            # æŸ¥æ‰¾åœæ»çš„å·¥ä½œæµå®ä¾‹
            stale_workflows = await self._find_stale_workflows()
            
            self.recovery_stats['total_scanned'] += len(stale_workflows) if stale_workflows else 0
            self.recovery_stats['stale_workflows_found'] = len(stale_workflows) if stale_workflows else 0
            self.recovery_stats['last_scan_time'] = datetime.utcnow().isoformat()
            
            if not stale_workflows:
                logger.debug("âœ… æœªå‘ç°åœæ»çš„å·¥ä½œæµå®ä¾‹")
                return
                
            logger.info(f"ğŸš¨ å‘ç° {len(stale_workflows)} ä¸ªåœæ»çš„å·¥ä½œæµå®ä¾‹ï¼Œå¼€å§‹æ¢å¤...")
            
            for workflow in stale_workflows:
                try:
                    await self._attempt_workflow_recovery(workflow)
                except Exception as e:
                    logger.error(f"æ¢å¤å·¥ä½œæµ {workflow['workflow_instance_id']} å¤±è´¥: {e}")
                    self.recovery_stats['failed_recoveries'] += 1
                    
        except Exception as e:
            logger.error(f"æ‰«æåœæ»å·¥ä½œæµå¼‚å¸¸: {e}")
            
    async def _find_stale_workflows(self) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾åœæ»çš„å·¥ä½œæµå®ä¾‹"""
        try:
            # è®¡ç®—åœæ»é˜ˆå€¼æ—¶é—´
            stale_threshold = datetime.utcnow() - timedelta(hours=self.stale_threshold_hours)
            
            # æŸ¥è¯¢æ½œåœ¨åœæ»çš„å·¥ä½œæµå®ä¾‹
            query = """
            SELECT 
                wi.workflow_instance_id,
                wi.workflow_instance_name,
                wi.status as workflow_status,
                wi.updated_at,
                wi.created_at,
                w.name as workflow_name,
                u.username as executor_name,
                -- ç»Ÿè®¡èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
                COUNT(ni.node_instance_id) as total_nodes,
                COUNT(CASE WHEN ni.status = 'completed' THEN 1 END) as completed_nodes,
                COUNT(CASE WHEN ni.status = 'pending' THEN 1 END) as pending_nodes,
                COUNT(CASE WHEN ni.status = 'running' THEN 1 END) as running_nodes,
                COUNT(CASE WHEN ni.status = 'failed' THEN 1 END) as failed_nodes,
                -- æœ€åæ›´æ–°æ—¶é—´
                MAX(ni.updated_at) as last_node_update,
                MAX(ti.updated_at) as last_task_update
            FROM workflow_instance wi
            LEFT JOIN workflow w ON wi.workflow_base_id = w.workflow_base_id AND w.is_current_version = 1
            LEFT JOIN user u ON wi.executor_id = u.user_id
            LEFT JOIN node_instance ni ON wi.workflow_instance_id = ni.workflow_instance_id AND ni.is_deleted = 0
            LEFT JOIN task_instance ti ON ni.node_instance_id = ti.node_instance_id AND ti.is_deleted = 0
            WHERE wi.is_deleted = 0
            AND wi.status IN ('running', 'pending')  -- åªæ£€æŸ¥è¿è¡Œä¸­æˆ–å¾…å¤„ç†çš„å·¥ä½œæµ
            AND wi.updated_at < %s  -- è¶…è¿‡é˜ˆå€¼æ—¶é—´æœªæ›´æ–°
            GROUP BY wi.workflow_instance_id, wi.workflow_instance_name, wi.status, wi.updated_at, wi.created_at, w.name, u.username
            HAVING pending_nodes > 0  -- å­˜åœ¨å¾…å¤„ç†èŠ‚ç‚¹
            AND completed_nodes > 0   -- ä½†æœ‰å·²å®ŒæˆèŠ‚ç‚¹ï¼ˆè¯´æ˜æ‰§è¡Œè¿‡ç¨‹ä¸­åœæ»ï¼‰
            ORDER BY wi.updated_at ASC
            LIMIT 50  -- é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡
            """
            
            results = await self.workflow_repo.db.fetch_all(query, stale_threshold)
            
            # è¿›ä¸€æ­¥ç­›é€‰çœŸæ­£åœæ»çš„å·¥ä½œæµ
            stale_workflows = []
            for result in results:
                # æ£€æŸ¥æ˜¯å¦çœŸçš„åœæ»ï¼ˆæœ‰å®Œæˆçš„èŠ‚ç‚¹ï¼Œä½†æœ‰ç­‰å¾…ä¸­çš„èŠ‚ç‚¹ä¸”é•¿æ—¶é—´æ— æ›´æ–°ï¼‰
                if await self._is_workflow_truly_stale(result):
                    stale_workflows.append(dict(result))
                    
            return stale_workflows
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾åœæ»å·¥ä½œæµå¤±è´¥: {e}")
            return []
            
    async def _is_workflow_truly_stale(self, workflow_data: Dict) -> bool:
        """åˆ¤æ–­å·¥ä½œæµæ˜¯å¦çœŸçš„åœæ»"""
        try:
            workflow_id = workflow_data['workflow_instance_id']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            running_tasks_query = """
            SELECT COUNT(*) as running_task_count
            FROM task_instance ti
            JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
            WHERE ni.workflow_instance_id = %s
            AND ti.status IN ('in_progress', 'assigned')
            AND ti.is_deleted = 0
            """
            
            running_task_result = await self.task_repo.db.fetch_one(running_tasks_query, workflow_id)
            if running_task_result and running_task_result['running_task_count'] > 0:
                # æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œä¸ç®—åœæ»
                return False
                
            # æ£€æŸ¥æ˜¯å¦æœ‰æ»¡è¶³æ¡ä»¶ä½†æœªæ‰§è¡Œçš„èŠ‚ç‚¹
            ready_nodes_query = """
            WITH node_dependencies AS (
                SELECT 
                    ni.node_instance_id,
                    ni.status as node_status,
                    COUNT(upstream_ni.node_instance_id) as upstream_count,
                    COUNT(CASE WHEN upstream_ni.status = 'completed' THEN 1 END) as completed_upstream_count
                FROM node_instance ni
                LEFT JOIN node_connection nc ON ni.node_id = nc.to_node_id
                LEFT JOIN node_instance upstream_ni ON nc.from_node_id = upstream_ni.node_id 
                    AND upstream_ni.workflow_instance_id = ni.workflow_instance_id
                    AND upstream_ni.is_deleted = 0
                WHERE ni.workflow_instance_id = %s 
                AND ni.is_deleted = 0
                GROUP BY ni.node_instance_id, ni.status
            )
            SELECT COUNT(*) as ready_pending_nodes
            FROM node_dependencies 
            WHERE node_status = 'pending'
            AND (upstream_count = 0 OR upstream_count = completed_upstream_count)
            """
            
            ready_result = await self.node_repo.db.fetch_one(ready_nodes_query, workflow_id)
            ready_nodes_count = ready_result['ready_pending_nodes'] if ready_result else 0
            
            # å¦‚æœæœ‰å‡†å¤‡å¥½ä½†æœªæ‰§è¡Œçš„èŠ‚ç‚¹ï¼Œåˆ™åˆ¤å®šä¸ºåœæ»
            return ready_nodes_count > 0
            
        except Exception as e:
            logger.error(f"åˆ¤æ–­å·¥ä½œæµåœæ»çŠ¶æ€å¤±è´¥: {e}")
            return False
            
    async def _attempt_workflow_recovery(self, workflow_data: Dict):
        """å°è¯•æ¢å¤åœæ»çš„å·¥ä½œæµ"""
        workflow_id = uuid.UUID(workflow_data['workflow_instance_id'])
        workflow_name = workflow_data.get('workflow_instance_name', 'æœªçŸ¥')
        
        logger.info(f"ğŸ”§ å¼€å§‹æ¢å¤åœæ»å·¥ä½œæµ: {workflow_name} ({workflow_id})")
        logger.info(f"   - åœæ»æ—¶é—´: {workflow_data.get('updated_at')}")
        logger.info(f"   - èŠ‚ç‚¹çŠ¶æ€: {workflow_data.get('completed_nodes', 0)}/{workflow_data.get('total_nodes', 0)} å®Œæˆ")
        
        self.recovery_stats['recovery_attempts'] += 1
        
        try:
            # 1. æ£€æŸ¥å¹¶æ¢å¤ä¸Šä¸‹æ–‡
            context_existed = self.context_manager.contexts.get(workflow_id) is not None
            if not context_existed:
                logger.info(f"   - ä¸Šä¸‹æ–‡ç¼ºå¤±ï¼Œä»æ•°æ®åº“æ¢å¤...")
                recovered_context = await self.context_manager.get_context(workflow_id)
                if not recovered_context:
                    raise Exception("æ— æ³•ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡")
                logger.info(f"   - ä¸Šä¸‹æ–‡æ¢å¤æˆåŠŸ")
            else:
                recovered_context = self.context_manager.contexts[workflow_id]
                logger.info(f"   - ä¸Šä¸‹æ–‡å·²å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨")
            
            # 2. æ£€æŸ¥å¾…è§¦å‘çš„èŠ‚ç‚¹
            ready_nodes = await recovered_context.get_ready_nodes()
            logger.info(f"   - å‘ç° {len(ready_nodes)} ä¸ªå¾…è§¦å‘èŠ‚ç‚¹")
            
            if not ready_nodes:
                logger.warning(f"   - æ²¡æœ‰å‘ç°å¾…è§¦å‘èŠ‚ç‚¹ï¼Œå¯èƒ½ä¾èµ–å…³ç³»é‡å»ºå¤±è´¥")
                # å°è¯•å¼ºåˆ¶é‡æ–°è·å–ä¸Šä¸‹æ–‡ï¼ˆè¿™ä¼šè§¦å‘ä¾èµ–å…³ç³»é‡å»ºï¼‰
                await self.context_manager.remove_context(workflow_id)
                recovered_context = await self.context_manager.get_context(workflow_id)
                if recovered_context:
                    ready_nodes = await recovered_context.get_ready_nodes()
                    logger.info(f"   - é‡å»ºä¸Šä¸‹æ–‡åå‘ç° {len(ready_nodes)} ä¸ªå¾…è§¦å‘èŠ‚ç‚¹")
                else:
                    logger.error(f"   - é‡æ–°è·å–ä¸Šä¸‹æ–‡å¤±è´¥")
            
            # 3. è§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹
            triggered_count = 0
            for node_instance_id in ready_nodes:
                try:
                    logger.info(f"   - è§¦å‘èŠ‚ç‚¹: {node_instance_id}")
                    await execution_engine._on_nodes_ready_to_execute(workflow_id, [node_instance_id])
                    triggered_count += 1
                except Exception as node_error:
                    logger.error(f"   - è§¦å‘èŠ‚ç‚¹å¤±è´¥ {node_instance_id}: {node_error}")
            
            if triggered_count > 0:
                self.recovery_stats['successful_recoveries'] += 1
                logger.info(f"âœ… å·¥ä½œæµæ¢å¤æˆåŠŸ: {workflow_name}")
                logger.info(f"   - è§¦å‘äº† {triggered_count} ä¸ªèŠ‚ç‚¹")
            else:
                logger.warning(f"âš ï¸ å·¥ä½œæµæ¢å¤éƒ¨åˆ†æˆåŠŸ: {workflow_name}")
                logger.warning(f"   - ä¸Šä¸‹æ–‡å·²æ¢å¤ï¼Œä½†æ²¡æœ‰è§¦å‘ä»»ä½•èŠ‚ç‚¹")
                
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµæ¢å¤å¤±è´¥: {workflow_name}")
            logger.error(f"   - é”™è¯¯: {e}")
            self.recovery_stats['failed_recoveries'] += 1
            raise
            
    async def get_monitor_stats(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'is_running': self.is_running,
            'scan_interval_seconds': self.scan_interval,
            'stale_threshold_hours': self.stale_threshold_hours,
            'max_recovery_attempts': self.max_recovery_attempts,
            'recovery_stats': self.recovery_stats.copy()
        }
        
    async def manual_scan_and_recover(self) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘æ‰«æå’Œæ¢å¤"""
        logger.info("ğŸ”§ æ‰‹åŠ¨è§¦å‘åœæ»å·¥ä½œæµæ‰«æå’Œæ¢å¤")
        
        before_stats = self.recovery_stats.copy()
        await self._scan_and_recover_stale_workflows()
        after_stats = self.recovery_stats.copy()
        
        # è®¡ç®—æœ¬æ¬¡æ‰«æçš„ç»“æœ
        scan_results = {
            'workflows_scanned': after_stats['total_scanned'] - before_stats['total_scanned'],
            'stale_found': after_stats['stale_workflows_found'],
            'recovery_attempts': after_stats['recovery_attempts'] - before_stats['recovery_attempts'],
            'successful_recoveries': after_stats['successful_recoveries'] - before_stats['successful_recoveries'],
            'failed_recoveries': after_stats['failed_recoveries'] - before_stats['failed_recoveries'],
            'scan_time': after_stats['last_scan_time']
        }
        
        return scan_results


# å…¨å±€ç›‘æ§æœåŠ¡å®ä¾‹
_workflow_monitor = None

def get_workflow_monitor() -> WorkflowMonitorService:
    """è·å–å·¥ä½œæµç›‘æ§æœåŠ¡å®ä¾‹"""
    global _workflow_monitor
    if _workflow_monitor is None:
        _workflow_monitor = WorkflowMonitorService()
    return _workflow_monitor