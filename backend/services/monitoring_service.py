"""
çŠ¶æ€è¿½è¸ªå’Œç›‘æ§æœåŠ¡
Status Tracking and Monitoring Service
"""

import uuid
import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from loguru import logger
import sys
from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
from ..repositories.instance.task_instance_repository import TaskInstanceRepository
from ..models.instance import (
    WorkflowInstanceStatus, TaskInstanceStatus, TaskInstanceType
)
from ..utils.helpers import now_utc


class MonitoringService:
    """ç›‘æ§æœåŠ¡"""
    
    def __init__(self):
        self.workflow_instance_repo = WorkflowInstanceRepository()
        self.task_instance_repo = TaskInstanceRepository()
        
        # ç›‘æ§é…ç½®
        self.is_monitoring = False
        self.monitor_interval = 15  # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰- ä¼˜åŒ–ä¸ºæ›´é¢‘ç¹
        self.alert_thresholds = {
            'workflow_timeout_minutes': 60,  # å·¥ä½œæµè¶…æ—¶é˜ˆå€¼
            'task_timeout_minutes': 30,      # ä»»åŠ¡è¶…æ—¶é˜ˆå€¼
            'failed_task_rate': 0.1,         # å¤±è´¥ä»»åŠ¡æ¯”ä¾‹é˜ˆå€¼
            'queue_size_threshold': 100      # é˜Ÿåˆ—å¤§å°é˜ˆå€¼
        }
        
        # å·¥ä½œæµå®Œæˆå›è°ƒæ³¨å†Œè¡¨
        self.workflow_completion_callbacks = {}  # {workflow_instance_id: [callback_functions]}
        
        # ç›‘æ§æ•°æ®
        self.metrics = {
            'workflows': {
                'total': 0,
                'running': 0,
                'completed': 0,
                'failed': 0,
                'cancelled': 0
            },
            'tasks': {
                'total': 0,
                'pending': 0,
                'in_progress': 0,
                'completed': 0,
                'failed': 0,
                'cancelled': 0
            },
            'performance': {
                'avg_workflow_duration': 0,
                'avg_task_duration': 0,
                'success_rate': 0
            }
        }
        
        # å‘Šè­¦å†å²
        self.alerts = []
    
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§æœåŠ¡"""
        if self.is_monitoring:
            logger.warning("ç›‘æ§æœåŠ¡å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_monitoring = True
        logger.info("å¯åŠ¨å·¥ä½œæµç›‘æ§æœåŠ¡")
        
        # å¯åŠ¨ç›‘æ§åç¨‹
        asyncio.create_task(self._monitoring_loop())
        asyncio.create_task(self._collect_metrics())
        asyncio.create_task(self._check_timeouts())
        asyncio.create_task(self._performance_analysis())
        asyncio.create_task(self._real_time_status_sync())  # æ–°å¢å®æ—¶çŠ¶æ€åŒæ­¥
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§æœåŠ¡"""
        self.is_monitoring = False
        logger.info("åœæ­¢å·¥ä½œæµç›‘æ§æœåŠ¡")
    
    async def _monitoring_loop(self):
        """ä¸»ç›‘æ§å¾ªç¯"""
        while self.is_monitoring:
            try:
                # æ”¶é›†åŸºç¡€æŒ‡æ ‡
                await self._update_basic_metrics()
                
                # æ£€æŸ¥å¼‚å¸¸æƒ…å†µ
                await self._check_anomalies()
                
                # æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å¹¶è§¦å‘å›è°ƒ
                await self._check_workflow_completion_and_trigger_callbacks()
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªç›‘æ§å‘¨æœŸ
                await asyncio.sleep(self.monitor_interval)
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                await asyncio.sleep(10)
    
    async def _update_basic_metrics(self):
        """æ›´æ–°åŸºç¡€æŒ‡æ ‡"""
        try:
            # è·å–å·¥ä½œæµç»Ÿè®¡
            workflow_stats = await self._get_workflow_statistics()
            task_stats = await self._get_task_statistics()
            
            # æ›´æ–°æŒ‡æ ‡
            self.metrics['workflows'] = workflow_stats
            self.metrics['tasks'] = task_stats
            
            # è®¡ç®—æˆåŠŸç‡
            total_workflows = workflow_stats['total']
            completed_workflows = workflow_stats['completed']
            
            if total_workflows > 0:
                self.metrics['performance']['success_rate'] = (
                    completed_workflows / total_workflows
                ) * 100
            
            logger.debug(f"æ›´æ–°åŸºç¡€æŒ‡æ ‡: å·¥ä½œæµæ€»æ•°={total_workflows}, ä»»åŠ¡æ€»æ•°={task_stats['total']}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°åŸºç¡€æŒ‡æ ‡å¤±è´¥: {e}")
    
    async def _get_workflow_statistics(self) -> Dict[str, int]:
        """è·å–å·¥ä½œæµç»Ÿè®¡"""
        try:
            # è·å–æ‰€æœ‰è¿è¡Œä¸­çš„å®ä¾‹
            running_instances = await self.workflow_instance_repo.get_running_instances(1000)
            
            # ç»Ÿè®¡å„çŠ¶æ€çš„å·¥ä½œæµæ•°é‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            stats = {
                'total': 0,
                'running': len(running_instances),
                'completed': 0,
                'failed': 0,
                'cancelled': 0,
                'paused': 0
            }
            
            # å®é™…åº”è¯¥æŸ¥è¯¢æ•°æ®åº“è·å–å®Œæ•´ç»Ÿè®¡
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–é€»è¾‘
            stats['total'] = stats['running'] + 100  # æ¨¡æ‹Ÿæ•°æ®
            stats['completed'] = 80
            stats['failed'] = 15
            stats['cancelled'] = 5
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµç»Ÿè®¡å¤±è´¥: {e}")
            return {'total': 0, 'running': 0, 'completed': 0, 'failed': 0, 'cancelled': 0}
    
    async def _get_task_statistics(self) -> Dict[str, int]:
        """è·å–ä»»åŠ¡ç»Ÿè®¡"""
        try:
            # è·å–ä»»åŠ¡ç»Ÿè®¡ï¼ˆå…¨å±€ï¼‰
            task_stats = await self.task_instance_repo.get_task_statistics()
            
            stats = {
                'total': task_stats.get('total_tasks', 0),
                'pending': task_stats.get('pending_tasks', 0),
                'in_progress': task_stats.get('in_progress_tasks', 0),
                'completed': task_stats.get('completed_tasks', 0),
                'failed': task_stats.get('failed_tasks', 0),
                'cancelled': 0  # éœ€è¦æ·»åŠ åˆ°ç»Ÿè®¡æŸ¥è¯¢ä¸­
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
            return {'total': 0, 'pending': 0, 'in_progress': 0, 'completed': 0, 'failed': 0, 'cancelled': 0}
    
    async def _check_anomalies(self):
        """æ£€æŸ¥å¼‚å¸¸æƒ…å†µ"""
        try:
            # æ£€æŸ¥å¤±è´¥ç‡
            await self._check_failure_rate()
            
            # æ£€æŸ¥é˜Ÿåˆ—å †ç§¯
            await self._check_queue_buildup()
            
            # æ£€æŸ¥ç³»ç»Ÿèµ„æº
            await self._check_system_resources()
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å¼‚å¸¸æƒ…å†µå¤±è´¥: {e}")
    
    async def _check_failure_rate(self):
        """æ£€æŸ¥å¤±è´¥ç‡"""
        try:
            failed_tasks = self.metrics['tasks']['failed']
            total_tasks = self.metrics['tasks']['total']
            
            if total_tasks > 0:
                failure_rate = failed_tasks / total_tasks
                
                if failure_rate > self.alert_thresholds['failed_task_rate']:
                    await self._create_alert(
                        'high_failure_rate',
                        f"ä»»åŠ¡å¤±è´¥ç‡è¿‡é«˜: {failure_rate:.2%}",
                        'warning'
                    )
                    
        except Exception as e:
            logger.error(f"æ£€æŸ¥å¤±è´¥ç‡å¤±è´¥: {e}")
    
    async def _check_queue_buildup(self):
        """æ£€æŸ¥é˜Ÿåˆ—å †ç§¯"""
        try:
            pending_tasks = self.metrics['tasks']['pending']
            
            if pending_tasks > self.alert_thresholds['queue_size_threshold']:
                await self._create_alert(
                    'queue_buildup',
                    f"å¾…å¤„ç†ä»»åŠ¡é˜Ÿåˆ—å †ç§¯: {pending_tasks} ä¸ªä»»åŠ¡",
                    'warning'
                )
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥é˜Ÿåˆ—å †ç§¯å¤±è´¥: {e}")
    
    async def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç³»ç»Ÿèµ„æºæ£€æŸ¥
            # å¦‚å†…å­˜ä½¿ç”¨ç‡ã€CPUä½¿ç”¨ç‡ã€æ•°æ®åº“è¿æ¥æ•°ç­‰
            pass
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
    
    async def _collect_metrics(self):
        """æ”¶é›†è¯¦ç»†æŒ‡æ ‡"""
        while self.is_monitoring:
            try:
                # æ¯5åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡è¯¦ç»†æŒ‡æ ‡
                await asyncio.sleep(300)
                
                # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                await self._collect_performance_metrics()
                
                # æ”¶é›†ä¸šåŠ¡æŒ‡æ ‡
                await self._collect_business_metrics()
                
            except Exception as e:
                logger.error(f"æ”¶é›†æŒ‡æ ‡å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    async def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è·å–å¹³å‡æ‰§è¡Œæ—¶é—´
            task_stats = await self.task_instance_repo.get_task_statistics()
            
            avg_duration = task_stats.get('average_duration')
            if avg_duration:
                self.metrics['performance']['avg_task_duration'] = float(avg_duration)
            
            logger.debug(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡: å¹³å‡ä»»åŠ¡æ‰§è¡Œæ—¶é—´={avg_duration}åˆ†é’Ÿ")
            
        except Exception as e:
            logger.error(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
    
    async def _collect_business_metrics(self):
        """æ”¶é›†ä¸šåŠ¡æŒ‡æ ‡"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸šåŠ¡ç›¸å…³çš„æŒ‡æ ‡æ”¶é›†
            # å¦‚ç”¨æˆ·æ´»è·ƒåº¦ã€å·¥ä½œæµç±»å‹åˆ†å¸ƒç­‰
            pass
            
        except Exception as e:
            logger.error(f"æ”¶é›†ä¸šåŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
    
    async def _check_timeouts(self):
        """æ£€æŸ¥è¶…æ—¶ä»»åŠ¡å’Œå·¥ä½œæµ"""
        while self.is_monitoring:
            try:
                # æ¯10åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡è¶…æ—¶
                await asyncio.sleep(600)
                
                # æ£€æŸ¥è¶…æ—¶å·¥ä½œæµ
                await self._check_workflow_timeouts()
                
                # æ£€æŸ¥è¶…æ—¶ä»»åŠ¡
                await self._check_task_timeouts()
                
            except Exception as e:
                logger.error(f"æ£€æŸ¥è¶…æ—¶å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    async def _check_workflow_timeouts(self):
        """æ£€æŸ¥è¶…æ—¶å·¥ä½œæµ"""
        try:
            # è·å–è¿è¡Œä¸­çš„å·¥ä½œæµå®ä¾‹
            running_instances = await self.workflow_instance_repo.get_running_instances(100)
            
            # ç¡®ä¿è¶…æ—¶é˜ˆå€¼æ˜¯æ•°å€¼ç±»å‹
            timeout_minutes = self.alert_thresholds['workflow_timeout_minutes']
            if isinstance(timeout_minutes, str):
                try:
                    timeout_minutes = int(timeout_minutes)
                except ValueError:
                    logger.warning(f"æ— æ³•è½¬æ¢workflow_timeout_minutesä¸ºæ•´æ•°: {timeout_minutes}ï¼Œä½¿ç”¨é»˜è®¤å€¼60")
                    timeout_minutes = 60
            
            timeout_threshold = datetime.now() - timedelta(minutes=timeout_minutes)
            
            for instance in running_instances:
                started_at = instance.get('started_at')
                if started_at:
                    try:
                        if isinstance(started_at, str):
                            start_time = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
                        else:
                            # å‡è®¾æ˜¯datetimeå¯¹è±¡
                            start_time = started_at
                        
                        if start_time.replace(tzinfo=None) < timeout_threshold:
                            await self._create_alert(
                                'workflow_timeout',
                                f"å·¥ä½œæµå®ä¾‹ {instance['instance_id']} æ‰§è¡Œè¶…æ—¶",
                                'error',
                                {'instance_id': instance['instance_id']}
                            )
                    except (ValueError, TypeError) as e:
                        logger.warning(f"è§£æå·¥ä½œæµå¼€å§‹æ—¶é—´å¤±è´¥: {started_at}, é”™è¯¯: {e}")
                        
        except Exception as e:
            logger.error(f"æ£€æŸ¥å·¥ä½œæµè¶…æ—¶å¤±è´¥: {e}")
    
    async def _check_task_timeouts(self):
        """æ£€æŸ¥è¶…æ—¶ä»»åŠ¡"""
        try:
            # è·å–è¿›è¡Œä¸­çš„ä»»åŠ¡
            in_progress_tasks = await self.task_instance_repo.get_tasks_by_workflow_instance(
                None, TaskInstanceStatus.IN_PROGRESS
            )
            
            # ç¡®ä¿è¶…æ—¶é˜ˆå€¼æ˜¯æ•°å€¼ç±»å‹
            task_timeout_minutes = self.alert_thresholds['task_timeout_minutes']
            if isinstance(task_timeout_minutes, str):
                try:
                    task_timeout_minutes = int(task_timeout_minutes)
                except ValueError:
                    logger.warning(f"æ— æ³•è½¬æ¢task_timeout_minutesä¸ºæ•´æ•°: {task_timeout_minutes}ï¼Œä½¿ç”¨é»˜è®¤å€¼30")
                    task_timeout_minutes = 30
            
            timeout_threshold = datetime.now() - timedelta(minutes=task_timeout_minutes)
            
            for task in in_progress_tasks:
                started_at = task.get('started_at')
                if started_at:
                    try:
                        if isinstance(started_at, str):
                            start_time = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
                        else:
                            # å‡è®¾æ˜¯datetimeå¯¹è±¡
                            start_time = started_at
                        
                        if start_time.replace(tzinfo=None) < timeout_threshold:
                            await self._create_alert(
                                'task_timeout',
                                f"ä»»åŠ¡ {task['task_instance_id']} æ‰§è¡Œè¶…æ—¶",
                                'warning',
                                {'task_id': task['task_instance_id']}
                            )
                    except (ValueError, TypeError) as e:
                        logger.warning(f"è§£æä»»åŠ¡å¼€å§‹æ—¶é—´å¤±è´¥: {started_at}, é”™è¯¯: {e}")
                        
        except Exception as e:
            logger.error(f"æ£€æŸ¥ä»»åŠ¡è¶…æ—¶å¤±è´¥: {e}")
    
    async def _performance_analysis(self):
        """æ€§èƒ½åˆ†æ"""
        while self.is_monitoring:
            try:
                # æ¯å°æ—¶è¿›è¡Œä¸€æ¬¡æ€§èƒ½åˆ†æ
                await asyncio.sleep(3600)
                
                # åˆ†ææ‰§è¡Œè¶‹åŠ¿
                await self._analyze_execution_trends()
                
                # åˆ†æç“¶é¢ˆ
                await self._analyze_bottlenecks()
                
            except Exception as e:
                logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
                await asyncio.sleep(300)
    
    async def _analyze_execution_trends(self):
        """åˆ†ææ‰§è¡Œè¶‹åŠ¿"""
        try:
            # è¿™é‡Œå¯ä»¥åˆ†ææ‰§è¡Œæ—¶é—´è¶‹åŠ¿ã€æˆåŠŸç‡è¶‹åŠ¿ç­‰
            logger.info("æ‰§è¡Œè¶‹åŠ¿åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆ†ææ‰§è¡Œè¶‹åŠ¿å¤±è´¥: {e}")
    
    async def _analyze_bottlenecks(self):
        """åˆ†æç“¶é¢ˆ"""
        try:
            # åˆ†æå¯èƒ½çš„ç“¶é¢ˆç‚¹
            # å¦‚æŸäº›èŠ‚ç‚¹ç±»å‹æ‰§è¡Œæ—¶é—´è¿‡é•¿ã€æŸäº›Agentå“åº”æ…¢ç­‰
            logger.info("ç“¶é¢ˆåˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆ†æç“¶é¢ˆå¤±è´¥: {e}")
    
    async def _real_time_status_sync(self):
        """å®æ—¶çŠ¶æ€åŒæ­¥ - æ¯5ç§’ä¸»åŠ¨æ£€æŸ¥è¿è¡Œä¸­å·¥ä½œæµçš„çŠ¶æ€å˜åŒ–"""
        while self.is_monitoring:
            try:
                # è·å–æ‰€æœ‰è¿è¡Œä¸­çš„å·¥ä½œæµ
                running_workflows = await self.workflow_instance_repo.db.fetch_all("""
                    SELECT workflow_instance_id, workflow_instance_name, status, updated_at
                    FROM workflow_instance 
                    WHERE status IN ('RUNNING', 'PENDING')
                    AND is_deleted = FALSE
                    AND status NOT IN ('cancelled', 'CANCELLED', 'failed', 'FAILED')
                    ORDER BY updated_at DESC
                """)
                
                if running_workflows:
                    logger.trace(f"ğŸ”„ [å®æ—¶åŒæ­¥] æ£€æŸ¥ {len(running_workflows)} ä¸ªè¿è¡Œä¸­çš„å·¥ä½œæµçŠ¶æ€")
                    
                    for workflow in running_workflows:
                        workflow_id = workflow['workflow_instance_id']
                        
                        # æ£€æŸ¥èŠ‚ç‚¹å®ä¾‹çŠ¶æ€æ˜¯å¦æœ‰å˜åŒ–
                        nodes_status = await self.workflow_instance_repo.db.fetch_all("""
                            SELECT node_instance_id, status, updated_at
                            FROM node_instance 
                            WHERE workflow_instance_id = $1 
                            AND is_deleted = FALSE
                            ORDER BY updated_at DESC
                        """, workflow_id)
                        
                        completed_nodes = sum(1 for n in nodes_status if n['status'] == 'completed')
                        total_nodes = len(nodes_status)
                        
                        # å¦‚æœæ‰€æœ‰èŠ‚ç‚¹éƒ½å®Œæˆäº†ï¼Œä½†å·¥ä½œæµçŠ¶æ€è¿˜æ˜¯RUNNINGï¼Œç«‹å³æ›´æ–°
                        if total_nodes > 0 and completed_nodes == total_nodes and workflow['status'] == 'RUNNING':
                            logger.info(f"ğŸ¯ [å®æ—¶åŒæ­¥] å‘ç°å®Œæˆçš„å·¥ä½œæµéœ€è¦çŠ¶æ€æ›´æ–°: {workflow['workflow_instance_name']}")
                            
                            # è§¦å‘çŠ¶æ€æ›´æ–°ï¼ˆé€šè¿‡æ‰§è¡Œå¼•æ“ï¼‰
                            try:
                                from .execution_service import execution_engine
                                # ğŸ”§ ä¿®å¤ï¼šè°ƒç”¨æ‰§è¡Œå¼•æ“çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯context_managerçš„æ–¹æ³•
                                await execution_engine._check_workflow_completion(workflow_id)
                            except Exception as sync_error:
                                logger.error(f"å®æ—¶åŒæ­¥è§¦å‘çŠ¶æ€æ›´æ–°å¤±è´¥: {sync_error}")
                
                # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œä¿æŒé«˜å®æ—¶æ€§
                await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"å®æ—¶çŠ¶æ€åŒæ­¥å¤±è´¥: {e}")
                await asyncio.sleep(10)  # é”™è¯¯æ—¶ç­‰å¾…10ç§’å†é‡è¯•
    
    async def _create_alert(self, alert_type: str, message: str, 
                          severity: str, context: Optional[Dict[str, Any]] = None):
        """åˆ›å»ºå‘Šè­¦"""
        try:
            alert = {
                'id': str(uuid.uuid4()),
                'type': alert_type,
                'message': message,
                'severity': severity,
                'context': context or {},
                'created_at': now_utc(),
                'acknowledged': False
            }
            
            self.alerts.append(alert)
            
            # é™åˆ¶å‘Šè­¦å†å²æ•°é‡
            if len(self.alerts) > 1000:
                self.alerts = self.alerts[-500:]
            
            logger.warning(f"åˆ›å»ºå‘Šè­¦: [{severity}] {message}")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å‘Šè­¦é€šçŸ¥é€»è¾‘
            # å¦‚å‘é€é‚®ä»¶ã€å‘é€åˆ°ç›‘æ§ç³»ç»Ÿç­‰
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå‘Šè­¦å¤±è´¥: {e}")
    
    async def get_current_metrics(self) -> Dict[str, Any]:
        """è·å–å½“å‰æŒ‡æ ‡"""
        try:
            await self._update_basic_metrics()
            
            return {
                'metrics': self.metrics,
                'alerts': {
                    'total': len(self.alerts),
                    'unacknowledged': len([a for a in self.alerts if not a['acknowledged']]),
                    'recent': self.alerts[-10:] if self.alerts else []
                },
                'system_status': {
                    'monitoring_active': self.is_monitoring,
                    'last_check': now_utc()
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–å½“å‰æŒ‡æ ‡å¤±è´¥: {e}")
            raise
    
    async def get_workflow_health(self, instance_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµå¥åº·çŠ¶æ€"""
        try:
            # è·å–å·¥ä½œæµå®ä¾‹ä¿¡æ¯
            instance = await self.workflow_instance_repo.get_instance_by_id(instance_id)
            if not instance:
                raise ValueError("å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨")
            
            # è·å–æ‰§è¡Œç»Ÿè®¡
            stats = await self.workflow_instance_repo.get_execution_statistics(instance_id)
            
            # è®¡ç®—å¥åº·åˆ†æ•°
            health_score = await self._calculate_health_score(instance, stats)
            
            # è¯†åˆ«é—®é¢˜
            issues = await self._identify_issues(instance, stats)
            
            return {
                'instance_id': instance_id,
                'health_score': health_score,
                'status': instance['status'],
                'statistics': stats,
                'issues': issues,
                'recommendations': await self._generate_recommendations(issues)
            }
            
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµå¥åº·çŠ¶æ€å¤±è´¥: {e}")
            raise
    
    async def _calculate_health_score(self, instance: Dict[str, Any], 
                                    stats: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—å¥åº·åˆ†æ•°"""
        try:
            score = 100.0
            
            # æ ¹æ®çŠ¶æ€æ‰£åˆ†
            status = instance['status']
            if status == WorkflowInstanceStatus.FAILED.value:
                score -= 50
            elif status == WorkflowInstanceStatus.PAUSED.value:
                score -= 20
            
            # æ ¹æ®ä»»åŠ¡å¤±è´¥ç‡æ‰£åˆ†
            if stats:
                total_tasks = stats.get('total_tasks', 0)
                failed_tasks = stats.get('failed_tasks', 0)
                
                if total_tasks > 0:
                    failure_rate = failed_tasks / total_tasks
                    score -= failure_rate * 30
            
            # æ ¹æ®æ‰§è¡Œæ—¶é—´æ‰£åˆ†
            if instance.get('started_at') and not instance.get('completed_at'):
                started_at = datetime.fromisoformat(instance['started_at'].replace('Z', '+00:00'))
                running_time = datetime.now().replace(tzinfo=started_at.tzinfo) - started_at
                running_hours = running_time.total_seconds() / 3600
                
                if running_hours > 2:  # è¿è¡Œè¶…è¿‡2å°æ—¶
                    score -= min(running_hours * 5, 30)
            
            return max(score, 0.0)
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¥åº·åˆ†æ•°å¤±è´¥: {e}")
            return 50.0
    
    async def _identify_issues(self, instance: Dict[str, Any], 
                             stats: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«é—®é¢˜"""
        issues = []
        
        try:
            # æ£€æŸ¥çŠ¶æ€é—®é¢˜
            if instance['status'] == WorkflowInstanceStatus.FAILED.value:
                issues.append({
                    'type': 'workflow_failed',
                    'severity': 'error',
                    'message': 'å·¥ä½œæµæ‰§è¡Œå¤±è´¥',
                    'details': instance.get('error_message', '')
                })
            
            # æ£€æŸ¥ä»»åŠ¡é—®é¢˜
            if stats:
                failed_tasks = stats.get('failed_tasks', 0)
                if failed_tasks > 0:
                    issues.append({
                        'type': 'failed_tasks',
                        'severity': 'warning',
                        'message': f'å­˜åœ¨ {failed_tasks} ä¸ªå¤±è´¥ä»»åŠ¡',
                        'details': f'å¤±è´¥ä»»åŠ¡æ•°é‡: {failed_tasks}'
                    })
                
                pending_tasks = stats.get('pending_tasks', 0)
                if pending_tasks > 10:
                    issues.append({
                        'type': 'task_backlog',
                        'severity': 'warning',
                        'message': f'å¾…å¤„ç†ä»»åŠ¡ç§¯å‹: {pending_tasks} ä¸ª',
                        'details': f'å¾…å¤„ç†ä»»åŠ¡æ•°é‡: {pending_tasks}'
                    })
            
            return issues
            
        except Exception as e:
            logger.error(f"è¯†åˆ«é—®é¢˜å¤±è´¥: {e}")
            return issues
    
    async def _generate_recommendations(self, issues: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        try:
            for issue in issues:
                issue_type = issue['type']
                
                if issue_type == 'workflow_failed':
                    recommendations.append("æ£€æŸ¥å·¥ä½œæµå®šä¹‰å’Œè¾“å…¥æ•°æ®")
                    recommendations.append("æŸ¥çœ‹é”™è¯¯æ—¥å¿—ç¡®å®šå¤±è´¥åŸå› ")
                elif issue_type == 'failed_tasks':
                    recommendations.append("æ£€æŸ¥å¤±è´¥ä»»åŠ¡çš„é”™è¯¯ä¿¡æ¯")
                    recommendations.append("è€ƒè™‘é‡è¯•å¤±è´¥çš„ä»»åŠ¡")
                elif issue_type == 'task_backlog':
                    recommendations.append("æ£€æŸ¥AgentæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
                    recommendations.append("è€ƒè™‘å¢åŠ å¤„ç†å™¨å¹¶å‘æ•°é‡")
            
            if not recommendations:
                recommendations.append("å·¥ä½œæµè¿è¡Œæ­£å¸¸ï¼Œæ— éœ€ç‰¹åˆ«å¤„ç†")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            return ["è¯·è”ç³»ç³»ç»Ÿç®¡ç†å‘˜"]
    
    async def acknowledge_alert(self, alert_id: str) -> bool:
        """ç¡®è®¤å‘Šè­¦"""
        try:
            for alert in self.alerts:
                if alert['id'] == alert_id:
                    alert['acknowledged'] = True
                    alert['acknowledged_at'] = now_utc()
                    logger.info(f"å‘Šè­¦å·²ç¡®è®¤: {alert_id}")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"ç¡®è®¤å‘Šè­¦å¤±è´¥: {e}")
            return False
    
    async def get_performance_report(self, days: int = 7) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        try:
            # è¿™é‡Œåº”è¯¥ä»å†å²æ•°æ®ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            # ç®€åŒ–å®ç°ï¼Œè¿”å›å½“å‰æŒ‡æ ‡
            
            report = {
                'period': f'æœ€è¿‘ {days} å¤©',
                'summary': {
                    'total_workflows': self.metrics['workflows']['total'],
                    'success_rate': self.metrics['performance']['success_rate'],
                    'avg_task_duration': self.metrics['performance']['avg_task_duration']
                },
                'trends': {
                    'workflow_count': [10, 15, 12, 18, 20, 16, 14],  # æ¨¡æ‹Ÿæ•°æ®
                    'success_rate': [95, 94, 96, 93, 97, 95, 96],      # æ¨¡æ‹Ÿæ•°æ®
                    'avg_duration': [25, 23, 27, 22, 24, 26, 23]       # æ¨¡æ‹Ÿæ•°æ®
                },
                'generated_at': now_utc()
            }
            
            return report
            
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    async def register_workflow_completion_callback(self, workflow_instance_id: uuid.UUID, 
                                                   callback_func) -> bool:
        """æ³¨å†Œå·¥ä½œæµå®Œæˆå›è°ƒ"""
        try:
            logger.info(f"ğŸ”” æ³¨å†Œå·¥ä½œæµå®Œæˆå›è°ƒ: {workflow_instance_id}")
            
            if workflow_instance_id not in self.workflow_completion_callbacks:
                self.workflow_completion_callbacks[workflow_instance_id] = []
            
            self.workflow_completion_callbacks[workflow_instance_id].append(callback_func)
            
            logger.info(f"âœ… å·¥ä½œæµå®Œæˆå›è°ƒæ³¨å†ŒæˆåŠŸ: {workflow_instance_id}")
            logger.info(f"   - è¯¥å·¥ä½œæµç°æœ‰å›è°ƒæ•°é‡: {len(self.workflow_completion_callbacks[workflow_instance_id])}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ³¨å†Œå·¥ä½œæµå®Œæˆå›è°ƒå¤±è´¥: {e}")
            return False
    
    async def _check_workflow_completion_and_trigger_callbacks(self):
        """æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å¹¶è§¦å‘å›è°ƒ"""
        try:
            if not self.workflow_completion_callbacks:
                return
            
            # æ£€æŸ¥æ³¨å†Œäº†å›è°ƒçš„å·¥ä½œæµçŠ¶æ€
            for workflow_instance_id, callbacks in list(self.workflow_completion_callbacks.items()):
                try:
                    # æŸ¥è¯¢å·¥ä½œæµçŠ¶æ€
                    workflow_instance = await self.workflow_instance_repo.get_workflow_instance_by_id(workflow_instance_id)
                    
                    if not workflow_instance:
                        # å·¥ä½œæµä¸å­˜åœ¨ï¼Œæ¸…ç†å›è°ƒ
                        logger.warning(f"å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨ï¼Œæ¸…ç†å›è°ƒ: {workflow_instance_id}")
                        del self.workflow_completion_callbacks[workflow_instance_id]
                        continue
                    
                    status = workflow_instance.get('status')
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆæˆåŠŸã€å¤±è´¥æˆ–å–æ¶ˆï¼‰
                    if status in ['completed', 'failed', 'cancelled', 'timeout']:
                        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°å·¥ä½œæµå®Œæˆ: {workflow_instance_id}, çŠ¶æ€: {status}")
                        
                        # æ”¶é›†æ‰§è¡Œç»“æœ
                        results = await self._collect_workflow_results(workflow_instance_id)
                        
                        # è§¦å‘æ‰€æœ‰å›è°ƒ
                        for callback in callbacks:
                            try:
                                await callback(workflow_instance_id, status, results)
                                logger.info(f"âœ… å·¥ä½œæµå®Œæˆå›è°ƒæ‰§è¡ŒæˆåŠŸ: {workflow_instance_id}")
                            except Exception as callback_e:
                                logger.error(f"å·¥ä½œæµå®Œæˆå›è°ƒæ‰§è¡Œå¤±è´¥: {callback_e}")
                        
                        # æ¸…ç†å·²å®Œæˆçš„å·¥ä½œæµå›è°ƒ
                        del self.workflow_completion_callbacks[workflow_instance_id]
                        logger.info(f"ğŸ§¹ å·²æ¸…ç†å·¥ä½œæµå›è°ƒ: {workflow_instance_id}")
                
                except Exception as check_e:
                    logger.error(f"æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å¤±è´¥: {workflow_instance_id}, é”™è¯¯: {check_e}")
                    
        except Exception as e:
            logger.error(f"æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å’Œè§¦å‘å›è°ƒå¤±è´¥: {e}")
    
    async def _collect_workflow_results(self, workflow_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ”¶é›†å·¥ä½œæµæ‰§è¡Œç»“æœ"""
        try:
            logger.info(f"ğŸ” æ”¶é›†å·¥ä½œæµæ‰§è¡Œç»“æœ: {workflow_instance_id}")
            
            # è·å–å·¥ä½œæµå®ä¾‹ä¿¡æ¯
            workflow_instance = await self.workflow_instance_repo.get_workflow_instance_by_id(workflow_instance_id)
            
            # è·å–è¯¥å·¥ä½œæµçš„æ‰€æœ‰ä»»åŠ¡
            tasks = await self.task_instance_repo.get_tasks_by_workflow_instance(workflow_instance_id)
            
            # ç»Ÿè®¡ä»»åŠ¡å®Œæˆæƒ…å†µ
            total_tasks = len(tasks)
            completed_tasks = len([t for t in tasks if t.get('status') == 'completed'])
            failed_tasks = len([t for t in tasks if t.get('status') == 'failed'])
            
            logger.info(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡: æ€»è®¡ {total_tasks}, å®Œæˆ {completed_tasks}, å¤±è´¥ {failed_tasks}")
            
            # æ”¶é›†ä»»åŠ¡ç»“æœ
            task_results = []
            
            for task in tasks:
                task_result = {
                    'task_id': task.get('task_instance_id'),
                    'title': task.get('task_title'),
                    'status': task.get('status'),
                    'output': task.get('output_data', ''),
                    'result_summary': task.get('result_summary', '')
                }
                task_results.append(task_result)
            
            # ğŸ”§ æ–°å¢ï¼šæŸ¥æ‰¾ç»“æŸèŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®ï¼Œè·å–å®Œæ•´çš„å·¥ä½œæµä¸Šä¸‹æ–‡
            end_node_output = None
            end_nodes_query = """
            SELECT ni.output_data, n.name as node_name, ni.node_instance_id
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = %s 
            AND n.type = 'end'
            AND ni.status = 'completed'
            ORDER BY ni.updated_at DESC
            LIMIT 1
            """
            
            try:
                end_node = await self.workflow_instance_repo.db.fetch_one(end_nodes_query, workflow_instance_id)
                if end_node and end_node['output_data']:
                    end_node_output = end_node['output_data']
                    logger.info(f"âœ… æ‰¾åˆ°ç»“æŸèŠ‚ç‚¹è¾“å‡º: {end_node['node_name']}, æ•°æ®é•¿åº¦: {len(str(end_node_output))} å­—ç¬¦")
                else:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç»“æŸèŠ‚ç‚¹è¾“å‡ºæ•°æ®")
            except Exception as end_node_e:
                logger.error(f"âŒ æŸ¥è¯¢ç»“æŸèŠ‚ç‚¹å¤±è´¥: {end_node_e}")
            
            # ğŸ”§ ç¡®å®šæœ€ç»ˆè¾“å‡ºï¼šä¼˜å…ˆä½¿ç”¨ç»“æŸèŠ‚ç‚¹è¾“å‡ºï¼Œå¦åˆ™ä½¿ç”¨ä»»åŠ¡è¾“å‡ºæ‹¼æ¥
            final_output = ""
            if end_node_output:
                # å¦‚æœç»“æŸèŠ‚ç‚¹æœ‰è¾“å‡ºï¼Œä½¿ç”¨ç»“æŸèŠ‚ç‚¹çš„å®Œæ•´ä¸Šä¸‹æ–‡
                if isinstance(end_node_output, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å–å®Œæ•´ä¸Šä¸‹æ–‡æˆ–æ ¼å¼åŒ–è¾“å‡º
                    if 'full_context' in end_node_output:
                        final_output = str(end_node_output['full_context'])
                    elif 'context_data' in end_node_output:
                        final_output = str(end_node_output['context_data'])
                    else:
                        # æ ¼å¼åŒ–å­—å…¸è¾“å‡º
                        final_output = self._format_dict_output(end_node_output)
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    final_output = str(end_node_output)
                
                logger.info(f"ğŸ“‹ ä½¿ç”¨ç»“æŸèŠ‚ç‚¹è¾“å‡ºä½œä¸ºæœ€ç»ˆç»“æœï¼Œé•¿åº¦: {len(final_output)} å­—ç¬¦")
            else:
                # å›é€€åˆ°åŸæ¥çš„é€»è¾‘ï¼šæ”¶é›†æ‰€æœ‰å®Œæˆä»»åŠ¡çš„è¾“å‡º
                final_outputs = []
                for task in tasks:
                    if task.get('status') == 'completed' and task.get('output_data'):
                        final_outputs.append(str(task.get('output_data')))
                
                final_output = '\n\n=== ä»»åŠ¡è¾“å‡ºåˆ†éš” ===\n\n'.join(final_outputs) if final_outputs else ''
                logger.info(f"ğŸ“‹ ä½¿ç”¨ä»»åŠ¡è¾“å‡ºæ‹¼æ¥ä½œä¸ºæœ€ç»ˆç»“æœï¼Œé•¿åº¦: {len(final_output)} å­—ç¬¦")
            
            # æ„å»ºç»“æœå¯¹è±¡
            results = {
                'workflow_instance_id': str(workflow_instance_id),
                'status': workflow_instance.get('status'),
                'total_tasks': total_tasks,
                'completed_tasks': completed_tasks,
                'failed_tasks': failed_tasks,
                'task_results': task_results,
                'final_output': final_output,
                'has_end_node_output': end_node_output is not None,
                'execution_duration': self._calculate_execution_duration(workflow_instance),
                'started_at': workflow_instance.get('created_at'),
                'completed_at': workflow_instance.get('updated_at')
            }
            
            logger.info(f"âœ… å·¥ä½œæµç»“æœæ”¶é›†å®Œæˆï¼Œæœ€ç»ˆè¾“å‡ºé•¿åº¦: {len(final_output)} å­—ç¬¦")
            return results
            
        except Exception as e:
            logger.error(f"æ”¶é›†å·¥ä½œæµæ‰§è¡Œç»“æœå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    def _format_dict_output(self, data: dict) -> str:
        """æ ¼å¼åŒ–å­—å…¸è¾“å‡ºä¸ºå¯è¯»æ–‡æœ¬"""
        try:
            if not data:
                return "æ— è¾“å‡ºæ•°æ®"
            
            # å°è¯•æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬
            output_parts = []
            
            for key, value in data.items():
                if key.startswith('_'):  # è·³è¿‡ç§æœ‰å­—æ®µ
                    continue
                
                if isinstance(value, dict):
                    output_parts.append(f"**{key}:**")
                    for sub_key, sub_value in value.items():
                        output_parts.append(f"  â€¢ {sub_key}: {str(sub_value)}")
                elif isinstance(value, list):
                    output_parts.append(f"**{key}:** ({len(value)} é¡¹)")
                    for i, item in enumerate(value[:5]):  # åªæ˜¾ç¤ºå‰5é¡¹
                        output_parts.append(f"  {i+1}. {str(item)}")
                    if len(value) > 5:
                        output_parts.append(f"  ... è¿˜æœ‰ {len(value) - 5} é¡¹")
                else:
                    output_parts.append(f"**{key}:** {str(value)}")
            
            return "\n".join(output_parts)
            
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–å­—å…¸è¾“å‡ºå¤±è´¥: {e}")
            return str(data)
    
    def _calculate_execution_duration(self, workflow_instance: Dict[str, Any]) -> str:
        """è®¡ç®—æ‰§è¡Œæ—¶é•¿"""
        try:
            started_at = workflow_instance.get('created_at')
            completed_at = workflow_instance.get('updated_at')
            
            if started_at and completed_at:
                if isinstance(started_at, str):
                    started_at = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
                if isinstance(completed_at, str):
                    completed_at = datetime.fromisoformat(completed_at.replace('Z', '+00:00'))
                
                duration = completed_at - started_at
                
                hours = duration.seconds // 3600
                minutes = (duration.seconds % 3600) // 60
                seconds = duration.seconds % 60
                
                if hours > 0:
                    return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’"
                elif minutes > 0:
                    return f"{minutes}åˆ†é’Ÿ{seconds}ç§’"
                else:
                    return f"{seconds}ç§’"
            
            return "æœªçŸ¥"
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ‰§è¡Œæ—¶é•¿å¤±è´¥: {e}")
            return "æœªçŸ¥"


# å…¨å±€ç›‘æ§æœåŠ¡å®ä¾‹
monitoring_service = MonitoringService()