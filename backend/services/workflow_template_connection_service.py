"""
å·¥ä½œæµæ¨¡æ¿è¿æ¥æœåŠ¡ - Linuså¼é‡æ„ç‰ˆæœ¬
Workflow Template Connection Service - Linus Refactored

æ ¸å¿ƒæ€æƒ³ï¼š
1. subdivisionå°±æ˜¯æ ‘ï¼Œä¸æ˜¯å¤æ‚çš„å›¾
2. ä¸€ä¸ªæŸ¥è¯¢ï¼Œä¸€ä¸ªæ•°æ®ç»“æ„ï¼Œä¸€å¥—ç®—æ³•
3. æ¶ˆé™¤æ‰€æœ‰ç‰¹æ®Šæƒ…å†µå’Œ4å±‚åµŒå¥—
4. "å¥½ç¨‹åºå‘˜å…³å¿ƒæ•°æ®ç»“æ„ï¼Œä¸æ˜¯ä»£ç " - Linus
"""

import uuid
from typing import Optional, Dict, Any, List
from loguru import logger

from ..repositories.base import BaseRepository
from ..utils.helpers import now_utc


class WorkflowTemplateConnectionService:
    """å·¥ä½œæµæ¨¡æ¿è¿æ¥æœåŠ¡ - é‡æ„ç‰ˆæœ¬"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_template_connection").db
    
    async def get_detailed_workflow_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """
        Linuså¼ç®€åŒ–ç‰ˆæœ¬ï¼šsubdivisionå°±æ˜¯æ ‘ï¼Œåˆ«æå¤æ‚äº†
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦ï¼ˆå®é™…ä¸Šç”¨ä¸åˆ°ï¼Œæ ‘å¤©ç„¶æœ‰é™æ·±åº¦ï¼‰
            
        Returns:
            ç®€åŒ–çš„è¿æ¥å›¾æ•°æ®ç»“æ„
        """
        # try:
        logger.info(f"ğŸŒ³ [Linuså¼ç®€åŒ–] è·å–subdivisionæ ‘: {workflow_instance_id}")
        
        # ç®€å•æŸ¥è¯¢ï¼šè·å–æ‰€æœ‰subdivisionï¼Œè®©æ ‘æ„å»ºå™¨å¤„ç†å±‚çº§å…³ç³»
        subdivisions = await self._get_all_subdivisions_simple(workflow_instance_id)
        
        if not subdivisions:
            logger.info(f"ğŸ“‹ æœªæ‰¾åˆ°subdivision: {workflow_instance_id}")
            return self._empty_connection_result(workflow_instance_id)
        
        # ä½¿ç”¨æ–°çš„å·¥ä½œæµæ¨¡æ¿æ ‘æ„å»ºå™¨
        from .workflow_template_tree import WorkflowTemplateTree
        tree = await WorkflowTemplateTree().build_from_subdivisions(subdivisions, workflow_instance_id)
        
        # ç›´æ¥ä»æ ‘è·å–å›¾å½¢æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
        graph_data = tree.to_graph_data()
        statistics = tree.get_statistics()
        
        result = {
            "workflow_instance_id": str(workflow_instance_id),
            "template_connections": [],  # ä¿æŒå…¼å®¹æ€§ï¼Œå®é™…æ•°æ®åœ¨graphé‡Œ
            "detailed_workflows": {},    # ç®€åŒ–åä¸éœ€è¦
            "merge_candidates": [],      # ç®€åŒ–åä¸éœ€è¦
            "detailed_connection_graph": graph_data,
            "statistics": statistics
        }
        
        logger.info(f"âœ… [Linuså¼ç®€åŒ–] subdivisionæ ‘æ„å»ºå®Œæˆ: {statistics}")
        return result
            
        # except Exception as e:
        #     logger.error(f"âŒ [Linuså¼ç®€åŒ–] è·å–subdivisionæ ‘å¤±è´¥: {e}")
        #     # å¦‚æœLinuså¼ç®€åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ—§ç‰ˆæœ¬
        #     logger.info(f"ğŸ”„ [å›é€€] ä½¿ç”¨æ—§ç‰ˆæœ¬æ–¹æ³•")
        #     return await self._get_detailed_workflow_connections_old(workflow_instance_id, max_depth)

    async def _get_all_subdivisions_simple(self, workflow_instance_id: uuid.UUID) -> List[Dict[str, Any]]:
        """
        é€’å½’æŸ¥è¯¢ï¼šè·å–æ‰€æœ‰subdivisionï¼ˆåŒ…æ‹¬åµŒå¥—çš„ï¼‰ï¼Œæ„å»ºå®Œæ•´çš„subdivisionæ ‘
        
        subdivisionè¡¨æœ‰parent_subdivision_idï¼Œä½†æˆ‘ä»¬éœ€è¦è·¨å·¥ä½œæµå®ä¾‹é€’å½’æŸ¥æ‰¾
        """
        try:
            logger.info(f"ğŸŒ³ å¼€å§‹é€’å½’æŸ¥è¯¢subdivision: {workflow_instance_id}")
            logger.info(f"ğŸ“Š [è°ƒè¯•] å·¥ä½œæµå®ä¾‹IDç±»å‹: {type(workflow_instance_id)}, å€¼: {workflow_instance_id}")
            
            all_subdivisions = []
            processed_workflows = set()
            
            # ğŸ”§ å¢åŠ è°ƒè¯•ï¼šæ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨
            workflow_check = await self.db.fetch_one("""
                SELECT workflow_instance_id, status, created_at 
                FROM workflow_instance 
                WHERE workflow_instance_id = %s
            """, workflow_instance_id)
            logger.info(f"ğŸ“Š [è°ƒè¯•] å·¥ä½œæµå®ä¾‹æ£€æŸ¥: {workflow_check}")
            
            if not workflow_check:
                logger.error(f"âŒ [ä¸¥é‡é”™è¯¯] å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {workflow_instance_id}")
                return []
            
            # ğŸ”§ å¢åŠ è°ƒè¯•ï¼šæ£€æŸ¥task_subdivisionè¡¨æ˜¯å¦æœ‰è®°å½•
            subdivision_count = await self.db.fetch_one("""
                SELECT COUNT(*) as count 
                FROM task_subdivision ts
                JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
                WHERE ti.workflow_instance_id = %s
                AND ts.is_deleted = FALSE
            """, workflow_instance_id)
            logger.info(f"ğŸ“Š [è°ƒè¯•] subdivisionæ€»æ•°é‡: {subdivision_count['count'] if subdivision_count else 0}")
            
            async def recursive_query(current_workflow_id: uuid.UUID, current_depth: int = 1, max_depth: int = 10):
                """é€’å½’æŸ¥è¯¢subdivision"""
                if current_depth > max_depth or str(current_workflow_id) in processed_workflows:
                    return
                
                processed_workflows.add(str(current_workflow_id))
                logger.info(f"  ğŸ” æŸ¥è¯¢ç¬¬{current_depth}å±‚: {current_workflow_id}")
                
                # ğŸ”§ å…ˆæ£€æŸ¥åŸºç¡€æ•°æ®
                basic_check = await self.db.fetch_all("""
                    SELECT COUNT(*) as total_subdivisions,
                           COUNT(ts.sub_workflow_instance_id) as subdivisions_with_workflow,
                           COUNT(ts.sub_workflow_base_id) as subdivisions_with_base_id
                    FROM task_subdivision ts
                    JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
                    WHERE ti.workflow_instance_id = %s
                    AND ts.is_deleted = FALSE
                    AND ti.is_deleted = FALSE
                """, current_workflow_id)
                logger.info(f"    ğŸ“Š åŸºç¡€ç»Ÿè®¡: {dict(basic_check[0]) if basic_check else 'None'}")
                
                # æŸ¥è¯¢å½“å‰å·¥ä½œæµçš„subdivisions - ä¿®å¤ç‰ˆæœ¬
                query = """
                SELECT 
                    ts.subdivision_id,
                    ts.parent_subdivision_id,
                    ts.subdivision_name,
                    ts.subdivision_description,
                    ts.subdivision_created_at,
                    ts.sub_workflow_base_id,
                    ts.sub_workflow_instance_id,
                    
                    -- ä»»åŠ¡ä¿¡æ¯
                    ti.task_title,
                    ti.task_description,
                    
                    -- èŠ‚ç‚¹ä¿¡æ¯
                    n.name as original_node_name,
                    n.type as original_node_type,
                    
                    -- å·¥ä½œæµä¿¡æ¯
                    sw.name as sub_workflow_name,
                    sw.description as sub_workflow_description,
                    
                    -- å®ä¾‹çŠ¶æ€
                    swi.status as sub_workflow_status,
                    swi.started_at as sub_workflow_started_at,
                    swi.completed_at as sub_workflow_completed_at,
                    
                    -- å±‚çº§ä¿¡æ¯
                    %s as depth,
                    %s as root_workflow_instance_id
                    
                FROM task_subdivision ts
                JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
                JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
                LEFT JOIN node n ON ni.node_id = n.node_id  -- ğŸ”§ ä¿®å¤ï¼šæ”¹ä¸ºLEFT JOINï¼Œå…¼å®¹ç©ºç‰ˆæœ¬å·¥ä½œæµ
                LEFT JOIN workflow sw ON ts.sub_workflow_base_id = sw.workflow_base_id AND sw.is_current_version = TRUE
                LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
                WHERE ti.workflow_instance_id = %s
                AND ts.is_deleted = FALSE
                AND ti.is_deleted = FALSE
                AND ni.is_deleted = FALSE
                -- ğŸ”§ ä¿®å¤ï¼šä¸è¦è¿‡æ»¤æ‰ sub_workflow_instance_id ä¸º NULL çš„è®°å½•
                -- å› ä¸ºæœ‰äº›subdivisionå¯èƒ½å¤„äºåˆ›å»ºä¸­æˆ–è€…æœ‰å…¶ä»–çŠ¶æ€
                ORDER BY ts.subdivision_created_at
                """
                
                logger.info(f"    ğŸ” æ‰§è¡ŒsubdivisionæŸ¥è¯¢...")
                subdivisions = await self.db.fetch_all(query, current_depth, current_workflow_id, current_workflow_id)
                current_level_subdivisions = [dict(row) for row in subdivisions]
                
                logger.info(f"    ğŸ“¦ ç¬¬{current_depth}å±‚åŸå§‹æŸ¥è¯¢ç»“æœ: {len(current_level_subdivisions)} ä¸ªsubdivision")
                
                # ğŸ”§ å¢åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                for i, sub in enumerate(current_level_subdivisions[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"      subdivision {i+1}:")
                    logger.info(f"        subdivision_id: {sub.get('subdivision_id')}")
                    logger.info(f"        sub_workflow_instance_id: {sub.get('sub_workflow_instance_id')}")
                    logger.info(f"        sub_workflow_base_id: {sub.get('sub_workflow_base_id')}")
                    logger.info(f"        subdivision_name: {sub.get('subdivision_name')}")
                    logger.info(f"        task_title: {sub.get('task_title')}")
                    logger.info(f"        sub_workflow_status: {sub.get('sub_workflow_status')}")
                
                # è¿‡æ»¤æœ‰æ•ˆçš„subdivisionï¼šå¿…é¡»æœ‰sub_workflow_base_id
                valid_subdivisions = []
                for sub in current_level_subdivisions:
                    if sub.get('sub_workflow_base_id'):
                        valid_subdivisions.append(sub)
                    else:
                        logger.warning(f"      âš ï¸ è·³è¿‡æ— æ•ˆsubdivision (ç¼ºå°‘sub_workflow_base_id): {sub.get('subdivision_id')}")
                
                logger.info(f"    âœ… ç¬¬{current_depth}å±‚æœ‰æ•ˆsubdivision: {len(valid_subdivisions)} ä¸ª")
                
                if valid_subdivisions:
                    all_subdivisions.extend(valid_subdivisions)
                    
                    # é€’å½’æŸ¥è¯¢å­å·¥ä½œæµçš„subdivisions - åªå¯¹æœ‰workflow_instance_idçš„ç»§ç»­é€’å½’
                    child_workflow_ids = []
                    for sub in valid_subdivisions:
                        if sub.get('sub_workflow_instance_id'):
                            child_workflow_ids.append(uuid.UUID(sub['sub_workflow_instance_id']))
                    
                    logger.info(f"    ğŸ”„ å‡†å¤‡é€’å½’æŸ¥è¯¢ {len(child_workflow_ids)} ä¸ªå­å·¥ä½œæµ")
                    
                    # å¯¹æ¯ä¸ªå­å·¥ä½œæµè¿›è¡Œé€’å½’æŸ¥è¯¢
                    for child_id in child_workflow_ids:
                        await recursive_query(child_id, current_depth + 1, max_depth)
                else:
                    logger.info(f"    ğŸ“­ ç¬¬{current_depth}å±‚æ— æœ‰æ•ˆsubdivision")
            
            # å¼€å§‹é€’å½’æŸ¥è¯¢
            await recursive_query(workflow_instance_id, 1)
            
            logger.info(f"ğŸŒ³ é€’å½’æŸ¥è¯¢å®Œæˆ: æ‰¾åˆ° {len(all_subdivisions)} ä¸ªsubdivisionè®°å½•ï¼ˆåŒ…æ‹¬åµŒå¥—ï¼‰")
            
            # è°ƒè¯•è¾“å‡ºå±‚çº§ä¿¡æ¯
            by_depth = {}
            for sub in all_subdivisions:
                depth = sub['depth']
                if depth not in by_depth:
                    by_depth[depth] = []
                by_depth[depth].append(sub['subdivision_name'])
            
            for depth in sorted(by_depth.keys()):
                names_str = ', '.join(by_depth[depth])
                logger.info(f"  ç¬¬{depth}å±‚: {names_str}")
            
            return all_subdivisions
            
        except Exception as e:
            logger.error(f"âŒ é€’å½’æŸ¥è¯¢subdivisionå¤±è´¥: {e}")
            raise
    
    def _empty_connection_result(self, workflow_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„è¿æ¥ç»“æœ"""
        logger.warning(f"ğŸ” è¿”å›ç©ºçš„è¿æ¥ç»“æœ: {workflow_instance_id}")
        return {
            "workflow_instance_id": str(workflow_instance_id),
            "template_connections": [],
            "detailed_workflows": {},
            "merge_candidates": [],
            "detailed_connection_graph": {
                "nodes": [],
                "edges": [],
                "layout": {
                    "algorithm": "simple_tree",
                    "max_depth": 0,
                    "total_nodes": 0,
                    "root_count": 0
                }
            },
            "statistics": {
                "total_subdivisions": 0,
                "completed_sub_workflows": 0,
                "unique_workflows": 0,
                "max_depth": 0
            }
        }
    
    # ä¿æŒå‘åå…¼å®¹çš„æ—§æ–¹æ³•ï¼ˆå§”æ‰˜ç»™æ–°æ–¹æ³•ï¼‰
    async def get_workflow_template_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """å‘åå…¼å®¹æ–¹æ³•ï¼Œå§”æ‰˜ç»™æ–°çš„ç®€åŒ–å®ç°"""
        result = await self.get_detailed_workflow_connections(workflow_instance_id, max_depth)
        
        # è½¬æ¢ä¸ºæ—§æ ¼å¼
        return {
            "workflow_instance_id": result["workflow_instance_id"],
            "template_connections": result["template_connections"],
            "connection_graph": result["detailed_connection_graph"],
            "recursive_levels": result["statistics"]["max_depth"],
            "statistics": result["statistics"]
        }
    
    async def get_workflow_template_connection_summary(self, workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµæ¨¡æ¿çš„è¿æ¥å…³ç³»æ‘˜è¦ï¼ˆç”¨äºæ˜¾ç¤ºæ¨¡æ¿çº§åˆ«çš„è¿æ¥ç»Ÿè®¡ï¼‰
        
        Args:
            workflow_base_id: å·¥ä½œæµåŸºç¡€ID
            
        Returns:
            è¿æ¥å…³ç³»æ‘˜è¦æ•°æ®
        """
        try:
            logger.info(f"ğŸ” è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦: {workflow_base_id}")
            
            summary_query = """
            SELECT 
                COUNT(DISTINCT ts.subdivision_id) as total_subdivisions,
                COUNT(DISTINCT ts.sub_workflow_base_id) as unique_sub_workflows,
                COUNT(DISTINCT ni.node_base_id) as connected_nodes,
                COUNT(DISTINCT swi.workflow_instance_id) as sub_workflow_instances,
                COUNT(CASE WHEN swi.status = 'completed' THEN 1 END) as completed_instances,
                MIN(ts.subdivision_created_at) as first_subdivision_at,
                MAX(ts.subdivision_created_at) as last_subdivision_at
            FROM task_subdivision ts
            JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
            JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
            LEFT JOIN node n ON ni.node_id = n.node_id  -- ğŸ”§ ä¿®å¤ï¼šæ”¹ä¸ºLEFT JOINï¼Œå…¼å®¹ç©ºç‰ˆæœ¬å·¥ä½œæµ
            LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
            WHERE n.workflow_base_id = $1
            AND ts.is_deleted = FALSE
            AND ti.is_deleted = FALSE
            AND ni.is_deleted = FALSE
            """
            
            result = await self.db.fetch_one(summary_query, workflow_base_id)
            
            if result:
                summary = {
                    "workflow_base_id": str(workflow_base_id),
                    "total_subdivisions": result["total_subdivisions"] or 0,
                    "unique_sub_workflows": result["unique_sub_workflows"] or 0,
                    "connected_nodes": result["connected_nodes"] or 0,
                    "sub_workflow_instances": result["sub_workflow_instances"] or 0,
                    "completed_instances": result["completed_instances"] or 0,
                    "success_rate": (result["completed_instances"] or 0) / max(result["sub_workflow_instances"] or 1, 1),
                    "first_subdivision_at": result["first_subdivision_at"].isoformat() if result["first_subdivision_at"] else None,
                    "last_subdivision_at": result["last_subdivision_at"].isoformat() if result["last_subdivision_at"] else None
                }
                
                logger.info(f"âœ… å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦: {summary}")
                return summary
            else:
                return {
                    "workflow_base_id": str(workflow_base_id),
                    "total_subdivisions": 0,
                    "unique_sub_workflows": 0,
                    "connected_nodes": 0,
                    "sub_workflow_instances": 0,
                    "completed_instances": 0,
                    "success_rate": 0,
                    "first_subdivision_at": None,
                    "last_subdivision_at": None
                }
                
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦å¤±è´¥: {e}")
            raise