"""
å·¥ä½œæµæ¨¡æ¿è¿æ¥æœåŠ¡
Workflow Template Connection Service

ç”¨äºè·å–å’Œåˆ†ææ‰§è¡Œå®ä¾‹å®Œæˆåçš„å·¥ä½œæµæ¨¡æ¿ä¹‹é—´çš„è¿æ¥å…³ç³»
"""

import uuid
from typing import Optional, Dict, Any, List
from loguru import logger

from ..repositories.base import BaseRepository
from ..utils.helpers import now_utc


class WorkflowTemplateConnectionService:
    """å·¥ä½œæµæ¨¡æ¿è¿æ¥æœåŠ¡"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_template_connection").db
    
    async def get_workflow_template_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """
        è·å–æ‰§è¡Œå®ä¾‹å®Œæˆåçš„å·¥ä½œæµæ¨¡æ¿è¿æ¥å›¾æ•°æ®ï¼ˆæ”¯æŒé€’å½’å±•å¼€ï¼‰
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦ï¼Œé˜²æ­¢æ— é™é€’å½’
            
        Returns:
            åŒ…å«æ¨¡æ¿è¿æ¥å…³ç³»çš„æ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šå±‚åµŒå¥—
        """
        try:
            logger.info(f"ğŸ” è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»(é€’å½’æ·±åº¦ {max_depth}): {workflow_instance_id}")
            
            # é€’å½’è·å–æ‰€æœ‰å±‚çº§çš„æ¨¡æ¿è¿æ¥å…³ç³»
            all_connections = await self._get_recursive_template_connections(workflow_instance_id, max_depth)
            
            if not all_connections:
                logger.info(f"ğŸ“‹ æœªæ‰¾åˆ°å·¥ä½œæµå®ä¾‹çš„ç»†åˆ†å…³ç³»: {workflow_instance_id}")
                return {
                    "workflow_instance_id": str(workflow_instance_id),
                    "template_connections": [],
                    "connection_graph": {
                        "nodes": [],
                        "edges": []
                    },
                    "recursive_levels": 0
                }
            
            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(all_connections)} ä¸ªå·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»ï¼ˆåŒ…å«é€’å½’ï¼‰")
            
            # æ„å»ºè¿æ¥å›¾æ•°æ®ç»“æ„ï¼ˆæ”¯æŒå¤šå±‚é€’å½’ï¼‰
            connection_graph = self._build_recursive_connection_graph(all_connections)
            
            result = {
                "workflow_instance_id": str(workflow_instance_id),
                "template_connections": all_connections,
                "connection_graph": connection_graph,
                "recursive_levels": self._calculate_max_depth(all_connections),
                "statistics": {
                    "total_subdivisions": len(all_connections),
                    "completed_sub_workflows": len([c for c in all_connections if c["sub_workflow"]["status"] == "completed"]),
                    "unique_parent_workflows": len(set(c["parent_workflow"]["workflow_base_id"] for c in all_connections)),
                    "unique_sub_workflows": len(set(c["sub_workflow"]["workflow_base_id"] for c in all_connections)),
                    "max_recursion_depth": self._calculate_max_depth(all_connections)
                }
            }
            
            logger.info(f"âœ… å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»è·å–æˆåŠŸ: {result['statistics']}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»å¤±è´¥: {e}")
            raise
    
    async def _get_recursive_template_connections(self, workflow_instance_id: uuid.UUID, max_depth: int, current_depth: int = 0) -> List[Dict[str, Any]]:
        """
        é€’å½’è·å–æ‰€æœ‰å±‚çº§çš„æ¨¡æ¿è¿æ¥å…³ç³»
        
        Args:
            workflow_instance_id: å½“å‰å±‚çº§çš„å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            current_depth: å½“å‰é€’å½’æ·±åº¦
            
        Returns:
            åŒ…å«æ‰€æœ‰å±‚çº§è¿æ¥å…³ç³»çš„åˆ—è¡¨
        """
        if current_depth >= max_depth:
            logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ {max_depth}ï¼Œåœæ­¢é€’å½’æŸ¥è¯¢")
            return []
            
        logger.debug(f"ğŸ”„ é€’å½’æŸ¥è¯¢å±‚çº§ {current_depth}: {workflow_instance_id}")
        
        # æŸ¥è¯¢å½“å‰å±‚çº§çš„ç›´æ¥å­å·¥ä½œæµ
        subdivisions_query = """
        SELECT 
            ts.subdivision_id,
            ts.original_task_id,
            ts.sub_workflow_base_id,
            ts.sub_workflow_instance_id,
            ts.subdivision_name,
            ts.subdivision_description,
            ts.subdivision_created_at,
            
            -- åŸå§‹ä»»åŠ¡ä¿¡æ¯
            ti.task_title,
            ti.task_description,
            ti.node_instance_id,
            ti.workflow_instance_id as parent_workflow_instance_id,
            
            -- åŸå§‹èŠ‚ç‚¹ä¿¡æ¯  
            ni.node_id as original_node_id,
            n.node_base_id as original_node_base_id,
            n.name as original_node_name,
            n.type as original_node_type,
            n.workflow_base_id as parent_workflow_base_id,
            
            -- çˆ¶å·¥ä½œæµä¿¡æ¯
            pw.name as parent_workflow_name,
            pw.description as parent_workflow_description,
            
            -- å­å·¥ä½œæµä¿¡æ¯
            sw.name as sub_workflow_name,
            sw.description as sub_workflow_description,
            
            -- å­å·¥ä½œæµå®ä¾‹å®ŒæˆçŠ¶æ€
            swi.status as sub_workflow_status,
            swi.started_at as sub_workflow_started_at,
            swi.completed_at as sub_workflow_completed_at,
            
            -- å­å·¥ä½œæµç»Ÿè®¡ä¿¡æ¯
            (SELECT COUNT(*) FROM node sn 
             WHERE sn.workflow_base_id = ts.sub_workflow_base_id 
             AND sn.is_current_version = TRUE 
             AND sn.is_deleted = FALSE) as sub_workflow_total_nodes,
             
            (SELECT COUNT(*) FROM node_instance sni 
             JOIN node sn ON sni.node_id = sn.node_id
             WHERE sn.workflow_base_id = ts.sub_workflow_base_id 
             AND sni.workflow_instance_id = ts.sub_workflow_instance_id
             AND sni.status = 'completed'
             AND sni.is_deleted = FALSE) as sub_workflow_completed_nodes
            
        FROM task_subdivision ts
        JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
        JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
        JOIN node n ON ni.node_id = n.node_id
        LEFT JOIN workflow pw ON n.workflow_base_id = pw.workflow_base_id AND pw.is_current_version = TRUE
        LEFT JOIN workflow sw ON ts.sub_workflow_base_id = sw.workflow_base_id AND sw.is_current_version = TRUE
        LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
        WHERE ti.workflow_instance_id = $1 
        AND ts.is_deleted = FALSE
        AND ti.is_deleted = FALSE
        AND ni.is_deleted = FALSE
        ORDER BY ts.subdivision_created_at
        """
        
        subdivisions = await self.db.fetch_all(subdivisions_query, workflow_instance_id)
        
        logger.info(f"ğŸ“Š [DEBUG] æŸ¥è¯¢subdivisionç»“æœ: å·¥ä½œæµå®ä¾‹ {workflow_instance_id}")
        logger.info(f"ğŸ“Š [DEBUG] æ‰¾åˆ° {len(subdivisions)} ä¸ªsubdivisionè®°å½•")
        
        for i, sub in enumerate(subdivisions):
            logger.info(f"ğŸ“Š [DEBUG] Subdivision {i+1}:")
            logger.info(f"    - subdivision_id: {sub['subdivision_id']}")
            logger.info(f"    - subdivision_name: {sub['subdivision_name']}")
            logger.info(f"    - sub_workflow_base_id: {sub['sub_workflow_base_id']}")
            logger.info(f"    - sub_workflow_instance_id: {sub['sub_workflow_instance_id']}")
            logger.info(f"    - original_node_name: {sub['original_node_name']}")
            logger.info(f"    - sub_workflow_name: {sub['sub_workflow_name']}")
        
        # è½¬æ¢å½“å‰å±‚çº§çš„è¿æ¥ä¸ºæ ‡å‡†æ ¼å¼
        current_connections = []
        child_instance_ids = []  # è®°å½•å­å·¥ä½œæµå®ä¾‹IDï¼Œç”¨äºä¸‹å±‚é€’å½’
        
        for subdivision in subdivisions:
            connection = {
                "subdivision_id": str(subdivision["subdivision_id"]),
                "subdivision_name": subdivision["subdivision_name"],
                "subdivision_description": subdivision["subdivision_description"] or "",
                "created_at": subdivision["subdivision_created_at"].isoformat() if subdivision["subdivision_created_at"] else None,
                "recursion_level": current_depth,  # æ·»åŠ é€’å½’å±‚çº§æ ‡è¯†
                
                # çˆ¶å·¥ä½œæµä¿¡æ¯
                "parent_workflow": {
                    "workflow_base_id": str(subdivision["parent_workflow_base_id"]),
                    "workflow_name": subdivision["parent_workflow_name"] or f"å·¥ä½œæµ_{subdivision['parent_workflow_base_id'][:8]}",
                    "workflow_description": subdivision["parent_workflow_description"] or "",
                    "workflow_instance_id": str(subdivision["parent_workflow_instance_id"]),
                    "connected_node": {
                        "node_base_id": str(subdivision["original_node_base_id"]),
                        "node_name": subdivision["original_node_name"],
                        "node_type": subdivision["original_node_type"],
                        "task_title": subdivision["task_title"],
                        "task_description": subdivision["task_description"] or ""
                    }
                },
                
                # å­å·¥ä½œæµä¿¡æ¯
                "sub_workflow": {
                    "workflow_base_id": str(subdivision["sub_workflow_base_id"]),
                    "workflow_name": subdivision["sub_workflow_name"] or f"å·¥ä½œæµ_{subdivision['sub_workflow_base_id'][:8]}",
                    "workflow_description": subdivision["sub_workflow_description"] or "",
                    "instance_id": str(subdivision["sub_workflow_instance_id"]) if subdivision["sub_workflow_instance_id"] else None,
                    "status": subdivision["sub_workflow_status"] or "unknown",
                    "started_at": subdivision["sub_workflow_started_at"].isoformat() if subdivision["sub_workflow_started_at"] else None,
                    "completed_at": subdivision["sub_workflow_completed_at"].isoformat() if subdivision["sub_workflow_completed_at"] else None,
                    "total_nodes": subdivision["sub_workflow_total_nodes"] or 0,
                    "completed_nodes": subdivision["sub_workflow_completed_nodes"] or 0
                }
            }
            current_connections.append(connection)
            
            # æ”¶é›†å­å·¥ä½œæµå®ä¾‹IDï¼Œç”¨äºé€’å½’æŸ¥è¯¢
            if subdivision["sub_workflow_instance_id"]:
                child_instance_ids.append(subdivision["sub_workflow_instance_id"])
        
        # é€’å½’æŸ¥è¯¢å­å·¥ä½œæµçš„è¿æ¥å…³ç³»
        all_connections = current_connections.copy()
        for child_instance_id in child_instance_ids:
            try:
                child_connections = await self._get_recursive_template_connections(
                    child_instance_id, max_depth, current_depth + 1
                )
                all_connections.extend(child_connections)
            except Exception as e:
                logger.warning(f"âš ï¸ é€’å½’æŸ¥è¯¢å­å·¥ä½œæµ {child_instance_id} å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–å­å·¥ä½œæµï¼Œä¸ä¸­æ–­æ•´ä¸ªé€’å½’è¿‡ç¨‹
                continue
        
        logger.debug(f"âœ… å±‚çº§ {current_depth} æŸ¥è¯¢å®Œæˆ: å½“å‰å±‚ {len(current_connections)} ä¸ªï¼Œæ€»è®¡ {len(all_connections)} ä¸ªè¿æ¥")
        return all_connections
    
    def _calculate_max_depth(self, connections: List[Dict[str, Any]]) -> int:
        """è®¡ç®—è¿æ¥å…³ç³»ä¸­çš„æœ€å¤§é€’å½’æ·±åº¦"""
        if not connections:
            return 0
        return max(conn.get("recursion_level", 0) for conn in connections) + 1
    
    def _build_recursive_connection_graph(self, template_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºæ”¯æŒé€’å½’çš„è¿æ¥å›¾æ•°æ®ç»“æ„
        
        Args:
            template_connections: åŒ…å«æ‰€æœ‰å±‚çº§çš„æ¨¡æ¿è¿æ¥å…³ç³»æ•°æ®
            
        Returns:
            å›¾å½¢æ•°æ®ç»“æ„ï¼ŒåŒ…å«å¤šå±‚çº§èŠ‚ç‚¹å’Œè¾¹
        """
        try:
            nodes = {}
            edges = []
            
            # æŒ‰é€’å½’å±‚çº§åˆ†ç»„è¿æ¥å…³ç³»
            levels = {}
            for connection in template_connections:
                level = connection.get("recursion_level", 0)
                if level not in levels:
                    levels[level] = []
                levels[level].append(connection)
            
            logger.debug(f"ğŸ—ï¸ æ„å»ºé€’å½’è¿æ¥å›¾: {len(levels)} ä¸ªå±‚çº§, æ€»è®¡ {len(template_connections)} ä¸ªè¿æ¥")
            
            # æ„å»ºèŠ‚ç‚¹ï¼ˆå·¥ä½œæµæ¨¡æ¿ï¼‰
            for connection in template_connections:
                parent_workflow = connection["parent_workflow"]
                sub_workflow = connection["sub_workflow"]
                recursion_level = connection.get("recursion_level", 0)
                
                # æ·»åŠ çˆ¶å·¥ä½œæµèŠ‚ç‚¹
                parent_id = parent_workflow["workflow_base_id"]
                if parent_id not in nodes:
                    nodes[parent_id] = {
                        "id": parent_id,
                        "type": "workflow_template",
                        "label": parent_workflow["workflow_name"],
                        "description": parent_workflow["workflow_description"],
                        "is_parent": recursion_level == 0,  # åªæœ‰é¡¶å±‚æ˜¯çœŸæ­£çš„çˆ¶å·¥ä½œæµ
                        "recursion_level": recursion_level,
                        "connected_nodes": [],
                        "workflow_instance_id": parent_workflow.get("workflow_instance_id")
                    }
                
                # è®°å½•è¿æ¥çš„èŠ‚ç‚¹
                nodes[parent_id]["connected_nodes"].append({
                    "node_base_id": parent_workflow["connected_node"]["node_base_id"],
                    "node_name": parent_workflow["connected_node"]["node_name"],
                    "node_type": parent_workflow["connected_node"]["node_type"],
                    "subdivision_name": connection["subdivision_name"]
                })
                
                # æ·»åŠ å­å·¥ä½œæµèŠ‚ç‚¹
                sub_id = sub_workflow["workflow_base_id"]
                if sub_id not in nodes:
                    nodes[sub_id] = {
                        "id": sub_id,
                        "type": "workflow_template",
                        "label": sub_workflow["workflow_name"],
                        "description": sub_workflow["workflow_description"],
                        "is_parent": False,
                        "recursion_level": recursion_level + 1,  # å­å·¥ä½œæµåœ¨ä¸‹ä¸€å±‚çº§
                        "status": sub_workflow["status"],
                        "total_nodes": sub_workflow["total_nodes"],
                        "completed_nodes": sub_workflow["completed_nodes"],
                        "completion_rate": sub_workflow["completed_nodes"] / max(sub_workflow["total_nodes"], 1) if sub_workflow["total_nodes"] else 0,
                        "started_at": sub_workflow["started_at"],
                        "completed_at": sub_workflow["completed_at"],
                        "workflow_instance_id": sub_workflow.get("instance_id"),
                        "connected_nodes": []  # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    }
                
                # æ·»åŠ è¿æ¥è¾¹
                edge = {
                    "id": f"{parent_id}_{sub_id}_{connection['subdivision_id']}",
                    "source": parent_id,
                    "target": sub_id,
                    "type": "subdivision_connection",
                    "label": connection["subdivision_name"],
                    "subdivision_id": connection["subdivision_id"],
                    "connected_node_name": parent_workflow["connected_node"]["node_name"],
                    "task_title": parent_workflow["connected_node"]["task_title"],
                    "created_at": connection["created_at"],
                    "recursion_level": recursion_level,
                    "edge_weight": recursion_level + 1  # ç”¨äºå¯è§†åŒ–æ—¶çš„è¾¹æƒé‡
                }
                edges.append(edge)
            
            # è½¬æ¢èŠ‚ç‚¹å­—å…¸ä¸ºåˆ—è¡¨
            node_list = list(nodes.values())
            
            # æŒ‰é€’å½’å±‚çº§å’Œåç§°æ’åºèŠ‚ç‚¹
            node_list.sort(key=lambda x: (x["recursion_level"], not x["is_parent"], x["label"]))
            
            # è®¡ç®—é€’å½’å¸ƒå±€ä½ç½®
            max_level = max(node["recursion_level"] for node in node_list) if node_list else 0
            level_node_counts = {}
            for node in node_list:
                level = node["recursion_level"]
                level_node_counts[level] = level_node_counts.get(level, 0) + 1
            
            # æ„å»ºèŠ‚ç‚¹ä½ç½®æ˜ å°„ï¼ˆç”¨äºæ–‡ä»¶ç³»ç»Ÿå¼å¸ƒå±€ï¼‰
            node_position_map = self._build_file_system_position_map(node_list, template_connections)
            
            # æ„å»ºæ ‘çŠ¶å¸ƒå±€çš„çˆ¶å­å…³ç³»å’Œä½ç½®æ˜ å°„
            tree_layout_data = self._build_tree_layout_data(node_list, template_connections)
            
            return {
                "nodes": node_list,
                "edges": edges,
                "layout": {
                    "algorithm": "recursive_hierarchical",
                    "direction": "TB",  # Top to Bottom
                    "node_spacing": 180,
                    "level_spacing": 120,
                    "max_recursion_level": max_level,
                    "level_node_counts": level_node_counts,
                    "node_position_map": node_position_map,  # æ–‡ä»¶ç³»ç»Ÿå¼å¸ƒå±€æ˜ å°„
                    "tree_layout_data": tree_layout_data  # æ–°å¢ï¼šæ ‘çŠ¶å¸ƒå±€æ•°æ®
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºé€’å½’è¿æ¥å›¾æ•°æ®ç»“æ„å¤±è´¥: {e}")
            return {
                "nodes": [],
                "edges": [],
                "layout": {}
            }
    
    def _build_tree_layout_data(self, node_list: List[Dict[str, Any]], template_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºæ ‘çŠ¶å¸ƒå±€çš„æ•°æ®ç»“æ„
        
        Args:
            node_list: èŠ‚ç‚¹åˆ—è¡¨
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            
        Returns:
            æ ‘çŠ¶å¸ƒå±€æ•°æ®ç»“æ„
        """
        try:
            # æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„
            parent_child_map = {}  # parent_id -> [child_ids]
            child_parent_map = {}  # child_id -> parent_id
            
            for connection in template_connections:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                child_id = connection["sub_workflow"]["workflow_base_id"]
                
                if parent_id not in parent_child_map:
                    parent_child_map[parent_id] = []
                parent_child_map[parent_id].append(child_id)
                child_parent_map[child_id] = parent_id
            
            # æ‰¾åˆ°æ ¹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰
            all_node_ids = set(node["id"] for node in node_list)
            root_nodes = []
            for node_id in all_node_ids:
                if node_id not in child_parent_map:
                    root_nodes.append(node_id)
            
            # æ„å»ºæ ‘çš„å±‚çº§ç»“æ„
            tree_levels = {}  # level -> [node_ids]
            node_levels = {}  # node_id -> level
            
            # ä½¿ç”¨BFSæ„å»ºå±‚çº§
            from collections import deque
            queue = deque()
            
            # åˆå§‹åŒ–æ ¹èŠ‚ç‚¹
            for root_id in root_nodes:
                queue.append((root_id, 0))
                node_levels[root_id] = 0
                if 0 not in tree_levels:
                    tree_levels[0] = []
                tree_levels[0].append(root_id)
            
            # BFSéå†æ„å»ºæ ‘å±‚çº§
            while queue:
                current_id, level = queue.popleft()
                
                # æ·»åŠ å­èŠ‚ç‚¹åˆ°ä¸‹ä¸€å±‚çº§
                if current_id in parent_child_map:
                    next_level = level + 1
                    if next_level not in tree_levels:
                        tree_levels[next_level] = []
                    
                    for child_id in parent_child_map[current_id]:
                        if child_id not in node_levels:  # é¿å…å¾ªç¯å¼•ç”¨
                            node_levels[child_id] = next_level
                            tree_levels[next_level].append(child_id)
                            queue.append((child_id, next_level))
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åœ¨å…¶å±‚çº§ä¸­çš„ä½ç½®
            node_positions = {}
            for level, node_ids in tree_levels.items():
                for index, node_id in enumerate(node_ids):
                    node_positions[node_id] = {
                        "level": level,
                        "index_in_level": index,
                        "total_in_level": len(node_ids),
                        "children": parent_child_map.get(node_id, []),
                        "parent": child_parent_map.get(node_id, None)
                    }
            
            logger.debug(f"ğŸŒ³ æ„å»ºæ ‘çŠ¶å¸ƒå±€æ•°æ®: {len(tree_levels)} å±‚, {len(node_positions)} ä¸ªèŠ‚ç‚¹")
            
            return {
                "tree_levels": tree_levels,
                "node_levels": node_levels,
                "node_positions": node_positions,
                "parent_child_map": parent_child_map,
                "child_parent_map": child_parent_map,
                "root_nodes": root_nodes,
                "max_level": max(tree_levels.keys()) if tree_levels else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ ‘çŠ¶å¸ƒå±€æ•°æ®å¤±è´¥: {e}")
            return {
                "tree_levels": {},
                "node_levels": {},
                "node_positions": {},
                "parent_child_map": {},
                "child_parent_map": {},
                "root_nodes": [],
                "max_level": 0
            }
    
    def _build_file_system_position_map(self, node_list: List[Dict[str, Any]], template_connections: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        æ„å»ºæ–‡ä»¶ç³»ç»Ÿå¼å¸ƒå±€çš„èŠ‚ç‚¹ä½ç½®æ˜ å°„
        
        Args:
            node_list: èŠ‚ç‚¹åˆ—è¡¨
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            
        Returns:
            èŠ‚ç‚¹ä½ç½®æ˜ å°„å­—å…¸
        """
        try:
            position_map = {}
            
            # æŒ‰é€’å½’å±‚çº§åˆ†ç»„èŠ‚ç‚¹
            levels = {}
            for node in node_list:
                level = node.get("recursion_level", 0)
                if level not in levels:
                    levels[level] = []
                levels[level].append(node)
            
            # æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„
            parent_child_map = {}
            for connection in template_connections:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                child_id = connection["sub_workflow"]["workflow_base_id"]
                
                if parent_id not in parent_child_map:
                    parent_child_map[parent_id] = []
                parent_child_map[parent_id].append(child_id)
            
            # ä¸ºæ¯ä¸ªå±‚çº§åˆ†é…ä½ç½®
            for level in sorted(levels.keys()):
                level_nodes = levels[level]
                
                if level == 0:  # é¡¶å±‚èŠ‚ç‚¹
                    # é¡¶å±‚èŠ‚ç‚¹æŒ‰åç§°æ’åº
                    level_nodes.sort(key=lambda x: x["label"])
                    for i, node in enumerate(level_nodes):
                        position_map[node["id"]] = {
                            "yIndex": i,
                            "indexInLevel": i,
                            "parentIndex": None
                        }
                else:  # å­å±‚çº§èŠ‚ç‚¹
                    # å­èŠ‚ç‚¹æŒ‰çˆ¶èŠ‚ç‚¹åˆ†ç»„ï¼Œç„¶ååœ¨çˆ¶èŠ‚ç‚¹ä¸‹æ–¹æ’åˆ—
                    y_index = 0
                    index_in_level = 0
                    
                    for node in level_nodes:
                        # æ‰¾åˆ°è¿™ä¸ªèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
                        parent_node = None
                        for connection in template_connections:
                            if connection["sub_workflow"]["workflow_base_id"] == node["id"]:
                                parent_id = connection["parent_workflow"]["workflow_base_id"]
                                parent_node = next((n for n in node_list if n["id"] == parent_id), None)
                                break
                        
                        parent_y_index = 0
                        if parent_node and parent_node["id"] in position_map:
                            parent_y_index = position_map[parent_node["id"]]["yIndex"]
                        
                        # å­èŠ‚ç‚¹æ”¾åœ¨çˆ¶èŠ‚ç‚¹çš„ä¸‹æ–¹
                        position_map[node["id"]] = {
                            "yIndex": parent_y_index + y_index + 1,  # åœ¨çˆ¶èŠ‚ç‚¹åŸºç¡€ä¸ŠåŠ åç§»
                            "indexInLevel": index_in_level,
                            "parentIndex": parent_y_index
                        }
                        
                        y_index += 1
                        index_in_level += 1
            
            logger.debug(f"ğŸ—‚ï¸ æ„å»ºæ–‡ä»¶ç³»ç»Ÿä½ç½®æ˜ å°„: {len(position_map)} ä¸ªèŠ‚ç‚¹")
            return position_map
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ–‡ä»¶ç³»ç»Ÿä½ç½®æ˜ å°„å¤±è´¥: {e}")
            return {}
    
    def _build_connection_graph(self, template_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºè¿æ¥å›¾æ•°æ®ç»“æ„
        
        Args:
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»æ•°æ®
            
        Returns:
            å›¾å½¢æ•°æ®ç»“æ„ï¼ŒåŒ…å«èŠ‚ç‚¹å’Œè¾¹
        """
        try:
            nodes = {}
            edges = []
            
            # æ„å»ºèŠ‚ç‚¹ï¼ˆå·¥ä½œæµæ¨¡æ¿ï¼‰
            for connection in template_connections:
                parent_workflow = connection["parent_workflow"]
                sub_workflow = connection["sub_workflow"]
                
                # æ·»åŠ çˆ¶å·¥ä½œæµèŠ‚ç‚¹
                parent_id = parent_workflow["workflow_base_id"]
                if parent_id not in nodes:
                    nodes[parent_id] = {
                        "id": parent_id,
                        "type": "workflow_template",
                        "label": parent_workflow["workflow_name"],
                        "description": parent_workflow["workflow_description"],
                        "is_parent": True,
                        "connected_nodes": []
                    }
                
                # è®°å½•è¿æ¥çš„èŠ‚ç‚¹
                nodes[parent_id]["connected_nodes"].append({
                    "node_base_id": parent_workflow["connected_node"]["node_base_id"],
                    "node_name": parent_workflow["connected_node"]["node_name"],
                    "node_type": parent_workflow["connected_node"]["node_type"],
                    "subdivision_name": connection["subdivision_name"]
                })
                
                # æ·»åŠ å­å·¥ä½œæµèŠ‚ç‚¹
                sub_id = sub_workflow["workflow_base_id"]
                if sub_id not in nodes:
                    nodes[sub_id] = {
                        "id": sub_id,
                        "type": "workflow_template",
                        "label": sub_workflow["workflow_name"],
                        "description": sub_workflow["workflow_description"],
                        "is_parent": False,
                        "status": sub_workflow["status"],
                        "total_nodes": sub_workflow["total_nodes"],
                        "completed_nodes": sub_workflow["completed_nodes"],
                        "completion_rate": sub_workflow["completed_nodes"] / max(sub_workflow["total_nodes"], 1),
                        "started_at": sub_workflow["started_at"],
                        "completed_at": sub_workflow["completed_at"],
                        "connected_nodes": []  # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    }
                
                # æ·»åŠ è¿æ¥è¾¹
                edge = {
                    "id": f"{parent_id}_{sub_id}_{connection['subdivision_id']}",
                    "source": parent_id,
                    "target": sub_id,
                    "type": "subdivision_connection",
                    "label": connection["subdivision_name"],
                    "subdivision_id": connection["subdivision_id"],
                    "connected_node_name": parent_workflow["connected_node"]["node_name"],
                    "task_title": parent_workflow["connected_node"]["task_title"],
                    "created_at": connection["created_at"]
                }
                edges.append(edge)
            
            # è½¬æ¢èŠ‚ç‚¹å­—å…¸ä¸ºåˆ—è¡¨
            node_list = list(nodes.values())
            
            # æŒ‰å±‚çº§æ’åºèŠ‚ç‚¹ï¼ˆçˆ¶å·¥ä½œæµåœ¨å‰ï¼‰
            node_list.sort(key=lambda x: (not x["is_parent"], x["label"]))
            
            return {
                "nodes": node_list,
                "edges": edges,
                "layout": {
                    "algorithm": "hierarchical",
                    "direction": "TB",  # Top to Bottom
                    "node_spacing": 150,
                    "level_spacing": 100
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¿æ¥å›¾æ•°æ®ç»“æ„å¤±è´¥: {e}")
            return {
                "nodes": [],
                "edges": [],
                "layout": {}
            }
    
    async def get_workflow_template_connection_summary(self, workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµæ¨¡æ¿çš„è¿æ¥å…³ç³»æ‘˜è¦ï¼ˆç”¨äºæ˜¾ç¤ºæ¨¡æ¿çº§åˆ«çš„è¿æ¥ç»Ÿè®¡ï¼‰
        
        Args:
            workflow_base_id: å·¥ä½œæµåŸºç¡€ID
            
        Returns:
            è¿æ¥å…³ç³»æ‘˜è¦æ•°æ®
        """
        try:
            logger.info(f"ğŸ” è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦: {workflow_base_id}")
            
            summary_query = """
            SELECT 
                COUNT(DISTINCT ts.subdivision_id) as total_subdivisions,
                COUNT(DISTINCT ts.sub_workflow_base_id) as unique_sub_workflows,
                COUNT(DISTINCT ni.node_base_id) as connected_nodes,
                COUNT(DISTINCT swi.workflow_instance_id) as sub_workflow_instances,
                COUNT(CASE WHEN swi.status = 'completed' THEN 1 END) as completed_instances,
                MIN(ts.subdivision_created_at) as first_subdivision_at,
                MAX(ts.subdivision_created_at) as last_subdivision_at
            FROM task_subdivision ts
            JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
            JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
            JOIN node n ON ni.node_id = n.node_id
            LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
            WHERE n.workflow_base_id = $1
            AND ts.is_deleted = FALSE
            AND ti.is_deleted = FALSE
            AND ni.is_deleted = FALSE
            """
            
            result = await self.db.fetch_one(summary_query, workflow_base_id)
            
            if result:
                summary = {
                    "workflow_base_id": str(workflow_base_id),
                    "total_subdivisions": result["total_subdivisions"] or 0,
                    "unique_sub_workflows": result["unique_sub_workflows"] or 0,
                    "connected_nodes": result["connected_nodes"] or 0,
                    "sub_workflow_instances": result["sub_workflow_instances"] or 0,
                    "completed_instances": result["completed_instances"] or 0,
                    "success_rate": (result["completed_instances"] or 0) / max(result["sub_workflow_instances"] or 1, 1),
                    "first_subdivision_at": result["first_subdivision_at"].isoformat() if result["first_subdivision_at"] else None,
                    "last_subdivision_at": result["last_subdivision_at"].isoformat() if result["last_subdivision_at"] else None
                }
                
                logger.info(f"âœ… å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦: {summary}")
                return summary
            else:
                return {
                    "workflow_base_id": str(workflow_base_id),
                    "total_subdivisions": 0,
                    "unique_sub_workflows": 0,
                    "connected_nodes": 0,
                    "sub_workflow_instances": 0,
                    "completed_instances": 0,
                    "success_rate": 0,
                    "first_subdivision_at": None,
                    "last_subdivision_at": None
                }
                
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦å¤±è´¥: {e}")
            raise
    
    async def get_detailed_workflow_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """
        è·å–åŒ…å«å†…éƒ¨èŠ‚ç‚¹è¯¦æƒ…çš„å·¥ä½œæµæ¨¡æ¿è¿æ¥å›¾æ•°æ®
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            
        Returns:
            åŒ…å«è¯¦ç»†å†…éƒ¨èŠ‚ç‚¹ä¿¡æ¯çš„è¿æ¥å›¾æ•°æ®
        """
        try:
            logger.info(f"ğŸ” è·å–è¯¦ç»†å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»(é€’å½’æ·±åº¦ {max_depth}): {workflow_instance_id}")
            
            # è·å–åŸºç¡€è¿æ¥å…³ç³»
            base_connections = await self.get_workflow_template_connections(workflow_instance_id, max_depth)
            
            # è·å–æ¯ä¸ªå·¥ä½œæµçš„è¯¦ç»†å†…éƒ¨ç»“æ„
            detailed_workflows = {}
            unique_workflow_ids = set()
            
            # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å·¥ä½œæµID
            for connection in base_connections["template_connections"]:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                sub_id = connection["sub_workflow"]["workflow_base_id"]
                unique_workflow_ids.add(parent_id)
                unique_workflow_ids.add(sub_id)
            
            logger.info(f"ğŸ“Š [DEBUG] æ”¶é›†åˆ°çš„å”¯ä¸€å·¥ä½œæµIDæ•°é‡: {len(unique_workflow_ids)}")
            logger.info(f"ğŸ“Š [DEBUG] å·¥ä½œæµIDåˆ—è¡¨: {list(unique_workflow_ids)}")
            logger.info(f"ğŸ“Š [DEBUG] æ¨¡æ¿è¿æ¥å…³ç³»æ•°é‡: {len(base_connections['template_connections'])}")
            
            # è·å–æ¯ä¸ªå·¥ä½œæµçš„è¯¦ç»†å†…éƒ¨ç»“æ„
            for workflow_base_id in unique_workflow_ids:
                logger.info(f"ğŸ” [DEBUG] è·å–å·¥ä½œæµ {workflow_base_id} çš„å†…éƒ¨ç»“æ„...")
                detailed_workflows[workflow_base_id] = await self._get_workflow_internal_structure(
                    uuid.UUID(workflow_base_id)
                )
            
            # åˆ†æå¯æ›¿æ¢çš„èŠ‚ç‚¹å¯¹
            merge_candidates = self._analyze_merge_candidates(
                base_connections["template_connections"], 
                detailed_workflows
            )
            
            # æ„å»ºè¯¦ç»†è¿æ¥å›¾
            detailed_graph = self._build_detailed_connection_graph(
                base_connections["template_connections"],
                detailed_workflows,
                merge_candidates
            )
            
            result = {
                **base_connections,
                "detailed_workflows": detailed_workflows,
                "merge_candidates": merge_candidates,
                "detailed_connection_graph": detailed_graph
            }
            
            logger.info(f"âœ… è¯¦ç»†å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»è·å–æˆåŠŸ: {len(detailed_workflows)} ä¸ªå·¥ä½œæµ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–è¯¦ç»†å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»å¤±è´¥: {e}")
            raise
    
    async def _get_workflow_internal_structure(self, workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµçš„å†…éƒ¨èŠ‚ç‚¹å’Œè¿æ¥ç»“æ„
        
        Args:
            workflow_base_id: å·¥ä½œæµåŸºç¡€ID
            
        Returns:
            å·¥ä½œæµå†…éƒ¨ç»“æ„æ•°æ®
        """
        try:
            # è·å–å·¥ä½œæµçš„æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«processorï¼‰
            nodes_query = """
            SELECT 
                n.node_id,
                n.node_base_id,
                n.name,
                n.type,
                n.task_description,
                n.position_x,
                n.position_y,
                n.created_at,
                n.updated_at
            FROM node n
            WHERE n.workflow_base_id = $1
            AND n.is_current_version = TRUE
            AND n.is_deleted = FALSE
            ORDER BY n.created_at
            """
            
            nodes = await self.db.fetch_all(nodes_query, workflow_base_id)
            
            # è·å–å·¥ä½œæµçš„æ‰€æœ‰è¿æ¥
            connections_query = """
            SELECT 
                CONCAT(nc.from_node_id, '-', nc.to_node_id) as connection_id,
                nc.from_node_id,
                nc.to_node_id,
                nc.connection_type,
                fn.node_base_id as from_node_base_id,
                fn.name as from_node_name,
                tn.node_base_id as to_node_base_id,
                tn.name as to_node_name
            FROM node_connection nc
            JOIN node fn ON nc.from_node_id = fn.node_id
            JOIN node tn ON nc.to_node_id = tn.node_id
            WHERE nc.workflow_id = (
                SELECT workflow_id 
                FROM workflow 
                WHERE workflow_base_id = %s 
                AND is_current_version = TRUE 
                LIMIT 1
            )
            ORDER BY nc.created_at
            """
            
            connections = await self.db.fetch_all(connections_query, workflow_base_id)
            
            # è½¬æ¢èŠ‚ç‚¹æ•°æ®æ ¼å¼
            formatted_nodes = []
            for node in nodes:
                formatted_nodes.append({
                    "node_id": str(node["node_id"]),
                    "node_base_id": str(node["node_base_id"]),
                    "name": node["name"],
                    "type": node["type"],
                    "task_description": node["task_description"] or "",
                    "position": {
                        "x": float(node["position_x"]) if node["position_x"] else 0,
                        "y": float(node["position_y"]) if node["position_y"] else 0
                    },
                    "created_at": node["created_at"].isoformat() if node["created_at"] else None,
                    "updated_at": node["updated_at"].isoformat() if node["updated_at"] else None
                })
            
            # è½¬æ¢è¿æ¥æ•°æ®æ ¼å¼
            formatted_connections = []
            for connection in connections:
                formatted_connections.append({
                    "connection_id": str(connection["connection_id"]),
                    "from_node": {
                        "node_id": str(connection["from_node_id"]),
                        "node_base_id": str(connection["from_node_base_id"]),
                        "name": connection["from_node_name"]
                    },
                    "to_node": {
                        "node_id": str(connection["to_node_id"]),
                        "node_base_id": str(connection["to_node_base_id"]),
                        "name": connection["to_node_name"]
                    },
                    "connection_type": connection["connection_type"]
                })
            
            return {
                "workflow_base_id": str(workflow_base_id),
                "nodes": formatted_nodes,
                "connections": formatted_connections,
                "node_count": len(formatted_nodes),
                "connection_count": len(formatted_connections)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµå†…éƒ¨ç»“æ„å¤±è´¥: {workflow_base_id}, {e}")
            return {
                "workflow_base_id": str(workflow_base_id),
                "nodes": [],
                "connections": [],
                "node_count": 0,
                "connection_count": 0
            }
    
    def _analyze_merge_candidates(self, template_connections: List[Dict[str, Any]], detailed_workflows: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åˆ†æå¯åˆå¹¶çš„èŠ‚ç‚¹å¯¹
        
        Args:
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            detailed_workflows: è¯¦ç»†å·¥ä½œæµç»“æ„
            
        Returns:
            å¯åˆå¹¶èŠ‚ç‚¹å¯¹åˆ—è¡¨
        """
        merge_candidates = []
        
        try:
            for connection in template_connections:
                parent_workflow_id = connection["parent_workflow"]["workflow_base_id"]
                sub_workflow_id = connection["sub_workflow"]["workflow_base_id"]
                connected_node_id = connection["parent_workflow"]["connected_node"]["node_base_id"]
                
                # è·å–çˆ¶å·¥ä½œæµå’Œå­å·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯
                parent_workflow = detailed_workflows.get(parent_workflow_id, {})
                sub_workflow = detailed_workflows.get(sub_workflow_id, {})
                
                if not parent_workflow or not sub_workflow:
                    continue
                
                # æ‰¾åˆ°è¢«ç»†åˆ†çš„èŠ‚ç‚¹
                connected_node = None
                for node in parent_workflow.get("nodes", []):
                    if node["node_base_id"] == connected_node_id:
                        connected_node = node
                        break
                
                if not connected_node:
                    continue
                
                # åˆ†æå­å·¥ä½œæµçš„å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
                start_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "start"]
                end_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "end"]
                
                merge_candidate = {
                    "subdivision_id": connection["subdivision_id"],
                    "parent_workflow_id": parent_workflow_id,
                    "sub_workflow_id": sub_workflow_id,
                    "replaceable_node": {
                        "node_base_id": connected_node["node_base_id"],
                        "name": connected_node["name"],
                        "type": connected_node["type"],
                        "task_description": connected_node["task_description"]
                    },
                    "replacement_structure": {
                        "start_nodes": start_nodes,
                        "end_nodes": end_nodes,
                        "total_nodes": len(sub_workflow.get("nodes", [])),
                        "total_connections": len(sub_workflow.get("connections", []))
                    },
                    "compatibility": self._check_merge_compatibility(connected_node, sub_workflow),
                    "merge_complexity": "simple" if len(sub_workflow.get("nodes", [])) <= 5 else "complex"
                }
                
                merge_candidates.append(merge_candidate)
            
            logger.debug(f"ğŸ” åˆ†æåˆ° {len(merge_candidates)} ä¸ªå¯åˆå¹¶èŠ‚ç‚¹å¯¹")
            return merge_candidates
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¯åˆå¹¶èŠ‚ç‚¹å¯¹å¤±è´¥: {e}")
            return []
    
    def _check_merge_compatibility(self, parent_node: Dict[str, Any], sub_workflow: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ£€æŸ¥èŠ‚ç‚¹åˆå¹¶çš„å…¼å®¹æ€§
        
        Args:
            parent_node: çˆ¶èŠ‚ç‚¹ä¿¡æ¯
            sub_workflow: å­å·¥ä½œæµç»“æ„
            
        Returns:
            å…¼å®¹æ€§æ£€æŸ¥ç»“æœ
        """
        try:
            compatibility = {
                "is_compatible": True,
                "issues": [],
                "recommendations": []
            }
            
            # æ£€æŸ¥èŠ‚ç‚¹ç±»å‹å…¼å®¹æ€§
            if parent_node["type"] != "processor":
                compatibility["is_compatible"] = False
                compatibility["issues"].append("åªæœ‰å¤„ç†å™¨ç±»å‹çš„èŠ‚ç‚¹å¯ä»¥è¢«æ›¿æ¢")
            
            # æ£€æŸ¥å­å·¥ä½œæµç»“æ„
            start_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "start"]
            end_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "end"]
            
            if len(start_nodes) == 0:
                compatibility["issues"].append("å­å·¥ä½œæµç¼ºå°‘å¼€å§‹èŠ‚ç‚¹")
                compatibility["is_compatible"] = False
            elif len(start_nodes) > 1:
                compatibility["recommendations"].append("å­å·¥ä½œæµæœ‰å¤šä¸ªå¼€å§‹èŠ‚ç‚¹ï¼Œåˆå¹¶åå¯èƒ½éœ€è¦è°ƒæ•´è¿æ¥")
            
            if len(end_nodes) == 0:
                compatibility["issues"].append("å­å·¥ä½œæµç¼ºå°‘ç»“æŸèŠ‚ç‚¹")
                compatibility["is_compatible"] = False
            elif len(end_nodes) > 1:
                compatibility["recommendations"].append("å­å·¥ä½œæµæœ‰å¤šä¸ªç»“æŸèŠ‚ç‚¹ï¼Œåˆå¹¶åå¯èƒ½éœ€è¦è°ƒæ•´è¿æ¥")
            
            # æ£€æŸ¥å¤æ‚åº¦
            node_count = len(sub_workflow.get("nodes", []))
            if node_count > 10:
                compatibility["recommendations"].append(f"å­å·¥ä½œæµè¾ƒå¤æ‚({node_count}ä¸ªèŠ‚ç‚¹)ï¼Œå»ºè®®ä»”ç»†å®¡æŸ¥åˆå¹¶ç»“æœ")
            
            return compatibility
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥åˆå¹¶å…¼å®¹æ€§å¤±è´¥: {e}")
            return {
                "is_compatible": False,
                "issues": ["å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥"],
                "recommendations": []
            }
    
    def _build_detailed_connection_graph(self, template_connections: List[Dict[str, Any]], detailed_workflows: Dict[str, Dict[str, Any]], merge_candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºåŒ…å«å†…éƒ¨èŠ‚ç‚¹çš„è¯¦ç»†è¿æ¥å›¾
        
        Args:
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            detailed_workflows: è¯¦ç»†å·¥ä½œæµç»“æ„
            merge_candidates: å¯åˆå¹¶èŠ‚ç‚¹å¯¹
            
        Returns:
            è¯¦ç»†è¿æ¥å›¾æ•°æ®ç»“æ„
        """
        try:
            nodes = []
            edges = []
            
            # è®¡ç®—å·¥ä½œæµå¸ƒå±€ä½ç½®
            workflow_positions = self._calculate_workflow_layout_positions(detailed_workflows)
            
            # ä¸ºæ¯ä¸ªå·¥ä½œæµæ·»åŠ èŠ‚ç‚¹
            for workflow_id, workflow_data in detailed_workflows.items():
                workflow_pos = workflow_positions.get(workflow_id, {"x": 0, "y": 0})
                
                logger.info(f"ğŸ—ï¸ [DEBUG] åˆ›å»ºå·¥ä½œæµå®¹å™¨èŠ‚ç‚¹: {workflow_id}")
                logger.info(f"    - ä½ç½®: x={workflow_pos['x']}, y={workflow_pos['y']}")
                logger.info(f"    - èŠ‚ç‚¹æ•°: {workflow_data.get('node_count', 0)}")
                logger.info(f"    - è¿æ¥æ•°: {workflow_data.get('connection_count', 0)}")
                
                # æ·»åŠ å·¥ä½œæµå®¹å™¨èŠ‚ç‚¹
                workflow_node = {
                    "id": f"workflow_{workflow_id}",
                    "type": "workflow_container",
                    "label": f"å·¥ä½œæµ {workflow_id[:8]}",
                    "position": {
                        "x": workflow_pos["x"],
                        "y": workflow_pos["y"]
                    },
                    "data": {
                        "workflow_base_id": workflow_id,
                        "node_count": workflow_data["node_count"],
                        "connection_count": workflow_data["connection_count"]
                    }
                }
                nodes.append(workflow_node)
                
                # æ·»åŠ å†…éƒ¨èŠ‚ç‚¹ï¼ŒåŸºäºå·¥ä½œæµå®¹å™¨ä½ç½®è¿›è¡Œåç§»
                internal_positions = self._calculate_internal_node_positions(
                    workflow_data.get("nodes", []),
                    workflow_pos,
                    workflow_data["node_count"]
                )
                
                for i, node in enumerate(workflow_data.get("nodes", [])):
                    internal_pos = internal_positions[i] if i < len(internal_positions) else {"x": 0, "y": 0}
                    
                    internal_node = {
                        "id": f"node_{node['node_base_id']}",
                        "type": "internal_node",
                        "label": node["name"],
                        "position": internal_pos,
                        "data": {
                            **node,
                            "parent_workflow_id": workflow_id,
                            "node_type": node["type"]
                        }
                    }
                    nodes.append(internal_node)
                
                # æ·»åŠ å†…éƒ¨è¿æ¥
                for connection in workflow_data.get("connections", []):
                    internal_edge = {
                        "id": f"edge_{connection['connection_id']}",
                        "source": f"node_{connection['from_node']['node_base_id']}",
                        "target": f"node_{connection['to_node']['node_base_id']}",
                        "sourceHandle": "source",  # æ·»åŠ é»˜è®¤çš„sourceHandle
                        "targetHandle": "target",  # æ·»åŠ é»˜è®¤çš„targetHandle
                        "type": "internal_connection",
                        "label": connection["connection_type"],
                        "data": connection
                    }
                    edges.append(internal_edge)
            
            # æ·»åŠ å·¥ä½œæµé—´çš„è¿æ¥
            for connection in template_connections:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                sub_id = connection["sub_workflow"]["workflow_base_id"]
                connected_node_id = connection["parent_workflow"]["connected_node"]["node_base_id"]
                
                # æ·»åŠ ä»çˆ¶å·¥ä½œæµèŠ‚ç‚¹åˆ°å­å·¥ä½œæµçš„è¿æ¥
                workflow_connection = {
                    "id": f"subdivision_{connection['subdivision_id']}",
                    "source": f"node_{connected_node_id}",
                    "target": f"workflow_{sub_id}",
                    "sourceHandle": "source",  # æ·»åŠ é»˜è®¤çš„sourceHandle
                    "targetHandle": "target",  # æ·»åŠ é»˜è®¤çš„targetHandle
                    "type": "subdivision_connection",
                    "label": connection["subdivision_name"],
                    "data": connection
                }
                edges.append(workflow_connection)
            
            logger.debug(f"ğŸ¨ æ„å»ºè¯¦ç»†è¿æ¥å›¾: {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
            
            return {
                "nodes": nodes,
                "edges": edges,
                "layout": {
                    "algorithm": "detailed_hierarchical",
                    "show_internal_nodes": True,
                    "node_spacing": 120,
                    "workflow_spacing": 300,
                    "level_spacing": 150
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¯¦ç»†è¿æ¥å›¾å¤±è´¥: {e}")
            return {
                "nodes": [],
                "edges": [],
                "layout": {}
            }
    
    def _calculate_workflow_layout_positions(self, detailed_workflows: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å·¥ä½œæµçš„å¸ƒå±€ä½ç½®"""
        positions = {}
        workflow_spacing = 400
        
        workflow_ids = list(detailed_workflows.keys())
        
        for i, workflow_id in enumerate(workflow_ids):
            positions[workflow_id] = {
                "x": i * workflow_spacing,
                "y": 0
            }
        
        return positions
    
    def _calculate_internal_node_positions(self, nodes: List[Dict[str, Any]], base_position: Dict[str, float], node_count: int) -> List[Dict[str, float]]:
        """è®¡ç®—å†…éƒ¨èŠ‚ç‚¹çš„ä½ç½®"""
        positions = []
        node_spacing = 150
        nodes_per_row = 3
        
        base_x = base_position["x"] + 50  # ç›¸å¯¹äºå·¥ä½œæµå®¹å™¨çš„åç§»
        base_y = base_position["y"] + 100
        
        for i, node in enumerate(nodes):
            row = i // nodes_per_row
            col = i % nodes_per_row
            
            # ä½¿ç”¨èŠ‚ç‚¹åŸå§‹ä½ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è®¡ç®—ä½ç½®
            original_pos = node.get("position", {})
            if original_pos.get("x") and original_pos.get("y"):
                # å¦‚æœæœ‰åŸå§‹ä½ç½®ï¼ŒåŸºäºå·¥ä½œæµå®¹å™¨è¿›è¡Œåç§»
                pos = {
                    "x": base_x + float(original_pos["x"]),
                    "y": base_y + float(original_pos["y"])
                }
            else:
                # å¦åˆ™ä½¿ç”¨ç½‘æ ¼å¸ƒå±€
                pos = {
                    "x": base_x + col * node_spacing,
                    "y": base_y + row * node_spacing
                }
            
            positions.append(pos)
        
        return positions