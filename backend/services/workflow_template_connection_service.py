"""
å·¥ä½œæµæ¨¡æ¿è¿æ¥æœåŠ¡
Workflow Template Connection Service

ç”¨äºè·å–å’Œåˆ†ææ‰§è¡Œå®ä¾‹å®Œæˆåçš„å·¥ä½œæµæ¨¡æ¿ä¹‹é—´çš„è¿æ¥å…³ç³»
"""

import uuid
from typing import Optional, Dict, Any, List
from loguru import logger

from ..repositories.base import BaseRepository
from ..utils.helpers import now_utc


class WorkflowTemplateConnectionService:
    """å·¥ä½œæµæ¨¡æ¿è¿æ¥æœåŠ¡"""
    
    def __init__(self):
        self.db = BaseRepository("workflow_template_connection").db
    
    async def get_detailed_workflow_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šåˆ©ç”¨parent_subdivision_idä¸€æ¬¡æ€§è·å–è¯¦ç»†çš„å·¥ä½œæµè¿æ¥å›¾æ•°æ®
        
        æ€§èƒ½æ”¹è¿›ï¼š
        1. ä½¿ç”¨WITH RECURSIVEä¸€æ¬¡æ€§è·å–æ‰€æœ‰å±‚çº§
        2. é¿å…é€’å½’æ•°æ®åº“è°ƒç”¨ 
        3. æ‰¹é‡è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        4. å†…å­˜ä¸­æ„å»ºå±‚çº§å…³ç³»
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            
        Returns:
            ä¼˜åŒ–åçš„è¿æ¥å›¾æ•°æ®ç»“æ„
        """
        try:
            logger.info(f"ğŸš€ [ä¼˜åŒ–ç‰ˆ] è·å–å·¥ä½œæµè¯¦ç»†è¿æ¥å…³ç³»: {workflow_instance_id} (æœ€å¤§æ·±åº¦: {max_depth})")
            
            # 1. ä½¿ç”¨å•ä¸ªWITH RECURSIVEæŸ¥è¯¢è·å–æ‰€æœ‰subdivisionå±‚çº§
            all_subdivisions = await self._get_all_subdivisions_with_hierarchy(workflow_instance_id, max_depth)
            
            if not all_subdivisions:
                logger.info(f"ğŸ“‹ æœªæ‰¾åˆ°å·¥ä½œæµå®ä¾‹çš„ç»†åˆ†å…³ç³»: {workflow_instance_id}")
                return self._empty_connection_result(workflow_instance_id)
            
            logger.info(f"ğŸ“Š ä¸€æ¬¡æ€§è·å–åˆ° {len(all_subdivisions)} ä¸ªsubdivisionè®°å½•")
            
            # 2. æ‰¹é‡è·å–å·¥ä½œæµç»Ÿè®¡ä¿¡æ¯
            workflow_stats = await self._batch_get_workflow_statistics(all_subdivisions)
            
            # 3. åœ¨å†…å­˜ä¸­æ„å»ºå®Œæ•´çš„è¿æ¥æ•°æ®ç»“æ„
            detailed_connections = self._build_detailed_connections(all_subdivisions, workflow_stats)
            
            # 4. æ„å»ºåˆå¹¶å€™é€‰æ•°æ®
            merge_candidates = await self._build_merge_candidates(all_subdivisions, workflow_stats)
            
            # 5. æ„å»ºè¯¦ç»†çš„è¿æ¥å›¾
            detailed_connection_graph = self._build_optimized_connection_graph(detailed_connections)
            
            # 6. æ„å»ºç»Ÿè®¡ä¿¡æ¯
            statistics = self._calculate_detailed_statistics(detailed_connections, all_subdivisions)
            
            result = {
                "workflow_instance_id": str(workflow_instance_id),
                "template_connections": detailed_connections,
                "detailed_workflows": self._extract_detailed_workflows(all_subdivisions, workflow_stats),
                "merge_candidates": merge_candidates,
                "detailed_connection_graph": detailed_connection_graph,
                "statistics": statistics,
                "performance_info": {
                    "total_subdivisions": len(all_subdivisions),
                    "optimization_used": "parent_subdivision_id_hierarchy",
                    "query_optimization": "single_recursive_query",
                    "max_depth_reached": max(s.get('depth', 0) for s in all_subdivisions) if all_subdivisions else 0
                }
            }
            
            logger.info(f"âœ… [ä¼˜åŒ–ç‰ˆ] è¿æ¥å›¾æ•°æ®æ„å»ºå®Œæˆ: {statistics}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ [ä¼˜åŒ–ç‰ˆ] è·å–è¯¦ç»†å·¥ä½œæµè¿æ¥å…³ç³»å¤±è´¥: {e}")
            raise
    
    async def _get_all_subdivisions_with_hierarchy(self, workflow_instance_id: uuid.UUID, max_depth: int) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨WITH RECURSIVEä¸€æ¬¡æ€§è·å–æ‰€æœ‰subdivisionå±‚çº§æ•°æ®
        
        è¿™æ˜¯å…³é”®ä¼˜åŒ–ï¼šé¿å…é€’å½’æ•°æ®åº“è°ƒç”¨ï¼Œåˆ©ç”¨parent_subdivision_idæ„å»ºå®Œæ•´å±‚çº§
        """
        try:
            # æ ¸å¿ƒä¼˜åŒ–ï¼šåˆ©ç”¨parent_subdivision_idçš„WITH RECURSIVEæŸ¥è¯¢
            hierarchy_query = """
            WITH RECURSIVE subdivision_hierarchy AS (
                -- åŸºç¡€æƒ…å†µï¼šæŸ¥æ‰¾æŒ‡å®šå·¥ä½œæµå®ä¾‹çš„ç›´æ¥subdivisionï¼ˆæ ¹çº§ï¼‰
                SELECT 
                    ts.subdivision_id,
                    ts.original_task_id,
                    ts.sub_workflow_base_id,
                    ts.sub_workflow_instance_id,
                    ts.subdivision_name,
                    ts.subdivision_description,
                    ts.subdivision_created_at,
                    ts.parent_subdivision_id,
                    
                    -- å±‚çº§ä¿¡æ¯
                    0 as depth,
                    CAST(CONCAT(ts.subdivision_id) AS CHAR(1000)) as hierarchy_path,
                    
                    -- åŸå§‹ä»»åŠ¡ä¿¡æ¯
                    ti.task_title,
                    ti.task_description,
                    ti.node_instance_id,
                    ti.workflow_instance_id as parent_workflow_instance_id,
                    
                    -- èŠ‚ç‚¹ä¿¡æ¯  
                    ni.node_id as original_node_id,
                    n.node_base_id as original_node_base_id,
                    n.name as original_node_name,
                    n.type as original_node_type,
                    n.workflow_base_id as parent_workflow_base_id,
                    
                    -- å·¥ä½œæµä¿¡æ¯ï¼ˆæ‰¹é‡JOINï¼Œé¿å…åç»­æŸ¥è¯¢ï¼‰
                    pw.name as parent_workflow_name,
                    pw.description as parent_workflow_description,
                    sw.name as sub_workflow_name,
                    sw.description as sub_workflow_description,
                    
                    -- å­å·¥ä½œæµå®ä¾‹çŠ¶æ€
                    swi.status as sub_workflow_status,
                    swi.started_at as sub_workflow_started_at,
                    swi.completed_at as sub_workflow_completed_at
                    
                FROM task_subdivision ts
                JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
                JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
                JOIN node n ON ni.node_id = n.node_id
                LEFT JOIN workflow pw ON n.workflow_base_id = pw.workflow_base_id AND pw.is_current_version = TRUE
                LEFT JOIN workflow sw ON ts.sub_workflow_base_id = sw.workflow_base_id AND sw.is_current_version = TRUE
                LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
                WHERE ti.workflow_instance_id = %s 
                AND ts.is_deleted = FALSE
                AND ti.is_deleted = FALSE
                AND ni.is_deleted = FALSE
                
                UNION ALL
                
                -- é€’å½’æƒ…å†µï¼šåŸºäºparent_subdivision_idæŸ¥æ‰¾å­çº§subdivision
                SELECT 
                    ts_child.subdivision_id,
                    ts_child.original_task_id,
                    ts_child.sub_workflow_base_id,
                    ts_child.sub_workflow_instance_id,
                    ts_child.subdivision_name,
                    ts_child.subdivision_description,
                    ts_child.subdivision_created_at,
                    ts_child.parent_subdivision_id,
                    
                    -- å±‚çº§ä¿¡æ¯é€’å¢
                    sh.depth + 1,
                    CONCAT(sh.hierarchy_path, ',', ts_child.subdivision_id),
                    
                    -- å­çº§ä»»åŠ¡ä¿¡æ¯
                    ti_child.task_title,
                    ti_child.task_description,
                    ti_child.node_instance_id,
                    ti_child.workflow_instance_id as parent_workflow_instance_id,
                    
                    -- å­çº§èŠ‚ç‚¹ä¿¡æ¯
                    ni_child.node_id as original_node_id,
                    n_child.node_base_id as original_node_base_id,
                    n_child.name as original_node_name,
                    n_child.type as original_node_type,
                    n_child.workflow_base_id as parent_workflow_base_id,
                    
                    -- å­çº§å·¥ä½œæµä¿¡æ¯
                    pw_child.name as parent_workflow_name,
                    pw_child.description as parent_workflow_description,
                    sw_child.name as sub_workflow_name,
                    sw_child.description as sub_workflow_description,
                    
                    -- å­çº§å·¥ä½œæµå®ä¾‹çŠ¶æ€
                    swi_child.status as sub_workflow_status,
                    swi_child.started_at as sub_workflow_started_at,
                    swi_child.completed_at as sub_workflow_completed_at
                    
                FROM subdivision_hierarchy sh
                JOIN task_subdivision ts_child ON sh.subdivision_id = ts_child.parent_subdivision_id
                JOIN task_instance ti_child ON ts_child.original_task_id = ti_child.task_instance_id
                JOIN node_instance ni_child ON ti_child.node_instance_id = ni_child.node_instance_id  
                JOIN node n_child ON ni_child.node_id = n_child.node_id
                LEFT JOIN workflow pw_child ON n_child.workflow_base_id = pw_child.workflow_base_id AND pw_child.is_current_version = TRUE
                LEFT JOIN workflow sw_child ON ts_child.sub_workflow_base_id = sw_child.workflow_base_id AND sw_child.is_current_version = TRUE
                LEFT JOIN workflow_instance swi_child ON ts_child.sub_workflow_instance_id = swi_child.workflow_instance_id
                WHERE ts_child.is_deleted = FALSE
                AND ti_child.is_deleted = FALSE
                AND ni_child.is_deleted = FALSE
                AND sh.depth < %s  -- é™åˆ¶æœ€å¤§æ·±åº¦
            )
            SELECT * FROM subdivision_hierarchy
            ORDER BY depth, subdivision_created_at
            """
            
            all_subdivisions = await self.db.fetch_all(hierarchy_query, workflow_instance_id, max_depth)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            subdivisions_list = [dict(row) for row in all_subdivisions]
            
            logger.info(f"ğŸš€ WITH RECURSIVEæŸ¥è¯¢å®Œæˆ: æ‰¾åˆ° {len(subdivisions_list)} ä¸ªsubdivisionè®°å½•")
            if subdivisions_list:
                depths = [s['depth'] for s in subdivisions_list]
                logger.info(f"ğŸ“ æ·±åº¦åˆ†å¸ƒ: æœ€å°{min(depths)}, æœ€å¤§{max(depths)}, æ€»å±‚çº§{max(depths)+1}")
                
            return subdivisions_list
            
        except Exception as e:
            logger.error(f"âŒ è·å–subdivisionå±‚çº§æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def _batch_get_workflow_statistics(self, subdivisions: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """æ‰¹é‡è·å–å·¥ä½œæµç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…å•ç‹¬æŸ¥è¯¢æ¯ä¸ªå·¥ä½œæµ"""
        try:
            if not subdivisions:
                return {}
            
            # æå–æ‰€æœ‰éœ€è¦ç»Ÿè®¡çš„å·¥ä½œæµID
            workflow_base_ids = set()
            workflow_instance_ids = set()
            
            for sub in subdivisions:
                if sub['sub_workflow_base_id']:
                    workflow_base_ids.add(sub['sub_workflow_base_id'])
                if sub['sub_workflow_instance_id']:
                    workflow_instance_ids.add(sub['sub_workflow_instance_id'])
            
            # æ‰¹é‡æŸ¥è¯¢èŠ‚ç‚¹ç»Ÿè®¡
            node_stats_query = """
            SELECT 
                n.workflow_base_id,
                COUNT(*) as total_nodes
            FROM node n
            WHERE n.workflow_base_id = ANY($1)
            AND n.is_current_version = TRUE 
            AND n.is_deleted = FALSE
            GROUP BY n.workflow_base_id
            """
            
            node_stats = await self.db.fetch_all(node_stats_query, list(workflow_base_ids))
            node_stats_map = {str(row['workflow_base_id']): row['total_nodes'] for row in node_stats}
            
            # æ‰¹é‡æŸ¥è¯¢å·²å®ŒæˆèŠ‚ç‚¹ç»Ÿè®¡
            completed_stats_query = """
            SELECT 
                n.workflow_base_id,
                ni.workflow_instance_id,
                COUNT(*) as completed_nodes
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE n.workflow_base_id = ANY($1)
            AND ni.workflow_instance_id = ANY($2)
            AND ni.status = 'completed'
            AND ni.is_deleted = FALSE
            GROUP BY n.workflow_base_id, ni.workflow_instance_id
            """
            
            completed_stats = await self.db.fetch_all(completed_stats_query, list(workflow_base_ids), list(workflow_instance_ids))
            completed_stats_map = {}
            for row in completed_stats:
                key = f"{row['workflow_base_id']}_{row['workflow_instance_id']}"
                completed_stats_map[key] = row['completed_nodes']
            
            # æ„å»ºå®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯æ˜ å°„
            stats_map = {}
            for workflow_base_id in workflow_base_ids:
                workflow_base_id_str = str(workflow_base_id)
                stats_map[workflow_base_id_str] = {
                    'total_nodes': node_stats_map.get(workflow_base_id_str, 0),
                    'completed_by_instance': {}
                }
                
                # ä¸ºæ¯ä¸ªå®ä¾‹æ·»åŠ å·²å®ŒæˆèŠ‚ç‚¹æ•°
                for workflow_instance_id in workflow_instance_ids:
                    key = f"{workflow_base_id}_{workflow_instance_id}"
                    if key in completed_stats_map:
                        stats_map[workflow_base_id_str]['completed_by_instance'][str(workflow_instance_id)] = completed_stats_map[key]
            
            logger.info(f"ğŸ“Š æ‰¹é‡ç»Ÿè®¡å®Œæˆ: {len(workflow_base_ids)} ä¸ªå·¥ä½œæµæ¨¡æ¿, {len(workflow_instance_ids)} ä¸ªå®ä¾‹")
            return stats_map
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è·å–å·¥ä½œæµç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _empty_connection_result(self, workflow_instance_id: uuid.UUID) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„è¿æ¥ç»“æœ"""
        return {
            "workflow_instance_id": str(workflow_instance_id),
            "template_connections": [],
            "detailed_workflows": {},
            "merge_candidates": [],
            "detailed_connection_graph": {
                "nodes": [],
                "edges": []
            },
            "statistics": {
                "total_subdivisions": 0,
                "completed_sub_workflows": 0,
                "unique_workflows": 0,
                "max_depth": 0
            },
            "performance_info": {
                "optimization_used": "parent_subdivision_id_hierarchy",
                "total_subdivisions": 0
            }
        }
    
    def _build_detailed_connections(self, all_subdivisions: List[Dict[str, Any]], workflow_stats: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åœ¨å†…å­˜ä¸­æ„å»ºè¯¦ç»†çš„è¿æ¥æ•°æ®ç»“æ„"""
        try:
            detailed_connections = []
            
            for subdivision in all_subdivisions:
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                sub_workflow_base_id = str(subdivision['sub_workflow_base_id'])
                sub_workflow_instance_id = str(subdivision['sub_workflow_instance_id']) if subdivision['sub_workflow_instance_id'] else None
                
                stats = workflow_stats.get(sub_workflow_base_id, {})
                total_nodes = stats.get('total_nodes', 0)
                completed_nodes = 0
                
                if sub_workflow_instance_id and 'completed_by_instance' in stats:
                    completed_nodes = stats['completed_by_instance'].get(sub_workflow_instance_id, 0)
                
                connection = {
                    "subdivision_id": str(subdivision["subdivision_id"]),
                    "subdivision_name": subdivision["subdivision_name"],
                    "subdivision_description": subdivision["subdivision_description"] or "",
                    "created_at": subdivision["subdivision_created_at"].isoformat() if subdivision["subdivision_created_at"] else None,
                    "depth": subdivision["depth"],  # ç›´æ¥ä½¿ç”¨WITH RECURSIVEè®¡ç®—çš„æ·±åº¦
                    "parent_subdivision_id": str(subdivision["parent_subdivision_id"]) if subdivision["parent_subdivision_id"] else None,
                    "hierarchy_path": subdivision["hierarchy_path"],  # å±‚çº§è·¯å¾„
                    
                    # çˆ¶å·¥ä½œæµä¿¡æ¯
                    "parent_workflow": {
                        "workflow_base_id": str(subdivision["parent_workflow_base_id"]),
                        "workflow_name": subdivision["parent_workflow_name"] or f"å·¥ä½œæµ_{str(subdivision['parent_workflow_base_id'])[:8]}",
                        "workflow_description": subdivision["parent_workflow_description"] or "",
                        "workflow_instance_id": str(subdivision["parent_workflow_instance_id"]),
                        "connected_node": {
                            "node_base_id": str(subdivision["original_node_base_id"]),
                            "node_name": subdivision["original_node_name"],
                            "node_type": subdivision["original_node_type"],
                            "task_title": subdivision["task_title"],
                            "task_description": subdivision["task_description"] or ""
                        }
                    },
                    
                    # å­å·¥ä½œæµä¿¡æ¯ï¼ˆåŒ…å«ç»Ÿè®¡æ•°æ®ï¼‰
                    "sub_workflow": {
                        "workflow_base_id": sub_workflow_base_id,
                        "workflow_name": subdivision["sub_workflow_name"] or f"å·¥ä½œæµ_{sub_workflow_base_id[:8]}",
                        "workflow_description": subdivision["sub_workflow_description"] or "",
                        "instance_id": sub_workflow_instance_id,
                        "status": subdivision["sub_workflow_status"] or "unknown",
                        "started_at": subdivision["sub_workflow_started_at"].isoformat() if subdivision["sub_workflow_started_at"] else None,
                        "completed_at": subdivision["sub_workflow_completed_at"].isoformat() if subdivision["sub_workflow_completed_at"] else None,
                        "total_nodes": total_nodes,
                        "completed_nodes": completed_nodes
                    }
                }
                
                detailed_connections.append(connection)
            
            logger.info(f"ğŸ“‹ æ„å»ºè¯¦ç»†è¿æ¥æ•°æ®å®Œæˆ: {len(detailed_connections)} ä¸ªè¿æ¥")
            return detailed_connections
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¯¦ç»†è¿æ¥æ•°æ®å¤±è´¥: {e}")
            return []
    
    # ä¿æŒåŸæœ‰æ–¹æ³•å‘åå…¼å®¹
    async def get_workflow_template_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """
        è·å–æ‰§è¡Œå®ä¾‹å®Œæˆåçš„å·¥ä½œæµæ¨¡æ¿è¿æ¥å›¾æ•°æ®ï¼ˆæ”¯æŒé€’å½’å±•å¼€ï¼‰
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦ï¼Œé˜²æ­¢æ— é™é€’å½’
            
        Returns:
            åŒ…å«æ¨¡æ¿è¿æ¥å…³ç³»çš„æ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šå±‚åµŒå¥—
        """
        try:
            logger.info(f"ğŸ” è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»(é€’å½’æ·±åº¦ {max_depth}): {workflow_instance_id}")
            
            # é€’å½’è·å–æ‰€æœ‰å±‚çº§çš„æ¨¡æ¿è¿æ¥å…³ç³»
            all_connections = await self._get_recursive_template_connections(workflow_instance_id, max_depth)
            
            if not all_connections:
                logger.info(f"ğŸ“‹ æœªæ‰¾åˆ°å·¥ä½œæµå®ä¾‹çš„ç»†åˆ†å…³ç³»: {workflow_instance_id}")
                return {
                    "workflow_instance_id": str(workflow_instance_id),
                    "template_connections": [],
                    "connection_graph": {
                        "nodes": [],
                        "edges": []
                    },
                    "recursive_levels": 0
                }
            
            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(all_connections)} ä¸ªå·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»ï¼ˆåŒ…å«é€’å½’ï¼‰")
            
            # æ„å»ºè¿æ¥å›¾æ•°æ®ç»“æ„ï¼ˆæ”¯æŒå¤šå±‚é€’å½’ï¼‰
            connection_graph = self._build_recursive_connection_graph(all_connections)
            
            result = {
                "workflow_instance_id": str(workflow_instance_id),
                "template_connections": all_connections,
                "connection_graph": connection_graph,
                "recursive_levels": self._calculate_max_depth(all_connections),
                "statistics": {
                    "total_subdivisions": len(all_connections),
                    "completed_sub_workflows": len([c for c in all_connections if c["sub_workflow"]["status"] == "completed"]),
                    "unique_parent_workflows": len(set(c["parent_workflow"]["workflow_base_id"] for c in all_connections)),
                    "unique_sub_workflows": len(set(c["sub_workflow"]["workflow_base_id"] for c in all_connections)),
                    "max_recursion_depth": self._calculate_max_depth(all_connections)
                }
            }
            
            logger.info(f"âœ… å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»è·å–æˆåŠŸ: {result['statistics']}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»å¤±è´¥: {e}")
            raise
    
    async def _get_recursive_template_connections(self, workflow_instance_id: uuid.UUID, max_depth: int, current_depth: int = 0, parent_subdivision_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        é€’å½’è·å–æ‰€æœ‰å±‚çº§çš„æ¨¡æ¿è¿æ¥å…³ç³»
        
        Args:
            workflow_instance_id: å½“å‰å±‚çº§çš„å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            current_depth: å½“å‰é€’å½’æ·±åº¦
            parent_subdivision_id: çˆ¶subdivisionçš„IDï¼Œç”¨äºå»ºç«‹çœŸå®çš„å±‚çº§å…³ç³»
            
        Returns:
            åŒ…å«æ‰€æœ‰å±‚çº§è¿æ¥å…³ç³»çš„åˆ—è¡¨
        """
        if current_depth >= max_depth:
            logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ {max_depth}ï¼Œåœæ­¢é€’å½’æŸ¥è¯¢")
            return []
            
        logger.debug(f"ğŸ”„ é€’å½’æŸ¥è¯¢å±‚çº§ {current_depth}: {workflow_instance_id}, çˆ¶subdivision: {parent_subdivision_id}")
        
        # æŸ¥è¯¢å½“å‰å±‚çº§çš„ç›´æ¥å­å·¥ä½œæµ
        subdivisions_query = """
        SELECT 
            ts.subdivision_id,
            ts.original_task_id,
            ts.sub_workflow_base_id,
            ts.sub_workflow_instance_id,
            ts.subdivision_name,
            ts.subdivision_description,
            ts.subdivision_created_at,
            ts.parent_subdivision_id,
            
            -- åŸå§‹ä»»åŠ¡ä¿¡æ¯
            ti.task_title,
            ti.task_description,
            ti.node_instance_id,
            ti.workflow_instance_id as parent_workflow_instance_id,
            
            -- åŸå§‹èŠ‚ç‚¹ä¿¡æ¯  
            ni.node_id as original_node_id,
            n.node_base_id as original_node_base_id,
            n.name as original_node_name,
            n.type as original_node_type,
            n.workflow_base_id as parent_workflow_base_id,
            
            -- çˆ¶å·¥ä½œæµä¿¡æ¯
            pw.name as parent_workflow_name,
            pw.description as parent_workflow_description,
            
            -- å­å·¥ä½œæµä¿¡æ¯
            sw.name as sub_workflow_name,
            sw.description as sub_workflow_description,
            
            -- å­å·¥ä½œæµå®ä¾‹å®ŒæˆçŠ¶æ€
            swi.status as sub_workflow_status,
            swi.started_at as sub_workflow_started_at,
            swi.completed_at as sub_workflow_completed_at,
            
            -- å­å·¥ä½œæµç»Ÿè®¡ä¿¡æ¯
            (SELECT COUNT(*) FROM node sn 
             WHERE sn.workflow_base_id = ts.sub_workflow_base_id 
             AND sn.is_current_version = TRUE 
             AND sn.is_deleted = FALSE) as sub_workflow_total_nodes,
             
            (SELECT COUNT(*) FROM node_instance sni 
             JOIN node sn ON sni.node_id = sn.node_id
             WHERE sn.workflow_base_id = ts.sub_workflow_base_id 
             AND sni.workflow_instance_id = ts.sub_workflow_instance_id
             AND sni.status = 'completed'
             AND sni.is_deleted = FALSE) as sub_workflow_completed_nodes
            
        FROM task_subdivision ts
        JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
        JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
        JOIN node n ON ni.node_id = n.node_id
        LEFT JOIN workflow pw ON n.workflow_base_id = pw.workflow_base_id AND pw.is_current_version = TRUE
        LEFT JOIN workflow sw ON ts.sub_workflow_base_id = sw.workflow_base_id AND sw.is_current_version = TRUE
        LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
        WHERE ti.workflow_instance_id = $1 
        AND ts.is_deleted = FALSE
        AND ti.is_deleted = FALSE
        AND ni.is_deleted = FALSE
        ORDER BY ts.subdivision_created_at
        """
        
        subdivisions = await self.db.fetch_all(subdivisions_query, workflow_instance_id)
        
        logger.info(f"ğŸ“Š [DEBUG] æŸ¥è¯¢subdivisionç»“æœ: å·¥ä½œæµå®ä¾‹ {workflow_instance_id}")
        logger.info(f"ğŸ“Š [DEBUG] æ‰¾åˆ° {len(subdivisions)} ä¸ªsubdivisionè®°å½•")
        
        for i, sub in enumerate(subdivisions):
            logger.info(f"ğŸ“Š [DEBUG] Subdivision {i+1}:")
            logger.info(f"    - subdivision_id: {sub['subdivision_id']}")
            logger.info(f"    - subdivision_name: {sub['subdivision_name']}")
            logger.info(f"    - sub_workflow_base_id: {sub['sub_workflow_base_id']}")
            logger.info(f"    - sub_workflow_instance_id: {sub['sub_workflow_instance_id']}")
            logger.info(f"    - original_node_name: {sub['original_node_name']}")
            logger.info(f"    - sub_workflow_name: {sub['sub_workflow_name']}")
        
        # è½¬æ¢å½“å‰å±‚çº§çš„è¿æ¥ä¸ºæ ‡å‡†æ ¼å¼
        current_connections = []
        child_instance_ids = []  # è®°å½•å­å·¥ä½œæµå®ä¾‹IDï¼Œç”¨äºä¸‹å±‚é€’å½’
        
        for subdivision in subdivisions:
            # è®¡ç®—çœŸå®çš„å±‚çº§å…³ç³»ï¼šå¦‚æœæœ‰çˆ¶subdivisionï¼Œåˆ™å±‚çº§+1
            actual_level = 0 if parent_subdivision_id is None else current_depth
            
            connection = {
                "subdivision_id": str(subdivision["subdivision_id"]),
                "subdivision_name": subdivision["subdivision_name"],
                "subdivision_description": subdivision["subdivision_description"] or "",
                "created_at": subdivision["subdivision_created_at"].isoformat() if subdivision["subdivision_created_at"] else None,
                "recursion_level": actual_level,  # ä½¿ç”¨å®é™…çš„å±‚çº§å…³ç³»
                "parent_subdivision_id": parent_subdivision_id,  # è®°å½•çˆ¶subdivisionå…³ç³»
                
                # çˆ¶å·¥ä½œæµä¿¡æ¯
                "parent_workflow": {
                    "workflow_base_id": str(subdivision["parent_workflow_base_id"]),
                    "workflow_name": subdivision["parent_workflow_name"] or f"å·¥ä½œæµ_{subdivision['parent_workflow_base_id'][:8]}",
                    "workflow_description": subdivision["parent_workflow_description"] or "",
                    "workflow_instance_id": str(subdivision["parent_workflow_instance_id"]),
                    "connected_node": {
                        "node_base_id": str(subdivision["original_node_base_id"]),
                        "node_name": subdivision["original_node_name"],
                        "node_type": subdivision["original_node_type"],
                        "task_title": subdivision["task_title"],
                        "task_description": subdivision["task_description"] or ""
                    }
                },
                
                # å­å·¥ä½œæµä¿¡æ¯
                "sub_workflow": {
                    "workflow_base_id": str(subdivision["sub_workflow_base_id"]),
                    "workflow_name": subdivision["sub_workflow_name"] or f"å·¥ä½œæµ_{subdivision['sub_workflow_base_id'][:8]}",
                    "workflow_description": subdivision["sub_workflow_description"] or "",
                    "instance_id": str(subdivision["sub_workflow_instance_id"]) if subdivision["sub_workflow_instance_id"] else None,
                    "status": subdivision["sub_workflow_status"] or "unknown",
                    "started_at": subdivision["sub_workflow_started_at"].isoformat() if subdivision["sub_workflow_started_at"] else None,
                    "completed_at": subdivision["sub_workflow_completed_at"].isoformat() if subdivision["sub_workflow_completed_at"] else None,
                    "total_nodes": subdivision["sub_workflow_total_nodes"] or 0,
                    "completed_nodes": subdivision["sub_workflow_completed_nodes"] or 0
                }
            }
            current_connections.append(connection)
            
            # æ”¶é›†å­å·¥ä½œæµå®ä¾‹IDï¼Œç”¨äºé€’å½’æŸ¥è¯¢
            if subdivision["sub_workflow_instance_id"]:
                child_instance_ids.append(subdivision["sub_workflow_instance_id"])
        
        # é€’å½’æŸ¥è¯¢å­å·¥ä½œæµçš„è¿æ¥å…³ç³»ï¼Œä¼ é€’subdivision_idå»ºç«‹çœŸå®çš„çˆ¶å­å…³ç³»
        all_connections = current_connections.copy()
        for i, child_instance_id in enumerate(child_instance_ids):
            try:
                # æ‰¾åˆ°å¯¹åº”çš„subdivision_idä½œä¸ºparent_subdivision_id
                current_subdivision_id = str(subdivisions[i]["subdivision_id"]) if i < len(subdivisions) else None
                
                child_connections = await self._get_recursive_template_connections(
                    child_instance_id, max_depth, current_depth + 1, current_subdivision_id
                )
                all_connections.extend(child_connections)
            except Exception as e:
                logger.warning(f"âš ï¸ é€’å½’æŸ¥è¯¢å­å·¥ä½œæµ {child_instance_id} å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–å­å·¥ä½œæµï¼Œä¸ä¸­æ–­æ•´ä¸ªé€’å½’è¿‡ç¨‹
                continue
        
        logger.debug(f"âœ… å±‚çº§ {current_depth} æŸ¥è¯¢å®Œæˆ: å½“å‰å±‚ {len(current_connections)} ä¸ªï¼Œæ€»è®¡ {len(all_connections)} ä¸ªè¿æ¥")
        return all_connections
    
    def _calculate_subdivision_level(self, target_connection: Dict[str, Any], all_connections: List[Dict[str, Any]]) -> int:
        """
        è®¡ç®—subdivisionçš„çœŸå®å±‚çº§
        
        Args:
            target_connection: ç›®æ ‡è¿æ¥
            all_connections: æ‰€æœ‰è¿æ¥å…³ç³»
            
        Returns:
            subdivisionçš„çœŸå®å±‚çº§ï¼ˆ0ä¸ºæ ¹çº§ï¼‰
        """
        try:
            subdivision_id = target_connection["subdivision_id"]
            parent_subdivision_id = target_connection.get("parent_subdivision_id")
            
            # å¦‚æœæ²¡æœ‰çˆ¶subdivisionï¼Œåˆ™ä¸ºæ ¹çº§
            if not parent_subdivision_id:
                return 0
            
            # é€’å½’æŸ¥æ‰¾çˆ¶subdivisionçš„å±‚çº§
            for connection in all_connections:
                if connection["subdivision_id"] == parent_subdivision_id:
                    parent_level = self._calculate_subdivision_level(connection, all_connections)
                    return parent_level + 1
            
            # å¦‚æœæ‰¾ä¸åˆ°çˆ¶subdivisionï¼ŒæŒ‰å·¥ä½œæµå®ä¾‹å…³ç³»åˆ¤æ–­
            # è¿™ç§æƒ…å†µè¯´æ˜çˆ¶subdivisionå¯èƒ½åœ¨ä¸Šä¸€è½®é€’å½’ä¸­ï¼Œåº”è¯¥æ˜¯å±‚çº§1
            return 1
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—subdivisionå±‚çº§å¤±è´¥: {e}")
            return 0  # é»˜è®¤è¿”å›0å±‚çº§
    
    def _calculate_max_depth(self, connections: List[Dict[str, Any]]) -> int:
        """è®¡ç®—è¿æ¥å…³ç³»ä¸­çš„æœ€å¤§é€’å½’æ·±åº¦"""
        if not connections:
            return 0
        return max(conn.get("recursion_level", 0) for conn in connections) + 1
    
    def _build_recursive_connection_graph(self, template_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºæ”¯æŒé€’å½’çš„è¿æ¥å›¾æ•°æ®ç»“æ„
        
        Args:
            template_connections: åŒ…å«æ‰€æœ‰å±‚çº§çš„æ¨¡æ¿è¿æ¥å…³ç³»æ•°æ®
            
        Returns:
            å›¾å½¢æ•°æ®ç»“æ„ï¼ŒåŒ…å«å¤šå±‚çº§èŠ‚ç‚¹å’Œè¾¹
        """
        try:
            nodes = {}
            edges = []
            
            # æŒ‰é€’å½’å±‚çº§åˆ†ç»„è¿æ¥å…³ç³»
            levels = {}
            for connection in template_connections:
                level = connection.get("recursion_level", 0)
                if level not in levels:
                    levels[level] = []
                levels[level].append(connection)
            
            logger.debug(f"ğŸ—ï¸ æ„å»ºé€’å½’è¿æ¥å›¾: {len(levels)} ä¸ªå±‚çº§, æ€»è®¡ {len(template_connections)} ä¸ªè¿æ¥")
            
            # æ„å»ºèŠ‚ç‚¹ï¼ˆå·¥ä½œæµæ¨¡æ¿ï¼‰- ä½¿ç”¨subdivisionçœŸå®å±‚çº§
            for connection in template_connections:
                parent_workflow = connection["parent_workflow"]
                sub_workflow = connection["sub_workflow"]
                subdivision_id = connection["subdivision_id"]
                parent_subdivision_id = connection.get("parent_subdivision_id")
                
                # è®¡ç®—çœŸå®çš„å±‚çº§ï¼šåŸºäºsubdivisionå±‚çº§é“¾
                actual_level = self._calculate_subdivision_level(connection, template_connections)
                
                # æ·»åŠ çˆ¶å·¥ä½œæµèŠ‚ç‚¹
                parent_id = parent_workflow["workflow_base_id"]
                if parent_id not in nodes:
                    # ä¸ºçˆ¶å·¥ä½œæµè®¡ç®—å±‚çº§
                    parent_level = 0 if not parent_subdivision_id else actual_level - 1
                    nodes[parent_id] = {
                        "id": parent_id,
                        "type": "workflow_template",
                        "label": parent_workflow["workflow_name"],
                        "description": parent_workflow["workflow_description"],
                        "is_parent": parent_level == 0,  # åªæœ‰å±‚çº§0æ˜¯çœŸæ­£çš„æ ¹å·¥ä½œæµ
                        "recursion_level": parent_level,
                        "connected_nodes": [],
                        "workflow_instance_id": parent_workflow.get("workflow_instance_id")
                    }
                
                # è®°å½•è¿æ¥çš„èŠ‚ç‚¹
                nodes[parent_id]["connected_nodes"].append({
                    "node_base_id": parent_workflow["connected_node"]["node_base_id"],
                    "node_name": parent_workflow["connected_node"]["node_name"],
                    "node_type": parent_workflow["connected_node"]["node_type"],
                    "subdivision_name": connection["subdivision_name"]
                })
                
                # æ·»åŠ å­å·¥ä½œæµèŠ‚ç‚¹
                sub_id = sub_workflow["workflow_base_id"]
                if sub_id not in nodes:
                    nodes[sub_id] = {
                        "id": sub_id,
                        "type": "workflow_template",
                        "label": sub_workflow["workflow_name"],
                        "description": sub_workflow["workflow_description"],
                        "is_parent": False,
                        "recursion_level": actual_level,  # ä½¿ç”¨çœŸå®çš„subdivisionå±‚çº§
                        "status": sub_workflow["status"],
                        "total_nodes": sub_workflow["total_nodes"],
                        "completed_nodes": sub_workflow["completed_nodes"],
                        "completion_rate": sub_workflow["completed_nodes"] / max(sub_workflow["total_nodes"], 1) if sub_workflow["total_nodes"] else 0,
                        "started_at": sub_workflow["started_at"],
                        "completed_at": sub_workflow["completed_at"],
                        "workflow_instance_id": sub_workflow.get("instance_id"),
                        "connected_nodes": []  # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    }
                
                # æ·»åŠ è¿æ¥è¾¹
                edge = {
                    "id": f"{parent_id}_{sub_id}_{connection['subdivision_id']}",
                    "source": parent_id,
                    "target": sub_id,
                    "type": "subdivision_connection",
                    "label": connection["subdivision_name"],
                    "subdivision_id": connection["subdivision_id"],
                    "connected_node_name": parent_workflow["connected_node"]["node_name"],
                    "task_title": parent_workflow["connected_node"]["task_title"],
                    "created_at": connection["created_at"],
                    "recursion_level": actual_level,
                    "edge_weight": actual_level + 1  # ç”¨äºå¯è§†åŒ–æ—¶çš„è¾¹æƒé‡
                }
                edges.append(edge)
            
            # è½¬æ¢èŠ‚ç‚¹å­—å…¸ä¸ºåˆ—è¡¨
            node_list = list(nodes.values())
            
            # æŒ‰é€’å½’å±‚çº§å’Œåç§°æ’åºèŠ‚ç‚¹
            node_list.sort(key=lambda x: (x["recursion_level"], not x["is_parent"], x["label"]))
            
            # è®¡ç®—é€’å½’å¸ƒå±€ä½ç½®
            max_level = max(node["recursion_level"] for node in node_list) if node_list else 0
            level_node_counts = {}
            for node in node_list:
                level = node["recursion_level"]
                level_node_counts[level] = level_node_counts.get(level, 0) + 1
            
            # æ„å»ºèŠ‚ç‚¹ä½ç½®æ˜ å°„ï¼ˆç”¨äºæ–‡ä»¶ç³»ç»Ÿå¼å¸ƒå±€ï¼‰
            node_position_map = self._build_file_system_position_map(node_list, template_connections)
            
            # æ„å»ºæ ‘çŠ¶å¸ƒå±€çš„çˆ¶å­å…³ç³»å’Œä½ç½®æ˜ å°„
            tree_layout_data = self._build_tree_layout_data(node_list, template_connections)
            
            return {
                "nodes": node_list,
                "edges": edges,
                "layout": {
                    "algorithm": "recursive_hierarchical",
                    "direction": "TB",  # Top to Bottom
                    "node_spacing": 180,
                    "level_spacing": 120,
                    "max_recursion_level": max_level,
                    "level_node_counts": level_node_counts,
                    "node_position_map": node_position_map,  # æ–‡ä»¶ç³»ç»Ÿå¼å¸ƒå±€æ˜ å°„
                    "tree_layout_data": tree_layout_data  # æ–°å¢ï¼šæ ‘çŠ¶å¸ƒå±€æ•°æ®
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºé€’å½’è¿æ¥å›¾æ•°æ®ç»“æ„å¤±è´¥: {e}")
            return {
                "nodes": [],
                "edges": [],
                "layout": {}
            }
    
    def _build_tree_layout_data(self, node_list: List[Dict[str, Any]], template_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºåŸºäºsubdivisionå±‚çº§å…³ç³»çš„æ ‘çŠ¶å¸ƒå±€æ•°æ®ç»“æ„
        
        Args:
            node_list: èŠ‚ç‚¹åˆ—è¡¨
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            
        Returns:
            æ ‘çŠ¶å¸ƒå±€æ•°æ®ç»“æ„
        """
        try:
            # æ„å»ºåŸºäºsubdivisionçš„çˆ¶å­å…³ç³»æ˜ å°„
            subdivision_parent_child_map = {}  # parent_subdivision_id -> [child_subdivision_ids]
            subdivision_child_parent_map = {}  # child_subdivision_id -> parent_subdivision_id
            subdivision_to_workflow_map = {}   # subdivision_id -> workflow_base_id
            
            # ä»template_connectionsæ„å»ºsubdivisionå±‚çº§å…³ç³»
            for connection in template_connections:
                subdivision_id = connection["subdivision_id"]
                parent_subdivision_id = connection.get("parent_subdivision_id")
                workflow_base_id = connection["sub_workflow"]["workflow_base_id"]
                
                # è®°å½•subdivisionåˆ°å·¥ä½œæµçš„æ˜ å°„
                subdivision_to_workflow_map[subdivision_id] = workflow_base_id
                
                # æ„å»ºsubdivisionçš„çˆ¶å­å…³ç³»
                if parent_subdivision_id:
                    # è¿™æ˜¯ä¸€ä¸ªå­subdivision
                    if parent_subdivision_id not in subdivision_parent_child_map:
                        subdivision_parent_child_map[parent_subdivision_id] = []
                    subdivision_parent_child_map[parent_subdivision_id].append(subdivision_id)
                    subdivision_child_parent_map[subdivision_id] = parent_subdivision_id
                # else: è¿™æ˜¯ä¸€ä¸ªæ ¹subdivision
            
            # æ„å»ºå·¥ä½œæµçº§åˆ«çš„çˆ¶å­å…³ç³»ï¼ˆåŸºäºsubdivisionå±‚çº§ï¼‰
            parent_child_map = {}  # parent_workflow_id -> [child_workflow_ids]
            child_parent_map = {}  # child_workflow_id -> parent_workflow_id
            
            for connection in template_connections:
                parent_workflow_id = connection["parent_workflow"]["workflow_base_id"]
                child_workflow_id = connection["sub_workflow"]["workflow_base_id"]
                subdivision_id = connection["subdivision_id"]
                parent_subdivision_id = connection.get("parent_subdivision_id")
                
                # å¦‚æœè¿™ä¸ªsubdivisionæœ‰çˆ¶subdivisionï¼Œåˆ™è¯¥å·¥ä½œæµçš„çˆ¶å·¥ä½œæµåº”è¯¥æ˜¯çˆ¶subdivisionå¯¹åº”çš„å·¥ä½œæµ
                if parent_subdivision_id:
                    # æ‰¾åˆ°çˆ¶subdivisionå¯¹åº”çš„å·¥ä½œæµ
                    actual_parent_workflow = subdivision_to_workflow_map.get(parent_subdivision_id)
                    if actual_parent_workflow:
                        parent_workflow_id = actual_parent_workflow
                
                if parent_workflow_id not in parent_child_map:
                    parent_child_map[parent_workflow_id] = []
                
                # é¿å…é‡å¤æ·»åŠ 
                if child_workflow_id not in parent_child_map[parent_workflow_id]:
                    parent_child_map[parent_workflow_id].append(child_workflow_id)
                child_parent_map[child_workflow_id] = parent_workflow_id
            
            # æ‰¾åˆ°æ ¹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰
            all_node_ids = set(node["id"] for node in node_list)
            root_nodes = []
            for node_id in all_node_ids:
                if node_id not in child_parent_map:
                    root_nodes.append(node_id)
            
            # æ„å»ºæ ‘çš„å±‚çº§ç»“æ„
            tree_levels = {}  # level -> [node_ids]
            node_levels = {}  # node_id -> level
            
            # ä½¿ç”¨BFSæ„å»ºå±‚çº§
            from collections import deque
            queue = deque()
            
            # åˆå§‹åŒ–æ ¹èŠ‚ç‚¹
            for root_id in root_nodes:
                queue.append((root_id, 0))
                node_levels[root_id] = 0
                if 0 not in tree_levels:
                    tree_levels[0] = []
                tree_levels[0].append(root_id)
            
            # BFSéå†æ„å»ºæ ‘å±‚çº§
            while queue:
                current_id, level = queue.popleft()
                
                # æ·»åŠ å­èŠ‚ç‚¹åˆ°ä¸‹ä¸€å±‚çº§
                if current_id in parent_child_map:
                    next_level = level + 1
                    if next_level not in tree_levels:
                        tree_levels[next_level] = []
                    
                    for child_id in parent_child_map[current_id]:
                        if child_id not in node_levels:  # é¿å…å¾ªç¯å¼•ç”¨
                            node_levels[child_id] = next_level
                            tree_levels[next_level].append(child_id)
                            queue.append((child_id, next_level))
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åœ¨å…¶å±‚çº§ä¸­çš„ä½ç½®
            node_positions = {}
            for level, node_ids in tree_levels.items():
                for index, node_id in enumerate(node_ids):
                    node_positions[node_id] = {
                        "level": level,
                        "index_in_level": index,
                        "total_in_level": len(node_ids),
                        "children": parent_child_map.get(node_id, []),
                        "parent": child_parent_map.get(node_id, None)
                    }
            
            logger.debug(f"ğŸŒ³ æ„å»ºåŸºäºsubdivisionçš„æ ‘çŠ¶å¸ƒå±€æ•°æ®: {len(tree_levels)} å±‚, {len(node_positions)} ä¸ªèŠ‚ç‚¹")
            logger.debug(f"ğŸ”— Subdivisionæ˜ å°„: {subdivision_to_workflow_map}")
            logger.debug(f"ğŸ“Š çˆ¶å­å…³ç³»: {parent_child_map}")
            
            return {
                "tree_levels": tree_levels,
                "node_levels": node_levels,
                "node_positions": node_positions,
                "parent_child_map": parent_child_map,
                "child_parent_map": child_parent_map,
                "root_nodes": root_nodes,
                "max_level": max(tree_levels.keys()) if tree_levels else 0,
                "subdivision_mapping": {
                    "subdivision_to_workflow": subdivision_to_workflow_map,
                    "subdivision_parent_child": subdivision_parent_child_map,
                    "subdivision_child_parent": subdivision_child_parent_map
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ ‘çŠ¶å¸ƒå±€æ•°æ®å¤±è´¥: {e}")
            return {
                "tree_levels": {},
                "node_levels": {},
                "node_positions": {},
                "parent_child_map": {},
                "child_parent_map": {},
                "root_nodes": [],
                "max_level": 0
            }
    
    def _build_file_system_position_map(self, node_list: List[Dict[str, Any]], template_connections: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        æ„å»ºæ–‡ä»¶ç³»ç»Ÿå¼å¸ƒå±€çš„èŠ‚ç‚¹ä½ç½®æ˜ å°„
        
        Args:
            node_list: èŠ‚ç‚¹åˆ—è¡¨
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            
        Returns:
            èŠ‚ç‚¹ä½ç½®æ˜ å°„å­—å…¸
        """
        try:
            position_map = {}
            
            # æŒ‰é€’å½’å±‚çº§åˆ†ç»„èŠ‚ç‚¹
            levels = {}
            for node in node_list:
                level = node.get("recursion_level", 0)
                if level not in levels:
                    levels[level] = []
                levels[level].append(node)
            
            # æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„
            parent_child_map = {}
            for connection in template_connections:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                child_id = connection["sub_workflow"]["workflow_base_id"]
                
                if parent_id not in parent_child_map:
                    parent_child_map[parent_id] = []
                parent_child_map[parent_id].append(child_id)
            
            # ä¸ºæ¯ä¸ªå±‚çº§åˆ†é…ä½ç½®
            for level in sorted(levels.keys()):
                level_nodes = levels[level]
                
                if level == 0:  # é¡¶å±‚èŠ‚ç‚¹
                    # é¡¶å±‚èŠ‚ç‚¹æŒ‰åç§°æ’åº
                    level_nodes.sort(key=lambda x: x["label"])
                    for i, node in enumerate(level_nodes):
                        position_map[node["id"]] = {
                            "yIndex": i,
                            "indexInLevel": i,
                            "parentIndex": None
                        }
                else:  # å­å±‚çº§èŠ‚ç‚¹
                    # å­èŠ‚ç‚¹æŒ‰çˆ¶èŠ‚ç‚¹åˆ†ç»„ï¼Œç„¶ååœ¨çˆ¶èŠ‚ç‚¹ä¸‹æ–¹æ’åˆ—
                    y_index = 0
                    index_in_level = 0
                    
                    for node in level_nodes:
                        # æ‰¾åˆ°è¿™ä¸ªèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹
                        parent_node = None
                        for connection in template_connections:
                            if connection["sub_workflow"]["workflow_base_id"] == node["id"]:
                                parent_id = connection["parent_workflow"]["workflow_base_id"]
                                parent_node = next((n for n in node_list if n["id"] == parent_id), None)
                                break
                        
                        parent_y_index = 0
                        if parent_node and parent_node["id"] in position_map:
                            parent_y_index = position_map[parent_node["id"]]["yIndex"]
                        
                        # å­èŠ‚ç‚¹æ”¾åœ¨çˆ¶èŠ‚ç‚¹çš„ä¸‹æ–¹
                        position_map[node["id"]] = {
                            "yIndex": parent_y_index + y_index + 1,  # åœ¨çˆ¶èŠ‚ç‚¹åŸºç¡€ä¸ŠåŠ åç§»
                            "indexInLevel": index_in_level,
                            "parentIndex": parent_y_index
                        }
                        
                        y_index += 1
                        index_in_level += 1
            
            logger.debug(f"ğŸ—‚ï¸ æ„å»ºæ–‡ä»¶ç³»ç»Ÿä½ç½®æ˜ å°„: {len(position_map)} ä¸ªèŠ‚ç‚¹")
            return position_map
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ–‡ä»¶ç³»ç»Ÿä½ç½®æ˜ å°„å¤±è´¥: {e}")
            return {}
    
    def _build_connection_graph(self, template_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºè¿æ¥å›¾æ•°æ®ç»“æ„
        
        Args:
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»æ•°æ®
            
        Returns:
            å›¾å½¢æ•°æ®ç»“æ„ï¼ŒåŒ…å«èŠ‚ç‚¹å’Œè¾¹
        """
        try:
            nodes = {}
            edges = []
            
            # æ„å»ºèŠ‚ç‚¹ï¼ˆå·¥ä½œæµæ¨¡æ¿ï¼‰
            for connection in template_connections:
                parent_workflow = connection["parent_workflow"]
                sub_workflow = connection["sub_workflow"]
                
                # æ·»åŠ çˆ¶å·¥ä½œæµèŠ‚ç‚¹
                parent_id = parent_workflow["workflow_base_id"]
                if parent_id not in nodes:
                    nodes[parent_id] = {
                        "id": parent_id,
                        "type": "workflow_template",
                        "label": parent_workflow["workflow_name"],
                        "description": parent_workflow["workflow_description"],
                        "is_parent": True,
                        "connected_nodes": []
                    }
                
                # è®°å½•è¿æ¥çš„èŠ‚ç‚¹
                nodes[parent_id]["connected_nodes"].append({
                    "node_base_id": parent_workflow["connected_node"]["node_base_id"],
                    "node_name": parent_workflow["connected_node"]["node_name"],
                    "node_type": parent_workflow["connected_node"]["node_type"],
                    "subdivision_name": connection["subdivision_name"]
                })
                
                # æ·»åŠ å­å·¥ä½œæµèŠ‚ç‚¹
                sub_id = sub_workflow["workflow_base_id"]
                if sub_id not in nodes:
                    nodes[sub_id] = {
                        "id": sub_id,
                        "type": "workflow_template",
                        "label": sub_workflow["workflow_name"],
                        "description": sub_workflow["workflow_description"],
                        "is_parent": False,
                        "status": sub_workflow["status"],
                        "total_nodes": sub_workflow["total_nodes"],
                        "completed_nodes": sub_workflow["completed_nodes"],
                        "completion_rate": sub_workflow["completed_nodes"] / max(sub_workflow["total_nodes"], 1),
                        "started_at": sub_workflow["started_at"],
                        "completed_at": sub_workflow["completed_at"],
                        "connected_nodes": []  # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                    }
                
                # æ·»åŠ è¿æ¥è¾¹
                edge = {
                    "id": f"{parent_id}_{sub_id}_{connection['subdivision_id']}",
                    "source": parent_id,
                    "target": sub_id,
                    "type": "subdivision_connection",
                    "label": connection["subdivision_name"],
                    "subdivision_id": connection["subdivision_id"],
                    "connected_node_name": parent_workflow["connected_node"]["node_name"],
                    "task_title": parent_workflow["connected_node"]["task_title"],
                    "created_at": connection["created_at"]
                }
                edges.append(edge)
            
            # è½¬æ¢èŠ‚ç‚¹å­—å…¸ä¸ºåˆ—è¡¨
            node_list = list(nodes.values())
            
            # æŒ‰å±‚çº§æ’åºèŠ‚ç‚¹ï¼ˆçˆ¶å·¥ä½œæµåœ¨å‰ï¼‰
            node_list.sort(key=lambda x: (not x["is_parent"], x["label"]))
            
            return {
                "nodes": node_list,
                "edges": edges,
                "layout": {
                    "algorithm": "hierarchical",
                    "direction": "TB",  # Top to Bottom
                    "node_spacing": 150,
                    "level_spacing": 100
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¿æ¥å›¾æ•°æ®ç»“æ„å¤±è´¥: {e}")
            return {
                "nodes": [],
                "edges": [],
                "layout": {}
            }
    
    async def get_workflow_template_connection_summary(self, workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµæ¨¡æ¿çš„è¿æ¥å…³ç³»æ‘˜è¦ï¼ˆç”¨äºæ˜¾ç¤ºæ¨¡æ¿çº§åˆ«çš„è¿æ¥ç»Ÿè®¡ï¼‰
        
        Args:
            workflow_base_id: å·¥ä½œæµåŸºç¡€ID
            
        Returns:
            è¿æ¥å…³ç³»æ‘˜è¦æ•°æ®
        """
        try:
            logger.info(f"ğŸ” è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦: {workflow_base_id}")
            
            summary_query = """
            SELECT 
                COUNT(DISTINCT ts.subdivision_id) as total_subdivisions,
                COUNT(DISTINCT ts.sub_workflow_base_id) as unique_sub_workflows,
                COUNT(DISTINCT ni.node_base_id) as connected_nodes,
                COUNT(DISTINCT swi.workflow_instance_id) as sub_workflow_instances,
                COUNT(CASE WHEN swi.status = 'completed' THEN 1 END) as completed_instances,
                MIN(ts.subdivision_created_at) as first_subdivision_at,
                MAX(ts.subdivision_created_at) as last_subdivision_at
            FROM task_subdivision ts
            JOIN task_instance ti ON ts.original_task_id = ti.task_instance_id
            JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
            JOIN node n ON ni.node_id = n.node_id
            LEFT JOIN workflow_instance swi ON ts.sub_workflow_instance_id = swi.workflow_instance_id
            WHERE n.workflow_base_id = $1
            AND ts.is_deleted = FALSE
            AND ti.is_deleted = FALSE
            AND ni.is_deleted = FALSE
            """
            
            result = await self.db.fetch_one(summary_query, workflow_base_id)
            
            if result:
                summary = {
                    "workflow_base_id": str(workflow_base_id),
                    "total_subdivisions": result["total_subdivisions"] or 0,
                    "unique_sub_workflows": result["unique_sub_workflows"] or 0,
                    "connected_nodes": result["connected_nodes"] or 0,
                    "sub_workflow_instances": result["sub_workflow_instances"] or 0,
                    "completed_instances": result["completed_instances"] or 0,
                    "success_rate": (result["completed_instances"] or 0) / max(result["sub_workflow_instances"] or 1, 1),
                    "first_subdivision_at": result["first_subdivision_at"].isoformat() if result["first_subdivision_at"] else None,
                    "last_subdivision_at": result["last_subdivision_at"].isoformat() if result["last_subdivision_at"] else None
                }
                
                logger.info(f"âœ… å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦: {summary}")
                return summary
            else:
                return {
                    "workflow_base_id": str(workflow_base_id),
                    "total_subdivisions": 0,
                    "unique_sub_workflows": 0,
                    "connected_nodes": 0,
                    "sub_workflow_instances": 0,
                    "completed_instances": 0,
                    "success_rate": 0,
                    "first_subdivision_at": None,
                    "last_subdivision_at": None
                }
                
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµæ¨¡æ¿è¿æ¥æ‘˜è¦å¤±è´¥: {e}")
            raise
    
    async def get_detailed_workflow_connections(self, workflow_instance_id: uuid.UUID, max_depth: int = 10) -> Dict[str, Any]:
        """
        è·å–åŒ…å«å†…éƒ¨èŠ‚ç‚¹è¯¦æƒ…çš„å·¥ä½œæµæ¨¡æ¿è¿æ¥å›¾æ•°æ®
        
        Args:
            workflow_instance_id: å·¥ä½œæµå®ä¾‹ID
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            
        Returns:
            åŒ…å«è¯¦ç»†å†…éƒ¨èŠ‚ç‚¹ä¿¡æ¯çš„è¿æ¥å›¾æ•°æ®
        """
        try:
            logger.info(f"ğŸ” è·å–è¯¦ç»†å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»(é€’å½’æ·±åº¦ {max_depth}): {workflow_instance_id}")
            
            # è·å–åŸºç¡€è¿æ¥å…³ç³»
            base_connections = await self.get_workflow_template_connections(workflow_instance_id, max_depth)
            
            # è·å–æ¯ä¸ªå·¥ä½œæµçš„è¯¦ç»†å†…éƒ¨ç»“æ„
            detailed_workflows = {}
            unique_workflow_ids = set()
            
            # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å·¥ä½œæµID
            for connection in base_connections["template_connections"]:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                sub_id = connection["sub_workflow"]["workflow_base_id"]
                unique_workflow_ids.add(parent_id)
                unique_workflow_ids.add(sub_id)
            
            logger.info(f"ğŸ“Š [DEBUG] æ”¶é›†åˆ°çš„å”¯ä¸€å·¥ä½œæµIDæ•°é‡: {len(unique_workflow_ids)}")
            logger.info(f"ğŸ“Š [DEBUG] å·¥ä½œæµIDåˆ—è¡¨: {list(unique_workflow_ids)}")
            logger.info(f"ğŸ“Š [DEBUG] æ¨¡æ¿è¿æ¥å…³ç³»æ•°é‡: {len(base_connections['template_connections'])}")
            
            # è·å–æ¯ä¸ªå·¥ä½œæµçš„è¯¦ç»†å†…éƒ¨ç»“æ„
            for workflow_base_id in unique_workflow_ids:
                logger.info(f"ğŸ” [DEBUG] è·å–å·¥ä½œæµ {workflow_base_id} çš„å†…éƒ¨ç»“æ„...")
                detailed_workflows[workflow_base_id] = await self._get_workflow_internal_structure(
                    uuid.UUID(workflow_base_id)
                )
            
            # åˆ†æå¯æ›¿æ¢çš„èŠ‚ç‚¹å¯¹
            merge_candidates = self._analyze_merge_candidates(
                base_connections["template_connections"], 
                detailed_workflows
            )
            
            # æ„å»ºè¯¦ç»†è¿æ¥å›¾
            detailed_graph = self._build_detailed_connection_graph(
                base_connections["template_connections"],
                detailed_workflows,
                merge_candidates
            )
            
            result = {
                **base_connections,
                "detailed_workflows": detailed_workflows,
                "merge_candidates": merge_candidates,
                "detailed_connection_graph": detailed_graph
            }
            
            logger.info(f"âœ… è¯¦ç»†å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»è·å–æˆåŠŸ: {len(detailed_workflows)} ä¸ªå·¥ä½œæµ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–è¯¦ç»†å·¥ä½œæµæ¨¡æ¿è¿æ¥å…³ç³»å¤±è´¥: {e}")
            raise
    
    async def _get_workflow_internal_structure(self, workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """
        è·å–å·¥ä½œæµçš„å†…éƒ¨èŠ‚ç‚¹å’Œè¿æ¥ç»“æ„
        
        Args:
            workflow_base_id: å·¥ä½œæµåŸºç¡€ID
            
        Returns:
            å·¥ä½œæµå†…éƒ¨ç»“æ„æ•°æ®
        """
        try:
            # è·å–å·¥ä½œæµçš„æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«processorï¼‰
            nodes_query = """
            SELECT 
                n.node_id,
                n.node_base_id,
                n.name,
                n.type,
                n.task_description,
                n.position_x,
                n.position_y,
                n.created_at,
                n.updated_at
            FROM node n
            WHERE n.workflow_base_id = $1
            AND n.is_current_version = TRUE
            AND n.is_deleted = FALSE
            ORDER BY n.created_at
            """
            
            nodes = await self.db.fetch_all(nodes_query, workflow_base_id)
            
            # è·å–å·¥ä½œæµçš„æ‰€æœ‰è¿æ¥
            connections_query = """
            SELECT 
                CONCAT(nc.from_node_id, '-', nc.to_node_id) as connection_id,
                nc.from_node_id,
                nc.to_node_id,
                nc.connection_type,
                fn.node_base_id as from_node_base_id,
                fn.name as from_node_name,
                tn.node_base_id as to_node_base_id,
                tn.name as to_node_name
            FROM node_connection nc
            JOIN node fn ON nc.from_node_id = fn.node_id
            JOIN node tn ON nc.to_node_id = tn.node_id
            WHERE nc.workflow_id = (
                SELECT workflow_id 
                FROM workflow 
                WHERE workflow_base_id = %s 
                AND is_current_version = TRUE 
                LIMIT 1
            )
            ORDER BY nc.created_at
            """
            
            connections = await self.db.fetch_all(connections_query, workflow_base_id)
            
            # è½¬æ¢èŠ‚ç‚¹æ•°æ®æ ¼å¼
            formatted_nodes = []
            for node in nodes:
                formatted_nodes.append({
                    "node_id": str(node["node_id"]),
                    "node_base_id": str(node["node_base_id"]),
                    "name": node["name"],
                    "type": node["type"],
                    "task_description": node["task_description"] or "",
                    "position": {
                        "x": float(node["position_x"]) if node["position_x"] else 0,
                        "y": float(node["position_y"]) if node["position_y"] else 0
                    },
                    "created_at": node["created_at"].isoformat() if node["created_at"] else None,
                    "updated_at": node["updated_at"].isoformat() if node["updated_at"] else None
                })
            
            # è½¬æ¢è¿æ¥æ•°æ®æ ¼å¼
            formatted_connections = []
            for connection in connections:
                formatted_connections.append({
                    "connection_id": str(connection["connection_id"]),
                    "from_node": {
                        "node_id": str(connection["from_node_id"]),
                        "node_base_id": str(connection["from_node_base_id"]),
                        "name": connection["from_node_name"]
                    },
                    "to_node": {
                        "node_id": str(connection["to_node_id"]),
                        "node_base_id": str(connection["to_node_base_id"]),
                        "name": connection["to_node_name"]
                    },
                    "connection_type": connection["connection_type"]
                })
            
            return {
                "workflow_base_id": str(workflow_base_id),
                "nodes": formatted_nodes,
                "connections": formatted_connections,
                "node_count": len(formatted_nodes),
                "connection_count": len(formatted_connections)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥ä½œæµå†…éƒ¨ç»“æ„å¤±è´¥: {workflow_base_id}, {e}")
            return {
                "workflow_base_id": str(workflow_base_id),
                "nodes": [],
                "connections": [],
                "node_count": 0,
                "connection_count": 0
            }
    
    def _analyze_merge_candidates(self, template_connections: List[Dict[str, Any]], detailed_workflows: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åˆ†æå¯åˆå¹¶çš„èŠ‚ç‚¹å¯¹
        
        Args:
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            detailed_workflows: è¯¦ç»†å·¥ä½œæµç»“æ„
            
        Returns:
            å¯åˆå¹¶èŠ‚ç‚¹å¯¹åˆ—è¡¨
        """
        merge_candidates = []
        
        try:
            for connection in template_connections:
                parent_workflow_id = connection["parent_workflow"]["workflow_base_id"]
                sub_workflow_id = connection["sub_workflow"]["workflow_base_id"]
                connected_node_id = connection["parent_workflow"]["connected_node"]["node_base_id"]
                
                # è·å–çˆ¶å·¥ä½œæµå’Œå­å·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯
                parent_workflow = detailed_workflows.get(parent_workflow_id, {})
                sub_workflow = detailed_workflows.get(sub_workflow_id, {})
                
                if not parent_workflow or not sub_workflow:
                    continue
                
                # æ‰¾åˆ°è¢«ç»†åˆ†çš„èŠ‚ç‚¹
                connected_node = None
                for node in parent_workflow.get("nodes", []):
                    if node["node_base_id"] == connected_node_id:
                        connected_node = node
                        break
                
                if not connected_node:
                    continue
                
                # åˆ†æå­å·¥ä½œæµçš„å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
                start_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "start"]
                end_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "end"]
                
                merge_candidate = {
                    "subdivision_id": connection["subdivision_id"],
                    "parent_workflow_id": parent_workflow_id,
                    "sub_workflow_id": sub_workflow_id,
                    "replaceable_node": {
                        "node_base_id": connected_node["node_base_id"],
                        "name": connected_node["name"],
                        "type": connected_node["type"],
                        "task_description": connected_node["task_description"]
                    },
                    "replacement_structure": {
                        "start_nodes": start_nodes,
                        "end_nodes": end_nodes,
                        "total_nodes": len(sub_workflow.get("nodes", [])),
                        "total_connections": len(sub_workflow.get("connections", []))
                    },
                    "compatibility": self._check_merge_compatibility(connected_node, sub_workflow),
                    "merge_complexity": "simple" if len(sub_workflow.get("nodes", [])) <= 5 else "complex"
                }
                
                merge_candidates.append(merge_candidate)
            
            logger.debug(f"ğŸ” åˆ†æåˆ° {len(merge_candidates)} ä¸ªå¯åˆå¹¶èŠ‚ç‚¹å¯¹")
            return merge_candidates
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¯åˆå¹¶èŠ‚ç‚¹å¯¹å¤±è´¥: {e}")
            return []
    
    def _check_merge_compatibility(self, parent_node: Dict[str, Any], sub_workflow: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ£€æŸ¥èŠ‚ç‚¹åˆå¹¶çš„å…¼å®¹æ€§
        
        Args:
            parent_node: çˆ¶èŠ‚ç‚¹ä¿¡æ¯
            sub_workflow: å­å·¥ä½œæµç»“æ„
            
        Returns:
            å…¼å®¹æ€§æ£€æŸ¥ç»“æœ
        """
        try:
            compatibility = {
                "is_compatible": True,
                "issues": [],
                "recommendations": []
            }
            
            # æ£€æŸ¥èŠ‚ç‚¹ç±»å‹å…¼å®¹æ€§
            if parent_node["type"] != "processor":
                compatibility["is_compatible"] = False
                compatibility["issues"].append("åªæœ‰å¤„ç†å™¨ç±»å‹çš„èŠ‚ç‚¹å¯ä»¥è¢«æ›¿æ¢")
            
            # æ£€æŸ¥å­å·¥ä½œæµç»“æ„
            start_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "start"]
            end_nodes = [n for n in sub_workflow.get("nodes", []) if n["type"] == "end"]
            
            if len(start_nodes) == 0:
                compatibility["issues"].append("å­å·¥ä½œæµç¼ºå°‘å¼€å§‹èŠ‚ç‚¹")
                compatibility["is_compatible"] = False
            elif len(start_nodes) > 1:
                compatibility["recommendations"].append("å­å·¥ä½œæµæœ‰å¤šä¸ªå¼€å§‹èŠ‚ç‚¹ï¼Œåˆå¹¶åå¯èƒ½éœ€è¦è°ƒæ•´è¿æ¥")
            
            if len(end_nodes) == 0:
                compatibility["issues"].append("å­å·¥ä½œæµç¼ºå°‘ç»“æŸèŠ‚ç‚¹")
                compatibility["is_compatible"] = False
            elif len(end_nodes) > 1:
                compatibility["recommendations"].append("å­å·¥ä½œæµæœ‰å¤šä¸ªç»“æŸèŠ‚ç‚¹ï¼Œåˆå¹¶åå¯èƒ½éœ€è¦è°ƒæ•´è¿æ¥")
            
            # æ£€æŸ¥å¤æ‚åº¦
            node_count = len(sub_workflow.get("nodes", []))
            if node_count > 10:
                compatibility["recommendations"].append(f"å­å·¥ä½œæµè¾ƒå¤æ‚({node_count}ä¸ªèŠ‚ç‚¹)ï¼Œå»ºè®®ä»”ç»†å®¡æŸ¥åˆå¹¶ç»“æœ")
            
            return compatibility
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥åˆå¹¶å…¼å®¹æ€§å¤±è´¥: {e}")
            return {
                "is_compatible": False,
                "issues": ["å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥"],
                "recommendations": []
            }
    
    def _build_detailed_connection_graph(self, template_connections: List[Dict[str, Any]], detailed_workflows: Dict[str, Dict[str, Any]], merge_candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºåŒ…å«å†…éƒ¨èŠ‚ç‚¹çš„è¯¦ç»†è¿æ¥å›¾
        
        Args:
            template_connections: æ¨¡æ¿è¿æ¥å…³ç³»
            detailed_workflows: è¯¦ç»†å·¥ä½œæµç»“æ„
            merge_candidates: å¯åˆå¹¶èŠ‚ç‚¹å¯¹
            
        Returns:
            è¯¦ç»†è¿æ¥å›¾æ•°æ®ç»“æ„
        """
        try:
            nodes = []
            edges = []
            
            # è®¡ç®—å·¥ä½œæµå¸ƒå±€ä½ç½®
            workflow_positions = self._calculate_workflow_layout_positions(detailed_workflows)
            
            # ä¸ºæ¯ä¸ªå·¥ä½œæµæ·»åŠ èŠ‚ç‚¹
            for workflow_id, workflow_data in detailed_workflows.items():
                workflow_pos = workflow_positions.get(workflow_id, {"x": 0, "y": 0})
                
                logger.info(f"ğŸ—ï¸ [DEBUG] åˆ›å»ºå·¥ä½œæµå®¹å™¨èŠ‚ç‚¹: {workflow_id}")
                logger.info(f"    - ä½ç½®: x={workflow_pos['x']}, y={workflow_pos['y']}")
                logger.info(f"    - èŠ‚ç‚¹æ•°: {workflow_data.get('node_count', 0)}")
                logger.info(f"    - è¿æ¥æ•°: {workflow_data.get('connection_count', 0)}")
                
                # æ·»åŠ å·¥ä½œæµå®¹å™¨èŠ‚ç‚¹
                workflow_node = {
                    "id": f"workflow_{workflow_id}",
                    "type": "workflow_container",
                    "label": f"å·¥ä½œæµ {workflow_id[:8]}",
                    "position": {
                        "x": workflow_pos["x"],
                        "y": workflow_pos["y"]
                    },
                    "data": {
                        "workflow_base_id": workflow_id,
                        "node_count": workflow_data["node_count"],
                        "connection_count": workflow_data["connection_count"]
                    }
                }
                nodes.append(workflow_node)
                
                # æ·»åŠ å†…éƒ¨èŠ‚ç‚¹ï¼ŒåŸºäºå·¥ä½œæµå®¹å™¨ä½ç½®è¿›è¡Œåç§»
                internal_positions = self._calculate_internal_node_positions(
                    workflow_data.get("nodes", []),
                    workflow_pos,
                    workflow_data["node_count"]
                )
                
                for i, node in enumerate(workflow_data.get("nodes", [])):
                    internal_pos = internal_positions[i] if i < len(internal_positions) else {"x": 0, "y": 0}
                    
                    internal_node = {
                        "id": f"node_{node['node_base_id']}",
                        "type": "internal_node",
                        "label": node["name"],
                        "position": internal_pos,
                        "data": {
                            **node,
                            "parent_workflow_id": workflow_id,
                            "node_type": node["type"]
                        }
                    }
                    nodes.append(internal_node)
                
                # æ·»åŠ å†…éƒ¨è¿æ¥
                for connection in workflow_data.get("connections", []):
                    internal_edge = {
                        "id": f"edge_{connection['connection_id']}",
                        "source": f"node_{connection['from_node']['node_base_id']}",
                        "target": f"node_{connection['to_node']['node_base_id']}",
                        "sourceHandle": "source",  # æ·»åŠ é»˜è®¤çš„sourceHandle
                        "targetHandle": "target",  # æ·»åŠ é»˜è®¤çš„targetHandle
                        "type": "internal_connection",
                        "label": connection["connection_type"],
                        "data": connection
                    }
                    edges.append(internal_edge)
            
            # æ·»åŠ å·¥ä½œæµé—´çš„è¿æ¥
            for connection in template_connections:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                sub_id = connection["sub_workflow"]["workflow_base_id"]
                connected_node_id = connection["parent_workflow"]["connected_node"]["node_base_id"]
                
                # æ·»åŠ ä»çˆ¶å·¥ä½œæµèŠ‚ç‚¹åˆ°å­å·¥ä½œæµçš„è¿æ¥
                workflow_connection = {
                    "id": f"subdivision_{connection['subdivision_id']}",
                    "source": f"node_{connected_node_id}",
                    "target": f"workflow_{sub_id}",
                    "sourceHandle": "source",  # æ·»åŠ é»˜è®¤çš„sourceHandle
                    "targetHandle": "target",  # æ·»åŠ é»˜è®¤çš„targetHandle
                    "type": "subdivision_connection",
                    "label": connection["subdivision_name"],
                    "data": connection
                }
                edges.append(workflow_connection)
            
            logger.debug(f"ğŸ¨ æ„å»ºè¯¦ç»†è¿æ¥å›¾: {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
            
            return {
                "nodes": nodes,
                "edges": edges,
                "layout": {
                    "algorithm": "detailed_hierarchical",
                    "show_internal_nodes": True,
                    "node_spacing": 120,
                    "workflow_spacing": 300,
                    "level_spacing": 150
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¯¦ç»†è¿æ¥å›¾å¤±è´¥: {e}")
            return {
                "nodes": [],
                "edges": [],
                "layout": {}
            }
    
    def _calculate_workflow_layout_positions(self, detailed_workflows: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å·¥ä½œæµçš„å¸ƒå±€ä½ç½®"""
        positions = {}
        workflow_spacing = 400
        
        workflow_ids = list(detailed_workflows.keys())
        
        for i, workflow_id in enumerate(workflow_ids):
            positions[workflow_id] = {
                "x": i * workflow_spacing,
                "y": 0
            }
        
        return positions
    
    def _calculate_internal_node_positions(self, nodes: List[Dict[str, Any]], base_position: Dict[str, float], node_count: int) -> List[Dict[str, float]]:
        """è®¡ç®—å†…éƒ¨èŠ‚ç‚¹çš„ä½ç½®"""
        positions = []
        node_spacing = 150
        nodes_per_row = 3
        
        base_x = base_position["x"] + 50  # ç›¸å¯¹äºå·¥ä½œæµå®¹å™¨çš„åç§»
        base_y = base_position["y"] + 100
        
        for i, node in enumerate(nodes):
            row = i // nodes_per_row
            col = i % nodes_per_row
            
            # ä½¿ç”¨èŠ‚ç‚¹åŸå§‹ä½ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è®¡ç®—ä½ç½®
            original_pos = node.get("position", {})
            if original_pos.get("x") and original_pos.get("y"):
                # å¦‚æœæœ‰åŸå§‹ä½ç½®ï¼ŒåŸºäºå·¥ä½œæµå®¹å™¨è¿›è¡Œåç§»
                pos = {
                    "x": base_x + float(original_pos["x"]),
                    "y": base_y + float(original_pos["y"])
                }
            else:
                # å¦åˆ™ä½¿ç”¨ç½‘æ ¼å¸ƒå±€
                pos = {
                    "x": base_x + col * node_spacing,
                    "y": base_y + row * node_spacing
                }
            
            positions.append(pos)
        
        return positions
    
    async def _build_merge_candidates(self, all_subdivisions: List[Dict[str, Any]], workflow_stats: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ„å»ºåˆå¹¶å€™é€‰æ•°æ®"""
        try:
            merge_candidates = []
            
            for subdivision in all_subdivisions:
                sub_workflow_base_id = str(subdivision['sub_workflow_base_id'])
                stats = workflow_stats.get(sub_workflow_base_id, {})
                
                # åªæœ‰å·²å®Œæˆçš„subdivisionæ‰èƒ½ä½œä¸ºåˆå¹¶å€™é€‰
                if subdivision['sub_workflow_status'] != 'completed':
                    continue
                
                # è®¡ç®—å…¼å®¹æ€§åˆ†æ•°
                compatibility_score = self._calculate_compatibility_score(subdivision, stats)
                
                merge_candidate = {
                    "subdivision_id": str(subdivision["subdivision_id"]),
                    "subdivision_name": subdivision["subdivision_name"],
                    "parent_subdivision_id": str(subdivision["parent_subdivision_id"]) if subdivision["parent_subdivision_id"] else None,
                    "depth": subdivision.get("depth", 0),
                    "replaceable_node": {
                        "node_base_id": str(subdivision["original_node_base_id"]),
                        "node_name": subdivision["original_node_name"],
                        "node_type": subdivision["original_node_type"]
                    },
                    "sub_workflow": {
                        "workflow_base_id": sub_workflow_base_id,
                        "workflow_name": subdivision["sub_workflow_name"],
                        "total_nodes": stats.get('total_nodes', 0),
                        "completed_nodes": stats.get('completed_by_instance', {}).get(str(subdivision['sub_workflow_instance_id']), 0)
                    },
                    "compatibility": {
                        "score": compatibility_score,
                        "is_recommended": compatibility_score > 0.7,
                        "complexity": "simple" if stats.get('total_nodes', 0) <= 5 else "complex"
                    }
                }
                
                merge_candidates.append(merge_candidate)
            
            logger.info(f"ğŸ”— æ„å»ºåˆå¹¶å€™é€‰æ•°æ®å®Œæˆ: {len(merge_candidates)} ä¸ªå€™é€‰")
            return merge_candidates
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºåˆå¹¶å€™é€‰æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _build_optimized_connection_graph(self, detailed_connections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºè¯¦ç»†çš„è¿æ¥å›¾æ•°æ®ç»“æ„"""
        try:
            nodes = []
            edges = []
            
            # æ„å»ºèŠ‚ç‚¹ä½ç½®æ˜ å°„
            node_positions = self._calculate_hierarchical_positions(detailed_connections)
            
            # åˆ›å»ºå·¥ä½œæµèŠ‚ç‚¹
            workflow_nodes = {}
            for connection in detailed_connections:
                parent_workflow = connection["parent_workflow"]
                sub_workflow = connection["sub_workflow"]
                
                # çˆ¶å·¥ä½œæµèŠ‚ç‚¹
                parent_id = parent_workflow["workflow_base_id"]
                if parent_id not in workflow_nodes:
                    position = node_positions.get(parent_id, {"x": 0, "y": 0})
                    workflow_nodes[parent_id] = {
                        "id": parent_id,
                        "type": "workflowTemplate",
                        "position": position,
                        "data": {
                            "label": parent_workflow["workflow_name"],
                            "description": parent_workflow["workflow_description"],
                            "workflow_base_id": parent_id,
                            "is_parent": True,
                            "depth": connection.get("depth", 0) - 1 if connection.get("depth", 0) > 0 else 0
                        }
                    }
                
                # å­å·¥ä½œæµèŠ‚ç‚¹
                sub_id = sub_workflow["workflow_base_id"]
                if sub_id not in workflow_nodes:
                    position = node_positions.get(sub_id, {"x": 200, "y": 200})
                    workflow_nodes[sub_id] = {
                        "id": sub_id,
                        "type": "workflowTemplate",
                        "position": position,
                        "data": {
                            "label": sub_workflow["workflow_name"],
                            "description": sub_workflow["workflow_description"],
                            "workflow_base_id": sub_id,
                            "status": sub_workflow["status"],
                            "total_nodes": sub_workflow["total_nodes"],
                            "completed_nodes": sub_workflow["completed_nodes"],
                            "is_parent": False,
                            "depth": connection.get("depth", 0),
                            "parent_workflow_id": parent_id
                        }
                    }
            
            # è½¬æ¢ä¸ºèŠ‚ç‚¹åˆ—è¡¨
            nodes = list(workflow_nodes.values())
            
            # åˆ›å»ºè¿æ¥è¾¹
            for connection in detailed_connections:
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                sub_id = connection["sub_workflow"]["workflow_base_id"]
                
                edge = {
                    "id": f"edge_{connection['subdivision_id']}",
                    "source": parent_id,
                    "target": sub_id,
                    "type": "smoothstep",
                    "animated": connection["sub_workflow"]["status"] == "running",
                    "label": connection["subdivision_name"],
                    "data": {
                        "subdivision_id": connection["subdivision_id"],
                        "connected_node_name": connection["parent_workflow"]["connected_node"]["node_name"],
                        "task_title": connection["parent_workflow"]["connected_node"]["task_title"]
                    }
                }
                edges.append(edge)
            
            logger.info(f"ğŸ¨ æ„å»ºè¯¦ç»†è¿æ¥å›¾å®Œæˆ: {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
            
            return {
                "nodes": nodes,
                "edges": edges,
                "layout": {
                    "algorithm": "hierarchical",
                    "direction": "TB",
                    "node_spacing": 200,
                    "level_spacing": 150
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¯¦ç»†è¿æ¥å›¾å¤±è´¥: {e}")
            return {"nodes": [], "edges": [], "layout": {}}
    
    def _extract_detailed_workflows(self, all_subdivisions: List[Dict[str, Any]], workflow_stats: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """æå–è¯¦ç»†çš„å·¥ä½œæµä¿¡æ¯"""
        try:
            detailed_workflows = {}
            
            # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å·¥ä½œæµID
            workflow_ids = set()
            for subdivision in all_subdivisions:
                workflow_ids.add(str(subdivision['parent_workflow_base_id']))
                workflow_ids.add(str(subdivision['sub_workflow_base_id']))
            
            # æ„å»ºè¯¦ç»†å·¥ä½œæµæ•°æ®
            for workflow_id in workflow_ids:
                stats = workflow_stats.get(workflow_id, {})
                
                # ä»subdivisionsä¸­æŸ¥æ‰¾å·¥ä½œæµä¿¡æ¯
                workflow_info = None
                for subdivision in all_subdivisions:
                    if str(subdivision['sub_workflow_base_id']) == workflow_id:
                        workflow_info = {
                            "workflow_base_id": workflow_id,
                            "name": subdivision["sub_workflow_name"],
                            "description": subdivision["sub_workflow_description"],
                            "total_nodes": stats.get('total_nodes', 0)
                        }
                        break
                    elif str(subdivision['parent_workflow_base_id']) == workflow_id:
                        workflow_info = {
                            "workflow_base_id": workflow_id,
                            "name": subdivision["parent_workflow_name"],
                            "description": subdivision["parent_workflow_description"],
                            "total_nodes": stats.get('total_nodes', 0)
                        }
                        break
                
                if workflow_info:
                    detailed_workflows[workflow_id] = workflow_info
            
            logger.info(f"ğŸ“Š æå–è¯¦ç»†å·¥ä½œæµå®Œæˆ: {len(detailed_workflows)} ä¸ªå·¥ä½œæµ")
            return detailed_workflows
            
        except Exception as e:
            logger.error(f"âŒ æå–è¯¦ç»†å·¥ä½œæµå¤±è´¥: {e}")
            return {}
    
    def _calculate_detailed_statistics(self, detailed_connections: List[Dict[str, Any]], all_subdivisions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not detailed_connections:
                return {
                    "total_subdivisions": 0,
                    "completed_sub_workflows": 0,
                    "unique_workflows": 0,
                    "max_depth": 0,
                    "hierarchy_stats": {}
                }
            
            # åŸºæœ¬ç»Ÿè®¡
            total_subdivisions = len(detailed_connections)
            completed_sub_workflows = len([c for c in detailed_connections if c["sub_workflow"]["status"] == "completed"])
            
            # å”¯ä¸€å·¥ä½œæµç»Ÿè®¡
            parent_workflows = set(c["parent_workflow"]["workflow_base_id"] for c in detailed_connections)
            sub_workflows = set(c["sub_workflow"]["workflow_base_id"] for c in detailed_connections)
            unique_workflows = len(parent_workflows | sub_workflows)
            
            # æ·±åº¦ç»Ÿè®¡
            depths = [c.get("depth", 0) for c in detailed_connections]
            max_depth = max(depths) if depths else 0
            
            # å±‚çº§ç»Ÿè®¡
            hierarchy_stats = {}
            for depth in range(max_depth + 1):
                count = len([c for c in detailed_connections if c.get("depth", 0) == depth])
                hierarchy_stats[f"level_{depth}"] = count
            
            statistics = {
                "total_subdivisions": total_subdivisions,
                "completed_sub_workflows": completed_sub_workflows,
                "unique_workflows": unique_workflows,
                "max_depth": max_depth,
                "hierarchy_stats": hierarchy_stats,
                "completion_rate": completed_sub_workflows / total_subdivisions if total_subdivisions > 0 else 0,
                "average_nodes_per_workflow": sum(c["sub_workflow"]["total_nodes"] for c in detailed_connections) / total_subdivisions if total_subdivisions > 0 else 0
            }
            
            logger.info(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡è®¡ç®—å®Œæˆ: {statistics}")
            return statistics
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—è¯¦ç»†ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total_subdivisions": 0, "completed_sub_workflows": 0, "unique_workflows": 0, "max_depth": 0}
    
    def _calculate_compatibility_score(self, subdivision: Dict[str, Any], stats: Dict[str, Any]) -> float:
        """è®¡ç®—subdivisionçš„å…¼å®¹æ€§åˆ†æ•°"""
        try:
            score = 1.0
            
            # åŸºäºèŠ‚ç‚¹ç±»å‹çš„è¯„åˆ†
            node_type = subdivision.get("original_node_type", "")
            if node_type != "processor":
                score *= 0.5
            
            # åŸºäºå¤æ‚åº¦çš„è¯„åˆ†
            total_nodes = stats.get('total_nodes', 0)
            if total_nodes > 10:
                score *= 0.7
            elif total_nodes > 5:
                score *= 0.9
            
            # åŸºäºå®ŒæˆçŠ¶æ€çš„è¯„åˆ†
            if subdivision.get("sub_workflow_status") == "completed":
                score *= 1.0
            else:
                score *= 0.3
            
            return min(score, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—å…¼å®¹æ€§åˆ†æ•°å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_hierarchical_positions(self, detailed_connections: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å±‚çº§å¸ƒå±€ä½ç½®"""
        try:
            positions = {}
            
            # æŒ‰æ·±åº¦åˆ†ç»„
            depth_groups = {}
            for connection in detailed_connections:
                depth = connection.get("depth", 0)
                if depth not in depth_groups:
                    depth_groups[depth] = []
                
                parent_id = connection["parent_workflow"]["workflow_base_id"]
                sub_id = connection["sub_workflow"]["workflow_base_id"]
                
                depth_groups[depth].extend([parent_id, sub_id])
            
            # ä¸ºæ¯ä¸ªæ·±åº¦åˆ†é…ä½ç½®
            y_offset = 0
            for depth in sorted(depth_groups.keys()):
                nodes_at_depth = list(set(depth_groups[depth]))  # å»é‡
                
                for i, node_id in enumerate(nodes_at_depth):
                    if node_id not in positions:
                        positions[node_id] = {
                            "x": i * 300,
                            "y": y_offset
                        }
                
                y_offset += 200
            
            return positions
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—å±‚çº§ä½ç½®å¤±è´¥: {e}")
            return {}