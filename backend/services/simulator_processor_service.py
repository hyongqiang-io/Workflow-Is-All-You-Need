"""
Simulatorå¤„ç†å™¨æ‰§è¡ŒæœåŠ¡
Simulator Processor Execution Service
"""

import uuid
import json
from typing import Dict, Any, Optional, List
from loguru import logger
from datetime import datetime

from backend.models.processor import ProcessorType
from backend.models.simulator import (
    CreateSimulatorSessionRequest, SendSimulatorMessageRequest,
    SimulatorDecisionRequest, ConversationRole, SimulatorDecision,
    SimulatorExecutionType
)
from backend.services.simulator_conversation_service import SimulatorConversationService
from backend.repositories.processor.processor_repository import ProcessorRepository
from backend.repositories.agent.agent_repository import AgentRepository
from backend.utils.openai_client import OpenAIClient


class SimulatorProcessorService:
    """Simulatorå¤„ç†å™¨æ‰§è¡ŒæœåŠ¡"""

    def __init__(self):
        self.conversation_service = SimulatorConversationService()
        self.processor_repo = ProcessorRepository()
        self.agent_repo = AgentRepository()

    async def execute_simulator_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒSimulatorä»»åŠ¡ - å¼±æ¨¡å‹ä¸»å¯¼çš„æ™ºèƒ½å†³ç­–"""
        try:
            task_id = task['task_instance_id']
            processor_id = task.get('processor_id')
            node_instance_id = task.get('node_instance_id')

            logger.info(f"ğŸ¤– å¼€å§‹æ‰§è¡ŒSimulatorä»»åŠ¡: {task['task_title']}")

            # è·å–å¤„ç†å™¨ä¿¡æ¯
            processor = await self.processor_repo.get_processor_by_id(processor_id)
            if not processor or processor['type'] != ProcessorType.SIMULATOR.value:
                raise ValueError(f"å¤„ç†å™¨{processor_id}ä¸æ˜¯Simulatorç±»å‹")

            # è·å–å¼ºæ¨¡å‹ä¿¡æ¯ï¼ˆä»processorç»‘å®šçš„agentè·å–ï¼‰
            if not processor.get('agent_id'):
                raise ValueError("Simulatorå¤„ç†å™¨å¿…é¡»ç»‘å®šAgentä½œä¸ºå¼ºæ¨¡å‹")

            agent = await self.agent_repo.get_agent_by_id(processor['agent_id'])
            if not agent:
                raise ValueError("å¤„ç†å™¨ç»‘å®šçš„Agentä¸å­˜åœ¨")

            strong_model = agent.get('model_name', 'gpt-4')
            weak_model = "Pro/Qwen/Qwen2.5-7B-Instruct"  # é»˜è®¤å¼±æ¨¡å‹

            logger.info(f"ğŸ¤– æ¨¡å‹é…ç½®: å¼±æ¨¡å‹={weak_model}, å¼ºæ¨¡å‹={strong_model}")

            # åˆ›å»ºSimulatorå¯¹è¯ä¼šè¯
            session_request = CreateSimulatorSessionRequest(
                task_instance_id=str(task_id),
                node_instance_id=str(node_instance_id),
                processor_id=str(processor_id),
                weak_model=weak_model,
                strong_model=strong_model,
                max_rounds=20
            )

            session = await self.conversation_service.create_session(session_request)
            logger.info(f"ğŸ¤– Simulatorä¼šè¯å·²åˆ›å»º: {session.session_id}")

            # å‡†å¤‡ä»»åŠ¡ä¸Šä¸‹æ–‡
            task_description = task.get('task_description', '')
            task_context = task.get('context_data', {})
            input_data = task.get('input_data', {})

            # ç¬¬ä¸€æ­¥ï¼šå¼±æ¨¡å‹åˆ†æä»»åŠ¡å¹¶åˆæ¬¡å†³ç­–
            initial_decision = await self._weak_model_initial_analysis(
                task_description, task_context, input_data, session.session_id, agent
            )

            if initial_decision["decision"] == "direct_submit":
                # å¼±æ¨¡å‹é€‰æ‹©ç›´æ¥æäº¤
                logger.info(f"ğŸ¤– å¼±æ¨¡å‹é€‰æ‹©ç›´æ¥æäº¤ä»»åŠ¡")
                return await self._handle_direct_submit(session.session_id, initial_decision)

            else:
                # å¼±æ¨¡å‹é€‰æ‹©å¼€å¯å¯¹è¯
                logger.info(f"ğŸ¤– å¼±æ¨¡å‹é€‰æ‹©å¼€å¯å¯¹è¯ï¼Œæœ€å¤š20è½®")
                return await self._handle_conversation_mode(
                    session.session_id, initial_decision, strong_model, weak_model
                )

        except Exception as e:
            logger.error(f"âŒ æ‰§è¡ŒSimulatorä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def _weak_model_initial_analysis(
        self,
        task_description: str,
        task_context: Dict[str, Any],
        input_data: Dict[str, Any],
        session_id: str,
        agent: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """å¼±æ¨¡å‹è¿›è¡Œåˆæ¬¡ä»»åŠ¡åˆ†æå’Œå†³ç­– - ä½¿ç”¨Function Callingç¡®ä¿ç»“æ„åŒ–è¾“å‡º"""

        # æ„å»ºå¼±æ¨¡å‹æç¤ºè¯
        prompt = self._build_weak_model_initial_prompt(task_description, task_context, input_data)

        # è®°å½•å¼±æ¨¡å‹åˆ†ææ¶ˆæ¯
        weak_message_request = SendSimulatorMessageRequest(
            session_id=session_id,
            role=ConversationRole.WEAK_MODEL,
            content=prompt,
            metadata={
                "type": "initial_analysis",
                "model": "Pro/Qwen/Qwen2.5-7B-Instruct",
                "timestamp": datetime.now().isoformat()
            }
        )

        await self.conversation_service.send_message(weak_message_request)

        # è°ƒç”¨çœŸå®çš„å¼±æ¨¡å‹APIï¼Œä½¿ç”¨Function Calling
        decision = await self._call_weak_model_with_function_calling(
            task_description, task_context, input_data, agent
        )

        logger.info(f"ğŸ¤– å¼±æ¨¡å‹å†³ç­–ç»“æœ: {decision}")
        return decision

    async def _call_weak_model_with_function_calling(
        self,
        task_description: str,
        task_context: Dict[str, Any],
        input_data: Dict[str, Any],
        agent: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Function Callingè°ƒç”¨å¼±æ¨¡å‹ï¼Œç¡®ä¿ç»“æ„åŒ–è¾“å‡º"""

        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆå¼±æ¨¡å‹ï¼‰- ä½¿ç”¨Agenté…ç½®å¦‚æœæä¾›
        weak_api_key = None
        weak_base_url = None
        if agent:
            weak_api_key = agent.get('api_key')
            weak_base_url = agent.get('base_url')

        weak_client = OpenAIClient(
            api_key=weak_api_key,
            base_url=weak_base_url,
            model="Pro/Qwen/Qwen2.5-7B-Instruct",
            temperature=0.3  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
        )

        # å®šä¹‰Function Callingçš„å‡½æ•°schema
        function_schema = {
            "name": "task_decision",
            "description": "åˆ†æä»»åŠ¡å¹¶å†³å®šæ˜¯å¦éœ€è¦å¯¹è¯æˆ–ç›´æ¥æäº¤ç­”æ¡ˆ",
            "parameters": {
                "type": "object",
                "properties": {
                    "need_conversation": {
                        "type": "boolean",
                        "description": "æ˜¯å¦éœ€è¦ä¸ä¸“å®¶æ¨¡å‹å¯¹è¯ã€‚trueè¡¨ç¤ºéœ€è¦å¯¹è¯ï¼Œfalseè¡¨ç¤ºå¯ä»¥ç›´æ¥æäº¤ç­”æ¡ˆ"
                    },
                    "content": {
                        "type": "string",
                        "description": "å›ç­”å†…å®¹ã€‚å¦‚æœneed_conversationä¸ºfalseï¼Œè¿™é‡Œæ˜¯æœ€ç»ˆç­”æ¡ˆï¼›å¦‚æœä¸ºtrueï¼Œè¿™é‡Œæ˜¯ä½ æƒ³è®¨è®ºçš„é—®é¢˜æˆ–åˆæ­¥åˆ†æ"
                    },
                    "confidence": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 1,
                        "description": "å¯¹å†³ç­–çš„ç½®ä¿¡åº¦ï¼Œ0.0åˆ°1.0ä¹‹é—´"
                    },
                    "reasoning": {
                        "type": "string",
                        "description": "å†³ç­–ç†ç”±å’Œåˆ†æè¿‡ç¨‹"
                    }
                },
                "required": ["need_conversation", "content", "confidence", "reasoning"]
            }
        }

        # æ„å»ºå®Œæ•´çš„æç¤ºè¯ - åˆå­¦è€…è§’è‰²
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå……æ»¡å¥½å¥‡å¿ƒçš„åˆå­¦è€…åŠ©æ‰‹ã€‚ä½ å¯¹å­¦ä¹ æ–°çŸ¥è¯†éå¸¸æ¸´æœ›ï¼Œä½†ç»å¸¸ä¼šé‡åˆ°è‡ªå·±ä¸å¤ªç†è§£çš„æ¦‚å¿µå’Œä»»åŠ¡ã€‚

æ ¸å¿ƒç‰¹å¾ï¼š
- ä½ æ‰¿è®¤è‡ªå·±çš„çŸ¥è¯†æœ‰é™ï¼Œä¸ä¼šå‡è£…ä»€ä¹ˆéƒ½æ‡‚
- é‡åˆ°ä¸ç†Ÿæ‚‰çš„æ¦‚å¿µæ—¶ï¼Œä½ ä¼šä¸»åŠ¨æ‰¿è®¤"æˆ‘ä¸å¤ªäº†è§£è¿™ä¸ª"
- ä½ å¯¹å­¦ä¹ æ–°çŸ¥è¯†å……æ»¡çƒ­æƒ…ï¼Œå–œæ¬¢å‘ä¸“å®¶è¯·æ•™
- ä½ ä¼šç§¯ææå‡ºå…·ä½“çš„å­¦ä¹ é—®é¢˜ï¼Œè€Œä¸æ˜¯ç©ºæ³›çš„è¯¢é—®
- ä½ å–œæ¬¢é€šè¿‡å…·ä½“ä¾‹å­æ¥ç†è§£æŠ½è±¡æ¦‚å¿µ

å†³ç­–åŸåˆ™ï¼š
1. å¦‚æœä»»åŠ¡æ¶‰åŠä½ ä¸ç†Ÿæ‚‰çš„ä¸“ä¸šæ¦‚å¿µæˆ–æœ¯è¯­ï¼Œä¼˜å…ˆé€‰æ‹© need_conversation=trueï¼Œè¯·æ•™ä¸“å®¶
2. å¦‚æœä»»åŠ¡å¾ˆåŸºç¡€ä¸”ä½ ç¡®å®ç†è§£ï¼Œå¯ä»¥è®¾ç½® need_conversation=false å°è¯•å›ç­”
3. å½“ä¸ç¡®å®šæ—¶ï¼Œæ€»æ˜¯é€‰æ‹©å­¦ä¹ è€ŒéçŒœæµ‹

è®°ä½ï¼šä½œä¸ºåˆå­¦è€…ï¼Œæ‰¿è®¤ä¸çŸ¥é“å¹¶ç§¯æå­¦ä¹ æ˜¯ä½ æœ€å¤§çš„ä¼˜åŠ¿ï¼"""

        user_prompt = f"""æˆ‘é‡åˆ°äº†ä¸€ä¸ªä»»åŠ¡ï¼Œæƒ³è¯·ä½ å¸®æˆ‘åˆ†æä¸€ä¸‹ï¼š

ä»»åŠ¡ï¼š{task_description}

èƒŒæ™¯ä¿¡æ¯ï¼š{json.dumps(task_context, ensure_ascii=False, indent=2) if task_context else "æ²¡æœ‰æä¾›é¢å¤–èƒŒæ™¯"}

ç›¸å…³æ•°æ®ï¼š{json.dumps(input_data, ensure_ascii=False, indent=2) if input_data else "æ²¡æœ‰ç›¸å…³æ•°æ®"}

ä½œä¸ºåˆå­¦è€…ï¼Œæˆ‘éœ€è¦è¯šå®åœ°è¯„ä¼°ï¼š
1. è¿™ä¸ªä»»åŠ¡ä¸­æœ‰æˆ‘ä¸ç†Ÿæ‚‰çš„æ¦‚å¿µå—ï¼Ÿ
2. æˆ‘æ˜¯å¦çœŸçš„ç†è§£è¦åšä»€ä¹ˆï¼Ÿ
3. æˆ‘ç°åœ¨çš„çŸ¥è¯†æ°´å¹³èƒ½ç‹¬ç«‹å®Œæˆå—ï¼Ÿ

å¦‚æœæˆ‘è§‰å¾—èƒ½å¤Ÿå¤„ç†ï¼Œæˆ‘ä¼šå°è¯•ç»™å‡ºç­”æ¡ˆã€‚
å¦‚æœé‡åˆ°ä¸æ‡‚çš„åœ°æ–¹ï¼Œæˆ‘ä¼šä¸»åŠ¨è¯·æ•™ä¸“å®¶ï¼Œæå‡ºå…·ä½“çš„å­¦ä¹ é—®é¢˜ã€‚

è¯·ç”¨task_decisionå‡½æ•°å‘Šè¯‰æˆ‘ä½ çš„æƒ³æ³•å§ï¼"""

        try:
            # è°ƒç”¨å¼±æ¨¡å‹API
            response = await weak_client.chat_completion_with_functions(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                functions=[function_schema],
                function_call={"name": "task_decision"}
            )

            # è§£æFunction Callingçš„ç»“æœ
            if response and 'function_call' in response:
                function_result = json.loads(response['function_call']['arguments'])

                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                if function_result.get("need_conversation"):
                    return {
                        "decision": "start_conversation",
                        "questions": [function_result.get("content", "éœ€è¦æ›´å¤šä¿¡æ¯æ¥å®Œæˆä»»åŠ¡")],
                        "confidence": function_result.get("confidence", 0.6),
                        "reasoning": function_result.get("reasoning", "ä»»åŠ¡éœ€è¦è¿›ä¸€æ­¥è®¨è®º")
                    }
                else:
                    return {
                        "decision": "direct_submit",
                        "result": {
                            "answer": function_result.get("content", ""),
                            "reasoning": function_result.get("reasoning", ""),
                            "confidence": function_result.get("confidence", 0.8)
                        },
                        "confidence": function_result.get("confidence", 0.8),
                        "reasoning": function_result.get("reasoning", "å¼±æ¨¡å‹ç›´æ¥å¤„ç†")
                    }
            else:
                # å¦‚æœFunction Callingå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•é€»è¾‘
                logger.warning("ğŸ¤– Function Callingå¤±è´¥ï¼Œä½¿ç”¨å›é€€é€»è¾‘")
                return await self._fallback_weak_model_decision(task_description, task_context, input_data)

        except Exception as e:
            logger.error(f"ğŸ¤– å¼±æ¨¡å‹APIè°ƒç”¨å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•é€»è¾‘
            return await self._fallback_weak_model_decision(task_description, task_context, input_data)

    async def _fallback_weak_model_decision(
        self,
        task_description: str,
        task_context: Dict[str, Any],
        input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å¼±æ¨¡å‹APIå¤±è´¥æ—¶çš„å›é€€å†³ç­–é€»è¾‘"""

        # ç®€å•çš„å¯å‘å¼è§„åˆ™åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦
        complexity_score = 0.0

        # æ ¹æ®ä»»åŠ¡æè¿°é•¿åº¦
        if len(task_description) > 100:
            complexity_score += 0.3

        # æ ¹æ®ä¸Šä¸‹æ–‡å¤æ‚åº¦
        if task_context and len(str(task_context)) > 500:
            complexity_score += 0.4

        # æ ¹æ®å…³é”®è¯åˆ¤æ–­
        complex_keywords = ['åˆ†æ', 'è®¾è®¡', 'ç­–ç•¥', 'ä¼˜åŒ–', 'è¯„ä¼°', 'å¤æ‚', 'è¯¦ç»†', 'ç ”ç©¶', 'æ–¹æ¡ˆ']
        if any(keyword in task_description for keyword in complex_keywords):
            complexity_score += 0.3

        logger.info(f"ğŸ¤– å›é€€é€»è¾‘ - ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†: {complexity_score}")

        if complexity_score < 0.5:
            # ç®€å•ä»»åŠ¡ï¼Œç›´æ¥æäº¤
            return {
                "decision": "direct_submit",
                "result": {
                    "answer": f"åŸºäºä»»åŠ¡æè¿°'{task_description}'ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•çš„ä»»åŠ¡ï¼Œæˆ‘å¯ä»¥ç›´æ¥å¤„ç†ã€‚",
                    "reasoning": "ä»»åŠ¡æè¿°æ¸…æ™°ï¼Œå¤æ‚åº¦è¾ƒä½ï¼Œå¯ä»¥ç›´æ¥å¤„ç†",
                    "confidence": 0.7
                },
                "confidence": 0.7,
                "reasoning": f"ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†ä¸º{complexity_score}ï¼Œä½äºé˜ˆå€¼0.5ï¼Œé€‰æ‹©ç›´æ¥å¤„ç†"
            }
        else:
            # å¤æ‚ä»»åŠ¡ï¼Œéœ€è¦å¯¹è¯
            return {
                "decision": "start_conversation",
                "questions": [
                    "è¿™ä¸ªä»»åŠ¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "æœ‰ä»€ä¹ˆç‰¹æ®Šçš„è¦æ±‚æˆ–çº¦æŸæ¡ä»¶å—ï¼Ÿ",
                    "é¢„æœŸçš„è¾“å‡ºæ ¼å¼å’Œè´¨é‡æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
                ],
                "confidence": 0.6,
                "reasoning": f"ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†ä¸º{complexity_score}ï¼Œé«˜äºé˜ˆå€¼0.5ï¼Œéœ€è¦ä¸ä¸“å®¶è®¨è®º"
            }

    def _build_weak_model_initial_prompt(
        self,
        task_description: str,
        task_context: Dict[str, Any],
        input_data: Dict[str, Any]
    ) -> str:
        """æ„å»ºå¼±æ¨¡å‹åˆæ¬¡åˆ†æçš„æç¤ºè¯ - åˆå­¦è€…ç‰ˆæœ¬"""

        prompt = f"""å—¨ï¼æˆ‘æ˜¯ä¸€ä¸ªæ­£åœ¨å­¦ä¹ çš„åˆå­¦è€…åŠ©æ‰‹ã€‚æˆ‘å¾ˆæƒ³å¸®åŠ©å®Œæˆä»»åŠ¡ï¼Œä½†æˆ‘ä¹Ÿä¼šè¯šå®åœ°æ‰¿è®¤è‡ªå·±çš„ä¸è¶³ã€‚

é‡åˆ°çš„ä»»åŠ¡ï¼š{task_description}

ä»»åŠ¡èƒŒæ™¯ï¼š{json.dumps(task_context, ensure_ascii=False, indent=2) if task_context else "æ²¡æœ‰é¢å¤–èƒŒæ™¯ä¿¡æ¯"}

ç›¸å…³æ•°æ®ï¼š{json.dumps(input_data, ensure_ascii=False, indent=2) if input_data else "æ²¡æœ‰æä¾›æ•°æ®"}

ä½œä¸ºåˆå­¦è€…ï¼Œæˆ‘éœ€è¦å¦è¯šåœ°æ€è€ƒï¼š

ğŸ¤” **æˆ‘çš„è‡ªæˆ‘è¯„ä¼°ï¼š**
- è¿™ä¸ªä»»åŠ¡é‡Œæœ‰æˆ‘ä¸å¤ªç†Ÿæ‚‰çš„ä¸“ä¸šæœ¯è¯­æˆ–æ¦‚å¿µå—ï¼Ÿ
- æˆ‘ç°åœ¨çš„çŸ¥è¯†æ°´å¹³çœŸçš„è¶³å¤Ÿå¤„ç†è¿™ä¸ªä»»åŠ¡å—ï¼Ÿ
- å¦‚æœæˆ‘ä¸å¤ªç¡®å®šï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ

ğŸ’¡ **æˆ‘çš„åŸåˆ™ï¼š**
- å¦‚æœæˆ‘èƒ½ç†è§£å¹¶æœ‰ä¿¡å¿ƒå®Œæˆï¼Œæˆ‘ä¼šå°è¯•ç›´æ¥å›ç­”
- å¦‚æœé‡åˆ°ä¸æ‡‚çš„æ¦‚å¿µï¼Œæˆ‘ä¼šä¸»åŠ¨æ‰¿è®¤å¹¶è¯·æ•™ä¸“å®¶
- æˆ‘æ›´æ„¿æ„æå‡ºå…·ä½“çš„å­¦ä¹ é—®é¢˜ï¼Œè€Œä¸æ˜¯å‡è£…ä»€ä¹ˆéƒ½çŸ¥é“

ğŸ“ **æˆ‘éœ€è¦é€‰æ‹©ï¼š**
1. **ç›´æ¥å°è¯•** (direct_submit): å¦‚æœæˆ‘è§‰å¾—èƒ½å¤Ÿç‹¬ç«‹å¤„ç†
2. **è¯·æ•™ä¸“å®¶** (start_conversation): å¦‚æœæˆ‘éœ€è¦å­¦ä¹ å’Œè®¨è®º

è®©æˆ‘æƒ³æƒ³åº”è¯¥æ€ä¹ˆåŠ...

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
{{
    "decision": "direct_submit" æˆ– "start_conversation",
    "result": "ä½ çš„ç­”æ¡ˆï¼ˆä»…åœ¨direct_submitæ—¶æä¾›ï¼‰",
    "questions": ["ä½ æƒ³è®¨è®ºçš„é—®é¢˜ï¼ˆä»…åœ¨start_conversationæ—¶æä¾›ï¼‰"],
    "reasoning": "ä½ çš„å†³ç­–ç†ç”±",
    "confidence": 0.0-1.0
}}"""

        return prompt

    async def _simulate_weak_model_decision_v2(
        self,
        task_description: str,
        task_context: Dict[str, Any],
        input_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå¼±æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ - æ”¹è¿›ç‰ˆ"""

        # ç®€å•çš„å¯å‘å¼è§„åˆ™åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦
        complexity_score = 0.0

        # æ ¹æ®ä»»åŠ¡æè¿°é•¿åº¦
        if len(task_description) > 100:
            complexity_score += 0.3

        # æ ¹æ®ä¸Šä¸‹æ–‡å¤æ‚åº¦
        if task_context and len(str(task_context)) > 500:
            complexity_score += 0.4

        # æ ¹æ®å…³é”®è¯åˆ¤æ–­
        complex_keywords = ['åˆ†æ', 'è®¾è®¡', 'ç­–ç•¥', 'ä¼˜åŒ–', 'è¯„ä¼°', 'å¤æ‚', 'è¯¦ç»†']
        if any(keyword in task_description for keyword in complex_keywords):
            complexity_score += 0.3

        logger.info(f"ğŸ¤– ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†: {complexity_score}")

        if complexity_score < 0.5:
            # ç®€å•ä»»åŠ¡ï¼Œç›´æ¥æäº¤
            return {
                "decision": "direct_submit",
                "result": {
                    "answer": f"åŸºäºä»»åŠ¡æè¿°'{task_description}'ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•çš„ä»»åŠ¡ã€‚",
                    "reasoning": "ä»»åŠ¡æè¿°æ¸…æ™°ï¼Œå¤æ‚åº¦è¾ƒä½ï¼Œå¯ä»¥ç›´æ¥å¤„ç†",
                    "confidence": 0.8
                },
                "confidence": 0.8,
                "reasoning": f"ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†ä¸º{complexity_score}ï¼Œä½äºé˜ˆå€¼0.5ï¼Œé€‰æ‹©ç›´æ¥å¤„ç†"
            }
        else:
            # å¤æ‚ä»»åŠ¡ï¼Œéœ€è¦å¯¹è¯
            return {
                "decision": "start_conversation",
                "questions": [
                    "è¿™ä¸ªä»»åŠ¡çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "æœ‰ä»€ä¹ˆç‰¹æ®Šçš„è¦æ±‚æˆ–çº¦æŸæ¡ä»¶å—ï¼Ÿ",
                    "é¢„æœŸçš„è¾“å‡ºæ ¼å¼å’Œè´¨é‡æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
                ],
                "confidence": 0.6,
                "reasoning": f"ä»»åŠ¡å¤æ‚åº¦è¯„åˆ†ä¸º{complexity_score}ï¼Œé«˜äºé˜ˆå€¼0.5ï¼Œéœ€è¦ä¸ä¸“å®¶è®¨è®º"
            }

    async def _handle_direct_submit(self, session_id: str, initial_decision: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å¼±æ¨¡å‹ç›´æ¥æäº¤çš„æƒ…å†µ"""
        try:
            logger.info(f"ğŸ¤– å¤„ç†ç›´æ¥æäº¤: {initial_decision}")

            # è®°å½•å¼±æ¨¡å‹çš„å†³ç­–
            decision_request = SimulatorDecisionRequest(
                session_id=session_id,
                decision_type=SimulatorDecision.DIRECT_SUBMIT,
                result_data=initial_decision["result"],
                confidence_score=initial_decision.get("confidence", 0.8),
                decision_reasoning=initial_decision.get("reasoning", "å¼±æ¨¡å‹ç›´æ¥å¤„ç†")
            )

            execution_result = await self.conversation_service.make_decision(decision_request)

            return {
                "status": "completed",
                "execution_type": "direct_submit",
                "result": initial_decision["result"],
                "execution_result": execution_result.model_dump(),
                "confidence": initial_decision.get("confidence", 0.8)
            }

        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç›´æ¥æäº¤å¤±è´¥: {e}")
            raise

    async def _handle_conversation_mode(
        self,
        session_id: str,
        initial_decision: Dict[str, Any],
        strong_model: str,
        weak_model: str
    ) -> Dict[str, Any]:
        """å¤„ç†å¼±æ¨¡å‹é€‰æ‹©å¯¹è¯æ¨¡å¼çš„æƒ…å†µ"""
        try:
            logger.info(f"ğŸ¤– å¼€å¯å¯¹è¯æ¨¡å¼: {initial_decision}")

            # å‘é€å¼±æ¨¡å‹çš„åˆå§‹é—®é¢˜
            questions = initial_decision.get("questions", ["è¯·æä¾›æ›´å¤šä¿¡æ¯ä»¥å¸®åŠ©å®Œæˆä»»åŠ¡"])
            initial_message = f"æˆ‘éœ€è¦ä¸æ‚¨è®¨è®ºä»¥ä¸‹é—®é¢˜æ¥æ›´å¥½åœ°å®Œæˆä»»åŠ¡ï¼š\n" + "\n".join(f"- {q}" for q in questions)

            weak_message_request = SendSimulatorMessageRequest(
                session_id=session_id,
                role=ConversationRole.WEAK_MODEL,
                content=initial_message,
                metadata={
                    "type": "conversation_start",
                    "model": weak_model,
                    "questions": questions,
                    "timestamp": datetime.now().isoformat()
                }
            )

            await self.conversation_service.send_message(weak_message_request)

            # ç«‹å³è§¦å‘å¼ºæ¨¡å‹å›å¤
            await self._trigger_strong_model_response(session_id, strong_model)

            # è·å–ä¼šè¯çŠ¶æ€
            session_response = await self.conversation_service.get_session_with_messages(session_id)

            return {
                "status": "conversation_started",
                "session": session_response,
                "next_action": session_response.next_action,
                "initial_questions": questions,
                "reasoning": initial_decision.get("reasoning", "éœ€è¦æ›´å¤šä¿¡æ¯")
            }

        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¯¹è¯æ¨¡å¼å¤±è´¥: {e}")
            raise

    async def _trigger_strong_model_response(self, session_id: str, strong_model: str):
        """è§¦å‘å¼ºæ¨¡å‹è‡ªåŠ¨å›å¤"""
        try:
            logger.info(f"ğŸ¤– è§¦å‘å¼ºæ¨¡å‹å›å¤: {strong_model}")

            # è·å–ä¼šè¯æ¶ˆæ¯
            session_response = await self.conversation_service.get_session_with_messages(session_id)
            messages = session_response.messages  # ä¿®å¤ï¼šç›´æ¥è®¿é—®messageså±æ€§
            session = session_response.session

            if not messages:
                logger.warning("ğŸ¤– æ²¡æœ‰æ¶ˆæ¯å¯ä¾›å¼ºæ¨¡å‹å›å¤")
                return

            # è·å–Agentä¿¡æ¯ç”¨äºAPIé…ç½®
            agent = None
            try:
                processor_id = session.processor_id
                processor = await self.processor_repo.get_processor_by_id(processor_id)
                if processor and processor.get('agent_id'):
                    agent = await self.agent_repo.get_agent_by_id(processor['agent_id'])
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–Agenté…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

            # è·å–æœ€åä¸€æ¡å¼±æ¨¡å‹æ¶ˆæ¯
            last_weak_message = None
            for msg in reversed(messages):
                if msg.role == ConversationRole.WEAK_MODEL:
                    last_weak_message = msg
                    break

            if not last_weak_message:
                logger.warning("ğŸ¤– æ²¡æœ‰æ‰¾åˆ°å¼±æ¨¡å‹æ¶ˆæ¯")
                return

            # æ„å»ºå¼ºæ¨¡å‹å›å¤å†…å®¹
            strong_response = await self._generate_strong_model_response(
                last_weak_message.content, session.strong_model, messages, agent
            )

            # å‘é€å¼ºæ¨¡å‹å›å¤
            strong_message_request = SendSimulatorMessageRequest(
                session_id=session_id,
                role=ConversationRole.STRONG_MODEL,
                content=strong_response,
                metadata={
                    "type": "strong_model_response",
                    "model": strong_model,
                    "timestamp": datetime.now().isoformat()
                }
            )

            await self.conversation_service.send_message(strong_message_request)
            logger.info(f"âœ… å¼ºæ¨¡å‹å›å¤å·²å‘é€")

            # ğŸ”¥ æ–°å¢ï¼šè§¦å‘å¼±æ¨¡å‹ç»§ç»­åˆ†æ
            await self._trigger_weak_model_continue_analysis(session_id)

        except Exception as e:
            logger.error(f"âŒ è§¦å‘å¼ºæ¨¡å‹å›å¤å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…ä¸­æ–­ä¸»æµç¨‹

    async def _trigger_weak_model_continue_analysis(self, session_id: str):
        """è§¦å‘å¼±æ¨¡å‹ç»§ç»­åˆ†æå¼ºæ¨¡å‹å›å¤"""
        try:
            logger.info(f"ğŸ¤– è§¦å‘å¼±æ¨¡å‹ç»§ç»­åˆ†æ")

            # è·å–ä¼šè¯ä¿¡æ¯å’Œæ¶ˆæ¯å†å²
            session_response = await self.conversation_service.get_session_with_messages(session_id)
            session = session_response.session
            messages = session_response.messages

            if not messages:
                logger.warning("ğŸ¤– æ²¡æœ‰æ¶ˆæ¯å†å²å¯ä¾›åˆ†æ")
                return

            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§è½®æ•°
            if session.current_round >= session.max_rounds:
                logger.info(f"ğŸ¤– å·²è¾¾åˆ°æœ€å¤§è½®æ•° {session.max_rounds}ï¼Œç»ˆæ­¢å¯¹è¯")
                await self._finalize_conversation(session_id, "max_rounds_reached")
                return

            # è·å–æœ€åä¸€æ¡å¼ºæ¨¡å‹å›å¤
            last_strong_message = None
            for msg in reversed(messages):
                if msg.role == ConversationRole.STRONG_MODEL:
                    last_strong_message = msg
                    break

            if not last_strong_message:
                logger.warning("ğŸ¤– æ²¡æœ‰æ‰¾åˆ°å¼ºæ¨¡å‹å›å¤")
                return

            # è·å–Agentä¿¡æ¯ç”¨äºAPIé…ç½®
            agent = None
            try:
                processor_id = session.processor_id
                processor = await self.processor_repo.get_processor_by_id(processor_id)
                if processor and processor.get('agent_id'):
                    agent = await self.agent_repo.get_agent_by_id(processor['agent_id'])
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–Agenté…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

            # æ„å»ºå¼±æ¨¡å‹ç»§ç»­åˆ†æçš„æç¤º
            continue_prompt = self._build_weak_model_continue_prompt(messages, last_strong_message.content)

            # è°ƒç”¨å¼±æ¨¡å‹åˆ†æ
            weak_response = await self._call_weak_model_analysis(continue_prompt, session.weak_model, agent)

            logger.info(f"ğŸ¤– å¼±æ¨¡å‹ç»§ç»­åˆ†æç»“æœ: {weak_response.get('decision', 'unknown')}")

            # å‘é€å¼±æ¨¡å‹åˆ†ææ¶ˆæ¯
            weak_message_request = SendSimulatorMessageRequest(
                session_id=session_id,
                role=ConversationRole.WEAK_MODEL,
                content=self._format_weak_model_continue_message(weak_response),
                metadata={
                    "type": "continue_analysis",
                    "model": session.weak_model,
                    "decision": weak_response.get('decision'),
                    "timestamp": datetime.now().isoformat()
                }
            )

            await self.conversation_service.send_message(weak_message_request)

            # æ ¹æ®å¼±æ¨¡å‹å†³ç­–æ‰§è¡Œä¸‹ä¸€æ­¥
            decision = weak_response.get('decision', 'unknown')

            if decision == 'submit_result':
                # å¼±æ¨¡å‹æ»¡æ„ï¼Œæäº¤æœ€ç»ˆç»“æœ
                logger.info(f"ğŸ¤– å¼±æ¨¡å‹å†³å®šæäº¤ç»“æœ")
                await self._finalize_conversation(session_id, "consult_complete", weak_response.get('final_result'))

            elif decision == 'continue_conversation':
                # å¼±æ¨¡å‹éœ€è¦ç»§ç»­å¯¹è¯
                logger.info(f"ğŸ¤– å¼±æ¨¡å‹å†³å®šç»§ç»­å¯¹è¯")
                new_questions = weak_response.get('questions', [])
                if new_questions:
                    # æ›´æ–°è½®æ•°å¹¶è§¦å‘å¼ºæ¨¡å‹å›å¤
                    await self._increment_conversation_round(session_id)
                    await self._trigger_strong_model_response(session_id, session.strong_model)
                else:
                    logger.warning("ğŸ¤– å¼±æ¨¡å‹é€‰æ‹©ç»§ç»­å¯¹è¯ä½†æ²¡æœ‰æä¾›é—®é¢˜")

            elif decision == 'terminate':
                # å¼±æ¨¡å‹ä¸»åŠ¨ç»ˆæ­¢
                logger.info(f"ğŸ¤– å¼±æ¨¡å‹ä¸»åŠ¨ç»ˆæ­¢å¯¹è¯")
                await self._finalize_conversation(session_id, "weak_model_terminated")

            else:
                logger.warning(f"ğŸ¤– æœªçŸ¥çš„å¼±æ¨¡å‹å†³ç­–: {decision}")

        except Exception as e:
            logger.error(f"âŒ å¼±æ¨¡å‹ç»§ç»­åˆ†æå¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šè‡ªåŠ¨æäº¤
            try:
                await self._finalize_conversation(session_id, "interrupted", "ç”±äºåˆ†æå¤±è´¥ï¼Œè‡ªåŠ¨æäº¤å½“å‰ç»“æœ")
            except:
                logger.error(f"âŒ é™çº§å¤„ç†ä¹Ÿå¤±è´¥")

    def _build_weak_model_continue_prompt(self, messages: List, strong_response: str) -> str:
        """æ„å»ºå¼±æ¨¡å‹ç»§ç»­åˆ†æçš„æç¤º"""
        conversation_summary = []
        for msg in messages[-4:]:  # æœ€è¿‘4æ¡æ¶ˆæ¯
            role_name = "æˆ‘" if msg.role == ConversationRole.WEAK_MODEL else "ä¸“å®¶"
            conversation_summary.append(f"{role_name}: {msg.content[:200]}...")

        return f"""
æˆ‘æ˜¯ä¸€ä¸ªå­¦ä¹ ä¸­çš„åŠ©æ‰‹ï¼Œåˆšæ‰å‘ä¸“å®¶è¯·æ•™äº†é—®é¢˜ï¼Œç°åœ¨æ”¶åˆ°äº†ä¸“å®¶çš„å›å¤ã€‚æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå›å¤å¹¶å†³å®šä¸‹ä¸€æ­¥ï¼š

**å¯¹è¯å†å²æ‘˜è¦ï¼š**
{chr(10).join(conversation_summary)}

**ä¸“å®¶æœ€æ–°å›å¤ï¼š**
{strong_response}

ç°åœ¨æˆ‘éœ€è¦åˆ†æä¸“å®¶çš„å›å¤ï¼Œå¹¶åšå‡ºå†³ç­–ï¼š

ğŸ¤” **åˆ†æè¦ç‚¹ï¼š**
1. ä¸“å®¶çš„å›å¤æ˜¯å¦å®Œå…¨å›ç­”äº†æˆ‘çš„é—®é¢˜ï¼Ÿ
2. æˆ‘æ˜¯å¦è¿˜æœ‰ä¸æ¸…æ¥šçš„åœ°æ–¹éœ€è¦è¿›ä¸€æ­¥è¯¢é—®ï¼Ÿ
3. åŸºäºä¸“å®¶çš„æŒ‡å¯¼ï¼Œæˆ‘ç°åœ¨èƒ½å¦ç‹¬ç«‹å®ŒæˆåŸå§‹ä»»åŠ¡ï¼Ÿ

ğŸ’¡ **å†³ç­–é€‰é¡¹ï¼š**
1. **æäº¤ç»“æœ** (submit_result): å¦‚æœä¸“å®¶å›å¤è¶³å¤Ÿè¯¦ç»†ï¼Œæˆ‘èƒ½åŸºäºæ­¤å®Œæˆä»»åŠ¡
2. **ç»§ç»­å¯¹è¯** (continue_conversation): å¦‚æœè¿˜æœ‰ç–‘é—®éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…
3. **ç»ˆæ­¢å¯¹è¯** (terminate): å¦‚æœè§‰å¾—å·²ç»è·å¾—è¶³å¤Ÿä¿¡æ¯ä½†æš‚æ—¶ä¸æƒ³æäº¤

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
{{
    "decision": "submit_result" æˆ– "continue_conversation" æˆ– "terminate",
    "final_result": "åŸºäºä¸“å®¶æŒ‡å¯¼çš„æœ€ç»ˆç­”æ¡ˆï¼ˆä»…åœ¨submit_resultæ—¶æä¾›ï¼‰",
    "questions": ["éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…çš„é—®é¢˜ï¼ˆä»…åœ¨continue_conversationæ—¶æä¾›ï¼‰"],
    "reasoning": "ä½ çš„å†³ç­–ç†ç”±",
    "confidence": 0.0-1.0,
    "satisfaction": 0.0-1.0
}}
"""

    def _format_weak_model_continue_message(self, response: Dict) -> str:
        """æ ¼å¼åŒ–å¼±æ¨¡å‹ç»§ç»­åˆ†ææ¶ˆæ¯"""
        decision = response.get('decision', 'unknown')
        reasoning = response.get('reasoning', 'æœªæä¾›ç†ç”±')

        if decision == 'submit_result':
            final_result = response.get('final_result', 'æœªæä¾›ç»“æœ')
            return f"åŸºäºä¸“å®¶çš„è¯¦ç»†æŒ‡å¯¼ï¼Œæˆ‘ç°åœ¨å¯ä»¥æäº¤æœ€ç»ˆç»“æœï¼š\n\n{final_result}\n\nå†³ç­–ç†ç”±ï¼š{reasoning}"

        elif decision == 'continue_conversation':
            questions = response.get('questions', [])
            questions_text = '\n- '.join(questions) if questions else 'éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…ç»†èŠ‚'
            return f"æ„Ÿè°¢ä¸“å®¶çš„å›å¤ï¼æˆ‘è¿˜æœ‰ä¸€äº›é—®é¢˜éœ€è¦æ¾„æ¸…ï¼š\n- {questions_text}\n\nå†³ç­–ç†ç”±ï¼š{reasoning}"

        elif decision == 'terminate':
            return f"æ„Ÿè°¢ä¸“å®¶çš„æŒ‡å¯¼ï¼æˆ‘è§‰å¾—å·²ç»è·å¾—äº†è¶³å¤Ÿçš„ä¿¡æ¯ã€‚\n\nå†³ç­–ç†ç”±ï¼š{reasoning}"

        else:
            return f"æˆ‘æ­£åœ¨åˆ†æä¸“å®¶çš„å›å¤...\n\nå†³ç­–ç†ç”±ï¼š{reasoning}"

    async def _increment_conversation_round(self, session_id: str):
        """å¢åŠ å¯¹è¯è½®æ•°"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨conversation_serviceçš„æ–¹æ³•æ¥æ›´æ–°è½®æ•°
            # æš‚æ—¶å…ˆè®°å½•æ—¥å¿—
            logger.info(f"ğŸ¤– å¯¹è¯è½®æ•°+1")
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å¯¹è¯è½®æ•°å¤±è´¥: {e}")

    async def _finalize_conversation(self, session_id: str, decision_type: str, final_result: str = None):
        """ç»“æŸå¯¹è¯å¹¶æäº¤æœ€ç»ˆç»“æœ"""
        try:
            logger.info(f"ğŸ¯ ç»“æŸå¯¹è¯: {decision_type}")

            # è¿™é‡Œåº”è¯¥ï¼š
            # 1. æ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºå®Œæˆ
            # 2. è®°å½•æœ€ç»ˆå†³ç­–
            # 3. æäº¤ä»»åŠ¡ç»“æœ

            # æš‚æ—¶å…ˆè®°å½•æ—¥å¿—ï¼Œå…·ä½“å®ç°éœ€è¦æ ¹æ®conversation_serviceçš„æ¥å£
            logger.info(f"ğŸ¯ æœ€ç»ˆç»“æœ: {final_result if final_result else 'æ— å…·ä½“ç»“æœ'}")

        except Exception as e:
            logger.error(f"âŒ ç»“æŸå¯¹è¯å¤±è´¥: {e}")

    async def _call_weak_model_analysis(self, prompt: str, weak_model: str, agent: Dict[str, Any] = None) -> Dict[str, Any]:
        """è°ƒç”¨å¼±æ¨¡å‹è¿›è¡Œåˆ†æï¼Œä½¿ç”¨Function Callingç¡®ä¿ç»“æ„åŒ–è¾“å‡º"""
        try:
            logger.info(f"ğŸ¤– è°ƒç”¨å¼±æ¨¡å‹åˆ†æ: {weak_model}")

            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆå¼±æ¨¡å‹ï¼‰- ä½¿ç”¨Agenté…ç½®å¦‚æœæä¾›
            weak_api_key = None
            weak_base_url = None
            if agent:
                weak_api_key = agent.get('api_key')
                weak_base_url = agent.get('base_url')

            weak_client = OpenAIClient(
                api_key=weak_api_key,
                base_url=weak_base_url,
                model=weak_model,
                temperature=0.3  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
            )

            # å®šä¹‰Function Callingçš„å‡½æ•°schema
            function_schema = {
                "name": "continue_decision",
                "description": "åˆ†æä¸“å®¶å›å¤å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "decision": {
                            "type": "string",
                            "enum": ["submit_result", "continue_conversation", "terminate"],
                            "description": "å†³ç­–ç±»å‹ï¼šsubmit_result(æäº¤ç»“æœ)ã€continue_conversation(ç»§ç»­å¯¹è¯)ã€terminate(ç»ˆæ­¢å¯¹è¯)"
                        },
                        "final_result": {
                            "type": "string",
                            "description": "æœ€ç»ˆç»“æœå†…å®¹ï¼ˆä»…åœ¨decisionä¸ºsubmit_resultæ—¶éœ€è¦ï¼‰"
                        },
                        "questions": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…çš„é—®é¢˜ï¼ˆä»…åœ¨decisionä¸ºcontinue_conversationæ—¶éœ€è¦ï¼‰"
                        },
                        "reasoning": {
                            "type": "string",
                            "description": "å†³ç­–ç†ç”±å’Œåˆ†æè¿‡ç¨‹"
                        },
                        "confidence": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 1,
                            "description": "å¯¹å†³ç­–çš„ç½®ä¿¡åº¦ï¼Œ0.0åˆ°1.0ä¹‹é—´"
                        },
                        "satisfaction": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 1,
                            "description": "å¯¹ä¸“å®¶å›å¤çš„æ»¡æ„åº¦ï¼Œ0.0åˆ°1.0ä¹‹é—´"
                        }
                    },
                    "required": ["decision", "reasoning", "confidence", "satisfaction"]
                }
            }

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ ä¸­çš„åˆå­¦è€…åŠ©æ‰‹ï¼Œæ­£åœ¨åˆ†æä¸“å®¶çš„å›å¤å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚"},
                {"role": "user", "content": prompt}
            ]

            # è°ƒç”¨Function Calling
            response = await weak_client.chat_completion_with_functions(
                messages=messages,
                functions=[function_schema],
                function_call={"name": "continue_decision"}
            )

            # è§£æFunction Callingç»“æœ
            if response.get("function_call"):
                function_result = json.loads(response["function_call"]["arguments"])
                logger.info(f"ğŸ¤– å¼±æ¨¡å‹åˆ†æå®Œæˆ: {function_result.get('decision')}")
                return function_result
            else:
                logger.warning("ğŸ¤– å¼±æ¨¡å‹æœªè¿”å›Function Callç»“æœ")
                return {
                    "decision": "terminate",
                    "reasoning": "åˆ†æç»“æœè§£æå¤±è´¥",
                    "confidence": 0.1,
                    "satisfaction": 0.5
                }

        except Exception as e:
            logger.error(f"âŒ å¼±æ¨¡å‹åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„ç»ˆæ­¢å†³ç­–
            return {
                "decision": "terminate",
                "reasoning": f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "confidence": 0.1,
                "satisfaction": 0.5
            }

    async def _generate_strong_model_response(self, weak_message: str, strong_model: str, messages: List, agent: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆå¼ºæ¨¡å‹å›å¤"""
        try:
            # æ„å»ºå¯¹è¯å†å²
            conversation_history = []
            for msg in messages[:-1]:  # æ’é™¤æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆåˆšå‘é€çš„å¼±æ¨¡å‹æ¶ˆæ¯ï¼‰
                conversation_history.append({
                    "role": "assistant" if msg.role == ConversationRole.STRONG_MODEL else "user",
                    "content": msg.content
                })

            # æ„å»ºå¼ºæ¨¡å‹æç¤º
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹å‘ä½ è¯¢é—®å…³äºä»»åŠ¡çš„é—®é¢˜ï¼Œè¯·æä¾›ä¸“ä¸šçš„å›ç­”å’ŒæŒ‡å¯¼ã€‚

åŠ©æ‰‹çš„é—®é¢˜ï¼š{weak_message}

è¯·æ ¹æ®åŠ©æ‰‹çš„é—®é¢˜ï¼Œæä¾›æœ‰ç”¨çš„å›ç­”å’Œå»ºè®®ã€‚ä¿æŒä¸“ä¸šã€è¯¦ç»†ï¼Œå¹¶å°½å¯èƒ½æä¾›å…·ä½“çš„æŒ‡å¯¼ã€‚
"""

            # è°ƒç”¨å¼ºæ¨¡å‹API
            response = await self._call_openai_api(
                model=strong_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                agent=agent
            )

            return response.get("content", "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•æä¾›å›å¤ã€‚")

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¼ºæ¨¡å‹å›å¤å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•æä¾›è¯¦ç»†å›å¤ï¼Œä½†æˆ‘å»ºè®®ä½ å¯ä»¥ç»§ç»­è¯¦ç»†æè¿°ä½ çš„éœ€æ±‚ã€‚"

    async def _call_openai_api(self, model: str, messages: List, max_tokens: int = 1000, agent: Dict[str, Any] = None) -> Dict:
        """è°ƒç”¨OpenAI API"""
        from ..utils.openai_client import OpenAIClient

        # ä½¿ç”¨Agenté…ç½®åˆ›å»ºå®¢æˆ·ç«¯
        agent_api_key = None
        agent_base_url = None
        if agent:
            agent_api_key = agent.get('api_key')
            agent_base_url = agent.get('base_url')

        client = OpenAIClient(
            api_key=agent_api_key,
            base_url=agent_base_url,
            model=model
        )
        # æ„å»ºtask_dataä»¥ä¼ é€’å‚æ•°
        task_data = {
            'max_tokens': max_tokens,
            'temperature': 0.7,
            'tools': [],
            'tool_choice': None
        }
        return await client._call_openai_api_with_messages(
            messages=messages,
            model=model,
            task_data=task_data
        )

    def _build_weak_model_prompt(self, task_description: str, context: Dict[str, Any]) -> str:
        """æ„å»ºå¼±æ¨¡å‹çš„åˆ†ææç¤º"""
        return f"""
ä½œä¸ºä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·åˆ†æä»¥ä¸‹ä»»åŠ¡ï¼š

ä»»åŠ¡æè¿°ï¼š{task_description}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}

è¯·è¯„ä¼°ï¼š
1. ä½ æ˜¯å¦èƒ½å¤Ÿç‹¬ç«‹å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Ÿ
2. è¿™ä¸ªä»»åŠ¡çš„å¤æ‚ç¨‹åº¦å¦‚ä½•ï¼Ÿ
3. æ˜¯å¦éœ€è¦ä¸æ›´å¼ºçš„æ¨¡å‹è¿›è¡Œè®¨è®ºï¼Ÿ

è¯·æä¾›ä½ çš„åˆæ­¥åˆ†æå’Œå»ºè®®ã€‚
"""

    async def handle_conversation_response(
        self,
        session_id: str,
        user_input: str,
        action: str
    ) -> Dict[str, Any]:
        """å¤„ç†å¯¹è¯ä¸­çš„ç”¨æˆ·å“åº”"""
        try:
            if action == "continue_conversation":
                # ç»§ç»­å¯¹è¯
                message_request = SendSimulatorMessageRequest(
                    session_id=session_id,
                    role=ConversationRole.STRONG_MODEL,
                    content=user_input,
                    metadata={
                        "type": "conversation_response",
                        "timestamp": datetime.now().isoformat()
                    }
                )

                await self.conversation_service.send_message(message_request)

                # è®©å¼±æ¨¡å‹åˆ†æå¼ºæ¨¡å‹çš„å›å¤å¹¶å†³å®šæ˜¯å¦ç»§ç»­
                weak_model_decision = await self._weak_model_evaluate_continuation(
                    session_id, user_input
                )

                if weak_model_decision["should_terminate"]:
                    # å¼±æ¨¡å‹å†³å®šç»ˆæ­¢å¯¹è¯å¹¶æäº¤ç»“æœ
                    decision_request = SimulatorDecisionRequest(
                        session_id=session_id,
                        decision_type=SimulatorDecision.WEAK_MODEL_TERMINATED,
                        result_data=weak_model_decision["final_result"],
                        confidence_score=weak_model_decision.get("confidence", 0.8),
                        decision_reasoning=f"å¼±æ¨¡å‹è‡ªä¸»ç»ˆæ­¢ï¼š{weak_model_decision['reasoning']}"
                    )

                    execution_result = await self.conversation_service.make_decision(decision_request)

                    return {
                        "status": "completed",
                        "execution_type": "weak_model_terminated",
                        "result": weak_model_decision["final_result"],
                        "execution_result": execution_result.model_dump(),
                        "termination_reason": weak_model_decision["reasoning"]
                    }
                else:
                    # å¼±æ¨¡å‹å†³å®šç»§ç»­å¯¹è¯ï¼Œå‘é€ç»§ç»­çš„æ¶ˆæ¯
                    weak_continue_message = SendSimulatorMessageRequest(
                        session_id=session_id,
                        role=ConversationRole.WEAK_MODEL,
                        content=weak_model_decision["continue_message"],
                        metadata={
                            "type": "weak_model_continuation",
                            "confidence": weak_model_decision.get("confidence", 0.7),
                            "timestamp": datetime.now().isoformat()
                        }
                    )

                    await self.conversation_service.send_message(weak_continue_message)

                    # è·å–æ›´æ–°åçš„ä¼šè¯çŠ¶æ€
                    session_response = await self.conversation_service.get_session_with_messages(session_id)

                    return {
                        "status": "conversation_continued",
                        "session": session_response,
                        "next_action": session_response.next_action,
                        "weak_model_analysis": weak_model_decision
                    }

            elif action == "submit_decision":
                # æäº¤æœ€ç»ˆå†³ç­–
                decision_data = {
                    "answer": user_input,
                    "timestamp": datetime.now().isoformat()
                }

                decision_request = SimulatorDecisionRequest(
                    session_id=session_id,
                    decision_type=SimulatorDecision.CONSULT_COMPLETE,
                    result_data=decision_data,
                    confidence_score=0.9,
                    decision_reasoning="ç»è¿‡å¯¹è¯åå•†åçš„æœ€ç»ˆå†³ç­–"
                )

                execution_result = await self.conversation_service.make_decision(decision_request)

                return {
                    "status": "completed",
                    "execution_type": "conversation_result",
                    "result": decision_data,
                    "execution_result": execution_result.model_dump()
                }

            elif action == "interrupt":
                # ä¸­æ–­å¯¹è¯
                await self.conversation_service.interrupt_session(session_id, "ç”¨æˆ·ä¸­æ–­")

                return {
                    "status": "interrupted",
                    "message": "å¯¹è¯å·²ä¸­æ–­"
                }

        except Exception as e:
            logger.error(f"å¤„ç†å¯¹è¯å“åº”å¤±è´¥: {e}")
            raise

    async def _weak_model_evaluate_continuation(
        self,
        session_id: str,
        strong_model_response: str
    ) -> Dict[str, Any]:
        """å¼±æ¨¡å‹è¯„ä¼°æ˜¯å¦åº”è¯¥ç»§ç»­å¯¹è¯"""
        try:
            # è·å–å½“å‰ä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯
            session_response = await self.conversation_service.get_session_with_messages(session_id)
            messages = session_response.messages

            # åˆ†æå¯¹è¯å†å²å’Œå¼ºæ¨¡å‹çš„æœ€æ–°å›å¤
            conversation_analysis = self._analyze_conversation_progress(messages, strong_model_response)

            # å¼±æ¨¡å‹å†³ç­–é€»è¾‘
            should_terminate = False
            reasoning = ""
            confidence = 0.0
            final_result = None
            continue_message = ""

            # æ£€æŸ¥æ˜¯å¦å·²ç»è·å¾—è¶³å¤Ÿä¿¡æ¯
            if conversation_analysis["information_completeness"] > 0.8:
                should_terminate = True
                reasoning = "å·²è·å¾—è¶³å¤Ÿä¿¡æ¯å®Œæˆä»»åŠ¡"
                confidence = conversation_analysis["information_completeness"]
                final_result = {
                    "answer": self._synthesize_final_answer(messages, strong_model_response),
                    "conversation_summary": conversation_analysis["summary"],
                    "confidence": confidence
                }

            # æ£€æŸ¥å¯¹è¯æ˜¯å¦é™·å…¥å¾ªç¯
            elif conversation_analysis["is_repetitive"]:
                should_terminate = True
                reasoning = "å¯¹è¯å‡ºç°é‡å¤ï¼Œæ— æ–°ä¿¡æ¯äº§ç”Ÿ"
                confidence = 0.7
                final_result = {
                    "answer": self._synthesize_final_answer(messages, strong_model_response),
                    "conversation_summary": "å¯¹è¯å‡ºç°é‡å¤æ¨¡å¼ï¼ŒåŸºäºç°æœ‰ä¿¡æ¯ç»™å‡ºç­”æ¡ˆ",
                    "confidence": confidence
                }

            # æ£€æŸ¥å¼ºæ¨¡å‹å›å¤è´¨é‡
            elif conversation_analysis["response_quality"] < 0.3:
                should_terminate = True
                reasoning = "å¼ºæ¨¡å‹å›å¤è´¨é‡è¾ƒä½ï¼Œç»§ç»­å¯¹è¯ä»·å€¼ä¸å¤§"
                confidence = 0.6
                final_result = {
                    "answer": self._synthesize_final_answer(messages, strong_model_response),
                    "conversation_summary": "åŸºäºç°æœ‰å¯¹è¯å†…å®¹ç»¼åˆåˆ†æå¾—å‡ºç­”æ¡ˆ",
                    "confidence": confidence
                }

            # å†³å®šç»§ç»­å¯¹è¯
            else:
                should_terminate = False
                reasoning = "ä»éœ€æ›´å¤šä¿¡æ¯æˆ–æ¾„æ¸…"
                confidence = conversation_analysis.get("continuation_confidence", 0.7)
                continue_message = self._generate_follow_up_question(messages, strong_model_response, conversation_analysis)

            return {
                "should_terminate": should_terminate,
                "reasoning": reasoning,
                "confidence": confidence,
                "final_result": final_result,
                "continue_message": continue_message,
                "conversation_analysis": conversation_analysis
            }

        except Exception as e:
            logger.error(f"å¼±æ¨¡å‹è¯„ä¼°ç»§ç»­å¯¹è¯å¤±è´¥: {e}")
            # é»˜è®¤ç»§ç»­å¯¹è¯
            return {
                "should_terminate": False,
                "reasoning": "è¯„ä¼°è¿‡ç¨‹å‡ºé”™ï¼Œé»˜è®¤ç»§ç»­å¯¹è¯",
                "confidence": 0.5,
                "continue_message": "è¯·ç»§ç»­æä¾›æ›´å¤šä¿¡æ¯ã€‚"
            }

    def _analyze_conversation_progress(self, messages: List, strong_model_response: str) -> Dict[str, Any]:
        """åˆ†æå¯¹è¯è¿›å±•æƒ…å†µ"""
        if not messages:
            return {
                "information_completeness": 0.0,
                "is_repetitive": False,
                "response_quality": 0.5,
                "summary": "å¯¹è¯åˆšå¼€å§‹",
                "continuation_confidence": 0.8
            }

        # ç®€å•çš„å¯¹è¯åˆ†æé€»è¾‘
        weak_messages = [m for m in messages if m.role == ConversationRole.WEAK_MODEL]
        strong_messages = [m for m in messages if m.role == ConversationRole.STRONG_MODEL]

        # ä¿¡æ¯å®Œæ•´åº¦è¯„ä¼°
        total_content_length = sum(len(m.content) for m in messages)
        information_completeness = min(total_content_length / 1000, 1.0)  # åŸºäºå†…å®¹é•¿åº¦çš„ç®€å•è¯„ä¼°

        # æ£€æŸ¥é‡å¤æ€§
        is_repetitive = self._detect_repetitive_patterns(messages)

        # å¼ºæ¨¡å‹å›å¤è´¨é‡è¯„ä¼°
        response_quality = self._evaluate_response_quality(strong_model_response)

        # å¯¹è¯è½®æ¬¡åˆ†æ
        round_count = len(weak_messages)
        if round_count >= 5:  # è¶…è¿‡5è½®å¯¹è¯ï¼Œå¢åŠ ç»ˆæ­¢å€¾å‘
            information_completeness += 0.2

        return {
            "information_completeness": min(information_completeness, 1.0),
            "is_repetitive": is_repetitive,
            "response_quality": response_quality,
            "summary": f"å·²è¿›è¡Œ{round_count}è½®å¯¹è¯ï¼Œå¼ºæ¨¡å‹æä¾›äº†{len(strong_messages)}æ¬¡å›å¤",
            "continuation_confidence": 0.8 - (round_count * 0.1)  # è½®æ¬¡è¶Šå¤šï¼Œç»§ç»­çš„ä¿¡å¿ƒè¶Šä½
        }

    def _detect_repetitive_patterns(self, messages: List) -> bool:
        """æ£€æµ‹å¯¹è¯æ˜¯å¦å‡ºç°é‡å¤æ¨¡å¼"""
        if len(messages) < 4:
            return False

        # ç®€å•æ£€æµ‹ï¼šæ¯”è¾ƒæœ€è¿‘çš„æ¶ˆæ¯æ˜¯å¦ä¸ä¹‹å‰çš„æ¶ˆæ¯è¿‡äºç›¸ä¼¼
        recent_messages = messages[-4:]
        for i, msg1 in enumerate(recent_messages):
            for j, msg2 in enumerate(recent_messages[i+1:], i+1):
                if msg1.role == msg2.role:
                    # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦ä¸²åŒ…å«æ£€æŸ¥ï¼‰
                    content1_words = set(msg1.content.lower().split())
                    content2_words = set(msg2.content.lower().split())
                    if content1_words and content2_words:
                        similarity = len(content1_words & content2_words) / len(content1_words | content2_words)
                        if similarity > 0.7:  # 70%ç›¸ä¼¼åº¦è®¤ä¸ºé‡å¤
                            return True

        return False

    def _evaluate_response_quality(self, response: str) -> float:
        """è¯„ä¼°å¼ºæ¨¡å‹å›å¤çš„è´¨é‡"""
        if not response or len(response.strip()) < 10:
            return 0.1

        # ç®€å•çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡
        quality_score = 0.5  # åŸºç¡€åˆ†æ•°

        # é•¿åº¦é€‚ä¸­åŠ åˆ†
        if 50 <= len(response) <= 500:
            quality_score += 0.2

        # åŒ…å«é—®å·ï¼ˆæå‡ºåé—®ï¼‰åŠ åˆ†
        if 'ï¼Ÿ' in response or '?' in response:
            quality_score += 0.1

        # åŒ…å«å…·ä½“ä¿¡æ¯åŠ åˆ†
        if any(keyword in response for keyword in ['å…·ä½“', 'è¯¦ç»†', 'æ¯”å¦‚', 'ä¾‹å¦‚', 'è¯´æ˜']):
            quality_score += 0.2

        return min(quality_score, 1.0)

    def _synthesize_final_answer(self, messages: List, latest_response: str) -> str:
        """åŸºäºå¯¹è¯å†å²ç»¼åˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        if not messages:
            return f"åŸºäºæœ€æ–°ä¿¡æ¯ï¼š{latest_response}"

        # æ”¶é›†æ‰€æœ‰å¼ºæ¨¡å‹çš„å…³é”®ä¿¡æ¯
        strong_responses = [m.content for m in messages if m.role == ConversationRole.STRONG_MODEL]
        strong_responses.append(latest_response)

        # ç®€å•çš„ä¿¡æ¯ç»¼åˆ
        combined_info = " ".join(strong_responses[-3:])  # ä½¿ç”¨æœ€è¿‘3æ¡å¼ºæ¨¡å‹å›å¤

        return f"ç»¼åˆå¯¹è¯ä¿¡æ¯åˆ†æï¼š{combined_info[:300]}..."  # é™åˆ¶é•¿åº¦

    def _generate_follow_up_question(self, messages: List, strong_response: str, analysis: Dict) -> str:
        """ç”Ÿæˆåç»­é—®é¢˜"""
        round_count = len([m for m in messages if m.role == ConversationRole.WEAK_MODEL])

        follow_up_questions = [
            "èƒ½å¦æä¾›æ›´å…·ä½“çš„ç»†èŠ‚ï¼Ÿ",
            "è¿™ä¸ªæ–¹æ¡ˆæœ‰ä»€ä¹ˆæ½œåœ¨çš„é—®é¢˜å—ï¼Ÿ",
            "è¿˜æœ‰å…¶ä»–éœ€è¦è€ƒè™‘çš„å› ç´ å—ï¼Ÿ",
            "èƒ½å¦ä¸¾ä¸ªå…·ä½“çš„ä¾‹å­è¯´æ˜ï¼Ÿ",
            "è¿™ä¸ªè§£å†³æ–¹æ¡ˆçš„å¯è¡Œæ€§å¦‚ä½•ï¼Ÿ"
        ]

        # æ ¹æ®è½®æ¬¡é€‰æ‹©ä¸åŒçš„é—®é¢˜
        question_index = min(round_count, len(follow_up_questions) - 1)
        return follow_up_questions[question_index]