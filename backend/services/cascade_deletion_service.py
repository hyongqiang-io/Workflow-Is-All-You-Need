"""
çº§è”åˆ é™¤æœåŠ¡
Cascade Deletion Service
"""

import uuid
from typing import Optional, Dict, Any, List
from datetime import datetime
from loguru import logger

from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
from ..repositories.instance.node_instance_repository import NodeInstanceRepository
from ..repositories.instance.task_instance_repository import TaskInstanceRepository
from ..repositories.workflow.workflow_repository import WorkflowRepository
from ..repositories.node.node_repository import NodeRepository
from ..utils.helpers import now_utc


class CascadeDeletionService:
    """çº§è”åˆ é™¤æœåŠ¡ - å¤„ç†å·¥ä½œæµç›¸å…³æ•°æ®çš„çº§è”åˆ é™¤"""
    
    def __init__(self):
        self.workflow_repo = WorkflowRepository()
        self.workflow_instance_repo = WorkflowInstanceRepository()
        self.node_instance_repo = NodeInstanceRepository()
        self.task_instance_repo = TaskInstanceRepository()
        self.node_repo = NodeRepository()
    
    async def delete_workflow_instance_cascade(self, instance_id: uuid.UUID, 
                                             soft_delete: bool = True) -> Dict[str, Any]:
        """çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®"""
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹: {instance_id}")
            logger.info(f"   åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            
            # ä½¿ç”¨å·¥ä½œæµå®ä¾‹ä»“åº“çš„çº§è”åˆ é™¤æ–¹æ³•
            deletion_result = await self.workflow_instance_repo.delete_instance_cascade(
                instance_id, soft_delete
            )
            
            logger.info(f"âœ… å·¥ä½œæµå®ä¾‹çº§è”åˆ é™¤å®Œæˆ")
            return deletion_result
            
        except Exception as e:
            logger.error(f"çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def delete_workflow_base_cascade(self, workflow_base_id: uuid.UUID, 
                                         soft_delete: bool = True) -> Dict[str, Any]:
        """çº§è”åˆ é™¤å·¥ä½œæµåŸºç¡€å®šä¹‰åŠå…¶æ‰€æœ‰å®ä¾‹å’Œç›¸å…³æ•°æ®"""
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹çº§è”åˆ é™¤å·¥ä½œæµåŸºç¡€å®šä¹‰: {workflow_base_id}")
            logger.info(f"   åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            
            # ç»Ÿè®¡åˆ é™¤çš„æ•°æ®é‡
            deletion_stats = {
                'workflow_base_id': str(workflow_base_id),
                'deleted_workflow_instances': 0,
                'deleted_tasks': 0,
                'deleted_nodes': 0,
                'deleted_workflow_base': False,
                'soft_delete': soft_delete,
                'instance_details': []
            }
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰åŸºäºæ­¤å·¥ä½œæµçš„å®ä¾‹
            logger.info(f"ğŸ“‹ æ­¥éª¤1: æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„å·¥ä½œæµå®ä¾‹")
            instances_query = """
                SELECT workflow_instance_id, workflow_instance_name, status 
                FROM workflow_instance 
                WHERE workflow_base_id = $1 AND is_deleted = FALSE
            """
            instances = await self.workflow_instance_repo.db.fetch_all(
                instances_query, workflow_base_id
            )
            
            logger.info(f"   æ‰¾åˆ° {len(instances)} ä¸ªå·¥ä½œæµå®ä¾‹éœ€è¦åˆ é™¤")
            
            # 2. é€ä¸ªçº§è”åˆ é™¤æ¯ä¸ªå·¥ä½œæµå®ä¾‹
            total_deleted_tasks = 0
            total_deleted_nodes = 0
            
            for instance in instances:
                instance_id = instance['workflow_instance_id']
                workflow_instance_name = instance.get('workflow_instance_name', 'æœªå‘½å')
                
                logger.info(f"ğŸ“‹ åˆ é™¤å·¥ä½œæµå®ä¾‹: {workflow_instance_name} ({instance_id})")
                
                # çº§è”åˆ é™¤å•ä¸ªå·¥ä½œæµå®ä¾‹
                instance_deletion = await self.delete_workflow_instance_cascade(
                    instance_id, soft_delete
                )
                
                total_deleted_tasks += instance_deletion['deleted_tasks']
                total_deleted_nodes += instance_deletion['deleted_nodes']
                deletion_stats['instance_details'].append({
                    'instance_id': str(instance_id),
                    'workflow_instance_name': workflow_instance_name,
                    'deleted_tasks': instance_deletion['deleted_tasks'],
                    'deleted_nodes': instance_deletion['deleted_nodes'],
                    'success': instance_deletion['deleted_workflow']
                })
            
            deletion_stats['deleted_workflow_instances'] = len(instances)
            deletion_stats['deleted_tasks'] = total_deleted_tasks
            deletion_stats['deleted_nodes'] = total_deleted_nodes
            
            # 3. åˆ é™¤å·¥ä½œæµåŸºç¡€å®šä¹‰æœ¬èº«
            logger.info(f"ğŸ“‹ æ­¥éª¤3: åˆ é™¤å·¥ä½œæµåŸºç¡€å®šä¹‰")
            if soft_delete:
                workflow_deleted = await self.workflow_repo.delete_workflow(
                    workflow_base_id, soft_delete=True
                )
            else:
                workflow_deleted = await self.workflow_repo.delete_workflow(
                    workflow_base_id, soft_delete=False
                )
            
            deletion_stats['deleted_workflow_base'] = workflow_deleted
            
            if workflow_deleted:
                logger.info(f"âœ… å·¥ä½œæµåŸºç¡€å®šä¹‰çº§è”åˆ é™¤æˆåŠŸ:")
                logger.info(f"   - å·¥ä½œæµåŸºç¡€ID: {workflow_base_id}")
                logger.info(f"   - åˆ é™¤çš„å·¥ä½œæµå®ä¾‹: {deletion_stats['deleted_workflow_instances']} ä¸ª")
                logger.info(f"   - åˆ é™¤çš„ä»»åŠ¡æ€»æ•°: {deletion_stats['deleted_tasks']} ä¸ª")
                logger.info(f"   - åˆ é™¤çš„èŠ‚ç‚¹å®ä¾‹æ€»æ•°: {deletion_stats['deleted_nodes']} ä¸ª")
                logger.info(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            else:
                logger.error(f"âŒ å·¥ä½œæµåŸºç¡€å®šä¹‰çº§è”åˆ é™¤å¤±è´¥: {workflow_base_id}")
            
            return deletion_stats
            
        except Exception as e:
            logger.error(f"çº§è”åˆ é™¤å·¥ä½œæµåŸºç¡€å®šä¹‰å¤±è´¥: {e}")
            raise
    
    async def get_deletion_preview(self, workflow_base_id: uuid.UUID) -> Dict[str, Any]:
        """é¢„è§ˆåˆ é™¤æ“ä½œå°†å½±å“çš„æ•°æ®é‡ï¼ˆä¸æ‰§è¡Œå®é™…åˆ é™¤ï¼‰"""
        try:
            logger.info(f"ğŸ” é¢„è§ˆå·¥ä½œæµåˆ é™¤å½±å“: {workflow_base_id}")
            
            # æŸ¥è¯¢æ‰€æœ‰ç›¸å…³çš„å·¥ä½œæµå®ä¾‹
            instances_query = """
                SELECT wi.workflow_instance_id, wi.workflow_instance_name, wi.status,
                       COUNT(DISTINCT ni.node_instance_id) as node_count,
                       COUNT(DISTINCT ti.task_instance_id) as task_count
                FROM workflow_instance wi
                LEFT JOIN node_instance ni ON wi.workflow_instance_id = ni.workflow_instance_id 
                                             AND ni.is_deleted = FALSE
                LEFT JOIN task_instance ti ON wi.workflow_instance_id = ti.workflow_instance_id 
                                             AND ti.is_deleted = FALSE
                WHERE wi.workflow_base_id = $1 AND wi.is_deleted = FALSE
                GROUP BY wi.workflow_instance_id, wi.workflow_instance_name, wi.status
            """
            instances = await self.workflow_instance_repo.db.fetch_all(
                instances_query, workflow_base_id
            )
            
            total_instances = len(instances)
            total_nodes = sum(int(inst.get('node_count', 0)) for inst in instances)
            total_tasks = sum(int(inst.get('task_count', 0)) for inst in instances)
            
            # æŒ‰çŠ¶æ€åˆ†ç»„ç»Ÿè®¡
            status_summary = {}
            for instance in instances:
                status = instance.get('status', 'unknown')
                if status not in status_summary:
                    status_summary[status] = 0
                status_summary[status] += 1
            
            preview = {
                'workflow_base_id': str(workflow_base_id),
                'total_workflow_instances': total_instances,
                'total_node_instances': total_nodes,
                'total_task_instances': total_tasks,
                'instance_status_summary': status_summary,
                'instance_details': [
                    {
                        'instance_id': str(inst['workflow_instance_id']),
                        'workflow_instance_name': inst.get('workflow_instance_name', 'æœªå‘½å'),
                        'status': inst.get('status'),
                        'node_count': int(inst.get('node_count', 0)),
                        'task_count': int(inst.get('task_count', 0))
                    }
                    for inst in instances
                ]
            }
            
            logger.info(f"ğŸ“Š åˆ é™¤é¢„è§ˆç»“æœ:")
            logger.info(f"   - å·¥ä½œæµå®ä¾‹: {total_instances} ä¸ª")
            logger.info(f"   - èŠ‚ç‚¹å®ä¾‹: {total_nodes} ä¸ª")
            logger.info(f"   - ä»»åŠ¡å®ä¾‹: {total_tasks} ä¸ª")
            logger.info(f"   - çŠ¶æ€åˆ†å¸ƒ: {status_summary}")
            
            return preview
            
        except Exception as e:
            logger.error(f"è·å–åˆ é™¤é¢„è§ˆå¤±è´¥: {e}")
            raise
    
    async def clear_processor_references(self, processor_id: uuid.UUID) -> Dict[str, Any]:
        """æ¸…ç©ºæ‰€æœ‰å·¥ä½œæµèŠ‚ç‚¹ä¸­å¯¹æŒ‡å®šå¤„ç†å™¨çš„å¼•ç”¨"""
        try:
            logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç©ºå¤„ç†å™¨å¼•ç”¨: {processor_id}")
            
            # æŸ¥è¯¢æ‰€æœ‰å¼•ç”¨æ­¤å¤„ç†å™¨çš„èŠ‚ç‚¹å¤„ç†å™¨å…³è”è®°å½•
            query = """
                SELECT np.node_processor_id, np.node_id, np.workflow_id, np.workflow_base_id,
                       n.name, n.type
                FROM node_processor np
                LEFT JOIN node n ON np.node_id = n.node_id
                WHERE np.processor_id = $1 AND np.is_deleted = FALSE
            """
            affected_records = await self.node_repo.db.fetch_all(query, str(processor_id))
            
            logger.info(f"   æ‰¾åˆ° {len(affected_records)} ä¸ªèŠ‚ç‚¹å¤„ç†å™¨å…³è”è®°å½•")
            
            if len(affected_records) == 0:
                return {
                    'processor_id': str(processor_id),
                    'cleared_records': 0,
                    'affected_workflows': [],
                    'success': True
                }
            
            # è½¯åˆ é™¤node_processorè®°å½•
            update_query = """
                UPDATE node_processor 
                SET is_deleted = TRUE, updated_at = $2
                WHERE processor_id = $1 AND is_deleted = FALSE
            """
            
            # æ·»åŠ updated_atå­—æ®µï¼Œå¦‚æœè¡¨ä¸­æ²¡æœ‰åˆ™ä½¿ç”¨created_at
            try:
                result = await self.node_repo.db.execute(
                    update_query, str(processor_id), now_utc()
                )
            except Exception as e:
                # å¦‚æœnode_processorè¡¨æ²¡æœ‰updated_atå­—æ®µï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                simple_update_query = """
                    UPDATE node_processor 
                    SET is_deleted = TRUE
                    WHERE processor_id = $1 AND is_deleted = FALSE
                """
                result = await self.node_repo.db.execute(
                    simple_update_query, str(processor_id)
                )
            
            # ç»Ÿè®¡å—å½±å“çš„å·¥ä½œæµ
            workflows = {}
            for record in affected_records:
                workflow_id = str(record['workflow_id'])
                if workflow_id not in workflows:
                    workflows[workflow_id] = {
                        'workflow_id': workflow_id,
                        'workflow_base_id': str(record['workflow_base_id']),
                        'affected_nodes': []
                    }
                workflows[workflow_id]['affected_nodes'].append({
                    'node_id': str(record['node_id']),
                    'name': record.get('name', 'æœªçŸ¥'),
                    'type': record.get('type', 'unknown')
                })
            
            clear_result = {
                'processor_id': str(processor_id),
                'cleared_records': len(affected_records),
                'affected_workflows': list(workflows.values()),
                'success': True
            }
            
            logger.info(f"âœ… å¤„ç†å™¨å¼•ç”¨æ¸…ç©ºå®Œæˆ:")
            logger.info(f"   - å¤„ç†å™¨ID: {processor_id}")
            logger.info(f"   - æ¸…ç©ºçš„å…³è”è®°å½•: {len(affected_records)} ä¸ª")
            logger.info(f"   - å—å½±å“çš„å·¥ä½œæµ: {len(workflows)} ä¸ª")
            
            return clear_result
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºå¤„ç†å™¨å¼•ç”¨å¤±è´¥: {e}")
            raise

    async def fix_orphan_instances(self) -> Dict[str, Any]:
        """ä¿®å¤å­¤å„¿å®ä¾‹ - åŒæ­¥è½¯åˆ é™¤å¼•ç”¨å·²åˆ é™¤å·¥ä½œæµçš„èŠ‚ç‚¹å®ä¾‹å’Œä»»åŠ¡å®ä¾‹"""
        try:
            logger.info(f"ğŸ”§ å¼€å§‹ä¿®å¤å­¤å„¿å®ä¾‹é—®é¢˜")

            # æŸ¥æ‰¾å¼•ç”¨è½¯åˆ é™¤å·¥ä½œæµçš„æ´»è·ƒèŠ‚ç‚¹å®ä¾‹
            orphan_nodes_query = """
                SELECT COUNT(*) as count
                FROM node_instance n
                JOIN workflow_instance w ON n.workflow_instance_id = w.workflow_instance_id
                WHERE w.is_deleted = TRUE AND n.is_deleted = FALSE
            """
            orphan_nodes_count = await self.node_instance_repo.db.fetch_one(orphan_nodes_query)
            orphan_nodes = int(orphan_nodes_count['count'])

            # æŸ¥æ‰¾å¼•ç”¨è½¯åˆ é™¤å·¥ä½œæµçš„æ´»è·ƒä»»åŠ¡å®ä¾‹
            orphan_tasks_query = """
                SELECT COUNT(*) as count
                FROM task_instance t
                JOIN workflow_instance w ON t.workflow_instance_id = w.workflow_instance_id
                WHERE w.is_deleted = TRUE AND t.is_deleted = FALSE
            """
            orphan_tasks_count = await self.task_instance_repo.db.fetch_one(orphan_tasks_query)
            orphan_tasks = int(orphan_tasks_count['count'])

            logger.info(f"   å‘ç°å­¤å„¿èŠ‚ç‚¹å®ä¾‹: {orphan_nodes} ä¸ª")
            logger.info(f"   å‘ç°å­¤å„¿ä»»åŠ¡å®ä¾‹: {orphan_tasks} ä¸ª")

            fixed_nodes = 0
            fixed_tasks = 0

            # ä¿®å¤å­¤å„¿èŠ‚ç‚¹å®ä¾‹
            if orphan_nodes > 0:
                fix_nodes_query = """
                    UPDATE node_instance n
                    JOIN workflow_instance w ON n.workflow_instance_id = w.workflow_instance_id
                    SET n.is_deleted = TRUE, n.updated_at = NOW()
                    WHERE w.is_deleted = TRUE AND n.is_deleted = FALSE
                """
                await self.node_instance_repo.db.execute(fix_nodes_query)
                fixed_nodes = orphan_nodes
                logger.info(f"âœ… å·²ä¿®å¤å­¤å„¿èŠ‚ç‚¹å®ä¾‹: {fixed_nodes} ä¸ª")

            # ä¿®å¤å­¤å„¿ä»»åŠ¡å®ä¾‹
            if orphan_tasks > 0:
                fix_tasks_query = """
                    UPDATE task_instance t
                    JOIN workflow_instance w ON t.workflow_instance_id = w.workflow_instance_id
                    SET t.is_deleted = TRUE, t.updated_at = NOW()
                    WHERE w.is_deleted = TRUE AND t.is_deleted = FALSE
                """
                await self.task_instance_repo.db.execute(fix_tasks_query)
                fixed_tasks = orphan_tasks
                logger.info(f"âœ… å·²ä¿®å¤å­¤å„¿ä»»åŠ¡å®ä¾‹: {fixed_tasks} ä¸ª")

            fix_result = {
                'orphan_nodes_found': orphan_nodes,
                'orphan_tasks_found': orphan_tasks,
                'fixed_nodes': fixed_nodes,
                'fixed_tasks': fixed_tasks,
                'success': True
            }

            if fixed_nodes > 0 or fixed_tasks > 0:
                logger.info(f"âœ… å­¤å„¿å®ä¾‹ä¿®å¤å®Œæˆ:")
                logger.info(f"   - ä¿®å¤çš„èŠ‚ç‚¹å®ä¾‹: {fixed_nodes} ä¸ª")
                logger.info(f"   - ä¿®å¤çš„ä»»åŠ¡å®ä¾‹: {fixed_tasks} ä¸ª")
            else:
                logger.info(f"âœ… æœªå‘ç°å­¤å„¿å®ä¾‹ï¼Œæ•°æ®çŠ¶æ€æ­£å¸¸")

            return fix_result

        except Exception as e:
            logger.error(f"ä¿®å¤å­¤å„¿å®ä¾‹å¤±è´¥: {e}")
            raise


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
cascade_deletion_service = CascadeDeletionService()