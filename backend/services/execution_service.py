"""
å·¥ä½œæµæ‰§è¡Œå¼•æ“æœåŠ¡
Workflow Execution Engine Service
"""

import uuid
import json
import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import sys
from loguru import logger
logger.remove()
from time import sleep
logger.add(sys.stderr,level="WARNING")

from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
from ..repositories.instance.task_instance_repository import TaskInstanceRepository
from ..repositories.workflow.workflow_repository import WorkflowRepository
from ..repositories.node.node_repository import NodeRepository
from ..repositories.processor.processor_repository import ProcessorRepository
from ..repositories.user.user_repository import UserRepository
from ..repositories.agent.agent_repository import AgentRepository
from ..models.instance import (
    WorkflowInstanceCreate, WorkflowInstanceUpdate, WorkflowInstanceStatus,
    TaskInstanceCreate, TaskInstanceUpdate, TaskInstanceStatus, TaskInstanceType,
    WorkflowExecuteRequest
)
from ..models.node import NodeType
from ..utils.helpers import now_utc
from .agent_task_service import agent_task_service
from .resource_cleanup_manager import ResourceCleanupManager

# ä½¿ç”¨æ–°çš„ç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨
from .workflow_execution_context import get_context_manager, WorkflowExecutionContext

def _json_serializer(obj):
    """è‡ªå®šä¹‰JSONåºåˆ—åŒ–å‡½æ•°ï¼Œå¤„ç†datetimeå¯¹è±¡"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {obj.__class__.__name__} is not JSON serializable")


class ExecutionEngine:
    """å·¥ä½œæµæ‰§è¡Œå¼•æ“ - ç®€åŒ–ç‰ˆæœ¬
    
    è´Ÿè´£ï¼š
    - å·¥ä½œæµå®ä¾‹çš„å¯åŠ¨å’Œç®¡ç†
    - èŠ‚ç‚¹å®ä¾‹çš„åˆ›å»ºå’Œæ‰§è¡Œ
    - ä»»åŠ¡å®ä¾‹çš„åˆ›å»ºå’Œè°ƒåº¦
    - å·¥ä½œæµçŠ¶æ€çš„ç»Ÿä¸€ç®¡ç†
    """
    
    def __init__(self):
        # æ•°æ®è®¿é—®å±‚
        self.workflow_instance_repo = WorkflowInstanceRepository()
        self.task_instance_repo = TaskInstanceRepository()
        self.workflow_repo = WorkflowRepository()
        self.node_repo = NodeRepository()
        self.processor_repo = ProcessorRepository()
        self.user_repo = UserRepository()
        self.agent_repo = AgentRepository()
        
        # æ‰§è¡Œé˜Ÿåˆ—å’ŒçŠ¶æ€è·Ÿè¸ª
        self.execution_queue = asyncio.Queue()
        self.running_instances = {}  # è¿è¡Œä¸­çš„å®ä¾‹è·Ÿè¸ª
        self.is_running = False
        
        # ä»»åŠ¡å®Œæˆå›è°ƒæ˜ å°„
        self.task_callbacks = {}
        
        # ç³»ç»Ÿç»„ä»¶
        self.resource_cleanup_manager = ResourceCleanupManager()
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¶æ„
        self.context_manager = get_context_manager()
        
        # ç›‘å¬å™¨è·Ÿè¸ª
        self.active_monitors = set()
        
        logger.debug("ğŸš€ åˆå§‹åŒ–ExecutionEngine")
    
    async def start_engine(self):
        """å¯åŠ¨æ‰§è¡Œå¼•æ“"""
        if self.is_running:
            logger.warning("æ‰§è¡Œå¼•æ“å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        logger.trace("å·¥ä½œæµæ‰§è¡Œå¼•æ“å¯åŠ¨")
        
        # å‘ä¸‹å…¼å®¹æ£€æŸ¥ - ç¡®ä¿context_managerå·²æ­£ç¡®åˆå§‹åŒ–
        if self.context_manager is None:
            logger.error("ä¸Šä¸‹æ–‡ç®¡ç†å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
            raise RuntimeError("ä¸Šä¸‹æ–‡ç®¡ç†å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
        
        # æ³¨å†Œä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å›è°ƒ
        self.context_manager.register_completion_callback(self._on_nodes_ready_to_execute)
        
        # æ³¨å†Œä¸ºAgentTaskServiceçš„å›è°ƒç›‘å¬å™¨
        logger.info(f"ğŸ”— [EXECUTION-ENGINE] æ³¨å†ŒAgentä»»åŠ¡å®Œæˆå›è°ƒç›‘å¬å™¨")
        logger.info(f"   - æ‰§è¡ŒæœåŠ¡å®ä¾‹: {self}")
        logger.info(f"   - æ³¨å†Œå‰å›è°ƒæ•°é‡: {len(agent_task_service.completion_callbacks)}")
        
        agent_task_service.register_completion_callback(self)
        
        logger.info(f"   - æ³¨å†Œåå›è°ƒæ•°é‡: {len(agent_task_service.completion_callbacks)}")
        logger.info(f"   - å›è°ƒåˆ—è¡¨: {[str(cb) for cb in agent_task_service.completion_callbacks]}")
        logger.info("âœ… å·²æ³¨å†ŒAgentä»»åŠ¡å›è°ƒç›‘å¬å™¨")
        
        # å¯åŠ¨ä»»åŠ¡å¤„ç†åç¨‹
        asyncio.create_task(self._process_execution_queue())
        asyncio.create_task(self._monitor_running_instances())
    
    async def stop_engine(self):
        """åœæ­¢æ‰§è¡Œå¼•æ“"""
        self.is_running = False
        
        # åœæ­¢èµ„æºæ¸…ç†ç®¡ç†å™¨
        if self.resource_cleanup_manager:
            await self.resource_cleanup_manager.stop_manager()
        
        logger.trace("å·¥ä½œæµæ‰§è¡Œå¼•æ“åœæ­¢")
    
    async def execute_workflow(self, request: WorkflowExecuteRequest, 
                             executor_id: uuid.UUID) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµ - è§£è€¦é‡æ„ç‰ˆæœ¬ï¼ˆä¿®å¤é”è¶…æ—¶é—®é¢˜ï¼‰"""
        
        # ğŸ” [è°ƒè¯•] å‚æ•°ç±»å‹æ£€æŸ¥
        logger.info(f"ğŸ” [UUIDè°ƒè¯•] execute_workflowå…¥å£å‚æ•°æ£€æŸ¥:")
        logger.info(f"ğŸ” [UUIDè°ƒè¯•] request.workflow_base_idç±»å‹: {type(request.workflow_base_id)}, å€¼: {request.workflow_base_id}")
        logger.info(f"ğŸ” [UUIDè°ƒè¯•] executor_idç±»å‹: {type(executor_id)}, å€¼: {executor_id}")
        
        # ğŸ”§ ä¿®å¤é”è¶…æ—¶ï¼šç¼©å°äº‹åŠ¡èŒƒå›´ï¼ŒåªåŒ…å«æ•°æ®åˆ›å»º
        workflow_data = None
        async with self.workflow_instance_repo.db.transaction() as conn:
            try:
                logger.trace(f"ğŸ”„ [ç¼–æ’å™¨] å¼€å§‹å·¥ä½œæµç¼–æ’: {request.workflow_base_id}")
                
                # 1. å¹‚ç­‰æ€§æ£€æŸ¥
                existing = await self._check_workflow_idempotency(
                    conn, request, executor_id
                )
                if existing:
                    return existing
                
                # 2. æ•°æ®å±‚ï¼šåˆ›å»ºå·¥ä½œæµæ•°æ®ï¼ˆçº¯æ•°æ®æ“ä½œ - å¿«é€Ÿå®Œæˆï¼‰
                workflow_data = await self._create_workflow_data(
                    conn, request, executor_id
                )
                
                logger.trace(f"âœ… [ç¼–æ’å™¨] æ•°æ®å±‚åˆ›å»ºå®Œæˆï¼Œäº‹åŠ¡å³å°†æäº¤")
                
                # äº‹åŠ¡åœ¨æ­¤å¤„è‡ªåŠ¨æäº¤ï¼Œé‡Šæ”¾é”
                
            except Exception as e:
                logger.error(f"âŒ [ç¼–æ’å™¨] æ•°æ®åˆ›å»ºå¤±è´¥: {e}")
                import traceback
                logger.error(f"   - é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                raise
        
        # 3. ä¸Šä¸‹æ–‡å±‚ï¼šæ³¨å†Œæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆäº‹åŠ¡å¤– - é¿å…é•¿æ—¶é—´æŒé”ï¼‰
        try:
            await self._register_execution_context(workflow_data)
            logger.trace(f"âœ… [ç¼–æ’å™¨] å·¥ä½œæµç¼–æ’å®Œæˆ")
            return workflow_data
        except Exception as e:
            logger.error(f"âŒ [ç¼–æ’å™¨] ä¸Šä¸‹æ–‡æ³¨å†Œå¤±è´¥: {e}")
            # æ•°æ®å·²åˆ›å»ºæˆåŠŸï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            workflow_data['message'] += f" (æ³¨æ„: ä¸Šä¸‹æ–‡æ³¨å†Œå¤±è´¥ï¼Œå¯èƒ½å½±å“è‡ªåŠ¨æ‰§è¡Œ: {str(e)})"
            return workflow_data
    
    async def _check_workflow_idempotency(self, conn, request: WorkflowExecuteRequest, 
                                        executor_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """å¹‚ç­‰æ€§æ£€æŸ¥ - çº¯æŸ¥è¯¢é€»è¾‘"""
        logger.trace(f"ğŸ” [å¹‚ç­‰æ£€æŸ¥] æ£€æŸ¥é‡å¤æ‰§è¡Œ")
        
        existing_check_query = """
            SELECT workflow_instance_id, status, workflow_instance_name, created_at
            FROM `workflow_instance` 
            WHERE workflow_base_id = %s 
            AND executor_id = %s 
            AND workflow_instance_name = %s
            AND status IN ('RUNNING', 'PENDING')
            AND is_deleted = FALSE
            ORDER BY created_at DESC
            LIMIT 1
        """
        
        existing = await conn.fetchrow(
            existing_check_query, 
            request.workflow_base_id, 
            executor_id, 
            request.workflow_instance_name
        )
        
        if existing:
            logger.info(f"ğŸ”„ [å¹‚ç­‰æ£€æŸ¥] å‘ç°å·²å­˜åœ¨çš„è¿è¡Œä¸­å®ä¾‹: {existing['workflow_instance_id']}")
            return {
                'instance_id': existing['workflow_instance_id'],
                'status': 'already_running',
                'message': f'å·¥ä½œæµå®ä¾‹ "{request.workflow_instance_name}" å·²åœ¨è¿è¡Œä¸­',
                'existing_instance': {
                    'id': existing['workflow_instance_id'],
                    'name': existing['workflow_instance_name'],
                    'status': existing['status'],
                    'created_at': existing['created_at']
                }
            }
        
        return None
    
    async def _create_workflow_data(self, conn, request: WorkflowExecuteRequest, 
                                  executor_id: uuid.UUID) -> Dict[str, Any]:
        """æ•°æ®å±‚ï¼šçº¯æ•°æ®åˆ›å»ºæ“ä½œ"""
        from ..models.instance import NodeInstanceStatus
        from ..models.node import NodeType
        import uuid, json
        from ..utils.helpers import now_utc
        
        logger.trace(f"ğŸ—ï¸ [æ•°æ®å±‚] å¼€å§‹åˆ›å»ºå·¥ä½œæµæ•°æ®")
        
        # 1. éªŒè¯å·¥ä½œæµ
        workflow = await self.workflow_repo.get_workflow_by_base_id(request.workflow_base_id)
        if not workflow:
            raise ValueError("å·¥ä½œæµä¸å­˜åœ¨")
        workflow_id = workflow['workflow_id']
        
        # 2. åˆ›å»ºå·¥ä½œæµå®ä¾‹
        instance_id = uuid.uuid4()
        create_instance_query = """
            INSERT INTO `workflow_instance` 
            (workflow_instance_id, workflow_id, workflow_base_id, executor_id, workflow_instance_name, 
             input_data, context_data, status, created_at, is_deleted)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        await conn.execute(
            create_instance_query,
            instance_id, workflow_id, request.workflow_base_id, executor_id, 
            request.workflow_instance_name,
            json.dumps(request.input_data or {}), json.dumps(request.context_data or {}),
            'RUNNING', now_utc(), False
        )
        
        # 3. æ‰¹é‡åˆ›å»ºèŠ‚ç‚¹å®ä¾‹å’Œå¯¹åº”çš„ä»»åŠ¡å®ä¾‹
        nodes = await self._get_workflow_nodes_by_version_id(workflow_id)
        if not nodes:
            raise ValueError(f"å·¥ä½œæµ {workflow_id} æ²¡æœ‰èŠ‚ç‚¹")
        
        node_instances = []
        start_nodes_count = 0
        created_tasks_count = 0
        
        # ğŸ”§ Linuså¼ä¿®å¤: å¯¼å…¥é™„ä»¶æœåŠ¡
        from ..services.file_association_service import FileAssociationService
        file_service = FileAssociationService()
        
        for node in nodes:
            node_instance_id = uuid.uuid4()
            
            # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
            create_node_query = """
                INSERT INTO `node_instance`
                (node_instance_id, workflow_instance_id, node_id, node_base_id, 
                 node_instance_name, task_description, status, input_data, output_data,
                 error_message, retry_count, created_at, is_deleted)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            await conn.execute(
                create_node_query,
                node_instance_id, instance_id, node['node_id'], node['node_base_id'],
                f"{node['name']}_instance", node.get('task_description', ''),
                NodeInstanceStatus.PENDING.value, json.dumps({}), json.dumps({}),
                None, 0, now_utc(), False
            )
            
            # ğŸ”§ Critical Fix: åˆ›å»ºèŠ‚ç‚¹å®ä¾‹åç«‹å³ç»§æ‰¿é™„ä»¶
            try:
                await file_service.inherit_node_files_to_instance(
                    node_id=uuid.UUID(node['node_id']), 
                    node_instance_id=node_instance_id
                )
            except Exception as e:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹å®ä¾‹ {node_instance_id} é™„ä»¶ç»§æ‰¿å¤±è´¥: {e}")
            
            node_instances.append({
                'node_instance_id': node_instance_id,
                'node_id': node['node_id'],
                'node_name': node['name'],
                'node_type': node['type']
            })
            
            if node['type'] == NodeType.START.value:
                start_nodes_count += 1
            
            # ğŸ”§ ä¿®å¤Critical Bug: ä¸è¦åœ¨æ­¤å¤„åˆ›å»ºä»»åŠ¡å®ä¾‹ï¼
            # ä»»åŠ¡å®ä¾‹åº”è¯¥åªåœ¨èŠ‚ç‚¹å‡†å¤‡æ‰§è¡Œæ—¶æ‰åˆ›å»ºï¼Œè€Œä¸æ˜¯åœ¨å·¥ä½œæµåˆ›å»ºæ—¶å…¨éƒ¨åˆ›å»º
            # è¿™æ ·å¯ä»¥ç¡®ä¿åªæœ‰æ»¡è¶³ä¾èµ–å…³ç³»çš„èŠ‚ç‚¹æ‰ä¼šæœ‰åˆ†é…çš„ä»»åŠ¡
            
            # æ³¨é‡Šæ‰åŸæ¥çš„ä»»åŠ¡åˆ›å»ºé€»è¾‘ï¼Œæ”¹ä¸ºèŠ‚ç‚¹æ‰§è¡Œæ—¶å†åˆ›å»º
            # TODO: åœ¨èŠ‚ç‚¹è§¦å‘æ—¶åŠ¨æ€åˆ›å»ºä»»åŠ¡å®ä¾‹
        
        logger.trace(f"âœ… [æ•°æ®å±‚] åˆ›å»ºå®Œæˆ: å®ä¾‹={instance_id}, èŠ‚ç‚¹={len(node_instances)}, ä»»åŠ¡={created_tasks_count}")
        
        return {
            'workflow_instance_id': instance_id,
            'workflow_id': workflow_id,
            'workflow_name': workflow.get('name', 'Unknown'),
            'workflow_instance_name': request.workflow_instance_name,
            'status': 'RUNNING',
            'executor_id': executor_id,
            'nodes_count': len(nodes),
            'tasks_count': created_tasks_count,
            'start_nodes_count': start_nodes_count,
            'created_at': now_utc().isoformat(),
            'node_instances': node_instances,
            'message': f'å·¥ä½œæµå®ä¾‹ "{request.workflow_instance_name}" åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(nodes)} ä¸ªèŠ‚ç‚¹ï¼Œ{created_tasks_count} ä¸ªä»»åŠ¡'
        }
    
    async def _register_execution_context(self, workflow_data: Dict[str, Any]):
        """ä¸Šä¸‹æ–‡å±‚ï¼šçº¯æ‰§è¡Œé€»è¾‘æ³¨å†Œ"""
        instance_id = workflow_data['workflow_instance_id']
        
        try:
            logger.trace(f"ğŸ”— [ä¸Šä¸‹æ–‡å±‚] å¼€å§‹æ³¨å†Œæ‰§è¡Œä¸Šä¸‹æ–‡: {instance_id}")
            
            # 1. åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            await self.context_manager.initialize_workflow_context(instance_id)
            
            # 2. è·å–ä¸Šä¸‹æ–‡å®ä¾‹
            from ..services.workflow_execution_context import get_context_manager
            context_manager = get_context_manager()
            workflow_context = await context_manager.get_or_create_context(instance_id)
            
            # 3. ä½¿ç”¨å·²æœ‰æ•°æ®æ³¨å†ŒèŠ‚ç‚¹ä¾èµ–ï¼ˆæ— éœ€æŸ¥è¯¢æ•°æ®åº“ï¼‰
            registered_count = 0
            start_nodes_triggered = 0
            
            for node_instance in workflow_data['node_instances']:
                node_instance_id = node_instance['node_instance_id']
                node_id = node_instance['node_id']
                node_type = node_instance['node_type']
                
                # è·å–ä¸Šæ¸¸èŠ‚ç‚¹ï¼ˆåŸºäºnode_idæŸ¥è¯¢ï¼‰ - ğŸ”§ ä¿®å¤å‚æ•°é¡ºåº
                upstream_node_instances = await self._get_upstream_node_instances(
                    node_id, instance_id
                )
                
                # æ³¨å†Œåˆ°ä¸Šä¸‹æ–‡
                await workflow_context.register_node_dependencies(
                    node_instance_id=node_instance_id,
                    node_id=node_id,
                    upstream_nodes=upstream_node_instances
                )
                
                registered_count += 1
                if node_type == 'START':
                    start_nodes_triggered += 1
                
                logger.trace(f"âœ… [ä¸Šä¸‹æ–‡å±‚] æ³¨å†ŒèŠ‚ç‚¹: {node_instance['node_name']}")
            
            logger.trace(f"ğŸ“Š [ä¸Šä¸‹æ–‡å±‚] æ³¨å†Œå®Œæˆ: {registered_count} ä¸ªèŠ‚ç‚¹")
            
            # 4. è§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹
            triggered_nodes = await workflow_context.scan_and_trigger_ready_nodes()
            
            expected_start_nodes = workflow_data['start_nodes_count']
            if triggered_nodes:
                logger.trace(f"ğŸš€ [ä¸Šä¸‹æ–‡å±‚] æˆåŠŸè§¦å‘ {len(triggered_nodes)} ä¸ªèŠ‚ç‚¹")
                logger.trace(f"   - é¢„æœŸSTARTèŠ‚ç‚¹: {expected_start_nodes}")
                logger.trace(f"   - è§¦å‘çš„èŠ‚ç‚¹: {triggered_nodes}")
                
                # ğŸ”§ ä¿®å¤Critical Bug: å°†è§¦å‘çš„èŠ‚ç‚¹å®é™…æäº¤æ‰§è¡Œ
                for node_instance_id in triggered_nodes:
                    try:
                        await self._execute_node_with_new_context(workflow_context, node_instance_id)
                        logger.trace(f"âœ… [æ‰§è¡Œæäº¤] å¯åŠ¨èŠ‚ç‚¹æ‰§è¡Œ: {node_instance_id}")
                    except Exception as e:
                        logger.error(f"âŒ [æ‰§è¡Œæäº¤] å¯åŠ¨èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥ {node_instance_id}: {e}")
                        
            else:
                logger.warning(f"âš ï¸ [ä¸Šä¸‹æ–‡å±‚] æœªè§¦å‘ä»»ä½•èŠ‚ç‚¹")
                logger.warning(f"   - é¢„æœŸSTARTèŠ‚ç‚¹: {expected_start_nodes}")  
                logger.warning(f"   - æ³¨å†Œçš„èŠ‚ç‚¹: {registered_count}")
                
                # è°ƒè¯•ä¿¡æ¯
                logger.trace(f"ğŸ” [è°ƒè¯•] ä¸Šä¸‹æ–‡çŠ¶æ€:")
                logger.trace(f"   - èŠ‚ç‚¹ä¾èµ–æ•°é‡: {len(workflow_context.node_dependencies)}")
                for node_id, deps in workflow_context.node_dependencies.items():
                    ready = deps.get('ready_to_execute', False)
                    upstream = deps.get('upstream_nodes', [])
                    logger.trace(f"   - èŠ‚ç‚¹ {node_id}: ready={ready}, upstream={len(upstream)}")
                
        except Exception as e:
            logger.error(f"âŒ [ä¸Šä¸‹æ–‡å±‚] æ³¨å†Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"   - è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œæ•°æ®å·²åˆ›å»ºæˆåŠŸ
    
    async def _get_workflow_nodes_by_version_id(self, workflow_id: uuid.UUID) -> List[Dict[str, Any]]:
        """é€šè¿‡å·¥ä½œæµç‰ˆæœ¬IDè·å–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆæœ¬ - ä½¿ç”¨å½“å‰ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        logger.debug(f"ğŸ” [èŠ‚ç‚¹æŸ¥è¯¢] æ­£åœ¨æŸ¥è¯¢å·¥ä½œæµç‰ˆæœ¬ {workflow_id} çš„èŠ‚ç‚¹...")
        try:
            # é¦–å…ˆè·å–workflow_base_idï¼Œç„¶åæŸ¥è¯¢å½“å‰ç‰ˆæœ¬çš„èŠ‚ç‚¹
            workflow_query = """
                SELECT workflow_base_id 
                FROM workflow 
                WHERE workflow_id = $1 AND is_deleted = FALSE
            """
            workflow_result = await self.node_repo.db.fetch_one(workflow_query, workflow_id)
            
            if not workflow_result:
                logger.error(f"å·¥ä½œæµç‰ˆæœ¬ä¸å­˜åœ¨: {workflow_id}")
                return []
            
            workflow_base_id = workflow_result['workflow_base_id']
            logger.trace(f"å·¥ä½œæµç‰ˆæœ¬ {workflow_id} å¯¹åº”çš„base_id: {workflow_base_id}")
            
            # æŸ¥è¯¢å½“å‰ç‰ˆæœ¬çš„æ‰€æœ‰èŠ‚ç‚¹ï¼ˆé¿å…ç¬›å¡å°”ç§¯é—®é¢˜ï¼‰
            query = """
                SELECT n.*
                FROM "node" n
                WHERE n.workflow_base_id = $1
                AND n.is_current_version = TRUE
                AND n.is_deleted = FALSE
                ORDER BY n.created_at ASC
            """
            results = await self.node_repo.db.fetch_all(query, workflow_base_id)
            logger.trace(f"âœ… é€šè¿‡base_id {workflow_base_id} è·å–å½“å‰ç‰ˆæœ¬èŠ‚ç‚¹ {len(results)} ä¸ª")

            # ä¸ºæ¯ä¸ªèŠ‚ç‚¹å•ç‹¬æŸ¥è¯¢å¤„ç†å™¨ä¿¡æ¯ï¼ˆé¿å…é‡å¤èŠ‚ç‚¹è®°å½•ï¼‰
            nodes_with_processors = []
            for node_result in results:
                node_dict = dict(node_result)

                # æŸ¥è¯¢è¯¥èŠ‚ç‚¹çš„å¤„ç†å™¨
                processor_query = """
                    SELECT processor_id FROM node_processor
                    WHERE node_id = $1 AND is_deleted = FALSE
                """
                processors = await self.node_repo.db.fetch_all(processor_query, node_dict['node_id'])

                # å¦‚æœæœ‰å¤„ç†å™¨ï¼Œå–ç¬¬ä¸€ä¸ªï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                if processors:
                    node_dict['processor_id'] = processors[0]['processor_id']
                else:
                    node_dict['processor_id'] = None

                nodes_with_processors.append(node_dict)

            results = nodes_with_processors
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å½“å‰ç‰ˆæœ¬èŠ‚ç‚¹ï¼Œå°è¯•ç›´æ¥ç”¨workflow_idæŸ¥è¯¢
            if not results:
                logger.warning(f"é€šè¿‡base_idæœªæ‰¾åˆ°èŠ‚ç‚¹ï¼Œå°è¯•ç›´æ¥æŸ¥è¯¢workflow_id: {workflow_id}")
                fallback_query = """
                    SELECT n.*
                    FROM "node" n
                    WHERE n.workflow_id = $1
                    AND n.is_deleted = FALSE
                    ORDER BY n.created_at ASC
                """
                fallback_results = await self.node_repo.db.fetch_all(fallback_query, workflow_id)
                logger.trace(f"âœ… é€šè¿‡workflow_id {workflow_id} fallbackæŸ¥è¯¢è·å–åˆ° {len(fallback_results)} ä¸ªèŠ‚ç‚¹")

                # ä¸ºfallbackç»“æœä¹Ÿå•ç‹¬æŸ¥è¯¢å¤„ç†å™¨ä¿¡æ¯
                nodes_with_processors = []
                for node_result in fallback_results:
                    node_dict = dict(node_result)

                    # æŸ¥è¯¢è¯¥èŠ‚ç‚¹çš„å¤„ç†å™¨
                    processor_query = """
                        SELECT processor_id FROM node_processor
                        WHERE node_id = $1 AND is_deleted = FALSE
                    """
                    processors = await self.node_repo.db.fetch_all(processor_query, node_dict['node_id'])

                    # å¦‚æœæœ‰å¤„ç†å™¨ï¼Œå–ç¬¬ä¸€ä¸ªï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                    if processors:
                        node_dict['processor_id'] = processors[0]['processor_id']
                    else:
                        node_dict['processor_id'] = None

                    nodes_with_processors.append(node_dict)

                results = nodes_with_processors
            
            return results
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµèŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def _check_running_instances(self, workflow_base_id: uuid.UUID, executor_id: uuid.UUID) -> List[Dict]:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰æ­£åœ¨è¿è¡Œçš„å·¥ä½œæµå®ä¾‹"""
        try:
            from ..models.instance import WorkflowInstanceStatus
            
            # æŸ¥è¯¢æ­£åœ¨è¿è¡Œçš„å·¥ä½œæµå®ä¾‹
            query = """
            SELECT wi.*
            FROM workflow_instance wi
            WHERE wi.workflow_base_id = $1
            AND wi.executor_id = $2
            AND wi.status IN ('RUNNING', 'PAUSED')
            AND wi.is_deleted = FALSE
            ORDER BY wi.created_at DESC
            """
            
            running_instances = await self.workflow_instance_repo.db.fetch_all(
                query, workflow_base_id, executor_id
            )
            
            logger.trace(f"æ‰¾åˆ° {len(running_instances)} ä¸ªæ­£åœ¨è¿è¡Œçš„å·¥ä½œæµå®ä¾‹")
            return running_instances
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥è¿è¡Œå®ä¾‹å¤±è´¥: {e}")
            return [] 
    async def _get_node_processors(self, node_id: uuid.UUID):
        """è·å–èŠ‚ç‚¹çš„å¤„ç†å™¨åˆ—è¡¨ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨å…·ä½“node_idï¼‰"""
        try:
            logger.debug(f"ğŸ” [å¤„ç†å™¨æŸ¥è¯¢] æ­£åœ¨æŸ¥è¯¢èŠ‚ç‚¹ {node_id} çš„å¤„ç†å™¨ç»‘å®š...")
            
            query = """
                SELECT np.*, p.name as processor_name, p.type as processor_type,
                       u.username, a.agent_name, p.user_id, p.agent_id
                FROM node_processor np
                JOIN processor p ON p.processor_id = np.processor_id AND p.is_deleted = FALSE
                LEFT JOIN "user" u ON u.user_id = p.user_id AND u.is_deleted = FALSE
                LEFT JOIN agent a ON a.agent_id = p.agent_id AND a.is_deleted = FALSE
                WHERE np.node_id = $1
                ORDER BY np.created_at ASC
            """
            results = await self.processor_repo.db.fetch_all(query, node_id)
            
            logger.debug(f"ğŸ” [å¤„ç†å™¨æŸ¥è¯¢] èŠ‚ç‚¹ {node_id} æŸ¥è¯¢ç»“æœ:")
            logger.debug(f"   - æ‰¾åˆ°å¤„ç†å™¨æ•°é‡: {len(results)}")
            
            if not results:
                # è¿›ä¸€æ­¥è¯Šæ–­ï¼šæ£€æŸ¥node_processorè¡¨ä¸­æ˜¯å¦æœ‰è¯¥èŠ‚ç‚¹çš„è®°å½•
                diagnostic_query = "SELECT COUNT(*) as count FROM node_processor WHERE node_id = $1"
                diagnostic_result = await self.processor_repo.db.fetch_one(diagnostic_query, node_id)
                total_records = diagnostic_result['count'] if diagnostic_result else 0
                
                logger.warning(f"ğŸš¨ [å¤„ç†å™¨æŸ¥è¯¢] èŠ‚ç‚¹ {node_id} æœªæ‰¾åˆ°å¤„ç†å™¨:")
                logger.warning(f"   - node_processorè¡¨ä¸­è¯¥èŠ‚ç‚¹è®°å½•æ•°: {total_records}")
                logger.warning(f"   - å¯èƒ½åŸå› : 1)èŠ‚ç‚¹æœªç»‘å®šå¤„ç†å™¨ 2)node_idä¸åŒ¹é… 3)å¤„ç†å™¨è¢«åˆ é™¤")
                
                # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
                node_check_query = "SELECT node_id, name, node_base_id FROM node WHERE node_id = $1"
                node_check = await self.processor_repo.db.fetch_one(node_check_query, node_id)
                if node_check:
                    logger.warning(f"   - èŠ‚ç‚¹å­˜åœ¨: {node_check['name']} (base_id: {node_check['node_base_id']})")
                else:
                    logger.warning(f"   - èŠ‚ç‚¹ä¸å­˜åœ¨äºnodeè¡¨ä¸­!")
            else:
                for i, result in enumerate(results):
                    logger.debug(f"   - å¤„ç†å™¨{i+1}: {result.get('processor_name')} (ç±»å‹: {result.get('processor_type')})")
            
            return results
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹å¤„ç†å™¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def _get_next_nodes(self, node_id: uuid.UUID):
        """è·å–èŠ‚ç‚¹çš„ä¸‹æ¸¸èŠ‚ç‚¹ï¼ˆæ”¯æŒæ¡ä»¶è¾¹ï¼‰"""
        try:
            # ä¿®æ”¹æŸ¥è¯¢ä»¥è·å–è¿æ¥ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¡ä»¶é…ç½®
            query = """
                SELECT
                    nc.to_node_id,
                    nc.connection_type,
                    nc.condition_config,
                    tn.node_base_id as to_node_base_id,
                    tn.name as to_node_name,
                    tn.type as to_node_type
                FROM node_connection nc
                JOIN node tn ON tn.node_id = nc.to_node_id
                WHERE nc.from_node_id = $1
                  AND tn.is_deleted = FALSE
                ORDER BY nc.created_at ASC
            """
            results = await self.node_repo.db.fetch_all(query, node_id)

            connections = []
            for result in results:
                connection = {
                    'to_node_id': result['to_node_id'],
                    'to_node_base_id': result['to_node_base_id'],
                    'to_node_name': result['to_node_name'],
                    'to_node_type': result['to_node_type'],
                    'connection_type': result['connection_type'] or 'normal',
                    'condition_config': {}
                }

                # è§£ææ¡ä»¶é…ç½®
                if result['condition_config']:
                    try:
                        if isinstance(result['condition_config'], str):
                            import json
                            connection['condition_config'] = json.loads(result['condition_config'])
                        else:
                            connection['condition_config'] = result['condition_config']
                    except (json.JSONDecodeError, TypeError) as e:
                        logger.warning(f"è§£ææ¡ä»¶é…ç½®å¤±è´¥: {e}")
                        connection['condition_config'] = {}

                connections.append(connection)

            logger.debug(f"è·å–èŠ‚ç‚¹ {node_id} çš„ä¸‹æ¸¸è¿æ¥: {len(connections)} ä¸ª")
            return connections

        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹ä¸‹æ¸¸è¿æ¥å¤±è´¥: {e}")
            return []
    
    def _determine_task_type(self, processor_type: str) -> TaskInstanceType:
        """æ ¹æ®å¤„ç†å™¨ç±»å‹ç¡®å®šä»»åŠ¡ç±»å‹"""
        processor_type_upper = processor_type.upper() if processor_type else ""
        
        if processor_type_upper == "HUMAN":
            return TaskInstanceType.HUMAN
        elif processor_type_upper == "AGENT":
            return TaskInstanceType.AGENT
        elif processor_type_upper == "MIX" or processor_type_upper == "MIXED":
            return TaskInstanceType.MIXED
        else:
            # è®°å½•è°ƒè¯•ä¿¡æ¯
            logger.warning(f"æœªçŸ¥çš„å¤„ç†å™¨ç±»å‹: '{processor_type}' (è½¬æ¢å: '{processor_type_upper}')ï¼Œé»˜è®¤ä¸ºäººå·¥ä»»åŠ¡")
            return TaskInstanceType.HUMAN  # é»˜è®¤ä¸ºäººå·¥ä»»åŠ¡
    
    def _determine_task_priority(self, task_type: TaskInstanceType, node_data: Dict[str, Any]) -> int:
        """ç¡®å®šä»»åŠ¡ä¼˜å…ˆçº§ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™æ–¹æ³•é¿å…è°ƒç”¨é”™è¯¯ï¼‰"""
        try:
            # ä¼˜å…ˆçº§å­—æ®µå·²åºŸå¼ƒï¼Œè¿”å›é»˜è®¤å€¼
            return 1
                
        except Exception as e:
            logger.warning(f"ç¡®å®šä»»åŠ¡ä¼˜å…ˆçº§å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            return 1
    
    def _determine_task_duration(self, task_type: TaskInstanceType, node_data: Dict[str, Any]) -> int:
        """æ ¹æ®ä»»åŠ¡ç±»å‹å’ŒèŠ‚ç‚¹é…ç½®ç¡®å®šé¢„ä¼°æ‰§è¡Œæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
        try:
            # ä»èŠ‚ç‚¹æ•°æ®ä¸­è·å–é¢„ä¼°æ—¶é—´é…ç½®
            node_duration = node_data.get('estimated_duration', None)
            if node_duration is not None:
                return max(5, min(480, int(node_duration)))  # é™åˆ¶åœ¨5åˆ†é’Ÿåˆ°8å°æ—¶ä¹‹é—´
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®é»˜è®¤é¢„ä¼°æ—¶é—´
            if task_type == TaskInstanceType.HUMAN:
                return 60  # äººå·¥ä»»åŠ¡é»˜è®¤1å°æ—¶
            elif task_type == TaskInstanceType.AGENT:
                return 15  # Agentä»»åŠ¡é»˜è®¤15åˆ†é’Ÿ
            elif task_type == TaskInstanceType.MIXED:
                return 45  # æ··åˆä»»åŠ¡é»˜è®¤45åˆ†é’Ÿ
            else:
                return 30  # é»˜è®¤30åˆ†é’Ÿ
                
        except Exception as e:
            logger.warning(f"ç¡®å®šä»»åŠ¡é¢„ä¼°æ—¶é—´å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            return 30
    
    
    async def _process_execution_queue(self):
        """å¤„ç†æ‰§è¡Œé˜Ÿåˆ—"""
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–æ‰§è¡Œé¡¹ç›®
                execution_item = await asyncio.wait_for(
                    self.execution_queue.get(), timeout=1.0
                )
                
                # å¤„ç†æ‰§è¡Œé¡¹ç›®
                await self._process_workflow_step(execution_item)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"å¤„ç†æ‰§è¡Œé˜Ÿåˆ—å¤±è´¥: {e}")
                await asyncio.sleep(1)
    
    async def _process_workflow_step(self, execution_item: Dict[str, Any]):
        """å¤„ç†å·¥ä½œæµæ­¥éª¤"""
        try:
            instance_id = execution_item['instance_id']
            workflow_id = execution_item['workflow_id']
            current_nodes = execution_item['current_nodes']
            
            logger.trace(f"å¤„ç†å·¥ä½œæµå®ä¾‹ {instance_id} çš„èŠ‚ç‚¹: {current_nodes}")
            
            # å¤„ç†å½“å‰èŠ‚ç‚¹
            next_nodes = []
            for node_id in current_nodes:
                node_result = await self._process_node(instance_id, workflow_id, node_id)
                if node_result.get('next_nodes'):
                    next_nodes.extend(node_result['next_nodes'])
            
            # å¦‚æœæœ‰ä¸‹ä¸€æ­¥èŠ‚ç‚¹ï¼Œç»§ç»­æ‰§è¡Œ
            if next_nodes:
                execution_item['current_nodes'] = next_nodes
                await self.execution_queue.put(execution_item)
            else:
                # å·¥ä½œæµå®Œæˆ
                await self._complete_workflow(instance_id)
                
        except Exception as e:
            logger.error(f"å¤„ç†å·¥ä½œæµæ­¥éª¤å¤±è´¥: {e}")
            await self._fail_workflow(execution_item['instance_id'], str(e))
    
    async def _process_node(self, instance_id: uuid.UUID, workflow_id: uuid.UUID, 
                          node_id: uuid.UUID) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªèŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨å…·ä½“çš„node_idï¼‰"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥é€šè¿‡node_idè·å–èŠ‚ç‚¹ä¿¡æ¯
            node = await self.node_repo.get_node_by_id(node_id)
            if not node:
                raise ValueError(f"èŠ‚ç‚¹ {node_id} ä¸å­˜åœ¨")
            
            node_type = node['type']
            logger.trace(f"å¤„ç†èŠ‚ç‚¹: {node['name']} (ç±»å‹: {node_type}, ID: {node_id})")
            
            if node_type == NodeType.START.value:
                # å¼€å§‹èŠ‚ç‚¹ç›´æ¥å®Œæˆ
                return await self._handle_start_node(instance_id, workflow_id, node_id)
            elif node_type == NodeType.END.value:
                # ç»“æŸèŠ‚ç‚¹
                return await self._handle_end_node(instance_id, workflow_id, node_id)
            elif node_type == NodeType.PROCESSOR.value:
                # å¤„ç†å™¨èŠ‚ç‚¹
                return await self._handle_processor_node(instance_id, workflow_id, node_id)
            else:
                logger.warning(f"æœªçŸ¥èŠ‚ç‚¹ç±»å‹: {node_type}")
                return {'next_nodes': []}
                
        except Exception as e:
            logger.error(f"å¤„ç†èŠ‚ç‚¹å¤±è´¥: {e}")
            raise
    
    
    
    
    async def _execute_task(self, task: Dict[str, Any]):
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        try:
            task_id = task['task_instance_id']
            task_type = task['task_type']
            
            logger.trace(f"æ‰§è¡Œä»»åŠ¡: {task['task_title']} (ç±»å‹: {task_type})")
            
            if task_type == TaskInstanceType.HUMAN.value:
                # äººå·¥ä»»åŠ¡ï¼šæ›´æ–°çŠ¶æ€ä¸ºå·²åˆ†é…ï¼Œç­‰å¾…äººå·¥å¤„ç†
                logger.trace(f"ğŸ‘¤ å¤„ç†äººå·¥ä»»åŠ¡: {task['task_title']}")
                logger.trace(f"   - ä»»åŠ¡ID: {task_id}")
                logger.trace(f"   - åˆ†é…ç›®æ ‡ç”¨æˆ·: {task.get('assigned_user_id')}")
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æœ‰åˆ†é…çš„ç”¨æˆ·
                assigned_user_id = task.get('assigned_user_id')
                if not assigned_user_id:
                    logger.warning(f"âš ï¸  äººå·¥ä»»åŠ¡æ²¡æœ‰åˆ†é…ç”¨æˆ·ï¼Œä»»åŠ¡å°†ä¿æŒPENDINGçŠ¶æ€")
                    logger.warning(f"   - ä»»åŠ¡ID: {task_id}")
                    logger.warning(f"   - ä»»åŠ¡æ ‡é¢˜: {task['task_title']}")
                    logger.warning(f"   - å»ºè®®: è¯·ä¸ºè¯¥ä»»åŠ¡çš„å¤„ç†å™¨é…ç½®ç”¨æˆ·")
                    return
                
                # ä»»åŠ¡åˆ›å»ºæ—¶å·²ç»è®¾ç½®äº†æ­£ç¡®çš„çŠ¶æ€ï¼Œè¿™é‡Œä¸éœ€è¦å†æ›´æ–°
                # ï¼ˆä»»åŠ¡åˆ›å»ºæ—¶å¦‚æœæœ‰assigned_user_idï¼ŒçŠ¶æ€å°±æ˜¯ASSIGNEDï¼‰
                logger.trace(f"   âœ… ä»»åŠ¡å·²å¤„äºæ­£ç¡®çŠ¶æ€ï¼Œæ— éœ€æ›´æ–°")
                
                # è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯ç”¨äºé€šçŸ¥
                task_title = task.get('task_title', 'æœªå‘½åä»»åŠ¡')
                workflow_name = task.get('workflow_name', 'æœªå‘½åå·¥ä½œæµ')
                estimated_duration = task.get('estimated_duration', 30)
                
                logger.trace(f"ğŸ“‹ äººå·¥ä»»åŠ¡åˆ†é…è¯¦æƒ…:")
                logger.trace(f"   - ä»»åŠ¡æ ‡é¢˜: {task_title}")
                logger.trace(f"   - å·¥ä½œæµ: {workflow_name}")
                logger.trace(f"   - åˆ†é…ç»™ç”¨æˆ·: {assigned_user_id}")
                logger.trace(f"   - é¢„ä¼°æ—¶é•¿: {estimated_duration}åˆ†é’Ÿ")
                logger.trace(f"   - ä»»åŠ¡æè¿°: {task.get('task_description', 'æ— æè¿°')[:100]}...")
                
                # å®æ—¶é€šçŸ¥ç”¨æˆ·æœ‰æ–°ä»»åŠ¡ - é‡è¦æ”¹è¿›ï¼
                try:
                    await self._notify_user_new_task(assigned_user_id, task_id, task_title)
                    logger.trace(f"   ğŸ“¬ ç”¨æˆ·é€šçŸ¥å·²å‘é€")
                except Exception as e:
                    logger.error(f"   âŒ å‘é€ç”¨æˆ·é€šçŸ¥å¤±è´¥: {e}")
                
                # è®°å½•ä»»åŠ¡åˆ†é…äº‹ä»¶ï¼ˆç”¨äºåç»­åˆ†æå’Œç›‘æ§ï¼‰
                await self._log_task_assignment_event(task_id, assigned_user_id, task_title)
                
                # è®°å½•åˆ°æ§åˆ¶å°ç”¨äºè°ƒè¯•
                print(f"\nğŸ¯ ã€ä»»åŠ¡æ¨é€ã€‘ æ–°çš„äººå·¥ä»»åŠ¡å·²åˆ†é…:")
                print(f"   ç”¨æˆ·ID: {assigned_user_id}")
                print(f"   ä»»åŠ¡ID: {task_id}")
                print(f"   ä»»åŠ¡æ ‡é¢˜: {task_title}")
                print(f"   å·¥ä½œæµ: {workflow_name}")
                print(f"   æ—¶é—´: {now_utc().strftime('%Y-%m-%d %H:%M:%S')}")
                print("=" * 60)
                
            elif task_type == TaskInstanceType.AGENT.value:
                # Agentä»»åŠ¡ï¼šè°ƒç”¨AIå¤„ç†
                await self._process_agent_task(task)
                
            elif task_type == TaskInstanceType.MIXED.value:
                # æ··åˆä»»åŠ¡ï¼šåŒæ—¶æäº¤ç»™äººå·¥å’ŒAgentå¤„ç†
                await self._process_mixed_task(task)
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def _process_agent_task(self, task: Dict[str, Any]):
        """å¤„ç†Agentä»»åŠ¡ - é›†æˆAgentTaskService"""
        try:
            task_id = task['task_instance_id']
            logger.trace(f"é›†æˆAgentä»»åŠ¡æœåŠ¡å¤„ç†ä»»åŠ¡: {task['task_title']}")
            
            # æ³¨å†Œä»»åŠ¡å®Œæˆå›è°ƒ
            callback_future = asyncio.Future()
            self.task_callbacks[task_id] = callback_future
            
            # æäº¤ä»»åŠ¡åˆ°AgentTaskServiceè¿›è¡Œå¤„ç†
            result = await agent_task_service.submit_task_to_agent(task_id)
            
            if result['status'] == 'queued':
                logger.trace(f"Agentä»»åŠ¡ {task_id} å·²æäº¤åˆ°æœåŠ¡é˜Ÿåˆ—")
                
                # ç­‰å¾…ä»»åŠ¡å¤„ç†å®Œæˆï¼ˆé€šè¿‡å›è°ƒæœºåˆ¶ï¼‰
                try:
                    await asyncio.wait_for(callback_future, timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    logger.trace(f"Agentä»»åŠ¡ {task_id} é€šè¿‡å›è°ƒæœºåˆ¶å®Œæˆ")
                except asyncio.TimeoutError:
                    logger.error(f"Agentä»»åŠ¡ {task_id} å¤„ç†è¶…æ—¶")
                    raise TimeoutError("Agentä»»åŠ¡å¤„ç†è¶…æ—¶")
                finally:
                    # æ¸…ç†å›è°ƒ
                    self.task_callbacks.pop(task_id, None)
                
            else:
                logger.warning(f"Agentä»»åŠ¡æäº¤å¤±è´¥: {result}")
                # æ¸…ç†å›è°ƒ
                self.task_callbacks.pop(task_id, None)
                raise RuntimeError(f"Agentä»»åŠ¡æäº¤å¤±è´¥: {result}")
            
        except Exception as e:
            logger.error(f"å¤„ç†Agentä»»åŠ¡å¤±è´¥: {e}")
            # æ¸…ç†å›è°ƒ
            self.task_callbacks.pop(task.get('task_instance_id'), None)
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            fail_update = TaskInstanceUpdate(
                status=TaskInstanceStatus.FAILED,
                error_message=str(e)
            )
            await self.task_instance_repo.update_task(task['task_instance_id'], fail_update)
            raise
    
    async def on_task_completed(self, task_id: uuid.UUID, result: Dict[str, Any]):
        """ä»»åŠ¡å®Œæˆå›è°ƒå¤„ç†"""
        try:
            logger.trace(f"æ”¶åˆ°ä»»åŠ¡å®Œæˆå›è°ƒ: {task_id}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç­‰å¾…çš„å›è°ƒ
            if task_id in self.task_callbacks:
                callback_future = self.task_callbacks[task_id]
                if not callback_future.done():
                    callback_future.set_result(result)
                    logger.trace(f"ä»»åŠ¡ {task_id} å›è°ƒå·²è§¦å‘")
            else:
                # è¿™æ˜¯æ­£å¸¸æƒ…å†µï¼šä»»åŠ¡å®Œæˆæ—¶ç­‰å¾…çš„Futureå¯èƒ½å·²ç»è¢«å¤„ç†å¹¶æ¸…ç†
                logger.debug(f"ä»»åŠ¡ {task_id} å®Œæˆï¼Œä½†å›è°ƒå·²è¢«å¤„ç†ï¼ˆæ­£å¸¸æƒ…å†µï¼‰")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯ä»¥ä¾¿æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
            task_info = await self.task_instance_repo.get_task_by_id(task_id)
            if task_info:
                workflow_instance_id = task_info['workflow_instance_id']
                node_instance_id = task_info['node_instance_id']
                
                logger.trace(f"ğŸ¯ Agentä»»åŠ¡å®Œæˆï¼Œæ›´æ–°èŠ‚ç‚¹çŠ¶æ€: workflow={workflow_instance_id}, node_instance={node_instance_id}")
                
                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node_query = """
                SELECT n.node_id, n.name
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.node_instance_id = $1
                """
                node_info = await self.task_instance_repo.db.fetch_one(node_query, node_instance_id)
                
                if node_info:
                    # ä½¿ç”¨WorkflowContextManageræ ‡è®°èŠ‚ç‚¹å®Œæˆ
                    output_data = {
                        'task_result': result.get('result', ''),
                        'task_summary': result.get('message', ''),
                        'execution_time': result.get('duration', 0),
                        'completion_time': datetime.utcnow().isoformat()
                    }
                    
                    await self.context_manager.mark_node_completed(
                        workflow_instance_id=workflow_instance_id,
                        node_id=node_info['node_id'],
                        node_instance_id=node_instance_id,
                        output_data=output_data
                    )
                    
                    logger.trace(f"âœ… Agentä»»åŠ¡å®ŒæˆåèŠ‚ç‚¹çŠ¶æ€æ›´æ–°å®Œæˆ")
                else:
                    logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹ä¿¡æ¯: node_instance_id={node_instance_id}")
            else:
                logger.error(f"æ— æ³•è·å–ä»»åŠ¡ä¿¡æ¯: task_id={task_id}")
                
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å®Œæˆå›è°ƒå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def on_task_failed(self, task_id: uuid.UUID, error_message: str):
        """ä»»åŠ¡å¤±è´¥å›è°ƒå¤„ç†"""
        try:
            logger.trace(f"æ”¶åˆ°ä»»åŠ¡å¤±è´¥å›è°ƒ: {task_id} - {error_message}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç­‰å¾…çš„å›è°ƒ
            if task_id in self.task_callbacks:
                callback_future = self.task_callbacks[task_id]
                if not callback_future.done():
                    callback_future.set_exception(RuntimeError(error_message))
                    logger.trace(f"ä»»åŠ¡ {task_id} å¤±è´¥å›è°ƒå·²è§¦å‘")
            else:
                # è¿™æ˜¯æ­£å¸¸æƒ…å†µï¼šä»»åŠ¡å¤±è´¥æ—¶ç­‰å¾…çš„Futureå¯èƒ½å·²ç»è¢«å¤„ç†å¹¶æ¸…ç†
                logger.debug(f"ä»»åŠ¡ {task_id} å¤±è´¥ï¼Œä½†å›è°ƒå·²è¢«å¤„ç†ï¼ˆæ­£å¸¸æƒ…å†µï¼‰")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯ä»¥ä¾¿æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ä¸ºå¤±è´¥
            task_info = await self.task_instance_repo.get_task_by_id(task_id)
            if task_info:
                workflow_instance_id = task_info['workflow_instance_id']
                node_instance_id = task_info['node_instance_id']
                
                logger.trace(f"âŒ Agentä»»åŠ¡å¤±è´¥ï¼Œæ ‡è®°èŠ‚ç‚¹å¤±è´¥: workflow={workflow_instance_id}, node_instance={node_instance_id}")
                
                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node_query = """
                SELECT n.node_id, n.name
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.node_instance_id = $1
                """
                node_info = await self.task_instance_repo.db.fetch_one(node_query, node_instance_id)
                
                if node_info:
                    # ä½¿ç”¨WorkflowContextManageræ ‡è®°èŠ‚ç‚¹å¤±è´¥
                    await self.context_manager.mark_node_failed(
                        workflow_instance_id=workflow_instance_id,
                        node_id=node_info['node_id'],
                        node_instance_id=node_instance_id,
                        error_info={'error': error_message}
                    )
                    
                    logger.trace(f"âŒ Agentä»»åŠ¡å¤±è´¥åèŠ‚ç‚¹çŠ¶æ€æ›´æ–°å®Œæˆ")
                else:
                    logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹ä¿¡æ¯: node_instance_id={node_instance_id}")
            else:
                logger.error(f"æ— æ³•è·å–ä»»åŠ¡ä¿¡æ¯: task_id={task_id}")
                
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥å›è°ƒå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _process_mixed_task(self, task: Dict[str, Any]):
        """å¤„ç†æ··åˆä»»åŠ¡ - äººæœºåä½œ"""
        try:
            task_id = task['task_instance_id']
            logger.trace(f"å¤„ç†æ··åˆä»»åŠ¡: {task['task_title']}")
            
            # 1. é¦–å…ˆåˆ†é…ç»™äººå·¥ç”¨æˆ·å¤„ç†
            human_update = TaskInstanceUpdate(status=TaskInstanceStatus.ASSIGNED)
            await self.task_instance_repo.update_task(task_id, human_update)
            logger.trace(f"æ··åˆä»»åŠ¡ {task_id} å·²åˆ†é…ç»™äººå·¥ç”¨æˆ·")
            
            # 2. åŒæ—¶æäº¤ç»™AgentæœåŠ¡è·å–AIå»ºè®®ï¼ˆä¸é˜»å¡ï¼‰
            try:
                # åˆ›å»ºAIå»ºè®®ä»»åŠ¡çš„å‰¯æœ¬æ•°æ®
                ai_suggestion_task = task.copy()
                ai_suggestion_task['task_title'] = f"[AIå»ºè®®] {task['task_title']}"
                ai_suggestion_task['task_description'] = f"ä¸ºäººå·¥ä»»åŠ¡æä¾›AIå»ºè®®: {task['task_description']}"
                
                # å¼‚æ­¥æäº¤åˆ°AgentæœåŠ¡è·å–å»ºè®®
                asyncio.create_task(self._provide_ai_assistance(task_id, ai_suggestion_task))
                logger.trace(f"ä¸ºæ··åˆä»»åŠ¡ {task_id} å¯åŠ¨AIååŠ©")
                
            except Exception as e:
                logger.warning(f"å¯åŠ¨AIååŠ©å¤±è´¥ï¼Œç»§ç»­äººå·¥å¤„ç†: {e}")
            
            # 3. æ··åˆä»»åŠ¡ä¸»è¦ç­‰å¾…äººå·¥å®Œæˆï¼ŒAIå»ºè®®ä½œä¸ºè¾…åŠ©
            logger.trace(f"æ··åˆä»»åŠ¡ {task_id} è¿›å…¥äººæœºåä½œæ¨¡å¼")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ··åˆä»»åŠ¡å¤±è´¥: {e}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            fail_update = TaskInstanceUpdate(
                status=TaskInstanceStatus.FAILED,
                error_message=str(e)
            )
            await self.task_instance_repo.update_task(task['task_instance_id'], fail_update)
            raise
    
    async def _provide_ai_assistance(self, original_task_id: uuid.UUID, ai_task: Dict[str, Any]):
        """ä¸ºäººå·¥ä»»åŠ¡æä¾›AIååŠ©å»ºè®®"""
        try:
            logger.trace(f"ä¸ºä»»åŠ¡ {original_task_id} ç”ŸæˆAIå»ºè®®")
            
            # è°ƒç”¨AgentTaskServiceç”ŸæˆAIå»ºè®®
            ai_result = await agent_task_service.process_agent_task(original_task_id)
            
            # å°†AIå»ºè®®å­˜å‚¨åˆ°åŸä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¸­
            if ai_result['status'] == TaskInstanceStatus.COMPLETED.value:
                ai_suggestions = {
                    'ai_analysis': ai_result['result'],
                    'suggestions_generated_at': now_utc().isoformat(),
                    'confidence_score': ai_result['result'].get('confidence_score', 0.8),
                    'ai_recommendations': ai_result['result'].get('recommendations', [])
                }
                
                # æ›´æ–°åŸä»»åŠ¡ï¼Œæ·»åŠ AIå»ºè®®åˆ°ä¸Šä¸‹æ–‡
                update_data = TaskInstanceUpdate(
                    context_data={'ai_assistance': ai_suggestions}
                )
                await self.task_instance_repo.update_task(original_task_id, update_data)
                
                logger.trace(f"AIå»ºè®®å·²æ·»åŠ åˆ°ä»»åŠ¡ {original_task_id} çš„ä¸Šä¸‹æ–‡ä¸­")
            
        except Exception as e:
            logger.warning(f"ç”ŸæˆAIååŠ©å»ºè®®å¤±è´¥: {e}")
            # AIååŠ©å¤±è´¥ä¸å½±å“ä¸»ä»»åŠ¡
    
    async def _complete_workflow(self, instance_id: uuid.UUID):
        """å®Œæˆå·¥ä½œæµ"""
        try:
            logger.trace(f"ğŸ å¼€å§‹å®Œæˆå·¥ä½œæµ: {instance_id}")
            
            # 1. ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡ºæ‘˜è¦
            logger.trace(f"ğŸ“Š ç”Ÿæˆå·¥ä½œæµè¾“å‡ºæ‘˜è¦")
            try:
                from .output_data_processor import OutputDataProcessor
                output_processor = OutputDataProcessor()
                
                # ç”Ÿæˆè¾“å‡ºæ‘˜è¦
                output_summary = await output_processor.generate_workflow_output_summary(instance_id)
                if output_summary:
                    logger.trace(f"âœ… å·¥ä½œæµè¾“å‡ºæ‘˜è¦ç”ŸæˆæˆåŠŸ")
                    
                    # å‡†å¤‡ç»“æ„åŒ–è¾“å‡ºæ•°æ®
                    summary_dict = output_summary.dict()
                    execution_summary = {
                        "execution_result": summary_dict.get("execution_result"),
                        "execution_stats": summary_dict.get("execution_stats")
                    }
                    quality_metrics = summary_dict.get("quality_metrics")
                    data_lineage = summary_dict.get("data_lineage")
                    
                    # è®¾ç½®åŸºç¡€è¾“å‡ºæ•°æ®
                    basic_output_data = {
                        'message': 'å·¥ä½œæµæ‰§è¡Œå®Œæˆ',
                        'completion_time': datetime.utcnow().isoformat(),
                        'result_type': summary_dict.get("execution_result", {}).get("result_type", "success")
                    }
                    
                    # å¦‚æœæœ‰å…·ä½“çš„ä¸šåŠ¡è¾“å‡ºæ•°æ®ï¼Œæ·»åŠ åˆ°åŸºç¡€è¾“å‡ºä¸­
                    if summary_dict.get("execution_result", {}).get("data_output"):
                        basic_output_data['workflow_results'] = summary_dict["execution_result"]["data_output"]
                    
                    logger.trace(f"ğŸ’¾ æ›´æ–°å·¥ä½œæµå®ä¾‹çŠ¶æ€å’Œè¾“å‡ºæ•°æ®")
                    
                    # 2. æ›´æ–°å·¥ä½œæµå®ä¾‹çŠ¶æ€ä¸ºå·²å®Œæˆï¼ŒåŒ…å«ç»“æ„åŒ–è¾“å‡º
                    update_data = WorkflowInstanceUpdate(
                        status=WorkflowInstanceStatus.COMPLETED,
                        output_data=basic_output_data,
                        execution_summary=execution_summary,
                        quality_metrics=quality_metrics,
                        data_lineage=data_lineage,
                        output_summary=output_summary
                    )
                    
                else:
                    logger.warning(f"âš ï¸ å·¥ä½œæµè¾“å‡ºæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€è¾“å‡ºæ•°æ®")
                    # ä½¿ç”¨åŸºç¡€è¾“å‡ºæ•°æ®
                    update_data = WorkflowInstanceUpdate(
                        status=WorkflowInstanceStatus.COMPLETED,
                        output_data={
                            'message': 'å·¥ä½œæµæ‰§è¡Œå®Œæˆ',
                            'completion_time': datetime.utcnow().isoformat(),
                            'result_type': 'success'
                        }
                    )
                    
            except Exception as output_error:
                logger.error(f"âŒ ç”Ÿæˆè¾“å‡ºæ‘˜è¦å¼‚å¸¸: {output_error}")
                import traceback
                logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                
                # å³ä½¿è¾“å‡ºæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¹Ÿè¦å®Œæˆå·¥ä½œæµ
                update_data = WorkflowInstanceUpdate(
                    status=WorkflowInstanceStatus.COMPLETED,
                    output_data={
                        'message': 'å·¥ä½œæµæ‰§è¡Œå®Œæˆ',
                        'completion_time': datetime.utcnow().isoformat(),
                        'result_type': 'success',
                        'note': 'è¾“å‡ºæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½†å·¥ä½œæµæ­£å¸¸å®Œæˆ'
                    }
                )
            
            # 3. æ›´æ–°æ•°æ®åº“
            await self.workflow_instance_repo.update_instance(instance_id, update_data)
            
            # 4. ä»è¿è¡Œå®ä¾‹ä¸­ç§»é™¤
            self.running_instances.pop(instance_id, None)
            
            logger.trace(f"âœ… å·¥ä½œæµå®ä¾‹ {instance_id} æ‰§è¡Œå®Œæˆ")
            logger.trace(f"ğŸ“‹ å·¥ä½œæµå®Œæˆç»Ÿè®¡:")
            logger.trace(f"   - å®ä¾‹ID: {instance_id}")
            logger.trace(f"   - å®Œæˆæ—¶é—´: {datetime.utcnow().isoformat()}")
            logger.trace(f"   - è¾“å‡ºæ‘˜è¦: {'å·²ç”Ÿæˆ' if 'output_summary' in locals() and output_summary else 'ç”Ÿæˆå¤±è´¥'}")
            
        except Exception as e:
            logger.error(f"âŒ å®Œæˆå·¥ä½œæµå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def _fail_workflow(self, instance_id: uuid.UUID, error_message: str):
        """å·¥ä½œæµæ‰§è¡Œå¤±è´¥"""
        try:
            # æ›´æ–°å·¥ä½œæµå®ä¾‹çŠ¶æ€ä¸ºå¤±è´¥
            update_data = WorkflowInstanceUpdate(
                status=WorkflowInstanceStatus.FAILED,
                error_message=error_message
            )
            await self.workflow_instance_repo.update_instance(instance_id, update_data)
            
            # ä»è¿è¡Œå®ä¾‹ä¸­ç§»é™¤
            self.running_instances.pop(instance_id, None)
            
            logger.error(f"å·¥ä½œæµå®ä¾‹ {instance_id} æ‰§è¡Œå¤±è´¥: {error_message}")
            
        except Exception as e:
            logger.error(f"æ ‡è®°å·¥ä½œæµå¤±è´¥çŠ¶æ€å¤±è´¥: {e}")
    
    async def _monitor_running_instances(self):
        """ç›‘æ§è¿è¡Œä¸­çš„å®ä¾‹"""
        while self.is_running:
            try:
                # æ£€æŸ¥è¶…æ—¶çš„å®ä¾‹
                for instance_id, execution_item in list(self.running_instances.items()):
                    # è¿™é‡Œå¯ä»¥æ·»åŠ è¶…æ—¶æ£€æŸ¥é€»è¾‘
                    pass
                            
                await asyncio.sleep(15)  # æ¯15ç§’æ£€æŸ¥ä¸€æ¬¡ - ä¼˜åŒ–ä¸ºæ›´é¢‘ç¹
                
            except Exception as e:
                logger.error(f"ç›‘æ§è¿è¡Œå®ä¾‹å¤±è´¥: {e}")
                await asyncio.sleep(10)

    async def pause_workflow(self, instance_id: uuid.UUID) -> bool:
        """æš‚åœå·¥ä½œæµ"""
        try:
            update_data = WorkflowInstanceUpdate(status=WorkflowInstanceStatus.PAUSED)
            result = await self.workflow_instance_repo.update_instance(instance_id, update_data)
            
            if result:
                logger.trace(f"å·¥ä½œæµå®ä¾‹ {instance_id} å·²æš‚åœ")
                return True
            return False
            
        except Exception as e:
            logger.error(f"æš‚åœå·¥ä½œæµå¤±è´¥: {e}")
            return False
    
    async def resume_workflow(self, instance_id: uuid.UUID) -> bool:
        """æ¢å¤å·¥ä½œæµ"""
        try:
            update_data = WorkflowInstanceUpdate(status=WorkflowInstanceStatus.RUNNING)
            result = await self.workflow_instance_repo.update_instance(instance_id, update_data)
            
            if result:
                logger.trace(f"å·¥ä½œæµå®ä¾‹ {instance_id} å·²æ¢å¤")
                return True
            return False
            
        except Exception as e:
            logger.error(f"æ¢å¤å·¥ä½œæµå¤±è´¥: {e}")
            return False
    
    async def cancel_workflow(self, instance_id: uuid.UUID) -> bool:
        """å–æ¶ˆå·¥ä½œæµ"""
        try:
            logger.trace(f"ğŸš« å¼€å§‹å–æ¶ˆå·¥ä½œæµå®ä¾‹: {instance_id}")
            
            # é¦–å…ˆæ£€æŸ¥å·¥ä½œæµå®ä¾‹æ˜¯å¦å­˜åœ¨
            instance = await self.workflow_instance_repo.get_instance_by_id(instance_id)
            if not instance:
                logger.error(f"âŒ å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {instance_id}")
                return False
                
            logger.trace(f"ğŸ“‹ æ‰¾åˆ°å·¥ä½œæµå®ä¾‹: {instance.get('workflow_instance_name', 'æœªå‘½å')}")
            logger.trace(f"   - å½“å‰çŠ¶æ€: {instance.get('status')}")
            logger.trace(f"   - æ‰§è¡Œè€…: {instance.get('executor_id')}")
            logger.trace(f"   - åˆ›å»ºæ—¶é—´: {instance.get('created_at')}")
            
            # 1. å–æ¶ˆæ­£åœ¨è¿è¡Œçš„å¼‚æ­¥ä»»åŠ¡
            logger.trace(f"ğŸ¯ æ­¥éª¤1: å–æ¶ˆæ­£åœ¨è¿è¡Œçš„å¼‚æ­¥ä»»åŠ¡")
            try:
                await self._cancel_running_tasks(instance_id)
                logger.trace(f"âœ… å¼‚æ­¥ä»»åŠ¡å–æ¶ˆå®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ å–æ¶ˆå¼‚æ­¥ä»»åŠ¡å¤±è´¥: {e}")
            
            # èµ„æºæ¸…ç†å’ŒçŠ¶æ€åŒæ­¥
            logger.trace(f"ğŸ¯ æ­¥éª¤2: æ¸…ç†å®ä¾‹çŠ¶æ€")
            try:
                # ä½¿ç”¨æ–°çš„ç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¸…ç†çŠ¶æ€
                if hasattr(self.context_manager, 'cleanup_workflow_context'):
                    await self.context_manager.cleanup_workflow_context(instance_id)
                    logger.trace(f"âœ… å·²ä»ç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ç§»é™¤å·¥ä½œæµ: {instance_id}")
                else:
                    logger.trace(f"   - ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸æ”¯æŒcleanupæ–¹æ³•")
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†å®ä¾‹çŠ¶æ€å¤±è´¥: {e}")
            
            # 3. æ›´æ–°æ•°æ®åº“çŠ¶æ€
            logger.trace(f"ğŸ¯ æ­¥éª¤3: æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºCANCELLED")
            try:
                update_data = WorkflowInstanceUpdate(status=WorkflowInstanceStatus.CANCELLED)
                logger.trace(f"   - å‡†å¤‡æ›´æ–°æ•°æ®: {update_data}")
                result = await self.workflow_instance_repo.update_instance(instance_id, update_data)
                logger.trace(f"   - æ•°æ®åº“æ›´æ–°ç»“æœ: {result}")
                
                if result:
                    logger.trace(f"âœ… æ•°æ®åº“çŠ¶æ€æ›´æ–°æˆåŠŸ")
                    
                    # ä»è¿è¡Œå®ä¾‹ä¸­ç§»é™¤
                    logger.trace(f"ğŸ¯ æ­¥éª¤4: ä»è¿è¡Œå®ä¾‹åˆ—è¡¨ä¸­ç§»é™¤")
                    if hasattr(self, 'running_instances') and instance_id in self.running_instances:
                        self.running_instances.pop(instance_id, None)
                        logger.trace(f"   - å·²ä»è¿è¡Œå®ä¾‹åˆ—è¡¨ä¸­ç§»é™¤")
                    else:
                        logger.trace(f"   - å®ä¾‹ä¸åœ¨è¿è¡Œåˆ—è¡¨ä¸­")
                    
                    # 5. é€šçŸ¥ç›¸å…³æœåŠ¡
                    logger.trace(f"ğŸ¯ æ­¥éª¤5: é€šçŸ¥ç›¸å…³æœåŠ¡")
                    try:
                        await self._notify_services_workflow_cancelled(instance_id)
                        logger.trace(f"âœ… æœåŠ¡é€šçŸ¥å®Œæˆ")
                    except Exception as e:
                        logger.error(f"âŒ é€šçŸ¥æœåŠ¡å¤±è´¥: {e}")
                    
                    logger.trace(f"âœ… å·¥ä½œæµå®ä¾‹ {instance_id} å·²æˆåŠŸå–æ¶ˆ (æ‰€æœ‰æ­¥éª¤å®Œæˆ)")
                    return True
                else:
                    logger.error(f"âŒ æ›´æ–°å·¥ä½œæµçŠ¶æ€å¤±è´¥: {instance_id}")
                    return False
            except Exception as e:
                logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¼‚å¸¸: {e}")
                import traceback
                logger.error(f"   - æ›´æ–°å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ å–æ¶ˆå·¥ä½œæµå¤±è´¥: {e}")
            import traceback
            logger.error(f"   - å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return False
    
    async def get_workflow_status(self, instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        try:
            instance = await self.workflow_instance_repo.get_instance_by_id(instance_id)
            if not instance:
                return None
            
            # è·å–æ‰§è¡Œç»Ÿè®¡
            stats = await self.workflow_instance_repo.get_execution_statistics(instance_id)
            
            return {
                'instance': instance,
                'statistics': stats,
                'is_running': instance_id in self.running_instances
            }
            
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")
            return None
    
    # =============================================================================
    # æ–°å¢ï¼šä¾èµ–ç­‰å¾…å’Œä¸Šä¸‹æ–‡ç®¡ç†æ–¹æ³•
    # =============================================================================
    
    async def _create_tasks_for_nodes(self, created_nodes: List[Dict], workflow_instance_id: uuid.UUID):
        """ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡å®ä¾‹ - ç»Ÿä¸€ä½¿ç”¨æ–°æ¶æ„"""
        logger.info(f"ğŸ”§ [ç»Ÿä¸€æ¶æ„] å¼€å§‹ä¸º {len(created_nodes)} ä¸ªèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡å®ä¾‹")
        
        task_creation_count = 0
        for i, created_node in enumerate(created_nodes, 1):
            logger.info(f"ğŸ“‹ [ç»Ÿä¸€æ¶æ„] å¤„ç†èŠ‚ç‚¹ {i}/{len(created_nodes)}: {created_node.get('node_data', {}).get('name', 'æœªçŸ¥èŠ‚ç‚¹')}")
            logger.info(f"   èŠ‚ç‚¹ç±»å‹: {created_node['node_type']}")
            logger.info(f"   èŠ‚ç‚¹å®ä¾‹ID: {created_node['node_instance_id']}")
            
            if created_node['node_type'] == NodeType.PROCESSOR.value:
                node_data = created_node['node_data']
                
                # ç›´æ¥è°ƒç”¨æ–°æ¶æ„çš„ä»»åŠ¡åˆ›å»ºæ–¹æ³•
                node_data_for_creation = {
                    'node_id': node_data['node_id'],
                    'name': node_data['name'],
                    'task_description': node_data.get('task_description') or node_data.get('description'),
                    'type': created_node['node_type'],
                    'input_data': node_data.get('input_data', {})
                }
                
                try:
                    await self._create_tasks_for_node_new_context(
                        node_data_for_creation,
                        created_node['node_instance_id'],
                        workflow_instance_id
                    )
                    
                    task_creation_count += 1
                    logger.info(f"âœ… [ç»Ÿä¸€æ¶æ„] èŠ‚ç‚¹ä»»åŠ¡åˆ›å»ºå®Œæˆ!")
                    
                except Exception as e:
                    logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„] ä»»åŠ¡å®ä¾‹åˆ›å»ºå¼‚å¸¸: {e}")
                    import traceback
                    logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    # ç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹
            else:
                logger.info(f"â„¹ï¸ [ç»Ÿä¸€æ¶æ„] è·³è¿‡éPROCESSORèŠ‚ç‚¹: {created_node['node_type']}")
                
        logger.info(f"ğŸ‰ [ç»Ÿä¸€æ¶æ„] æ‰€æœ‰èŠ‚ç‚¹ä»»åŠ¡åˆ›å»ºå®Œæˆï¼Œå…±åˆ›å»º {task_creation_count} ä¸ªä»»åŠ¡")
    
    
    
    async def _resume_workflow_execution(self, workflow_instance_id: uuid.UUID):
        """æ¢å¤å·¥ä½œæµæ‰§è¡Œï¼Œæ£€æŸ¥å¹¶è§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹"""
        try:
            logger.trace(f"ğŸ”„ å¼€å§‹æ¢å¤å·¥ä½œæµæ‰§è¡Œ: {workflow_instance_id}")
            
            # ğŸ”’ é¦–å…ˆæ£€æŸ¥å·¥ä½œæµçŠ¶æ€ï¼Œé¿å…å¤„ç†å·²å–æ¶ˆ/å¤±è´¥çš„å·¥ä½œæµ
            workflow_status_query = """
            SELECT status FROM workflow_instance 
            WHERE workflow_instance_id = $1 AND is_deleted = FALSE
            """
            workflow_status_result = await self.workflow_instance_repo.db.fetch_one(
                workflow_status_query, workflow_instance_id
            )
            
            if not workflow_status_result:
                logger.warning(f"âš ï¸ [æ¢å¤æ‰§è¡Œ] å·¥ä½œæµ {workflow_instance_id} ä¸å­˜åœ¨ï¼Œåœæ­¢æ¢å¤")
                return
                
            workflow_status = workflow_status_result['status']
            if workflow_status.lower() in ['cancelled', 'failed', 'completed']:
                logger.trace(f"ğŸš« [æ¢å¤æ‰§è¡Œ] å·¥ä½œæµ {workflow_instance_id} çŠ¶æ€ä¸º {workflow_status}ï¼Œè·³è¿‡æ¢å¤")
                return
            
            # æŸ¥æ‰¾æ‰€æœ‰pendingçŠ¶æ€çš„èŠ‚ç‚¹
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_instance_repo = NodeInstanceRepository()
            
            pending_query = """
            SELECT ni.*, n.type as node_type, n.name as node_name
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = $1
            AND ni.status IN ('pending', 'PENDING', 'waiting', 'WAITING')
            AND ni.is_deleted = FALSE
            AND n.is_deleted = FALSE
            ORDER BY ni.created_at ASC
            """
            
            pending_nodes = await node_instance_repo.db.fetch_all(pending_query, workflow_instance_id)
            logger.trace(f"æ‰¾åˆ° {len(pending_nodes)} ä¸ªå¾…å¤„ç†çŠ¶æ€çš„èŠ‚ç‚¹ (pending/waiting)")
            
            if pending_nodes:
                for node in pending_nodes:
                    node_name = node.get('node_name', 'æœªçŸ¥')
                    node_type = node.get('node_type', 'æœªçŸ¥')
                    logger.trace(f"  - å¾…å¤„ç†èŠ‚ç‚¹: {node_name} (ç±»å‹: {node_type}, çŠ¶æ€: {node.get('status', 'æœªçŸ¥')})")
                
                # ç¡®ä¿å·²å®Œæˆçš„STARTèŠ‚ç‚¹å·²é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                await self._ensure_completed_start_nodes_notified(workflow_instance_id)
                
                # æ£€æŸ¥è¿™äº›èŠ‚ç‚¹çš„ä¾èµ–æ˜¯å¦å·²æ»¡è¶³ï¼Œå¦‚æœæ»¡è¶³åˆ™è§¦å‘æ‰§è¡Œ
                logger.trace(f"æ£€æŸ¥pendingèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»")
                triggered_count = await self._check_and_trigger_ready_nodes(workflow_instance_id, pending_nodes)
                logger.trace(f"è§¦å‘äº† {triggered_count} ä¸ªå‡†å¤‡å°±ç»ªçš„èŠ‚ç‚¹")
            else:
                logger.trace(f"æ²¡æœ‰æ‰¾åˆ°pendingçŠ¶æ€çš„èŠ‚ç‚¹ï¼Œå·¥ä½œæµå¯èƒ½å·²å®Œæˆæˆ–å‡ºç°å¼‚å¸¸")
                
        except Exception as e:
            logger.error(f"æ¢å¤å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
    
    
    async def _check_and_trigger_ready_nodes(self, workflow_instance_id: uuid.UUID, pending_nodes: List[Dict]) -> int:
        """æ£€æŸ¥å¹¶è§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹ï¼Œè¿”å›è§¦å‘çš„èŠ‚ç‚¹æ•°é‡"""
        triggered_count = 0
        try:
            logger.trace(f"æ£€æŸ¥ {len(pending_nodes)} ä¸ªpendingèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»")
            
            for node in pending_nodes:
                node_instance_id = node['node_instance_id']
                node_name = node.get('node_name', 'æœªçŸ¥')
                
                # æ£€æŸ¥èŠ‚ç‚¹ä¾èµ–æ˜¯å¦æ»¡è¶³
                if await self._check_node_dependencies_satisfied(workflow_instance_id, node_instance_id):
                    logger.trace(f"âœ… èŠ‚ç‚¹ {node_name} çš„ä¾èµ–å·²æ»¡è¶³ï¼Œè§¦å‘æ‰§è¡Œ")
                    # è·å–å·¥ä½œæµä¸Šä¸‹æ–‡å¹¶æ‰§è¡ŒèŠ‚ç‚¹
                    from .workflow_execution_context import get_context_manager
                    context_manager = get_context_manager()
                    workflow_context = await context_manager.get_context(workflow_instance_id)
                    if workflow_context:
                        await self._execute_node_with_unified_context(workflow_context, workflow_instance_id, node_instance_id)
                    triggered_count += 1
                else:
                    logger.trace(f"â³ èŠ‚ç‚¹ {node_name} çš„ä¾èµ–å°šæœªæ»¡è¶³ï¼Œç­‰å¾…ä¸­")
            
            return triggered_count
                    
        except Exception as e:
            logger.error(f"æ£€æŸ¥å’Œè§¦å‘å‡†å¤‡å¥½çš„èŠ‚ç‚¹å¤±è´¥: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return triggered_count
    
    # _collect_task_context_data æ–¹æ³•å·²è¢« WorkflowContextManager.get_task_context_data æ›¿æ¢

    async def _check_node_dependencies_satisfied(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹çš„ä¾èµ–æ˜¯å¦å·²æ»¡è¶³ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒè‡ªåŠ¨æ¢å¤ï¼‰"""
        try:
            # ğŸ” ä½¿ç”¨æ–°çš„ get_context æ–¹æ³•ï¼Œè‡ªåŠ¨æ”¯æŒæ¢å¤
            context = await self.context_manager.get_context(workflow_instance_id)
            if not context:
                logger.warning(f"âš ï¸ [ä¾èµ–æ£€æŸ¥] æ— æ³•è·å–æˆ–æ¢å¤å·¥ä½œæµä¸Šä¸‹æ–‡: {workflow_instance_id}")
                return False
            
            # ğŸ” ä¸¥æ ¼æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€ - é˜²æ­¢é‡å¤æ‰§è¡Œ
            node_state = self.context_manager.node_completion_status.get(node_instance_id)
            if node_state in ['EXECUTING', 'COMPLETED']:
                logger.trace(f"ğŸš« [ä¾èµ–æ£€æŸ¥] èŠ‚ç‚¹ {node_instance_id} å†…å­˜çŠ¶æ€ä¸º {node_state}ï¼Œè·³è¿‡è§¦å‘")
                return False
            
            # ğŸ” åŒé‡æ£€æŸ¥ï¼šæ•°æ®åº“çŠ¶æ€éªŒè¯
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_instance_repo = NodeInstanceRepository()
            
            node_info = await node_instance_repo.get_instance_by_id(node_instance_id)
            if node_info and node_info.get('status') in ['running', 'completed']:
                logger.trace(f"ğŸš« [ä¾èµ–æ£€æŸ¥-DB] èŠ‚ç‚¹ {node_instance_id} æ•°æ®åº“çŠ¶æ€ä¸º {node_info.get('status')}ï¼Œè·³è¿‡è§¦å‘")
                return False
            
            # ğŸ” è·å–èŠ‚ç‚¹ä¾èµ–ä¿¡æ¯ï¼ˆä¿®å¤ç‰ˆï¼šæ”¯æŒé™çº§æŸ¥è¯¢ï¼‰
            deps = self.context_manager.get_node_dependency_info(node_instance_id)
            if not deps:
                logger.warning(f"âš ï¸ [ä¾èµ–æ£€æŸ¥] èŠ‚ç‚¹ {node_instance_id} åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­æ²¡æœ‰ä¾èµ–ä¿¡æ¯ï¼Œå°è¯•ä»æ•°æ®åº“æ¢å¤")
                # ğŸ”„ é™çº§ç­–ç•¥ï¼šä»æ•°æ®åº“é‡æ–°æ„å»ºä¾èµ–ä¿¡æ¯
                try:
                    await self._rebuild_node_dependencies_from_db(workflow_instance_id, node_instance_id)
                    
                    # é‡æ–°å°è¯•è·å–ä¾èµ–ä¿¡æ¯
                    deps = self.context_manager.get_node_dependency_info(node_instance_id)
                    if not deps:
                        logger.error(f"âŒ [ä¾èµ–æ£€æŸ¥] æ— æ³•ä»æ•°æ®åº“æ¢å¤èŠ‚ç‚¹ {node_instance_id} çš„ä¾èµ–ä¿¡æ¯")
                        return False
                    else:
                        logger.info(f"âœ… [ä¾èµ–æ£€æŸ¥] æˆåŠŸä»æ•°æ®åº“æ¢å¤èŠ‚ç‚¹ {node_instance_id} çš„ä¾èµ–ä¿¡æ¯")
                except Exception as e:
                    logger.error(f"âŒ [ä¾èµ–æ£€æŸ¥] ä»æ•°æ®åº“æ¢å¤ä¾èµ–ä¿¡æ¯å¤±è´¥: {e}")
                    return False
            
            node_id = deps.get('node_id')
            upstream_nodes = deps.get('upstream_nodes', [])
            
            # ğŸ” å¦‚æœæ²¡æœ‰ä¸Šæ¸¸ä¾èµ–ï¼ˆSTARTèŠ‚ç‚¹ï¼‰ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œ
            if not upstream_nodes:
                # STARTèŠ‚ç‚¹ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ä¸Šä¸‹æ–‡ä¸­æ ‡è®°ä¸ºå®Œæˆ
                context = self.context_manager.contexts[workflow_instance_id]
                if node_instance_id in context.execution_context.get('completed_nodes', set()):
                    logger.trace(f"ğŸš« [ä¾èµ–æ£€æŸ¥] STARTèŠ‚ç‚¹ {node_instance_id} å·²åœ¨ä¸Šä¸‹æ–‡ä¸­å®Œæˆ")
                    return False
                logger.trace(f"âœ… [ä¾èµ–æ£€æŸ¥] STARTèŠ‚ç‚¹ {node_instance_id} æ— ä¾èµ–ï¼Œå¯ä»¥æ‰§è¡Œ")
                return True
            
            # ğŸ” ä¸¥æ ¼æ£€æŸ¥æ‰€æœ‰ä¸Šæ¸¸ä¾èµ–çš„å®ŒæˆçŠ¶æ€
            context = self.context_manager.contexts[workflow_instance_id]
            completed_nodes = context.execution_context.get('completed_nodes', set())
            
            # æ£€æŸ¥æ¯ä¸ªä¸Šæ¸¸èŠ‚ç‚¹æ˜¯å¦éƒ½å·²å®Œæˆ
            all_upstream_completed = True
            completed_count = 0
            
            for upstream_node_id in upstream_nodes:
                # æ£€æŸ¥ä¸Šä¸‹æ–‡çŠ¶æ€
                if upstream_node_id in completed_nodes:
                    completed_count += 1
                    logger.trace(f"  âœ… ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} åœ¨ä¸Šä¸‹æ–‡ä¸­å·²å®Œæˆ")
                else:
                    # åŒé‡æ£€æŸ¥ï¼šéªŒè¯æ•°æ®åº“çŠ¶æ€
                    upstream_query = """
                    SELECT ni.status, ni.node_instance_id, n.name
                    FROM node_instance ni 
                    JOIN node n ON ni.node_id = n.node_id
                    WHERE ni.node_id = $1 
                    AND ni.workflow_instance_id = $2 
                    AND ni.is_deleted = FALSE
                    ORDER BY ni.created_at DESC LIMIT 1
                    """
                    
                    upstream_result = await node_instance_repo.db.fetch_one(
                        upstream_query, upstream_node_id, workflow_instance_id
                    )
                    
                    if upstream_result and upstream_result.get('status') == 'completed':
                        completed_count += 1
                        logger.trace(f"  âœ… ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} åœ¨æ•°æ®åº“ä¸­å·²å®Œæˆ")
                        # åŒæ­¥åˆ°ä¸Šä¸‹æ–‡ï¼ˆä¿®å¤çŠ¶æ€ä¸ä¸€è‡´ï¼‰
                        context['completed_nodes'].add(upstream_node_id)
                    else:
                        all_upstream_completed = False
                        status = upstream_result.get('status') if upstream_result else 'not_found'
                        name = upstream_result.get('name') if upstream_result else 'Unknown'
                        logger.trace(f"  âŒ ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} ({name}) çŠ¶æ€ä¸º {status}ï¼Œä¾èµ–æœªæ»¡è¶³")
                        break
            
            # ğŸ” æœ€ç»ˆä¾èµ–æ£€æŸ¥ç»“æœ
            dependencies_satisfied = all_upstream_completed and completed_count == len(upstream_nodes)
            
            if dependencies_satisfied:
                # å†æ¬¡ç¡®è®¤èŠ‚ç‚¹æœªè¢«æ‰§è¡Œ
                if node_instance_id in context.execution_context.get('completed_nodes', set()):
                    logger.trace(f"ğŸš« [ä¾èµ–æ£€æŸ¥] èŠ‚ç‚¹ {node_instance_id} å·²åœ¨ä¸Šä¸‹æ–‡ä¸­å®Œæˆï¼Œè·³è¿‡")
                    return False
                
                logger.trace(f"âœ… [ä¾èµ–æ£€æŸ¥] èŠ‚ç‚¹ {node_instance_id} æ‰€æœ‰ä¾èµ–å·²æ»¡è¶³ ({completed_count}/{len(upstream_nodes)})")
                return True
            else:
                logger.trace(f"â³ [ä¾èµ–æ£€æŸ¥] èŠ‚ç‚¹ {node_instance_id} ä¾èµ–æœªæ»¡è¶³ ({completed_count}/{len(upstream_nodes)})")
                return False
                
        except Exception as e:
            logger.error(f"âŒ [ä¾èµ–æ£€æŸ¥] æ£€æŸ¥èŠ‚ç‚¹ä¾èµ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
                
    async def _rebuild_node_dependencies_from_db(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """ä»æ•°æ®åº“é‡æ–°æ„å»ºèŠ‚ç‚¹ä¾èµ–ä¿¡æ¯ï¼ˆä¿®å¤æ–¹æ³•ï¼‰"""
        try:
            logger.debug(f"ğŸ”„ [ä¾èµ–é‡å»º] å¼€å§‹ä»æ•°æ®åº“é‡å»ºèŠ‚ç‚¹ {node_instance_id} çš„ä¾èµ–ä¿¡æ¯")
            
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            
            # è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            node_instance = await node_repo.get_instance_by_id(node_instance_id)
            if not node_instance:
                logger.error(f"âŒ [ä¾èµ–é‡å»º] èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨: {node_instance_id}")
                return False
            
            node_id = node_instance['node_id']
            logger.debug(f"  èŠ‚ç‚¹ID: {node_id}")
            
            # æŸ¥è¯¢ä¸Šæ¸¸è¿æ¥å…³ç³»
            upstream_query = """
            SELECT DISTINCT 
                nc.from_node_id as upstream_node_id,
                n.name as upstream_node_name,
                n.type as upstream_node_type
            FROM node_connection nc
            JOIN node n ON nc.from_node_id = n.node_id
            JOIN node_instance ni ON ni.node_id = n.node_id
            WHERE nc.to_node_id = $1 
            AND ni.workflow_instance_id = $2
            AND ni.is_deleted = FALSE
            ORDER BY n.name
            """
            
            upstream_connections = await node_repo.db.fetch_all(
                upstream_query, node_id, workflow_instance_id
            )
            
            logger.debug(f"  æŸ¥è¯¢åˆ° {len(upstream_connections)} ä¸ªä¸Šæ¸¸è¿æ¥")
            
            upstream_node_instance_ids = []
            for upstream in upstream_connections:
                upstream_node_id = upstream['upstream_node_id']
                
                # æŸ¥æ‰¾å¯¹åº”çš„node_instance_id
                instance_query = """
                SELECT node_instance_id 
                FROM node_instance 
                WHERE node_id = $1 AND workflow_instance_id = $2 AND is_deleted = FALSE
                """
                upstream_instance_result = await node_repo.db.fetch_one(
                    instance_query, upstream_node_id, workflow_instance_id
                )
                
                if upstream_instance_result:
                    upstream_node_instance_id = upstream_instance_result['node_instance_id']
                    upstream_node_instance_ids.append(upstream_node_instance_id)
                    logger.debug(f"    ä¸Šæ¸¸èŠ‚ç‚¹: {upstream.get('upstream_node_name', 'Unknown')} (node_id: {upstream_node_id} -> instance_id: {upstream_node_instance_id})")
                else:
                    logger.warning(f"    âš ï¸ æœªæ‰¾åˆ°ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} å¯¹åº”çš„å®ä¾‹")
            
            # é‡æ–°æ³¨å†Œä¾èµ–å…³ç³»
            await self.context_manager.register_node_dependencies(
                workflow_instance_id,
                node_instance_id,
                node_id,
                upstream_node_instance_ids
            )
            
            logger.debug(f"âœ… [ä¾èµ–é‡å»º] æˆåŠŸé‡å»ºèŠ‚ç‚¹ {node_instance_id} çš„ä¾èµ–ä¿¡æ¯: {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹")
            return True
            
        except Exception as e:
            logger.error(f"âŒ [ä¾èµ–é‡å»º] é‡å»ºä¾èµ–ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    
    async def _try_recover_node_context_state(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """å°è¯•æ¢å¤èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡çŠ¶æ€"""
        try:
            logger.debug(f"ğŸ”„ [çŠ¶æ€æ¢å¤] å°è¯•æ¢å¤èŠ‚ç‚¹ {node_instance_id} çš„ä¸Šä¸‹æ–‡çŠ¶æ€")
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ˜¯å¦æœ‰ç”Ÿå‘½å‘¨æœŸä¸€è‡´æ€§æ£€æŸ¥æ–¹æ³•
            if hasattr(self.context_manager, 'ensure_context_lifecycle_consistency'):
                await self.context_manager.ensure_context_lifecycle_consistency(workflow_instance_id)
                logger.debug(f"âœ… [çŠ¶æ€æ¢å¤] å®Œæˆå·¥ä½œæµç”Ÿå‘½å‘¨æœŸä¸€è‡´æ€§æ£€æŸ¥")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡æ–°åˆå§‹åŒ–ä¸Šä¸‹æ–‡
            if workflow_instance_id not in self.context_manager.contexts:
                logger.info(f"ğŸ”„ [çŠ¶æ€æ¢å¤] é‡æ–°åˆå§‹åŒ–å·¥ä½œæµä¸Šä¸‹æ–‡: {workflow_instance_id}")
                await self.context_manager.initialize_workflow_context(workflow_instance_id, restore_from_snapshot=True)
                
                # é‡æ–°æ³¨å†Œä¾èµ–å…³ç³»
                await self._rebuild_workflow_dependencies(workflow_instance_id)
            
        except Exception as e:
            logger.error(f"âŒ [çŠ¶æ€æ¢å¤] æ¢å¤èŠ‚ç‚¹ä¸Šä¸‹æ–‡çŠ¶æ€å¤±è´¥: {e}")
    
    async def _rebuild_workflow_dependencies(self, workflow_instance_id: uuid.UUID):
        """é‡å»ºå·¥ä½œæµçš„ä¾èµ–å…³ç³»"""
        try:
            logger.debug(f"ğŸ”§ [ä¾èµ–é‡å»º] å¼€å§‹é‡å»ºå·¥ä½œæµ {workflow_instance_id} çš„ä¾èµ–å…³ç³»")
            
            # è·å–å·¥ä½œæµçš„æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            
            nodes_query = """
            SELECT ni.node_instance_id, ni.node_id, n.type
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = $1 AND ni.is_deleted = FALSE
            ORDER BY ni.created_at
            """
            
            nodes = await node_repo.db.fetch_all(nodes_query, workflow_instance_id)
            
            for node in nodes:
                node_instance_id = node['node_instance_id']
                node_id = node['node_id']
                
                # è·å–ä¸Šæ¸¸ä¾èµ–
                upstream_query = """
                SELECT nc.from_node_id
                FROM node_connection nc
                WHERE nc.to_node_id = $1
                """
                
                upstream_results = await node_repo.db.fetch_all(upstream_query, node_id)
                upstream_node_ids = [result['from_node_id'] for result in upstream_results]
                
                # è½¬æ¢ä¸ºnode_instance_id
                upstream_node_instance_ids = []
                for upstream_node_id in upstream_node_ids:
                    instance_query = """
                    SELECT node_instance_id 
                    FROM node_instance 
                    WHERE node_id = $1 AND workflow_instance_id = $2 AND is_deleted = FALSE
                    """
                    upstream_instance_result = await node_repo.db.fetch_one(
                        instance_query, upstream_node_id, workflow_instance_id
                    )
                    
                    if upstream_instance_result:
                        upstream_node_instance_ids.append(upstream_instance_result['node_instance_id'])
                    else:
                        logger.warning(f"    âš ï¸ æœªæ‰¾åˆ°ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} å¯¹åº”çš„å®ä¾‹")
                
                # é‡æ–°æ³¨å†Œä¾èµ– - ä¿®å¤å‚æ•°é¡ºåº
                await self.context_manager.register_node_dependencies(
                    workflow_instance_id, node_instance_id, node_id, upstream_node_instance_ids
                )
                
                logger.trace(f"ğŸ”§ [ä¾èµ–é‡å»º] èŠ‚ç‚¹ {node_instance_id} ä¾èµ–å·²é‡å»º: {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹")
            
            logger.debug(f"âœ… [ä¾èµ–é‡å»º] å·¥ä½œæµ {workflow_instance_id} ä¾èµ–å…³ç³»é‡å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ [ä¾èµ–é‡å»º] é‡å»ºä¾èµ–å…³ç³»å¤±è´¥: {e}")
    
    async def _execute_start_node_directly(self, workflow_instance_id: uuid.UUID, start_node: Dict[str, Any]):
        """ç›´æ¥æ‰§è¡ŒSTARTèŠ‚ç‚¹"""
        try:
            node_instance_id = start_node['node_instance_id']
            node_name = start_node.get('node_name', 'æœªçŸ¥')
            logger.trace(f"â–¶ï¸ å¼€å§‹ç›´æ¥æ‰§è¡ŒSTARTèŠ‚ç‚¹: {node_name} (ID: {node_instance_id})")
            
            # æ›´æ–°èŠ‚ç‚¹å®ä¾‹çŠ¶æ€ä¸ºæ‰§è¡Œä¸­
            logger.trace(f"  æ­¥éª¤1: æ›´æ–°èŠ‚ç‚¹å®ä¾‹çŠ¶æ€ä¸º RUNNING")
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
            
            node_instance_repo = NodeInstanceRepository()
            
            # æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
            update_data = NodeInstanceUpdate(
                status=NodeInstanceStatus.RUNNING
            )
            await node_instance_repo.update_node_instance(node_instance_id, update_data)
            logger.trace(f"  âœ… èŠ‚ç‚¹çŠ¶æ€æ›´æ–°ä¸º RUNNING æˆåŠŸ")
            
            # STARTèŠ‚ç‚¹æ²¡æœ‰å®é™…ä»»åŠ¡ï¼Œç›´æ¥å®Œæˆï¼Œä½†è¦åŒ…å«task_descriptionä¾›ä¸‹æ¸¸ä½¿ç”¨
            logger.trace(f"  æ­¥éª¤2: STARTèŠ‚ç‚¹æ— å®é™…ä»»åŠ¡ï¼Œç›´æ¥æ ‡è®°ä¸º COMPLETED")
            
            # è·å–èŠ‚ç‚¹çš„task_description
            task_description = start_node.get('task_description', '')
            if not task_description:
                # å¦‚æœèŠ‚ç‚¹å®ä¾‹æ²¡æœ‰task_descriptionï¼Œä»èŠ‚ç‚¹å®šä¹‰ä¸­è·å–
                from ..repositories.node.node_repository import NodeRepository
                node_repo = NodeRepository()
                node_data = await node_repo.get_node_by_id(start_node['node_id'])
                if node_data:
                    task_description = node_data.get('task_description', '')
            
            completed_data = NodeInstanceUpdate(
                status=NodeInstanceStatus.COMPLETED,
                output_data={
                    'message': 'STARTèŠ‚ç‚¹è‡ªåŠ¨å®Œæˆ',
                    'task_description': task_description,  # æ·»åŠ task_descriptionä¾›ä¸‹æ¸¸èŠ‚ç‚¹ä½¿ç”¨
                    'completed_at': datetime.utcnow().isoformat()
                }
            )
            await node_instance_repo.update_node_instance(node_instance_id, completed_data)
            logger.trace(f"  âœ… èŠ‚ç‚¹çŠ¶æ€æ›´æ–°ä¸º COMPLETED æˆåŠŸ")
            
            # æ­¥éª¤3: é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨èŠ‚ç‚¹å®Œæˆ
            logger.trace(f"  æ­¥éª¤3: é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨STARTèŠ‚ç‚¹å®Œæˆ")
            node_instance_data = await node_instance_repo.get_instance_by_id(node_instance_id)
            if node_instance_data and self.context_manager:
                node_id = node_instance_data['node_id']
                output_data = completed_data.output_data
                
                logger.trace(f"    - é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨: node_id={node_id}")
                logger.trace(f"    - ä¼ é€’çš„output_dataç±»å‹: {type(output_data)}")
                logger.trace(f"    - ä¼ é€’çš„output_dataå†…å®¹: {output_data}")
                await self.context_manager.mark_node_completed(
                    workflow_instance_id, node_id, node_instance_id, output_data
                )
                logger.trace(f"    âœ… ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€šçŸ¥å®Œæˆ")
            else:
                logger.warning(f"    âš ï¸ æ— æ³•é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨: node_instance_data={node_instance_data is not None}, context_manager={self.context_manager is not None}")
            
            # è·å–ä¸‹æ¸¸èŠ‚ç‚¹å¹¶å¯åŠ¨æ‰§è¡Œï¼ˆWorkflowExecutionContextå·²ç»å¤„ç†äº†ä¸‹æ¸¸è§¦å‘ï¼Œé¿å…é‡å¤è§¦å‘ï¼‰
            logger.trace(f"  æ­¥éª¤4: è·³è¿‡é¢å¤–çš„ä¸‹æ¸¸èŠ‚ç‚¹è§¦å‘ï¼ˆWorkflowExecutionContextå·²è‡ªåŠ¨å¤„ç†ï¼‰")
            # await self._trigger_downstream_nodes(workflow_instance_id, start_node)  # æ³¨é‡Šæ‰é¿å…é‡å¤è§¦å‘
            logger.trace(f"  âœ… ä¾èµ–WorkflowExecutionContextè‡ªåŠ¨è§¦å‘æœºåˆ¶")
            
            
            logger.trace(f"  âœ… STARTèŠ‚ç‚¹æ‰§è¡Œå®Œæˆ: {node_name} (ID: {node_instance_id})")
            
        except Exception as e:
            node_name = start_node.get('node_name', 'æœªçŸ¥')
            logger.error(f"âŒ æ‰§è¡ŒSTARTèŠ‚ç‚¹å¤±è´¥ {node_name}: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆè¯¦æƒ…: {traceback.format_exc()}")
            raise
    
            # ç§»é™¤åºŸå¼ƒçš„_trigger_downstream_nodesæ–¹æ³• - åŠŸèƒ½å·²ç”±å·¥ä½œæµä¸Šä¸‹æ–‡ç®¡ç†å™¨æ›¿ä»£
    
    # æ—§æ¶æ„æ–¹æ³•å·²ç§»é™¤ï¼š_execute_node_when_ready
    
    
    async def _update_node_tasks_with_context(self, node_instance_id: uuid.UUID, upstream_context: Dict[str, Any]):
        """æ›´æ–°èŠ‚ç‚¹ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ•°æ®ï¼Œå¹¶åŒæ­¥æ›´æ–°èŠ‚ç‚¹çš„è¾“å…¥æ•°æ®"""
        try:
            # æ„å»ºå®Œæ•´çš„ä»»åŠ¡ä¸Šä¸‹æ–‡ - ä¿®å¤æ•°æ®æ ¼å¼è½¬æ¢é—®é¢˜
            immediate_upstream_results = upstream_context.get('immediate_upstream_results', {})
            logger.trace(f"ğŸ”„ [æ•°æ®æ ¼å¼è½¬æ¢] åŸå§‹ä¸Šæ¸¸ç»“æœ: {immediate_upstream_results}")
            
            task_context = {
                'immediate_upstream': immediate_upstream_results,  # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨immediate_upstream_results
                'workflow_global': upstream_context.get('workflow_global', {}),
                'node_info': {
                    'node_instance_id': str(node_instance_id),
                    'upstream_node_count': upstream_context.get('upstream_node_count', 0)
                }
            }
            
            logger.trace(f"ğŸ”„ [æ•°æ®æ ¼å¼è½¬æ¢] æœ€ç»ˆä»»åŠ¡ä¸Šä¸‹æ–‡åŒ…å« {len(immediate_upstream_results)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹æ•°æ®")
            
            # é¦–å…ˆæ›´æ–°èŠ‚ç‚¹å®ä¾‹çš„è¾“å…¥æ•°æ®ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
            logger.trace(f"ğŸ“ [èŠ‚ç‚¹ä¸Šä¸‹æ–‡] æ›´æ–°èŠ‚ç‚¹ {node_instance_id} çš„è¾“å…¥æ•°æ®")
            logger.trace(f"   - ä¸Šæ¸¸ç»“æœæ•°é‡: {len(task_context.get('immediate_upstream', {}))}")
            logger.trace(f"   - å·¥ä½œæµå…¨å±€æ•°æ®: {len(task_context.get('workflow_global', {}).get('global_data', {}))}")
            
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceUpdate
            node_instance_repo = NodeInstanceRepository()
            
            node_update = NodeInstanceUpdate(input_data=task_context)
            await node_instance_repo.update_node_instance(node_instance_id, node_update)
            logger.trace(f"   âœ… èŠ‚ç‚¹è¾“å…¥æ•°æ®å·²æ›´æ–°ï¼šåŒ…å« {len(task_context)} ä¸ªé¡¶çº§å­—æ®µ")
            
            # ç„¶åè·å–èŠ‚ç‚¹çš„æ‰€æœ‰ä»»åŠ¡å¹¶æ›´æ–°å®ƒä»¬çš„ä¸Šä¸‹æ–‡
            tasks = await self.task_instance_repo.get_tasks_by_node_instance(node_instance_id)
            
            for task in tasks:
                
                # å°†ä¸Šä¸‹æ–‡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆæ•°æ®åº“input_dataå­—æ®µæ˜¯TEXTç±»å‹ï¼‰
                from ..utils.helpers import safe_json_dumps
                task_context_json = safe_json_dumps(task_context)
                
                # æ›´æ–°ä»»åŠ¡çš„è¾“å…¥æ•°æ®ï¼ˆä½†ä¸æ”¹å˜å·²å®Œæˆæˆ–å¤±è´¥ä»»åŠ¡çš„çŠ¶æ€ï¼‰
                current_status = task.get('status', 'PENDING')
                
                # åªæœ‰PENDINGçŠ¶æ€çš„ä»»åŠ¡æ‰éœ€è¦æ›´æ–°çŠ¶æ€
                if current_status == 'PENDING':
                    new_status = TaskInstanceStatus.ASSIGNED if task.get('assigned_user_id') or task.get('assigned_agent_id') else TaskInstanceStatus.PENDING
                    update_data = TaskInstanceUpdate(
                        input_data=task_context_json,
                        status=new_status
                    )
                    logger.warning(f"ä»»åŠ¡ {task['task_instance_id']} çŠ¶æ€æ›´æ–°: {current_status} â†’ {new_status.value}")
                    logger.warning(f"ä»»åŠ¡ {task['task_instance_id']} ä¸Šä¸‹æ–‡æ•°æ®: {len(task_context_json)} å­—ç¬¦")
                else:
                    # å·²å®Œæˆ/å¤±è´¥/è¿›è¡Œä¸­çš„ä»»åŠ¡åªæ›´æ–°ä¸Šä¸‹æ–‡ï¼Œä¸æ”¹å˜çŠ¶æ€
                    update_data = TaskInstanceUpdate(
                        input_data=task_context_json
                    )
                    logger.warning(f"ä»»åŠ¡ {task['task_instance_id']} çŠ¶æ€ä¿æŒ: {current_status}ï¼ˆä»…æ›´æ–°ä¸Šä¸‹æ–‡ï¼‰")
                    logger.warning(f"ä»»åŠ¡ {task['task_instance_id']} ä¸Šä¸‹æ–‡æ•°æ®: {len(task_context_json)} å­—ç¬¦")
                
                await self.task_instance_repo.update_task(task['task_instance_id'], update_data)
                logger.warning(f"æ›´æ–°ä»»åŠ¡ {task['task_instance_id']} çš„ä¸Šä¸‹æ–‡æ•°æ®")
                
        except Exception as e:
            logger.error(f"æ›´æ–°èŠ‚ç‚¹ä»»åŠ¡ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            raise
    
    async def _execute_node_tasks(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """æ‰§è¡ŒèŠ‚ç‚¹çš„ä»»åŠ¡"""
        try:
            # è·å–èŠ‚ç‚¹çš„æ‰€æœ‰ä»»åŠ¡
            tasks = await self.task_instance_repo.get_tasks_by_node_instance(node_instance_id)
            
            if not tasks:
                # è·å–èŠ‚ç‚¹ä¿¡æ¯åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºä»»åŠ¡
                from ..repositories.instance.node_instance_repository import NodeInstanceRepository
                node_repo = NodeInstanceRepository()
                node_instance_data = await node_repo.get_instance_by_id(node_instance_id)
                
                if not node_instance_data:
                    logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯: {node_instance_id}")
                    return
                
                # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
                node_query = """
                SELECT n.*, ni.workflow_instance_id
                FROM node n 
                JOIN node_instance ni ON n.node_id = ni.node_id
                WHERE ni.node_instance_id = $1
                """
                node_info = await self.task_instance_repo.db.fetch_one(node_query, node_instance_id)
                
                if not node_info:
                    logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯: {node_instance_id}")
                    return
                
                # å¦‚æœæ˜¯PROCESSORèŠ‚ç‚¹ä½†æ²¡æœ‰ä»»åŠ¡ï¼Œéœ€è¦å…ˆåˆ›å»ºä»»åŠ¡ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                if node_info['type'].upper() == 'PROCESSOR':
                    logger.trace(f"ğŸ”§ PROCESSORèŠ‚ç‚¹ {node_info['name']} æ²¡æœ‰ä»»åŠ¡ï¼Œå¼€å§‹åˆ›å»ºä»»åŠ¡...")
                    
                    # æ„é€ èŠ‚ç‚¹æ•°æ®ç”¨äºä»»åŠ¡åˆ›å»º
                    created_node = {
                        'node_instance_id': node_instance_id,
                        'node_type': node_info['type'],
                        'node_data': {
                            'node_id': node_info['node_id'],
                            'name': node_info['name'],
                            'description': node_info.get('description', ''),
                            'task_description': node_info.get('task_description', ''),
                            'input_data': {}
                        }
                    }
                    
                    logger.trace(f"ğŸ“‹ å¼€å§‹ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡ï¼ŒèŠ‚ç‚¹æ•°æ®: {created_node}")
                    
                    try:
                        # ä¸ºè¿™ä¸ªèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡
                        await self._create_tasks_for_nodes([created_node], workflow_instance_id)
                        logger.trace(f"âœ… èŠ‚ç‚¹ {node_info['name']} ä»»åŠ¡åˆ›å»ºå®Œæˆ")
                    except Exception as task_creation_error:
                        logger.error(f"âŒ èŠ‚ç‚¹ {node_info['name']} ä»»åŠ¡åˆ›å»ºå¤±è´¥: {task_creation_error}")
                        import traceback
                        logger.error(f"ä»»åŠ¡åˆ›å»ºé”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                        # ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½æ— æ³•æ‰¾åˆ°ä»»åŠ¡
                    
                    # é‡æ–°è·å–ä»»åŠ¡
                    tasks = await self.task_instance_repo.get_tasks_by_node_instance(node_instance_id)
                    
                    if not tasks:
                        logger.error(f"âŒ PROCESSORèŠ‚ç‚¹ {node_info['name']} ä»»åŠ¡åˆ›å»ºå¤±è´¥ï¼Œæ²¡æœ‰é…ç½®å¤„ç†å™¨")
                        await self._complete_node_without_tasks(workflow_instance_id, node_instance_id)
                        return
                else:
                    # å¦‚æœæ˜¯STARTæˆ–ENDèŠ‚ç‚¹ç­‰éPROCESSORèŠ‚ç‚¹ï¼Œç›´æ¥æ ‡è®°å®Œæˆ
                    logger.trace(f"â­ï¸ éPROCESSORèŠ‚ç‚¹ {node_info.get('name', 'Unknown')} æ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥æ ‡è®°å®Œæˆ")
                    await self._complete_node_without_tasks(workflow_instance_id, node_instance_id)
                    return
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            for task in tasks:
                if task['task_type'] == TaskInstanceType.AGENT.value:
                    # Agentä»»åŠ¡ï¼šæäº¤ç»™AgentTaskServiceå¤„ç†
                    await self._execute_agent_task(task)
                elif task['task_type'] == TaskInstanceType.HUMAN.value:
                    # Humanä»»åŠ¡ï¼šè°ƒç”¨_execute_taskæ–¹æ³•å¤„ç†ï¼ˆåŒ…å«ç”¨æˆ·é€šçŸ¥ï¼‰
                    await self._execute_task(task)
                    logger.trace(f"Humanä»»åŠ¡ {task['task_instance_id']} å·²åˆ†é…å¹¶é€šçŸ¥ç”¨æˆ·")
                elif task['task_type'] == TaskInstanceType.MIXED.value:
                    # Mixedä»»åŠ¡ï¼šå…ˆåˆ†é…ç»™ç”¨æˆ·ï¼ŒåŒæ—¶æä¾›AIè¾…åŠ©
                    await self._execute_mixed_task(task)
            
            # æ³¨å†Œä»»åŠ¡å®Œæˆç›‘å¬
            await self._register_node_completion_monitor(workflow_instance_id, node_instance_id)
            
        except Exception as e:
            logger.error(f"æ‰§è¡ŒèŠ‚ç‚¹ä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def _complete_node_without_tasks(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """å®Œæˆæ²¡æœ‰ä»»åŠ¡çš„èŠ‚ç‚¹ï¼ˆå¦‚STARTã€ENDèŠ‚ç‚¹ï¼‰"""
        try:
            from datetime import datetime as dt
            from ..models.instance import NodeInstanceStatus, NodeInstanceUpdate
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            
            # è·å–node_idç”¨äºæ ‡è®°å®Œæˆ
            node_repo = NodeInstanceRepository()
            node_instance_data = await node_repo.get_instance_by_id(node_instance_id)
            if not node_instance_data:
                logger.error(f"æ— æ³•æ‰¾åˆ°èŠ‚ç‚¹å®ä¾‹: {node_instance_id}")
                return
            
            node_id = node_instance_data['node_id']
            
            # æ ‡è®°èŠ‚ç‚¹å®Œæˆ
            output_data = {
                'completed_at': dt.utcnow().isoformat(),
                'node_type': 'system',
                'message': 'ç³»ç»ŸèŠ‚ç‚¹è‡ªåŠ¨å®Œæˆ'
            }
            
            await self.context_manager.mark_node_completed(
                workflow_instance_id, node_id, node_instance_id, output_data
            )
            
            # åŒæ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
            try:
                node_repo = NodeInstanceRepository()
                node_update = NodeInstanceUpdate(
                    status=NodeInstanceStatus.COMPLETED,
                    output_data=output_data,
                    completed_at=dt.utcnow()
                )
                await node_repo.update_node_instance(node_instance_id, node_update)
                logger.trace(f"ğŸ’¾ [ç³»ç»ŸèŠ‚ç‚¹] èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ•°æ®åº“çŠ¶æ€å·²æ›´æ–°ä¸ºCOMPLETED")
            except Exception as e:
                logger.error(f"âŒ [ç³»ç»ŸèŠ‚ç‚¹] æ›´æ–°èŠ‚ç‚¹å®ä¾‹æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}")
            
            logger.trace(f"âœ… ç³»ç»ŸèŠ‚ç‚¹ {node_id} è‡ªåŠ¨å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å®Œæˆç³»ç»ŸèŠ‚ç‚¹å¤±è´¥: {e}")
            raise
    
    async def _execute_agent_task(self, task: Dict[str, Any]):
        """æ‰§è¡ŒAgentä»»åŠ¡"""
        try:
            task_id = task['task_instance_id']
            task_title = task.get('task_title', 'unknown')
            task_type = task.get('task_type', 'unknown')
            current_status = task.get('status', 'unknown')
            assigned_agent_id = task.get('assigned_agent_id', 'none')
            processor_id = task.get('processor_id', 'none')
            node_instance_id = task.get('node_instance_id', 'none')
            workflow_instance_id = task.get('workflow_instance_id', 'none')
            
            logger.info(f"ğŸš€ [EXECUTION-ENGINE] === å¼€å§‹æ‰§è¡ŒAgentä»»åŠ¡ ===")
            logger.info(f"   ğŸ“‹ ä»»åŠ¡ID: {task_id}")
            logger.info(f"   ğŸ·ï¸  ä»»åŠ¡æ ‡é¢˜: {task_title}")
            logger.info(f"   ğŸ“ ä»»åŠ¡ç±»å‹: {task_type}")
            logger.info(f"   ğŸ“Š å½“å‰çŠ¶æ€: {current_status}")
            logger.info(f"   ğŸ¤– åˆ†é…Agent: {assigned_agent_id}")
            logger.info(f"   âš™ï¸  å¤„ç†å™¨ID: {processor_id}")
            logger.info(f"   ğŸ”— èŠ‚ç‚¹å®ä¾‹ID: {node_instance_id}")
            logger.info(f"   ğŸŒŠ å·¥ä½œæµå®ä¾‹ID: {workflow_instance_id}")
            
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦çœŸçš„éœ€è¦æ‰§è¡Œ
            if current_status in ['completed', 'failed', 'cancelled']:
                logger.warning(f"âš ï¸ [EXECUTION-ENGINE] ä»»åŠ¡ {task_id} çŠ¶æ€ä¸º {current_status}ï¼Œè·³è¿‡æ‰§è¡Œ")
                return
            
            # è®°å½•æ‰§è¡Œå‰çš„æ—¶é—´æˆ³
            start_time = datetime.now()
            logger.info(f"â° [EXECUTION-ENGINE] ä»»åŠ¡å¼€å§‹æ‰§è¡Œæ—¶é—´: {start_time.isoformat()}")
            
            # æ£€æŸ¥AgentæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ
            logger.info(f"ğŸ” [EXECUTION-ENGINE] æ£€æŸ¥AgentæœåŠ¡çŠ¶æ€...")
            logger.info(f"   - AgentæœåŠ¡è¿è¡ŒçŠ¶æ€: {agent_task_service.is_running}")
            logger.info(f"   - AgentæœåŠ¡å¤„ç†é˜Ÿåˆ—å¤§å°: {agent_task_service.processing_queue.qsize()}")
            logger.info(f"   - AgentæœåŠ¡å›è°ƒæ•°é‡: {len(agent_task_service.completion_callbacks)}")
            
            # è°ƒç”¨AgentTaskServiceå¤„ç†ä»»åŠ¡
            logger.info(f"ğŸ”„ [EXECUTION-ENGINE] è°ƒç”¨AgentTaskService.process_agent_task()")
            logger.info(f"   - ä¼ é€’ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - é¢„æœŸæµç¨‹: assigned â†’ in_progress â†’ completed")
            
            try:
                # åœ¨è°ƒç”¨å‰å†æ¬¡æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                current_task = await self.task_instance_repo.get_task_by_id(task_id)
                if current_task:
                    logger.info(f"ğŸ“Š [EXECUTION-ENGINE] è°ƒç”¨å‰ä»»åŠ¡çŠ¶æ€éªŒè¯:")
                    logger.info(f"   - æ•°æ®åº“ä¸­çŠ¶æ€: {current_task.get('status', 'unknown')}")
                    logger.info(f"   - æ˜¯å¦å·²åˆ†é…Agent: {'æ˜¯' if current_task.get('assigned_agent_id') else 'å¦'}")
                    logger.info(f"   - Agent IDåŒ¹é…: {'æ˜¯' if str(current_task.get('assigned_agent_id', '')) == str(assigned_agent_id) else 'å¦'}")
                
                # å®é™…è°ƒç”¨Agentå¤„ç†
                result = await agent_task_service.process_agent_task(task_id)
                
                # è®°å½•æ‰§è¡Œåçš„æ—¶é—´å’Œç»“æœ
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                logger.info(f"âœ… [EXECUTION-ENGINE] AgentTaskServiceè°ƒç”¨å®Œæˆ!")
                logger.info(f"   â±ï¸  æ‰§è¡Œè€—æ—¶: {duration:.2f}ç§’")
                logger.info(f"   ğŸ¯ è¿”å›ç»“æœç±»å‹: {type(result)}")
                
                if isinstance(result, dict):
                    result_status = result.get('status', 'unknown')
                    result_message = result.get('message', 'no message')
                    logger.info(f"   ğŸ“Š ç»“æœçŠ¶æ€: {result_status}")
                    logger.info(f"   ğŸ’¬ ç»“æœæ¶ˆæ¯: {result_message}")
                    
                    # å¦‚æœæœ‰å…·ä½“çš„ç»“æœå†…å®¹ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
                    if 'result' in result:
                        result_content = str(result['result'])
                        content_preview = result_content[:200] + '...' if len(result_content) > 200 else result_content
                        logger.info(f"   ğŸ“„ ç»“æœå†…å®¹é¢„è§ˆ: {content_preview}")
                else:
                    logger.info(f"   ğŸ“„ ç»“æœå†…å®¹: {result}")
                
                # éªŒè¯ä»»åŠ¡æ˜¯å¦çœŸçš„å®Œæˆäº†
                updated_task = await self.task_instance_repo.get_task_by_id(task_id)
                if updated_task:
                    final_status = updated_task.get('status', 'unknown')
                    has_output = bool(updated_task.get('output_data', '').strip())
                    
                    logger.info(f"ğŸ” [EXECUTION-ENGINE] æ‰§è¡Œåä»»åŠ¡çŠ¶æ€éªŒè¯:")
                    logger.info(f"   - æœ€ç»ˆçŠ¶æ€: {final_status}")
                    logger.info(f"   - æœ‰è¾“å‡ºæ•°æ®: {'æ˜¯' if has_output else 'å¦'}")
                    logger.info(f"   - å®Œæˆæ—¶é—´: {updated_task.get('completed_at', 'æœªè®¾ç½®')}")
                    logger.info(f"   - å®é™…æ‰§è¡Œæ—¶é•¿: {updated_task.get('actual_duration', 'æœªè®¾ç½®')}åˆ†é’Ÿ")
                    
                    if final_status == 'completed':
                        logger.info(f"ğŸ‰ [EXECUTION-ENGINE] Agentä»»åŠ¡çœŸå®æ‰§è¡ŒæˆåŠŸ!")
                        if has_output:
                            output_preview = str(updated_task.get('output_data', ''))[:150]
                            logger.info(f"   ğŸ“‹ è¾“å‡ºæ•°æ®: {output_preview}...")
                    else:
                        logger.warning(f"âš ï¸ [EXECUTION-ENGINE] Agentä»»åŠ¡çŠ¶æ€å¼‚å¸¸: {final_status}")
                        if updated_task.get('error_message'):
                            logger.warning(f"   âŒ é”™è¯¯ä¿¡æ¯: {updated_task['error_message']}")
                else:
                    logger.error(f"âŒ [EXECUTION-ENGINE] æ— æ³•éªŒè¯ä»»åŠ¡æ‰§è¡Œç»“æœï¼Œä»»åŠ¡å¯èƒ½å·²è¢«åˆ é™¤")
                
            except Exception as process_error:
                logger.error(f"âŒ [EXECUTION-ENGINE] AgentTaskServiceå¤„ç†å¤±è´¥!")
                logger.error(f"   ğŸš« é”™è¯¯ç±»å‹: {type(process_error).__name__}")
                logger.error(f"   ğŸ’¬ é”™è¯¯æ¶ˆæ¯: {str(process_error)}")
                import traceback
                logger.error(f"   ğŸ“š å®Œæ•´å †æ ˆ:")
                for line in traceback.format_exc().split('\n'):
                    if line.strip():
                        logger.error(f"      {line}")
                raise
            
            logger.info(f"âœ… [EXECUTION-ENGINE] === Agentä»»åŠ¡æ‰§è¡Œæµç¨‹å®Œæˆ ===")
            logger.info(f"   ğŸ ä»»åŠ¡ID: {task_id}")
            logger.info(f"   â±ï¸  æ€»è€—æ—¶: {(datetime.now() - start_time).total_seconds():.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ [EXECUTION-ENGINE] === Agentä»»åŠ¡æ‰§è¡Œå¤±è´¥ ===")
            logger.error(f"   ğŸš« ä»»åŠ¡ID: {task.get('task_instance_id', 'unknown')}")
            logger.error(f"   ğŸ“ ä»»åŠ¡æ ‡é¢˜: {task.get('task_title', 'unknown')}")
            logger.error(f"   ğŸ’¥ å¤±è´¥åŸå› : {str(e)}")
            logger.error(f"   ğŸ“Š åŸå§‹ä»»åŠ¡æ•°æ®: {task}")
            import traceback
            logger.error(f"   ğŸ“š å®Œæ•´é”™è¯¯å †æ ˆ:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.error(f"      {line}")
            raise
    
    async def _execute_mixed_task(self, task: Dict[str, Any]):
        """æ‰§è¡ŒMixedä»»åŠ¡ï¼ˆäººæœºåä½œï¼‰"""
        try:
            # Mixedä»»åŠ¡åˆ†é…ç»™ç”¨æˆ·ï¼ŒåŒæ—¶å¯åŠ¨AIè¾…åŠ©
            task_id = task['task_instance_id']
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºASSIGNEDï¼ˆåˆ†é…ç»™ç”¨æˆ·ï¼‰
            update_data = TaskInstanceUpdate(status=TaskInstanceStatus.ASSIGNED)
            await self.task_instance_repo.update_task(task_id, update_data)
            
            # å¯åŠ¨AIè¾…åŠ©ï¼ˆå¯é€‰ï¼‰
            asyncio.create_task(self._provide_ai_assistance(task))
            
            logger.trace(f"Mixedä»»åŠ¡ {task_id} å·²åˆ†é…ç»™ç”¨æˆ·ï¼ŒAIè¾…åŠ©å·²å¯åŠ¨")
            
        except Exception as e:
            logger.error(f"æ‰§è¡ŒMixedä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def _provide_ai_assistance(self, task: Dict[str, Any]):
        """ä¸ºMixedä»»åŠ¡æä¾›AIè¾…åŠ©"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°AIè¾…åŠ©é€»è¾‘
            # ä¾‹å¦‚ï¼šåˆ†æä»»åŠ¡å†…å®¹ï¼Œæä¾›å»ºè®®ç­‰
            logger.trace(f"ä¸ºä»»åŠ¡ {task['task_instance_id']} æä¾›AIè¾…åŠ©")
            
        except Exception as e:
            logger.error(f"æä¾›AIè¾…åŠ©å¤±è´¥: {e}")
    
    async def _register_node_completion_monitor(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """æ³¨å†ŒèŠ‚ç‚¹å®Œæˆç›‘å¬å™¨ï¼ˆé˜²é‡å¤ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ´»è·ƒçš„ç›‘å¬å™¨
            if node_instance_id in self.active_monitors:
                logger.warning(f"ğŸ”„ [ç›‘å¬å™¨æ³¨å†Œ-é˜²é‡å¤] èŠ‚ç‚¹ {node_instance_id} å·²æœ‰æ´»è·ƒç›‘å¬å™¨ï¼Œè·³è¿‡é‡å¤æ³¨å†Œ")
                return
            
            logger.trace(f"ğŸ“‹ [ç›‘å¬å™¨æ³¨å†Œ] ä¸ºèŠ‚ç‚¹ {node_instance_id} æ³¨å†Œå®Œæˆç›‘å¬å™¨")
            logger.trace(f"   - å·¥ä½œæµå®ä¾‹: {workflow_instance_id}")
            
            # æ·»åŠ åˆ°æ´»è·ƒç›‘å¬å™¨é›†åˆ
            self.active_monitors.add(node_instance_id)
            
            # å¯åŠ¨èŠ‚ç‚¹å®Œæˆç›‘å¬åç¨‹
            task = asyncio.create_task(self._monitor_node_completion(workflow_instance_id, node_instance_id))
            logger.trace(f"âœ… [ç›‘å¬å™¨æ³¨å†Œ] èŠ‚ç‚¹ {node_instance_id} ç›‘å¬åç¨‹å·²å¯åŠ¨")
            
        except Exception as e:
            logger.error(f"âŒ [ç›‘å¬å™¨æ³¨å†Œ] æ³¨å†ŒèŠ‚ç‚¹å®Œæˆç›‘å¬å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _monitor_node_completion(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """ç›‘å¬èŠ‚ç‚¹å®Œæˆ"""
        try:
            logger.trace(f"ğŸ” [èŠ‚ç‚¹ç›‘å¬] å¼€å§‹ç›‘å¬èŠ‚ç‚¹å®Œæˆ: {node_instance_id}")
            
            while True:
                # æ£€æŸ¥èŠ‚ç‚¹çš„æ‰€æœ‰ä»»åŠ¡æ˜¯å¦å®Œæˆ
                tasks = await self.task_instance_repo.get_tasks_by_node_instance(node_instance_id)
                
                if not tasks:
                    logger.trace(f"âš ï¸ [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} æ²¡æœ‰ä»»åŠ¡ï¼Œåœæ­¢ç›‘å¬")
                    break
                
                completed_tasks = [t for t in tasks if t['status'] == TaskInstanceStatus.COMPLETED.value]
                failed_tasks = [t for t in tasks if t['status'] == TaskInstanceStatus.FAILED.value]
                
                logger.trace(f"ğŸ“Š [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} ä»»åŠ¡çŠ¶æ€:")
                logger.trace(f"   - æ€»ä»»åŠ¡æ•°: {len(tasks)}")
                logger.trace(f"   - å·²å®Œæˆ: {len(completed_tasks)}")
                logger.trace(f"   - å¤±è´¥: {len(failed_tasks)}")
                
                # æ˜¾ç¤ºæ¯ä¸ªä»»åŠ¡çš„è¯¦ç»†çŠ¶æ€
                for i, task in enumerate(tasks):
                    task_id = task.get('task_instance_id', 'unknown')
                    task_status = task.get('status', 'unknown')
                    task_title = task.get('task_title', 'unknown')
                    logger.trace(f"   - ä»»åŠ¡{i+1}: {task_title} (ID: {task_id}) - çŠ¶æ€: {task_status}")
                
                if len(completed_tasks) == len(tasks):
                    # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ ‡è®°èŠ‚ç‚¹å®Œæˆ
                    logger.trace(f"ğŸ‰ [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œå¼€å§‹æ ‡è®°èŠ‚ç‚¹å®Œæˆ")
                    output_data = await self._aggregate_node_output(completed_tasks)
                    
                    # æ£€æŸ¥context manageræ˜¯å¦å¯ç”¨
                    if self.context_manager is None:
                        logger.error(f"âŒ [èŠ‚ç‚¹ç›‘å¬] context_manager ä¸º Noneï¼Œæ— æ³•æ ‡è®°èŠ‚ç‚¹å®Œæˆ")
                        break
                    
                    # è·å–node_idç”¨äºä¾èµ–åŒ¹é…ï¼Œå› ä¸ºä¾èµ–å…³ç³»æ˜¯åŸºäºnode_idæ³¨å†Œçš„
                    from ..repositories.instance.node_instance_repository import NodeInstanceRepository
                    node_repo = NodeInstanceRepository()
                    node_instance_data = await node_repo.get_instance_by_id(node_instance_id)
                    node_id = node_instance_data['node_id'] if node_instance_data else None
                    
                    if node_id:
                        logger.trace(f"ğŸ¯ [èŠ‚ç‚¹ç›‘å¬] æ ‡è®°èŠ‚ç‚¹å®Œæˆ: node_id={node_id}")
                        await self.context_manager.mark_node_completed(
                            workflow_instance_id, node_id, node_instance_id, output_data
                        )
                    else:
                        logger.error(f"âŒ [èŠ‚ç‚¹ç›‘å¬] æ— æ³•è·å–node_idï¼Œæ— æ³•æ ‡è®°èŠ‚ç‚¹å®Œæˆ")
                        break
                    
                    # åŒæ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
                    try:
                        from datetime import datetime
                        from ..models.instance import NodeInstanceStatus, NodeInstanceUpdate
                        from ..repositories.instance.node_instance_repository import NodeInstanceRepository
                        
                        node_repo = NodeInstanceRepository()
                        node_update = NodeInstanceUpdate(
                            status=NodeInstanceStatus.COMPLETED,
                            output_data=output_data,
                            completed_at=datetime.utcnow()
                        )
                        await node_repo.update_node_instance(node_instance_id, node_update)
                        logger.trace(f"ğŸ’¾ [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹å®ä¾‹ {node_instance_id} æ•°æ®åº“çŠ¶æ€å·²æ›´æ–°ä¸ºCOMPLETED")
                    except Exception as e:
                        logger.error(f"âŒ [èŠ‚ç‚¹ç›‘å¬] æ›´æ–°èŠ‚ç‚¹å®ä¾‹æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}")
                    
                    logger.trace(f"âœ… [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} å·²æ ‡è®°ä¸ºå®Œæˆï¼Œåœæ­¢ç›‘å¬")
                    # ä»æ´»è·ƒç›‘å¬å™¨é›†åˆä¸­ç§»é™¤
                    self.active_monitors.discard(node_instance_id)
                    break
                elif len(failed_tasks) > 0:
                    # æœ‰ä»»åŠ¡å¤±è´¥ï¼Œæ ‡è®°èŠ‚ç‚¹å¤±è´¥
                    logger.error(f"âŒ [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} æœ‰ä»»åŠ¡å¤±è´¥ï¼Œæ ‡è®°èŠ‚ç‚¹å¤±è´¥")
                    error_info = {'failed_tasks': [str(t['task_instance_id']) for t in failed_tasks]}
                    
                    # è·å–node_idç”¨äºæ ‡è®°å¤±è´¥
                    from ..repositories.instance.node_instance_repository import NodeInstanceRepository
                    node_repo = NodeInstanceRepository()
                    node_instance_data = await node_repo.get_instance_by_id(node_instance_id)
                    node_id = node_instance_data['node_id'] if node_instance_data else None
                    
                    if node_id:
                        await self.context_manager.mark_node_failed(
                            workflow_instance_id, node_id, node_instance_id, error_info
                        )
                    else:
                        logger.error(f"âŒ [èŠ‚ç‚¹ç›‘å¬] æ— æ³•è·å–node_idï¼Œæ— æ³•æ ‡è®°èŠ‚ç‚¹å¤±è´¥")
                    # ä»æ´»è·ƒç›‘å¬å™¨é›†åˆä¸­ç§»é™¤
                    self.active_monitors.discard(node_instance_id)
                    break
                
                # ç­‰å¾…5ç§’åå†æ¬¡æ£€æŸ¥
                logger.trace(f"â³ [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} ä»æœ‰ä»»åŠ¡æœªå®Œæˆï¼Œ5ç§’åå†æ¬¡æ£€æŸ¥")
                await asyncio.sleep(5)
                
        except Exception as e:
            logger.error(f"ç›‘å¬èŠ‚ç‚¹å®Œæˆå¤±è´¥: {e}")
            # å¼‚å¸¸æ—¶ä¹Ÿè¦ä»æ´»è·ƒç›‘å¬å™¨é›†åˆä¸­ç§»é™¤
            self.active_monitors.discard(node_instance_id)
        finally:
            # ç¡®ä¿ç›‘å¬å™¨è¢«æ¸…ç†
            self.active_monitors.discard(node_instance_id)
            logger.trace(f"ğŸ§¹ [èŠ‚ç‚¹ç›‘å¬] èŠ‚ç‚¹ {node_instance_id} ç›‘å¬å™¨å·²æ¸…ç†")
    
    def _make_json_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„å½¢å¼"""
        if isinstance(obj, uuid.UUID):
            return str(obj)
        elif isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        else:
            return obj

    async def _aggregate_node_output(self, completed_tasks: List[Dict]) -> Dict[str, Any]:
        """èšåˆèŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®"""
        try:
            aggregated = {
                'task_count': len(completed_tasks),
                'completed_at': datetime.utcnow().isoformat(),
                'task_results': []
            }
            
            combined_output = {}
            
            for task_index, task in enumerate(completed_tasks):
                task_result = {
                    'task_id': str(task['task_instance_id']),  # è½¬æ¢UUIDä¸ºå­—ç¬¦ä¸²
                    'task_title': task.get('task_title', ''),
                    'output_data': task.get('output_data', ''),  # ç°åœ¨æ˜¯æ–‡æœ¬æ ¼å¼
                    'result_summary': task.get('result_summary', '')
                }
                aggregated['task_results'].append(task_result)
                
                # åˆå¹¶ä»»åŠ¡è¾“å‡ºæ•°æ®ï¼ˆç°åœ¨æ˜¯æ–‡æœ¬æ ¼å¼ï¼‰
                if task.get('output_data'):
                    output_data = task['output_data']
                    # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ªé”®å€¼å¯¹
                    task_key = f"task_{task_index + 1}_output"
                    combined_output[task_key] = str(output_data)
            
            aggregated['combined_output'] = combined_output
            
            return self._make_json_serializable(aggregated)
            
        except Exception as e:
            logger.error(f"èšåˆèŠ‚ç‚¹è¾“å‡ºå¤±è´¥: {e}")
            return {'error': str(e)}
    
    async def _on_nodes_ready_to_execute(self, workflow_instance_id: uuid.UUID, ready_node_instance_ids: List[uuid.UUID]):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å›è°ƒï¼šæœ‰èŠ‚ç‚¹å‡†å¤‡æ‰§è¡Œï¼ˆæ–°æ¶æ„ç‰ˆæœ¬ï¼‰"""
        try:
            logger.info(f"ğŸ”” [ç»Ÿä¸€æ¶æ„-å›è°ƒ] å·¥ä½œæµ {workflow_instance_id} ä¸­æœ‰ {len(ready_node_instance_ids)} ä¸ªèŠ‚ç‚¹å‡†å¤‡æ‰§è¡Œ")
            
            # è·å–å·¥ä½œæµä¸Šä¸‹æ–‡
            from .workflow_execution_context import get_context_manager
            context_manager = get_context_manager()
            workflow_context = await context_manager.get_context(workflow_instance_id)
            
            if not workflow_context:
                logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„-å›è°ƒ] æœªæ‰¾åˆ°å·¥ä½œæµä¸Šä¸‹æ–‡: {workflow_instance_id}")
                return
            
            # ä½¿ç”¨æ–°æ¶æ„æ‰§è¡Œå‡†å¤‡å¥½çš„èŠ‚ç‚¹
            for node_instance_id in ready_node_instance_ids:
                try:
                    logger.info(f"âš¡ [ç»Ÿä¸€æ¶æ„-å›è°ƒ] å¼€å§‹æ‰§è¡ŒèŠ‚ç‚¹: {node_instance_id}")
                    await self._execute_node_with_unified_context(workflow_context, workflow_instance_id, node_instance_id)
                except Exception as e:
                    logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„-å›è°ƒ] æ‰§è¡ŒèŠ‚ç‚¹ {node_instance_id} å¤±è´¥: {e}")
                    import traceback
                    logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                
        except Exception as e:
            logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„-å›è°ƒ] æ‰§è¡Œå‡†å¤‡å¥½çš„èŠ‚ç‚¹å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _execute_node_with_unified_context(self, workflow_context, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """ä½¿ç”¨ç»Ÿä¸€ä¸Šä¸‹æ–‡æ‰§è¡ŒèŠ‚ç‚¹"""
        try:
            # è·å–èŠ‚ç‚¹ä¿¡æ¯
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..repositories.node.node_repository import NodeRepository
            
            node_instance_repo = NodeInstanceRepository()
            node_repo = NodeRepository()
            
            node_instance_info = await node_instance_repo.get_instance_by_id(node_instance_id)
            if not node_instance_info:
                logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] æ— æ³•è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯: {node_instance_id}")
                return
                
            node_info = await node_repo.get_node_by_id(node_instance_info['node_id'])
            if not node_info:
                logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] æ— æ³•è·å–èŠ‚ç‚¹ä¿¡æ¯: {node_instance_info['node_id']}")
                return
            
            logger.info(f"ğŸ“‹ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] æ‰§è¡ŒèŠ‚ç‚¹: {node_info.get('name')} (ç±»å‹: {node_info.get('type')})")
            
            # æ ¹æ®èŠ‚ç‚¹ç±»å‹å¤„ç†
            if node_info.get('type') == NodeType.PROCESSOR.value:
                # PROCESSORèŠ‚ç‚¹ï¼šåˆ›å»ºä»»åŠ¡å¹¶æ‰§è¡Œ
                logger.info(f"ğŸ”¨ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] PROCESSORèŠ‚ç‚¹ {node_info.get('name')} - åˆ›å»ºå¹¶åˆ†é…ä»»åŠ¡")
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»åŠ¡å®ä¾‹
                from ..repositories.instance.task_instance_repository import TaskInstanceRepository
                task_repo = TaskInstanceRepository()
                existing_tasks = await task_repo.get_tasks_by_node_instance(node_instance_id)
                
                if not existing_tasks:
                    # åˆ›å»ºä»»åŠ¡å®ä¾‹
                    await self._create_tasks_for_node_new_context(node_info, node_instance_id, workflow_instance_id)
                    logger.info(f"âœ… [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] PROCESSORèŠ‚ç‚¹ {node_info.get('name')} ä»»åŠ¡åˆ›å»ºå®Œæˆ")
                    
                    # é‡æ–°è·å–åˆšåˆ›å»ºçš„ä»»åŠ¡
                    existing_tasks = await task_repo.get_tasks_by_node_instance(node_instance_id)
                else:
                    logger.info(f"â„¹ï¸ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] PROCESSORèŠ‚ç‚¹ {node_info.get('name')} å·²æœ‰ {len(existing_tasks)} ä¸ªä»»åŠ¡å®ä¾‹")
                
                # âœ¨ å…³é”®ä¿®å¤ï¼šæ‰§è¡Œä»»åŠ¡ï¼
                logger.info(f"ğŸš€ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] å¼€å§‹æ‰§è¡ŒPROCESSORèŠ‚ç‚¹ {node_info.get('name')} çš„ä»»åŠ¡")
                logger.info(f"   - å¾…æ‰§è¡Œä»»åŠ¡æ•°é‡: {len(existing_tasks)}")
                
                for i, task in enumerate(existing_tasks, 1):
                    task_title = task.get('task_title', 'unknown')
                    task_type = task.get('task_type', 'unknown') 
                    task_status = task.get('status', 'unknown')
                    logger.info(f"   - ä»»åŠ¡{i}: {task_title} (ç±»å‹: {task_type}, çŠ¶æ€: {task_status})")
                
                # è°ƒç”¨ä»»åŠ¡æ‰§è¡Œæ–¹æ³•
                await self._execute_node_tasks(workflow_instance_id, node_instance_id)
                logger.info(f"âœ… [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] PROCESSORèŠ‚ç‚¹ {node_info.get('name')} ä»»åŠ¡æ‰§è¡Œè°ƒç”¨å®Œæˆ")
            
            elif node_info.get('type') == NodeType.END.value:
                # ğŸ”§ ä¿®å¤ï¼šENDèŠ‚ç‚¹åº”è¯¥æ”¶é›†æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹çš„è¾“å‡ºç»“æœ
                logger.info(f"ğŸ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] ENDèŠ‚ç‚¹ {node_info.get('name')} - æ”¶é›†ä¸Šæ¸¸ç»“æœ")
                
                # æ”¶é›†ä¸Šæ¸¸èŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®
                upstream_outputs = {}
                try:
                    # è·å–å®Œæ•´çš„å·¥ä½œæµä¸Šä¸‹æ–‡æ•°æ®
                    context_data = await self.context_manager.get_task_context_data(workflow_instance_id, node_instance_id)
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é”®å 'immediate_upstream_results' è€Œä¸æ˜¯ 'upstream_outputs'
                    upstream_outputs = context_data.get('immediate_upstream_results', {})
                    logger.info(f"   ğŸ“Š æ”¶é›†åˆ° {len(upstream_outputs)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹çš„è¾“å‡º")
                    
                    # è°ƒè¯•è¾“å‡º
                    logger.info(f"   ğŸ” ä¸Šæ¸¸è¾“å‡ºè¯¦æƒ…: {list(upstream_outputs.keys())}")
                    for node_name, node_data in upstream_outputs.items():
                        output_preview = str(node_data.get('output_data', ''))[:100]
                        logger.info(f"     - {node_name}: {output_preview}...")
                        
                except Exception as e:
                    logger.warning(f"   âš ï¸ æ”¶é›†ä¸Šæ¸¸è¾“å‡ºå¤±è´¥: {e}")
                    import traceback
                    logger.error(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                
                # æ„å»ºåŒ…å«ä¸Šæ¸¸ç»“æœçš„å®Œæ•´è¾“å‡º
                end_output = {
                    'workflow_completed': True,
                    'completion_time': datetime.utcnow().isoformat(),
                    'end_node': node_info.get('name'),
                    'upstream_results': upstream_outputs,  # ğŸ”§ å…³é”®ä¿®å¤ï¼šåŒ…å«ä¸Šæ¸¸ç»“æœ
                    'full_context': self._format_workflow_final_output(upstream_outputs)  # ğŸ”§ æ ¼å¼åŒ–çš„å®Œæ•´ç»“æœ
                }
                
                logger.info(f"   ğŸ“‹ æœ€ç»ˆè¾“å‡ºåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(str(end_output.get('full_context', '')))}")
                
                await workflow_context.mark_node_completed(
                    node_info.get('node_id'),
                    node_instance_id, 
                    end_output
                )
                
                logger.info(f"âœ… [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] ENDèŠ‚ç‚¹ {node_info.get('name')} å·²æ ‡è®°ä¸ºå®Œæˆï¼ˆåŒ…å«ä¸Šæ¸¸ç»“æœï¼‰")
            
            else:
                logger.warning(f"âš ï¸ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] æœªçŸ¥èŠ‚ç‚¹ç±»å‹: {node_info.get('type')}")
                
        except Exception as e:
            logger.error(f"âŒ [ç»Ÿä¸€æ¶æ„-æ‰§è¡Œ] æ‰§è¡ŒèŠ‚ç‚¹å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _format_workflow_final_output(self, upstream_outputs: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å·¥ä½œæµçš„æœ€ç»ˆè¾“å‡ºä¸ºå¯è¯»æ–‡æœ¬"""
        if not upstream_outputs:
            return "å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä¸Šæ¸¸èŠ‚ç‚¹è¾“å‡ºã€‚"
        
        output_lines = ["=== å·¥ä½œæµæ‰§è¡Œç»“æœæ±‡æ€» ===\n"]
        
        for node_name, node_data in upstream_outputs.items():
            output_lines.append(f"ã€èŠ‚ç‚¹ï¼š{node_name}ã€‘")
            
            # æå–èŠ‚ç‚¹è¾“å‡ºæ•°æ®
            if isinstance(node_data, dict):
                # å¤„ç†ç»“æ„åŒ–çš„è¾“å‡ºæ•°æ®
                if 'output_data' in node_data:
                    output_data = node_data['output_data']
                    if isinstance(output_data, str) and output_data.strip():
                        output_lines.append(f"è¾“å‡ºç»“æœï¼š{output_data}")
                    elif isinstance(output_data, dict):
                        # æ ¼å¼åŒ–å­—å…¸è¾“å‡º
                        formatted_output = self._format_dict_as_text(output_data)
                        output_lines.append(f"è¾“å‡ºç»“æœï¼š{formatted_output}")
                    else:
                        output_lines.append("è¾“å‡ºç»“æœï¼š[æ— æœ‰æ•ˆè¾“å‡ºæ•°æ®]")
                
                # æ·»åŠ æ‰§è¡Œç»Ÿè®¡
                if 'status' in node_data:
                    output_lines.append(f"æ‰§è¡ŒçŠ¶æ€ï¼š{node_data['status']}")
                if 'completed_at' in node_data:
                    output_lines.append(f"å®Œæˆæ—¶é—´ï¼š{node_data['completed_at']}")
            else:
                # å¤„ç†ç®€å•çš„è¾“å‡ºæ•°æ®
                output_lines.append(f"è¾“å‡ºç»“æœï¼š{str(node_data)}")
            
            output_lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        output_lines.append("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œæ‰€æœ‰èŠ‚ç‚¹å¤„ç†ç»“æœå·²æ±‡æ€»ã€‚")
        
        return "\n".join(output_lines)
    
    def _format_dict_as_text(self, data: dict) -> str:
        """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬"""
        if not data:
            return "[ç©ºæ•°æ®]"
        
        lines = []
        for key, value in data.items():
            if isinstance(value, (str, int, float, bool)):
                lines.append(f"  â€¢ {key}: {value}")
            elif isinstance(value, dict):
                lines.append(f"  â€¢ {key}: {json.dumps(value, ensure_ascii=False, indent=2)}")
            elif isinstance(value, list):
                lines.append(f"  â€¢ {key}: [{len(value)}é¡¹]")
            else:
                lines.append(f"  â€¢ {key}: {str(value)}")
        
        return "\n".join(lines) if lines else "[æ— æ•°æ®]"
    
    async def _log_task_assignment_event(self, task_id: uuid.UUID, assigned_user_id: Optional[uuid.UUID], task_title: str):
        """è®°å½•ä»»åŠ¡åˆ†é…äº‹ä»¶"""
        try:
            from datetime import datetime
            event_data = {
                'event_type': 'task_assigned',
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'task_id': str(task_id),
                'assigned_user_id': str(assigned_user_id) if assigned_user_id else None
            }
            
            # è¿™é‡Œå¯ä»¥è®°å½•åˆ°äº‹ä»¶æ—¥å¿—è¡¨æˆ–å‘é€åˆ°æ¶ˆæ¯é˜Ÿåˆ—
            logger.trace(f"ğŸ“ ä»»åŠ¡åˆ†é…äº‹ä»¶è®°å½•: {event_data}")
            
            # è®°å½•åˆ°ä¸“é—¨çš„äº‹ä»¶æ—¥å¿—æ–‡ä»¶
            try:
                event_log_entry = f"{event_data['timestamp']}|{event_data['event_type']}|{event_data['task_id']}|{event_data['assigned_user_id']}|{task_title[:50]}"
                with open("task_events.log", "a", encoding="utf-8") as f:
                    f.write(event_log_entry + "\n")
                logger.warning(f"   äº‹ä»¶å·²è®°å½•åˆ°æ–‡ä»¶")
            except Exception as e:
                logger.warning(f"   äº‹ä»¶æ–‡ä»¶è®°å½•å¤±è´¥: {e}")
            
            
        except Exception as e:
            logger.error(f"è®°å½•ä»»åŠ¡åˆ†é…äº‹ä»¶å¤±è´¥: {e}")
    
    async def _log_workflow_execution_summary(self, workflow_instance_id: uuid.UUID):
        """è®°å½•å·¥ä½œæµæ‰§è¡Œæ‘˜è¦"""
        try:
            logger.info(f"ğŸ“Š [æ‰§è¡Œæ‘˜è¦] å¼€å§‹ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œæ‘˜è¦: {workflow_instance_id}")
            
            # è·å–å·¥ä½œæµå®ä¾‹ä¿¡æ¯
            instance = await self.workflow_instance_repo.get_instance_by_id(workflow_instance_id)
            if not instance:
                logger.warning(f"   [æ‰§è¡Œæ‘˜è¦] å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {workflow_instance_id}")
                return
            
            # è·å–æ‰€æœ‰ä»»åŠ¡
            tasks = await self.task_instance_repo.get_tasks_by_workflow_instance(workflow_instance_id)
            logger.info(f"ğŸ“‹ [æ‰§è¡Œæ‘˜è¦] æŸ¥è¯¢åˆ°æ€»ä»»åŠ¡æ•°: {len(tasks) if tasks else 0}")
            
            if tasks:
                logger.info(f"ğŸ“‹ [æ‰§è¡Œæ‘˜è¦] ä»»åŠ¡è¯¦ç»†ä¿¡æ¯:")
                for i, task in enumerate(tasks, 1):
                    logger.info(f"   ä»»åŠ¡{i}: {task.get('task_title')} (çŠ¶æ€: {task.get('status')}, ç±»å‹: {task.get('task_type')}, åˆ†é…ç”¨æˆ·: {task.get('assigned_user_id')})")
            else:
                logger.warning(f"âš ï¸ [æ‰§è¡Œæ‘˜è¦] æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡å®ä¾‹")
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_tasks = len(tasks) if tasks else 0
            human_tasks = len([t for t in tasks if t['task_type'] == TaskInstanceType.HUMAN.value]) if tasks else 0
            agent_tasks = len([t for t in tasks if t['task_type'] == TaskInstanceType.AGENT.value]) if tasks else 0
            assigned_tasks = len([t for t in tasks if t['status'] in ['ASSIGNED', 'IN_PROGRESS', 'COMPLETED']]) if tasks else 0
            pending_tasks = len([t for t in tasks if t['status'] == 'PENDING']) if tasks else 0
            
            logger.info(f"ğŸ“Š [æ‰§è¡Œæ‘˜è¦] ä»»åŠ¡ç»Ÿè®¡:")
            logger.info(f"   - æ€»ä»»åŠ¡æ•°: {total_tasks}")
            logger.info(f"   - äººå·¥ä»»åŠ¡: {human_tasks}")
            logger.info(f"   - Agentä»»åŠ¡: {agent_tasks}")
            logger.info(f"   - å·²åˆ†é…: {assigned_tasks} (çŠ¶æ€ä¸º ASSIGNED/IN_PROGRESS/COMPLETED)")
            logger.info(f"   - ç­‰å¾…ä¸­: {pending_tasks} (çŠ¶æ€ä¸º PENDING)")
            
            # è¯¦ç»†åˆ†æçŠ¶æ€åˆ†å¸ƒ
            if tasks:
                status_distribution = {}
                for task in tasks:
                    status = task.get('status')
                    status_distribution[status] = status_distribution.get(status, 0) + 1
                logger.info(f"ğŸ“Š [æ‰§è¡Œæ‘˜è¦] çŠ¶æ€åˆ†å¸ƒ: {status_distribution}")
            
            # è¾“å‡ºæ‘˜è¦
            print(f"\nğŸ“Š ã€å·¥ä½œæµæ‰§è¡Œæ‘˜è¦ã€‘")
            print(f"å·¥ä½œæµå®ä¾‹: {instance.get('workflow_instance_name', 'Unknown')}")
            print(f"å®ä¾‹ID: {workflow_instance_id}")
            print(f"çŠ¶æ€: {instance.get('status', 'Unknown')}")
            print(f"æ€»ä»»åŠ¡æ•°: {total_tasks}")
            print(f"  - äººå·¥ä»»åŠ¡: {human_tasks}")
            print(f"  - Agentä»»åŠ¡: {agent_tasks}")
            print(f"  - å·²åˆ†é…: {assigned_tasks}")
            print(f"  - ç­‰å¾…ä¸­: {pending_tasks}")
            print(f"åˆ›å»ºæ—¶é—´: {instance.get('created_at', 'Unknown')}")
            print("=" * 50)
            
            # åˆ—å‡ºæ‰€æœ‰å·²åˆ†é…çš„äººå·¥ä»»åŠ¡
            human_assigned_tasks = [t for t in tasks if t['task_type'] == TaskInstanceType.HUMAN.value and t.get('assigned_user_id')]
            if human_assigned_tasks:
                print(f"ğŸ“‹ å·²åˆ†é…çš„äººå·¥ä»»åŠ¡:")
                for i, task in enumerate(human_assigned_tasks, 1):
                    print(f"  {i}. {task['task_title']}")
                    print(f"     ç”¨æˆ·: {task.get('assigned_user_id')}")
                    print(f"     çŠ¶æ€: {task['status']}")
            else:
                print(f"ğŸ“‹ æš‚æ— å·²åˆ†é…çš„äººå·¥ä»»åŠ¡")
                if tasks:
                    print(f"ğŸ“‹ æ‰€æœ‰ä»»åŠ¡è¯¦æƒ…:")
                    for i, task in enumerate(tasks, 1):
                        print(f"  {i}. æ ‡é¢˜: {task['task_title']}")
                        print(f"     çŠ¶æ€: {task['status']}")
                        print(f"     ç±»å‹: {task['task_type']}")
                        print(f"     åˆ†é…ç”¨æˆ·: {task.get('assigned_user_id', 'None')}")
                        print(f"     ä»»åŠ¡ID: {task.get('task_instance_id')}")
                        print("     ---")
                else:
                    print(f"ğŸ“‹ âš ï¸ å·¥ä½œæµä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä»»åŠ¡å®ä¾‹ï¼")
            print("=" * 50)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œæ‘˜è¦å¤±è´¥: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
    
    async def _notify_user_new_task(self, user_id: uuid.UUID, task_id: uuid.UUID, task_title: str):
        """é€šçŸ¥ç”¨æˆ·æœ‰æ–°ä»»åŠ¡åˆ†é…"""
        try:
            logger.trace(f"ğŸ”” å¼€å§‹å‘é€ä»»åŠ¡é€šçŸ¥ç»™ç”¨æˆ·: {user_id}")
            
            # è·å–ç”¨æˆ·ä¿¡æ¯ç”¨äºé€šçŸ¥
            user_info = await self.user_repo.get_by_id(user_id, id_column="user_id")
            username = user_info.get('username', 'Unknown') if user_info else 'Unknown'
            
            notification_data = {
                'user_id': str(user_id),
                'username': username,
                'task_id': str(task_id),
                'task_title': task_title,
                'notification_type': 'new_task_assigned',
                'timestamp': now_utc().isoformat(),
                'message': f'æ‚¨æœ‰æ–°çš„ä»»åŠ¡: {task_title}',
                'action_url': f'/tasks/{task_id}'
            }
            
            logger.trace(f"ğŸ“¨ é€šçŸ¥æ•°æ®å‡†å¤‡å®Œæˆ:")
            logger.trace(f"   - ç”¨æˆ·: {username} ({user_id})")
            logger.trace(f"   - ä»»åŠ¡: {task_title}")
            logger.trace(f"   - æ—¶é—´: {notification_data['timestamp']}")
            
            # æ–¹å¼1: æ§åˆ¶å°é€šçŸ¥ï¼ˆç”¨äºå¼€å‘è°ƒè¯•ï¼‰
            print(f"\nğŸ”” ã€ç”¨æˆ·é€šçŸ¥ã€‘")
            print(f"ç”¨æˆ·: {username} ({user_id})")
            print(f"æ¶ˆæ¯: æ‚¨æœ‰æ–°çš„ä»»åŠ¡åˆ†é…")
            print(f"ä»»åŠ¡: {task_title}")
            print(f"ä»»åŠ¡ID: {task_id}")
            print(f"æ—¶é—´: {notification_data['timestamp']}")
            print(f"æ“ä½œ: è¯·ç™»å½•ç³»ç»ŸæŸ¥çœ‹ä»»åŠ¡è¯¦æƒ…")
            print("=" * 50)
            
            # æ–¹å¼2: è®°å½•åˆ°æ•°æ®åº“ï¼ˆç”¨æˆ·é€šçŸ¥å†å²ï¼‰
            try:
                await self._store_user_notification(notification_data)
                logger.trace(f"   âœ… é€šçŸ¥å·²å­˜å‚¨åˆ°æ•°æ®åº“")
            except Exception as e:
                logger.warning(f"   âš ï¸  å­˜å‚¨é€šçŸ¥å¤±è´¥: {e}")
            
            # æ–¹å¼3: æ–‡ä»¶æ—¥å¿—è®°å½•ï¼ˆå¯ç”¨äºå…¶ä»–ç³»ç»Ÿè¯»å–ï¼‰
            try:
                notification_log_entry = f"{now_utc().isoformat()}|TASK_ASSIGNED|{user_id}|{username}|{task_id}|{task_title}"
                with open("user_notifications.log", "a", encoding="utf-8") as f:
                    f.write(notification_log_entry + "\n")
                logger.trace(f"   âœ… é€šçŸ¥å·²è®°å½•åˆ°æ–‡ä»¶")
            except Exception as e:
                logger.warning(f"   âš ï¸  æ–‡ä»¶è®°å½•å¤±è´¥: {e}")
            
            # TODO: æ–¹å¼4: å®æ—¶æ¨é€ï¼ˆæœªæ¥å®ç°ï¼‰
            # å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€å®ç°ï¼š
            # 1. WebSocket æ¨é€: await self.websocket_manager.send_to_user(user_id, notification_data)
            # 2. Server-Sent Events (SSE): await self.sse_manager.send_event(user_id, notification_data)
            # 3. æ¶ˆæ¯é˜Ÿåˆ—: await self.message_queue.publish(f"user.{user_id}.notifications", notification_data)
            # 4. é‚®ä»¶é€šçŸ¥: await self.email_service.send_task_notification(user_info.get('email'), notification_data)
            
            logger.trace(f"   ğŸ‰ ç”¨æˆ·é€šçŸ¥å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å‘é€ç”¨æˆ·é€šçŸ¥å¤±è´¥: {e}")
            import traceback
            logger.error(f"   å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
    
    async def _store_user_notification(self, notification_data: dict):
        """å­˜å‚¨ç”¨æˆ·é€šçŸ¥åˆ°æ•°æ®åº“ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥å­˜å‚¨åˆ°ä¸“é—¨çš„é€šçŸ¥è¡¨ä¸­
            # å¦‚æœæ²¡æœ‰é€šçŸ¥è¡¨ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„è®°å½•è¡¨
            logger.warning(f"å­˜å‚¨é€šçŸ¥æ•°æ®: {notification_data}")
            # æš‚æ—¶è·³è¿‡æ•°æ®åº“å­˜å‚¨ï¼Œé¿å…è¡¨ç»“æ„ä¾èµ–
        except Exception as e:
            logger.warning(f"å­˜å‚¨ç”¨æˆ·é€šçŸ¥å¤±è´¥: {e}")
    
    # ================================================================================
    # å»¶è¿Ÿä»»åŠ¡åˆ›å»ºæœºåˆ¶ - æ ¸å¿ƒæ–¹æ³•
    # ================================================================================
    
    async def _check_node_prerequisites(self, workflow_instance_id: uuid.UUID, 
                                      node_instance_id: uuid.UUID) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹çš„å‰ç½®æ¡ä»¶æ˜¯å¦æ»¡è¶³ï¼ˆä¿®å¤ç‰ˆï¼šåŒé‡éªŒè¯ï¼‰"""
        try:
            logger.trace(f"ğŸ” æ£€æŸ¥èŠ‚ç‚¹å‰ç½®æ¡ä»¶: {node_instance_id}")
            
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥å¹¶æ¢å¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨çŠ¶æ€
            if workflow_instance_id not in self.context_manager.contexts:
                logger.warning(f"âš ï¸ [ä¸Šä¸‹æ–‡æ¢å¤] å·¥ä½œæµå®ä¾‹ {workflow_instance_id} ä¸åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼Œå°è¯•æ¢å¤...")
                
                # é‡æ–°åˆ›å»ºä¸Šä¸‹æ–‡
                context = await self.context_manager.get_or_create_context(workflow_instance_id)
                
                # ä»æ•°æ®åº“æ¢å¤å·²å®ŒæˆèŠ‚ç‚¹çŠ¶æ€
                await self._recover_context_state_from_database(workflow_instance_id, context)
                
                logger.info(f"âœ… [ä¸Šä¸‹æ–‡æ¢å¤] å·¥ä½œæµå®ä¾‹ {workflow_instance_id} ä¸Šä¸‹æ–‡æ¢å¤æˆåŠŸ")
            
            # ä»æ•°æ®åº“æŸ¥è¯¢èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            node_instance = await node_repo.get_instance_by_id(node_instance_id)
            
            if not node_instance:
                logger.error(f"âŒ èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨: {node_instance_id}")
                return False
            
            node_id = node_instance['node_id']
            logger.trace(f"  èŠ‚ç‚¹ID: {node_id}")
            
            # ğŸ”’ ä¸¥æ ¼æ£€æŸ¥ï¼šé˜²æ­¢é‡å¤æ‰§è¡Œ
            current_status = node_instance.get('status')
            if current_status in ['running', 'completed', 'failed']:
                logger.trace(f"  ğŸš« èŠ‚ç‚¹å·²å¤„äº {current_status} çŠ¶æ€ï¼Œè·³è¿‡æ£€æŸ¥")
                return False  # å·²å¤„ç†è¿‡çš„èŠ‚ç‚¹ä¸å†å¤„ç†
            
            # ğŸ” åŒé‡æ£€æŸ¥ï¼šä¸Šä¸‹æ–‡ç®¡ç†å™¨çŠ¶æ€
            if hasattr(self.context_manager, 'contexts') and workflow_instance_id in self.context_manager.contexts:
                context = self.context_manager.contexts[workflow_instance_id]
                if node_instance_id in context.execution_context.get('completed_nodes', set()):
                    logger.trace(f"  ğŸš« èŠ‚ç‚¹åœ¨ä¸Šä¸‹æ–‡ä¸­å·²å®Œæˆï¼Œè·³è¿‡æ£€æŸ¥")
                    return False
                if node_instance_id in context.execution_context.get('current_executing_nodes', set()):
                    logger.trace(f"  ğŸš« èŠ‚ç‚¹åœ¨ä¸Šä¸‹æ–‡ä¸­æ­£åœ¨æ‰§è¡Œï¼Œè·³è¿‡æ£€æŸ¥")
                    return False
            
            # æŸ¥è¯¢è¯¥èŠ‚ç‚¹çš„å‰ç½®èŠ‚ç‚¹ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼çš„æŸ¥è¯¢ï¼‰
            prerequisite_query = '''
            SELECT source_n.node_id as prerequisite_node_id, source_n.name as prerequisite_name,
                   source_ni.node_instance_id as prerequisite_instance_id, source_ni.status as prerequisite_status,
                   c.workflow_id
            FROM node_connection c
            JOIN node source_n ON c.from_node_id = source_n.node_id  
            JOIN node target_n ON c.to_node_id = target_n.node_id
            JOIN node_instance source_ni ON source_n.node_id = source_ni.node_id
            WHERE target_n.node_id = $1 
              AND source_ni.workflow_instance_id = $2
              AND source_ni.is_deleted = FALSE
            ORDER BY source_n.name
            '''
            
            prerequisites = await self.workflow_instance_repo.db.fetch_all(
                prerequisite_query, node_id, workflow_instance_id
            )
            
            logger.trace(f"  æ‰¾åˆ° {len(prerequisites)} ä¸ªå‰ç½®èŠ‚ç‚¹")
            
            # å¦‚æœæ²¡æœ‰å‰ç½®èŠ‚ç‚¹ï¼ˆå¦‚STARTèŠ‚ç‚¹ï¼‰ï¼Œéœ€è¦å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if not prerequisites:
                # å¯¹äºSTARTèŠ‚ç‚¹ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡
                if current_status == 'completed':
                    logger.trace(f"  ğŸš« STARTèŠ‚ç‚¹å·²å®Œæˆï¼Œè·³è¿‡")
                    return False
                logger.trace(f"  âœ… æ— å‰ç½®èŠ‚ç‚¹ï¼ˆSTARTèŠ‚ç‚¹ï¼‰ï¼Œæ»¡è¶³æ¡ä»¶")
                return True
            
            # æ£€æŸ¥æ‰€æœ‰å‰ç½®èŠ‚ç‚¹æ˜¯å¦éƒ½å·²å®Œæˆ
            all_completed = True
            completed_count = 0
            
            for prerequisite in prerequisites:
                status = prerequisite['prerequisite_status']
                name = prerequisite['prerequisite_name']
                prereq_node_id = prerequisite['prerequisite_node_id']
                
                logger.trace(f"    å‰ç½®èŠ‚ç‚¹ {name} (node_id: {prereq_node_id}): {status}")
                
                if status == 'completed':
                    completed_count += 1
                    
                    # ğŸ” åŒé‡éªŒè¯ï¼šæ£€æŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­çš„çŠ¶æ€
                    if hasattr(self.context_manager, 'contexts') and workflow_instance_id in self.context_manager.contexts:
                        context = self.context_manager.contexts[workflow_instance_id]
                        if prereq_node_id not in context.execution_context.get('completed_nodes', set()):
                            logger.warning(f"    âš ï¸ å‰ç½®èŠ‚ç‚¹ {name} æ•°æ®åº“æ˜¾ç¤ºå·²å®Œæˆä½†ä¸Šä¸‹æ–‡æœªæ›´æ–°ï¼ŒåŒæ­¥çŠ¶æ€")
                            # åŒæ­¥çŠ¶æ€åˆ°ä¸Šä¸‹æ–‡
                            context.execution_context['completed_nodes'].add(prereq_node_id)
                    
                    logger.trace(f"    âœ… å‰ç½®èŠ‚ç‚¹ {name} å·²å®Œæˆ")
                else:
                    all_completed = False
                    logger.trace(f"    âŒ å‰ç½®èŠ‚ç‚¹ {name} æœªå®Œæˆ: {status}")
            
            # æœ€ç»ˆç»“æœæ£€æŸ¥
            if all_completed and completed_count == len(prerequisites):
                logger.trace(f"  âœ… æ‰€æœ‰å‰ç½®èŠ‚ç‚¹å·²å®Œæˆ ({completed_count}/{len(prerequisites)})ï¼Œæ»¡è¶³ä»»åŠ¡åˆ›å»ºæ¡ä»¶")
                return True
            else:
                logger.trace(f"  â³ å‰ç½®èŠ‚ç‚¹æœªå…¨éƒ¨å®Œæˆ ({completed_count}/{len(prerequisites)})ï¼Œç­‰å¾…ä¸­")
                return False
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥èŠ‚ç‚¹å‰ç½®æ¡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    
    async def _recover_context_state_from_database(self, workflow_instance_id: uuid.UUID, context):
        """ä»æ•°æ®åº“æ¢å¤ä¸Šä¸‹æ–‡çŠ¶æ€ï¼ˆä¿®å¤ä¸¢å¤±çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨çŠ¶æ€ï¼‰"""
        try:
            logger.debug(f"ğŸ”„ [ä¸Šä¸‹æ–‡æ¢å¤] å¼€å§‹ä»æ•°æ®åº“æ¢å¤å·¥ä½œæµ {workflow_instance_id} çš„çŠ¶æ€...")
            
            # æŸ¥è¯¢æ‰€æœ‰å·²å®Œæˆçš„èŠ‚ç‚¹
            completed_nodes_query = '''
            SELECT ni.node_id, ni.node_instance_id, ni.output_data, n.name
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id  
            WHERE ni.workflow_instance_id = $1 
              AND ni.status = 'completed'
              AND ni.is_deleted = FALSE
            '''
            
            completed_nodes = await self.workflow_instance_repo.db.fetch_all(
                completed_nodes_query, workflow_instance_id
            )
            
            logger.debug(f"  ğŸ” å‘ç° {len(completed_nodes)} ä¸ªå·²å®Œæˆçš„èŠ‚ç‚¹")
            
            # æ¢å¤å·²å®ŒæˆèŠ‚ç‚¹çŠ¶æ€åˆ°ä¸Šä¸‹æ–‡
            for node in completed_nodes:
                node_id = node['node_id']
                node_instance_id = node['node_instance_id']
                output_data = node['output_data'] or {}
                node_name = node['name']
                
                # æ·»åŠ åˆ°å·²å®ŒæˆèŠ‚ç‚¹é›†åˆ - ä¿®å¤ï¼šä½¿ç”¨node_instance_id
                context.execution_context['completed_nodes'].add(node_instance_id)
                
                # æ¢å¤èŠ‚ç‚¹è¾“å‡ºæ•°æ® - ä¿®å¤ï¼šä½¿ç”¨node_instance_idä½œä¸ºkey
                context.execution_context['node_outputs'][node_instance_id] = output_data
                
                logger.debug(f"    âœ… æ¢å¤èŠ‚ç‚¹ {node_name} ({node_instance_id}) çš„å®ŒæˆçŠ¶æ€")
            
            # æŸ¥è¯¢æ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹
            executing_nodes_query = '''
            SELECT ni.node_id, ni.node_instance_id, n.name
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = $1 
              AND ni.status = 'running'
              AND ni.is_deleted = FALSE
            '''
            
            executing_nodes = await self.workflow_instance_repo.db.fetch_all(
                executing_nodes_query, workflow_instance_id
            )
            
            logger.debug(f"  ğŸ” å‘ç° {len(executing_nodes)} ä¸ªæ‰§è¡Œä¸­çš„èŠ‚ç‚¹")
            
            # æ¢å¤æ‰§è¡Œä¸­èŠ‚ç‚¹çŠ¶æ€
            for node in executing_nodes:
                node_id = node['node_id']
                node_instance_id = node['node_instance_id']
                node_name = node['name']
                
                # ä¿®å¤ï¼šä½¿ç”¨node_instance_id
                context.execution_context['current_executing_nodes'].add(node_instance_id)
                logger.debug(f"    ğŸƒ æ¢å¤èŠ‚ç‚¹ {node_name} ({node_instance_id}) çš„æ‰§è¡ŒçŠ¶æ€")
            
            # é‡æ–°æ„å»ºèŠ‚ç‚¹ä¾èµ–ä¿¡æ¯
            await self._rebuild_all_node_dependencies(workflow_instance_id)
            
            logger.info(f"âœ… [ä¸Šä¸‹æ–‡æ¢å¤] å·¥ä½œæµ {workflow_instance_id} çŠ¶æ€æ¢å¤å®Œæˆ")
            logger.info(f"    - å·²å®ŒæˆèŠ‚ç‚¹: {len(completed_nodes)} ä¸ª")
            logger.info(f"    - æ‰§è¡Œä¸­èŠ‚ç‚¹: {len(executing_nodes)} ä¸ª")
            
        except Exception as e:
            logger.error(f"âŒ [ä¸Šä¸‹æ–‡æ¢å¤] æ¢å¤å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _rebuild_all_node_dependencies(self, workflow_instance_id: uuid.UUID):
        """é‡å»ºæ‰€æœ‰èŠ‚ç‚¹çš„ä¾èµ–å…³ç³»"""
        try:
            logger.debug(f"ğŸ”„ é‡å»ºå·¥ä½œæµ {workflow_instance_id} çš„æ‰€æœ‰èŠ‚ç‚¹ä¾èµ–å…³ç³»...")
            
            # æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
            nodes_query = '''
            SELECT ni.node_instance_id, ni.node_id, n.name
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = $1 AND ni.is_deleted = FALSE
            '''
            
            nodes = await self.workflow_instance_repo.db.fetch_all(nodes_query, workflow_instance_id)
            
            for node in nodes:
                node_instance_id = node['node_instance_id']
                node_id = node['node_id']
                
                # æŸ¥è¯¢è¯¥èŠ‚ç‚¹çš„ä¸Šæ¸¸ä¾èµ–
                upstream_query = '''
                SELECT DISTINCT nc.from_node_id
                FROM node_connection nc
                WHERE nc.to_node_id = $1
                '''
                
                upstream_results = await self.workflow_instance_repo.db.fetch_all(upstream_query, node_id)
                upstream_node_ids = [result['from_node_id'] for result in upstream_results]
                
                # è½¬æ¢ä¸ºnode_instance_id
                upstream_node_instance_ids = []
                for upstream_node_id in upstream_node_ids:
                    instance_query = """
                    SELECT node_instance_id 
                    FROM node_instance 
                    WHERE node_id = $1 AND workflow_instance_id = $2 AND is_deleted = FALSE
                    """
                    upstream_instance_result = await self.workflow_instance_repo.db.fetch_one(
                        instance_query, upstream_node_id, workflow_instance_id
                    )
                    
                    if upstream_instance_result:
                        upstream_node_instance_ids.append(upstream_instance_result['node_instance_id'])
                    else:
                        logger.warning(f"    âš ï¸ æœªæ‰¾åˆ°ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} å¯¹åº”çš„å®ä¾‹")
                
                # æ³¨å†Œä¾èµ–å…³ç³»åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                await self.context_manager.register_node_dependencies(
                    workflow_instance_id, node_instance_id, node_id, upstream_node_instance_ids
                )
                
                logger.debug(f"    âœ… é‡å»ºèŠ‚ç‚¹ {node['name']} ä¾èµ–: {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹")
            
            logger.debug(f"âœ… æ‰€æœ‰èŠ‚ç‚¹ä¾èµ–å…³ç³»é‡å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ é‡å»ºèŠ‚ç‚¹ä¾èµ–å…³ç³»å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _create_tasks_when_ready(self, workflow_instance_id: uuid.UUID, 
                                     node_instance_id: uuid.UUID) -> bool:
        """å½“èŠ‚ç‚¹æ»¡è¶³å‰ç½®æ¡ä»¶æ—¶åˆ›å»ºä»»åŠ¡"""
        try:
            logger.trace(f"ğŸ¯ å°è¯•ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡: {node_instance_id}")
            
            # æ£€æŸ¥å‰ç½®æ¡ä»¶
            prerequisites_met = await self._check_node_prerequisites(workflow_instance_id, node_instance_id)
            if not prerequisites_met:
                logger.trace(f"  â³ å‰ç½®æ¡ä»¶æœªæ»¡è¶³ï¼Œæš‚ä¸åˆ›å»ºä»»åŠ¡")
                return False
            
            # è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            node_instance = await node_repo.get_instance_by_id(node_instance_id)
            
            if not node_instance:
                logger.error(f"âŒ èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨: {node_instance_id}")
                return False
            
            # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
            node = await self.node_repo.get_node_by_id(node_instance['node_id'])
            if not node:
                logger.error(f"âŒ èŠ‚ç‚¹ä¸å­˜åœ¨: {node_instance['node_id']}")
                return False
            
            # åªä¸ºPROCESSORèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡
            if node['type'] != NodeType.PROCESSOR.value:
                logger.trace(f"  â­ï¸ èŠ‚ç‚¹ç±»å‹ä¸æ˜¯PROCESSOR ({node['type']})ï¼Œè‡ªåŠ¨å®Œæˆ")
                
                # å¯¹äºéPROCESSORèŠ‚ç‚¹çš„å¤„ç†ç­–ç•¥
                if node['type'] == NodeType.END.value:
                    # ğŸ”§ ä¿®å¤ENDèŠ‚ç‚¹è¿‡æ—©æ‰§è¡Œé—®é¢˜ï¼šä¸åœ¨è¿™é‡Œç›´æ¥æ‰§è¡Œï¼Œè®©ä¾èµ–é©±åŠ¨çš„è§¦å‘æœºåˆ¶å¤„ç†
                    logger.trace(f"  ğŸ ENDèŠ‚ç‚¹ {node_instance_id} ç­‰å¾…ä¾èµ–é©±åŠ¨çš„è§¦å‘æœºåˆ¶å¤„ç†")
                    return False  # ä¸åˆ›å»ºä»»åŠ¡ï¼Œç­‰å¾…æ­£ç¡®çš„è§¦å‘æ—¶æœº
                elif node['type'] == NodeType.START.value:
                    from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
                    update_data = NodeInstanceUpdate(status=NodeInstanceStatus.COMPLETED)
                    await node_repo.update_node_instance(node_instance_id, update_data)
                    
                    # ä¿®å¤ï¼šSTARTèŠ‚ç‚¹å®Œæˆæ—¶ä¹Ÿéœ€è¦è°ƒç”¨mark_node_completedå­˜å‚¨è¾“å‡ºæ•°æ®
                    logger.trace(f"ğŸš€ STARTèŠ‚ç‚¹å®Œæˆï¼Œå­˜å‚¨è¾“å‡ºæ•°æ®åˆ°WorkflowContextManager")
                    task_description = node.get('task_description', 'å¼€å§‹èŠ‚ç‚¹å·²å®Œæˆ')
                    start_output_data = {
                        'task_result': task_description,  # å°†task_descriptionä½œä¸ºè¾“å‡ºç»“æœä¼ é€’ç»™ä¸‹æ¸¸
                        'task_summary': 'STARTèŠ‚ç‚¹å¤„ç†å®Œæˆ',
                        'execution_time': 0,
                        'completion_time': datetime.utcnow().isoformat()
                    }
                    logger.trace(f"  - STARTèŠ‚ç‚¹è¾“å‡ºæ•°æ®: {start_output_data}")
                    
                    await self.context_manager.mark_node_completed(
                        workflow_instance_id=workflow_instance_id,
                        node_id=node['node_id'],
                        node_instance_id=node_instance_id,
                        output_data=start_output_data
                    )
                else:
                    logger.error(f"error node type:{node['type']}")

                # ğŸ”§ ä¿®å¤ï¼šåªæœ‰STARTèŠ‚ç‚¹æ‰éœ€è¦ç«‹å³æ£€æŸ¥ä¸‹æ¸¸ï¼ŒENDèŠ‚ç‚¹é€šè¿‡ä¾èµ–æœºåˆ¶è§¦å‘
                if node['type'] == NodeType.START.value:
                    await self._check_downstream_nodes_for_task_creation(workflow_instance_id)
                return True
            
            # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€ï¼Œå¦‚æœå·²å®Œæˆæˆ–æ­£åœ¨è¿è¡Œåˆ™æ— éœ€å¤„ç†
            current_status = node_instance['status']
            if current_status in ['completed', 'running', 'failed']:
                logger.trace(f"  âœ… èŠ‚ç‚¹çŠ¶æ€ä¸º{current_status}ï¼Œæ— éœ€é‡å¤å¤„ç†")
                return True
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ›å»ºè¿‡ä»»åŠ¡
            existing_tasks_query = '''
            SELECT task_instance_id FROM task_instance 
            WHERE node_instance_id = $1 AND is_deleted = FALSE
            '''
            existing_tasks = await self.task_instance_repo.db.fetch_all(existing_tasks_query, node_instance_id)
            
            if existing_tasks:
                logger.trace(f"  âœ… ä»»åŠ¡å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤åˆ›å»º")
                return True
            
            # å¯¹äºå·²ç»æ˜¯pendingçŠ¶æ€çš„èŠ‚ç‚¹ï¼Œé¿å…é‡å¤è®¾ç½®
            if current_status == 'pending':
                logger.trace(f"  â„¹ï¸ èŠ‚ç‚¹å·²æ˜¯pendingçŠ¶æ€ï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°ç›´æ¥åˆ›å»ºä»»åŠ¡")
                # è·³è¿‡çŠ¶æ€æ›´æ–°ï¼Œç›´æ¥è¿›å…¥ä»»åŠ¡åˆ›å»ºæµç¨‹
            else:
                # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ä¸ºå‡†å¤‡ä¸­
                from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
                update_data = NodeInstanceUpdate(status=NodeInstanceStatus.PENDING)
                
                # æ·»åŠ é‡è¯•æœºåˆ¶æ¥å¤„ç†å¯èƒ½çš„æ—¶åºé—®é¢˜
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        result = await node_repo.update_node_instance(node_instance_id, update_data)
                        if result:
                            logger.trace(f"  âœ… èŠ‚ç‚¹çŠ¶æ€æ›´æ–°ä¸ºpendingæˆåŠŸ")
                            break
                        elif attempt < max_retries - 1:
                            logger.warning(f"èŠ‚ç‚¹å®ä¾‹æ›´æ–°å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾…åé‡è¯•...")
                            await asyncio.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
                        else:
                            logger.error(f"èŠ‚ç‚¹å®ä¾‹æ›´æ–°å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                            # ç»§ç»­æ‰§è¡Œï¼Œä¸å› ä¸ºçŠ¶æ€æ›´æ–°å¤±è´¥è€Œä¸­æ–­æ•´ä¸ªæµç¨‹
                    except Exception as e:
                        if attempt < max_retries - 1:
                            logger.warning(f"èŠ‚ç‚¹å®ä¾‹æ›´æ–°å¼‚å¸¸ï¼Œå°è¯• {attempt + 1}/{max_retries}: {e}")
                            await asyncio.sleep(0.1)
                        else:
                            logger.error(f"èŠ‚ç‚¹å®ä¾‹æ›´æ–°å¼‚å¸¸ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                            # ç»§ç»­æ‰§è¡Œ
            
            # ä¸ºè¯¥èŠ‚ç‚¹åˆ›å»ºä»»åŠ¡
            created_node = {
                'node_instance_id': node_instance_id,
                'node_id': node['node_id'],  # ä½¿ç”¨node_idè€Œä¸æ˜¯node_base_id
                'node_type': node['type'],
                'node_data': node
            }
            
            await self._create_tasks_for_nodes([created_node], workflow_instance_id)
            
            logger.trace(f"  âœ… èŠ‚ç‚¹ä»»åŠ¡åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºèŠ‚ç‚¹ä»»åŠ¡å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    
    async def _check_downstream_nodes_for_task_creation(self, workflow_instance_id: uuid.UUID):
        """æ£€æŸ¥ä¸‹æ¸¸èŠ‚ç‚¹æ˜¯å¦å¯ä»¥åˆ›å»ºä»»åŠ¡ - å¢å¼ºå¹¶å‘å¤„ç†ç‰ˆæœ¬"""
        max_retries = 3
        retry_delay = 0.5
        
        for attempt in range(max_retries):
            try:
                logger.trace(f"ğŸ”„ æ£€æŸ¥ä¸‹æ¸¸èŠ‚ç‚¹ä»»åŠ¡åˆ›å»ºæœºä¼š (å°è¯• {attempt + 1}/{max_retries})")
                
                # ğŸ”§ å¼ºåˆ¶åˆ·æ–°å·¥ä½œæµä¸Šä¸‹æ–‡çŠ¶æ€ï¼ˆé˜²æ­¢çŠ¶æ€å»¶è¿Ÿï¼‰
                await self._refresh_workflow_context_state(workflow_instance_id)
                
                # æŸ¥è¯¢å·¥ä½œæµä¸­æ‰€æœ‰ç­‰å¾…çŠ¶æ€çš„èŠ‚ç‚¹
                waiting_nodes_query = '''
                SELECT ni.node_instance_id, ni.node_id, n.name, n.type
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.workflow_instance_id = %s 
                  AND ni.status = 'pending'
                  AND ni.is_deleted = FALSE
                '''
                
                waiting_nodes = await self.workflow_instance_repo.db.fetch_all(
                    waiting_nodes_query, workflow_instance_id
                )
                
                logger.trace(f"  æ‰¾åˆ° {len(waiting_nodes)} ä¸ªç­‰å¾…ä¸­çš„èŠ‚ç‚¹")
                
                created_any_task = False
                
                # ä¸ºæ¯ä¸ªç­‰å¾…èŠ‚ç‚¹æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ›å»ºä»»åŠ¡
                for node in waiting_nodes:
                    node_instance_id = node['node_instance_id']
                    node_name = node['name']
                    
                    logger.trace(f"  æ£€æŸ¥èŠ‚ç‚¹: {node_name} ({node_instance_id})")
                    
                    # ğŸ”§ æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²ç»æœ‰ä»»åŠ¡ï¼ˆé˜²æ­¢é‡å¤åˆ›å»ºï¼‰
                    existing_tasks = await self.task_instance_repo.db.fetch_all(
                        "SELECT task_instance_id FROM task_instance WHERE node_instance_id = %s AND is_deleted = FALSE",
                        node_instance_id
                    )
                    
                    if existing_tasks:
                        logger.trace(f"    èŠ‚ç‚¹ {node_name} å·²æœ‰ {len(existing_tasks)} ä¸ªä»»åŠ¡ï¼Œè·³è¿‡")
                        continue
                    
                    # å°è¯•åˆ›å»ºä»»åŠ¡
                    try:
                        created = await self._create_tasks_when_ready(workflow_instance_id, node_instance_id)
                        if created:
                            logger.info(f"  âœ… ä¸ºèŠ‚ç‚¹ {node_name} åˆ›å»ºäº†ä»»åŠ¡")
                            created_any_task = True
                        else:
                            logger.trace(f"  â³ èŠ‚ç‚¹ {node_name} ä¾èµ–æœªæ»¡è¶³æˆ–ä¸ç¬¦åˆåˆ›å»ºæ¡ä»¶")
                    except Exception as e:
                        logger.warning(f"  âŒ ä¸ºèŠ‚ç‚¹ {node_name} åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
                        continue
                
                # å¦‚æœæˆåŠŸåˆ›å»ºäº†ä»»åŠ¡æˆ–æ²¡æœ‰ç­‰å¾…èŠ‚ç‚¹ï¼Œåˆ™é€€å‡ºé‡è¯•
                if created_any_task or len(waiting_nodes) == 0:
                    if created_any_task:
                        logger.info(f"âœ… ä¸‹æ¸¸èŠ‚ç‚¹æ£€æŸ¥å®Œæˆï¼Œåˆ›å»ºäº†æ–°ä»»åŠ¡")
                    return
                
                # å¦‚æœæ²¡æœ‰åˆ›å»ºä»»ä½•ä»»åŠ¡ä¸”è¿˜æœ‰ç­‰å¾…èŠ‚ç‚¹ï¼Œå¯èƒ½éœ€è¦é‡è¯•
                if attempt < max_retries - 1:
                    logger.debug(f"  â±ï¸ æ²¡æœ‰åˆ›å»ºä»»åŠ¡ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5  # æŒ‡æ•°é€€é¿
                
            except Exception as e:
                logger.error(f"æ£€æŸ¥ä¸‹æ¸¸èŠ‚ç‚¹å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    import traceback
                    logger.error(f"ä¸‹æ¸¸èŠ‚ç‚¹æ£€æŸ¥æœ€ç»ˆå¤±è´¥: {traceback.format_exc()}")
        
        logger.trace(f"ğŸ ä¸‹æ¸¸èŠ‚ç‚¹æ£€æŸ¥å®Œæˆ")
    
    async def _refresh_workflow_context_state(self, workflow_instance_id: uuid.UUID):
        """åˆ·æ–°å·¥ä½œæµä¸Šä¸‹æ–‡çŠ¶æ€ï¼ˆé˜²æ­¢çŠ¶æ€å»¶è¿Ÿé—®é¢˜ï¼‰"""
        try:
            from .workflow_execution_context import get_context_manager
            context_manager = get_context_manager()
            workflow_context = await context_manager.get_context(workflow_instance_id)
            
            if not workflow_context:
                logger.debug(f"   ğŸ“‹ å·¥ä½œæµä¸Šä¸‹æ–‡ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ·æ–°")
                return
            
            # ğŸ”§ ä»æ•°æ®åº“é‡æ–°åŒæ­¥èŠ‚ç‚¹çŠ¶æ€
            nodes_query = """
            SELECT ni.node_instance_id, ni.status, n.node_id
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = %s
            AND ni.is_deleted = FALSE
            """
            
            nodes = await self.workflow_instance_repo.db.fetch_all(nodes_query, workflow_instance_id)
            
            updated_count = 0
            for node in nodes:
                node_instance_id = node['node_instance_id']
                db_status = node['status']
                
                # å°†æ•°æ®åº“çŠ¶æ€è½¬æ¢ä¸ºä¸Šä¸‹æ–‡çŠ¶æ€
                context_status = {
                    'pending': 'PENDING',
                    'running': 'EXECUTING', 
                    'completed': 'COMPLETED',
                    'failed': 'FAILED'
                }.get(db_status, 'UNKNOWN')
                
                current_status = workflow_context.node_states.get(node_instance_id)
                
                if current_status != context_status:
                    workflow_context.node_states[node_instance_id] = context_status
                    updated_count += 1
                    
                    # æ›´æ–°å®ŒæˆèŠ‚ç‚¹é›†åˆ
                    if context_status == 'COMPLETED':
                        workflow_context.execution_context['completed_nodes'].add(node_instance_id)
                    elif context_status in ['PENDING', 'EXECUTING']:
                        workflow_context.execution_context['completed_nodes'].discard(node_instance_id)
            
            if updated_count > 0:
                logger.debug(f"   ğŸ”„ åˆ·æ–°äº† {updated_count} ä¸ªèŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡çŠ¶æ€")
            
        except Exception as e:
            logger.warning(f"åˆ·æ–°å·¥ä½œæµä¸Šä¸‹æ–‡çŠ¶æ€å¤±è´¥: {e}")
    
    async def _execute_end_node(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID):
        """æ‰§è¡ŒENDèŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆï¼šä¸¥æ ¼ä¾èµ–æ£€æŸ¥ï¼‰"""
        try:
            logger.trace(f"ğŸ æ‰§è¡ŒENDèŠ‚ç‚¹: {node_instance_id}")
            
            # ğŸ”’ ä¸¥æ ¼æ£€æŸ¥ï¼šé˜²æ­¢é‡å¤æ‰§è¡Œ  
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus
            
            node_repo = NodeInstanceRepository()
            node_info = await node_repo.get_instance_by_id(node_instance_id)
            
            if not node_info:
                logger.error(f"âŒ [ENDèŠ‚ç‚¹] èŠ‚ç‚¹å®ä¾‹ä¸å­˜åœ¨: {node_instance_id}")
                return
            
            # æ£€æŸ¥èŠ‚ç‚¹å½“å‰çŠ¶æ€
            current_status = node_info.get('status')
            if current_status in ['running', 'completed']:
                logger.trace(f"ğŸš« [ENDèŠ‚ç‚¹] èŠ‚ç‚¹å·²å¤„äº {current_status} çŠ¶æ€ï¼Œè·³è¿‡æ‰§è¡Œ")
                return
            
            # ğŸ” åŒé‡ä¾èµ–æ£€æŸ¥ï¼šä¸Šä¸‹æ–‡ç®¡ç†å™¨ + æ•°æ®åº“
            node_id = node_info['node_id']
            
            # 1. æ£€æŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å‡†å¤‡çŠ¶æ€
            is_ready = self.context_manager.is_node_ready_to_execute(node_instance_id)
            if not is_ready:
                logger.warning(f"âŒ [ENDèŠ‚ç‚¹] èŠ‚ç‚¹ {node_instance_id} åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­æœªå‡†å¤‡å°±ç»ª")
                return
            
            # 2. æ£€æŸ¥æ•°æ®åº“ä¾èµ–çŠ¶æ€
            dependencies_satisfied = await self._check_node_dependencies_satisfied(workflow_instance_id, node_instance_id)
            if not dependencies_satisfied:
                logger.warning(f"âŒ [ENDèŠ‚ç‚¹] èŠ‚ç‚¹ {node_instance_id} ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œ")
                return
            
            logger.trace(f"âœ… [ENDèŠ‚ç‚¹] ä¾èµ–æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹æ‰§è¡Œ")
            
            # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ä¸ºè¿è¡Œä¸­
            logger.trace(f"ğŸƒ [ENDèŠ‚ç‚¹] æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­")
            update_data = NodeInstanceUpdate(status=NodeInstanceStatus.RUNNING)
            await node_repo.update_node_instance(node_instance_id, update_data)
            
            # æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
            await self.context_manager.mark_node_executing(
                workflow_instance_id=workflow_instance_id,
                node_id=node_id,
                node_instance_id=node_instance_id
            )
            
            # æ”¶é›†ç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹çš„è¾“å‡ºç»“æœï¼ˆç®€åŒ–ç‰ˆï¼‰
            logger.trace(f"ğŸ“‹ [ENDèŠ‚ç‚¹] æ”¶é›†ç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹ç»“æœ")
            context_data = await self._collect_immediate_upstream_results(workflow_instance_id, node_instance_id)
            
            # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ä¸ºå®Œæˆï¼Œå¹¶ä¿å­˜ä¸Šä¸‹æ–‡æ•°æ®
            logger.trace(f"âœ… [ENDèŠ‚ç‚¹] æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ")
            final_update = NodeInstanceUpdate(
                status=NodeInstanceStatus.COMPLETED,
                output_data=context_data
            )
            await node_repo.update_node_instance(node_instance_id, final_update)
            
            # é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨èŠ‚ç‚¹å®Œæˆ
            logger.trace(f"ğŸ‰ [ENDèŠ‚ç‚¹] é€šçŸ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨èŠ‚ç‚¹å®Œæˆ")
            await self.context_manager.mark_node_completed(
                workflow_instance_id=workflow_instance_id,
                node_id=node_id,
                node_instance_id=node_instance_id,
                output_data=context_data
            )
            
            logger.trace(f"âœ… ENDèŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
            
            # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å¯ä»¥å®Œæˆ
            await self._check_workflow_completion(workflow_instance_id)
            
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡ŒENDèŠ‚ç‚¹å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _collect_immediate_upstream_results(self, workflow_instance_id: uuid.UUID, node_instance_id: uuid.UUID) -> Dict[str, Any]:
        """æ”¶é›†ç»“æŸèŠ‚ç‚¹çš„ç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            logger.trace(f"ğŸ“‹ æ”¶é›†ç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹ç»“æœ: {node_instance_id}")
            
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–ç›´æ¥ä¸Šæ¸¸ç»“æœ
            context_data = await self.context_manager.get_task_context_data(workflow_instance_id, node_instance_id)
            immediate_upstream = context_data.get('immediate_upstream_results', {})
            
            # ç®€å•æ•´ç†ä¸Šæ¸¸ç»“æœ
            end_node_output = {
                'workflow_completed': True,
                'completion_time': datetime.utcnow().isoformat(),
                'upstream_results': immediate_upstream,  # ç›´æ¥ä½¿ç”¨ä¸Šæ¸¸ç»“æœ
                'upstream_count': len(immediate_upstream),
                'summary': f"å·¥ä½œæµå®Œæˆï¼Œæ•´åˆäº†{len(immediate_upstream)}ä¸ªä¸Šæ¸¸èŠ‚ç‚¹çš„ç»“æœ",
                'workflow_instance_id': str(workflow_instance_id)
            }
            
            logger.trace(f"âœ… æ”¶é›†åˆ° {len(immediate_upstream)} ä¸ªç›´æ¥ä¸Šæ¸¸èŠ‚ç‚¹çš„ç»“æœ")
            for node_name, result in immediate_upstream.items():
                logger.trace(f"  - {node_name}: {len(str(result.get('output_data', '')))} å­—ç¬¦è¾“å‡º")
            
            return end_node_output
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†ç›´æ¥ä¸Šæ¸¸ç»“æœå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {
                'workflow_completed': True,
                'completion_time': datetime.utcnow().isoformat(),
                'error': f"æ”¶é›†ä¸Šæ¸¸ç»“æœå¤±è´¥: {str(e)}",
                'workflow_instance_id': str(workflow_instance_id)
            }
    
    async def _check_workflow_completion(self, workflow_instance_id: uuid.UUID):
        """æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å¯ä»¥å®Œæˆï¼Œå¹¶è§¦å‘å‡†å¤‡å°±ç»ªçš„èŠ‚ç‚¹"""
        try:
            logger.info(f"ğŸ” æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å’Œè§¦å‘å‡†å¤‡å°±ç»ªèŠ‚ç‚¹: {workflow_instance_id}")
            
            # æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹çš„çŠ¶æ€
            nodes_status_query = '''
            SELECT ni.node_instance_id, ni.status, n.name, n.type
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.workflow_instance_id = $1 AND ni.is_deleted = FALSE
            '''
            
            all_nodes = await self.workflow_instance_repo.db.fetch_all(
                nodes_status_query, workflow_instance_id
            )
            
            logger.info(f"  ğŸ“Š å·¥ä½œæµæ€»èŠ‚ç‚¹æ•°: {len(all_nodes)}")
            
            # æ£€æŸ¥å„ç§çŠ¶æ€çš„èŠ‚ç‚¹
            completed_nodes = [n for n in all_nodes if n['status'] == 'completed']
            failed_nodes = [n for n in all_nodes if n['status'] == 'failed']
            pending_nodes = [n for n in all_nodes if n['status'] == 'pending']
            running_nodes = [n for n in all_nodes if n['status'] == 'running']
            
            logger.info(f"  ğŸ“Š èŠ‚ç‚¹çŠ¶æ€åˆ†å¸ƒ: å®Œæˆ {len(completed_nodes)}, å¤±è´¥ {len(failed_nodes)}, ç­‰å¾… {len(pending_nodes)}, è¿è¡Œä¸­ {len(running_nodes)}")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å¹¶è§¦å‘å‡†å¤‡å°±ç»ªçš„èŠ‚ç‚¹
            if pending_nodes:
                logger.info(f"ğŸ”„ æ£€æŸ¥ {len(pending_nodes)} ä¸ªç­‰å¾…èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å°±ç»ª:")
                for node in pending_nodes:
                    logger.info(f"  - {node['name']} ({node['node_instance_id']}) çŠ¶æ€: {node['status']}")
                
                # è§¦å‘å‡†å¤‡å°±ç»ªçš„èŠ‚ç‚¹
                triggered_count = await self._check_and_trigger_ready_nodes(workflow_instance_id, pending_nodes)
                if triggered_count > 0:
                    logger.info(f"âœ… æˆåŠŸè§¦å‘äº† {triggered_count} ä¸ªå‡†å¤‡å°±ç»ªçš„èŠ‚ç‚¹")
                else:
                    logger.info(f"â„¹ï¸ æ²¡æœ‰èŠ‚ç‚¹å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…æ›´å¤šä¾èµ–å®Œæˆ")
            
            # å¦‚æœæœ‰å¤±è´¥èŠ‚ç‚¹ï¼Œæ ‡è®°å·¥ä½œæµä¸ºå¤±è´¥
            if failed_nodes:
                from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus
                update_data = WorkflowInstanceUpdate(
                    status=WorkflowInstanceStatus.FAILED,
                    error_message=f"å·¥ä½œæµåŒ…å« {len(failed_nodes)} ä¸ªå¤±è´¥èŠ‚ç‚¹"
                )
                await self.workflow_instance_repo.update_instance(workflow_instance_id, update_data)
                logger.info(f"âŒ å·¥ä½œæµæ ‡è®°ä¸ºå¤±è´¥")
                return
            
            # ğŸ†• ä½¿ç”¨åŸºäºè·¯å¾„çŠ¶æ€çš„å·¥ä½œæµå®Œæˆæ£€æŸ¥
            workflow_context = await self.context_manager.get_context(workflow_instance_id)
            if workflow_context:
                # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å®Œæˆï¼ˆåŸºäºè·¯å¾„çŠ¶æ€ï¼‰
                is_completed = await workflow_context.is_workflow_completed()

                if is_completed:
                    from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus
                    update_data = WorkflowInstanceUpdate(status=WorkflowInstanceStatus.COMPLETED)
                    await self.workflow_instance_repo.update_instance(workflow_instance_id, update_data)
                    logger.info(f"âœ… åŸºäºè·¯å¾„çŠ¶æ€æ£€æµ‹ï¼Œå·¥ä½œæµæ ‡è®°ä¸ºå®Œæˆ")
                else:
                    # æä¾›è¯¦ç»†çš„è·¯å¾„çŠ¶æ€ä¿¡æ¯
                    active_paths = len(workflow_context.execution_context.get('active_paths', set()))
                    completed_paths = len(workflow_context.execution_context.get('completed_paths', set()))
                    failed_paths = len(workflow_context.execution_context.get('failed_paths', set()))

                    logger.info(f"â³ å·¥ä½œæµä»åœ¨è¿›è¡Œä¸­: æ´»è·ƒè·¯å¾„={active_paths}, å®Œæˆè·¯å¾„={completed_paths}, å¤±è´¥è·¯å¾„={failed_paths}")
                    logger.info(f"   ä¼ ç»Ÿç»Ÿè®¡: {len(completed_nodes)}/{len(all_nodes)} èŠ‚ç‚¹å®Œæˆ, {len(pending_nodes)} èŠ‚ç‚¹ç­‰å¾…, {len(running_nodes)} èŠ‚ç‚¹è¿è¡Œä¸­")
            else:
                # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
                logger.warning("âš ï¸ å·¥ä½œæµä¸Šä¸‹æ–‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼ ç»Ÿå®Œæˆæ£€æŸ¥é€»è¾‘")

                # å¦‚æœæ‰€æœ‰èŠ‚ç‚¹éƒ½å·²å®Œæˆï¼Œæ ‡è®°å·¥ä½œæµä¸ºå®Œæˆ
                if len(completed_nodes) == len(all_nodes) and len(all_nodes) > 0:
                    from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus
                    update_data = WorkflowInstanceUpdate(status=WorkflowInstanceStatus.COMPLETED)
                    await self.workflow_instance_repo.update_instance(workflow_instance_id, update_data)
                    logger.info(f"âœ… ä¼ ç»Ÿé€»è¾‘ï¼šå·¥ä½œæµæ ‡è®°ä¸ºå®Œæˆ")
                else:
                    logger.info(f"â³ ä¼ ç»Ÿé€»è¾‘ï¼šå·¥ä½œæµä»åœ¨è¿›è¡Œä¸­: {len(completed_nodes)}/{len(all_nodes)} èŠ‚ç‚¹å®Œæˆ, {len(pending_nodes)} èŠ‚ç‚¹ç­‰å¾…, {len(running_nodes)} èŠ‚ç‚¹è¿è¡Œä¸­")
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥å·¥ä½œæµå®ŒæˆçŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

    # =============================================================================
    # æ–°æ¶æ„æ–¹æ³• - æ”¯æŒWorkflowInstanceContext
    # =============================================================================
    
    async def _get_upstream_node_instances(self, node_id: uuid.UUID, workflow_instance_id: uuid.UUID) -> List[uuid.UUID]:
        """è·å–èŠ‚ç‚¹çš„ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹IDåˆ—è¡¨ï¼ˆä¿®å¤ç‰ˆï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å·¥ä½œæµå®ä¾‹ï¼‰"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            
            # æŸ¥è¯¢ä¸Šæ¸¸è¿æ¥å…³ç³»
            upstream_query = """
            SELECT DISTINCT 
                nc.from_node_id as upstream_node_id,
                n.name as upstream_node_name
            FROM node_connection nc
            JOIN node n ON nc.from_node_id = n.node_id
            WHERE nc.to_node_id = $1
            ORDER BY n.name
            """
            
            logger.info(f"ğŸ” [ä¸Šæ¸¸æŸ¥è¯¢] æŸ¥è¯¢èŠ‚ç‚¹ {node_id} çš„ä¸Šæ¸¸ä¾èµ–")
            upstream_connections = await node_repo.db.fetch_all(upstream_query, node_id)
            logger.info(f"ğŸ” [ä¸Šæ¸¸æŸ¥è¯¢] æŸ¥è¯¢åˆ° {len(upstream_connections)} ä¸ªä¸Šæ¸¸è¿æ¥")
            
            # è¾“å‡ºæ‰€æœ‰ä¸Šæ¸¸è¿æ¥çš„è¯¦ç»†ä¿¡æ¯
            for conn in upstream_connections:
                logger.info(f"ğŸ” [ä¸Šæ¸¸æŸ¥è¯¢] å‘ç°è¿æ¥: {conn.get('upstream_node_name', 'Unknown')} ({conn['upstream_node_id']}) -> å½“å‰èŠ‚ç‚¹({node_id})")
            
            upstream_node_instance_ids = []
            for upstream in upstream_connections:
                upstream_node_id = upstream['upstream_node_id']
                logger.info(f"ğŸ” [ä¸Šæ¸¸æŸ¥è¯¢] å¤„ç†ä¸Šæ¸¸èŠ‚ç‚¹ {upstream.get('upstream_node_name', 'Unknown')} (node_id: {upstream_node_id})")
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æŸ¥è¯¢çš„æ˜¯å½“å‰å·¥ä½œæµå®ä¾‹çš„èŠ‚ç‚¹å®ä¾‹ï¼Œè€Œä¸æ˜¯å…¶ä»–å®ä¾‹çš„
                instance_query = """
                SELECT node_instance_id 
                FROM node_instance 
                WHERE node_id = $1 AND workflow_instance_id = $2 AND is_deleted = FALSE
                ORDER BY created_at DESC
                LIMIT 1
                """
                upstream_instance_result = await node_repo.db.fetch_one(
                    instance_query, upstream_node_id, workflow_instance_id
                )
                
                if upstream_instance_result:
                    upstream_node_instance_id = upstream_instance_result['node_instance_id']
                    upstream_node_instance_ids.append(upstream_node_instance_id)
                    logger.info(f"  âœ… æ‰¾åˆ°ä¸Šæ¸¸å®ä¾‹: {upstream.get('upstream_node_name', 'Unknown')} -> {upstream_node_instance_id}")
                    logger.info(f"    (ç¡®è®¤å±äºå·¥ä½œæµå®ä¾‹: {workflow_instance_id})")
                else:
                    logger.warning(f"  âš ï¸ æœªæ‰¾åˆ°ä¸Šæ¸¸èŠ‚ç‚¹ {upstream_node_id} åœ¨å·¥ä½œæµå®ä¾‹ {workflow_instance_id} ä¸­çš„å¯¹åº”å®ä¾‹")
            
            logger.info(f"âœ… [ä¸Šæ¸¸æŸ¥è¯¢] è·å–åˆ° {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹ID: {upstream_node_instance_ids}")
            return upstream_node_instance_ids
            
        except Exception as e:
            logger.error(f"è·å–ä¸Šæ¸¸èŠ‚ç‚¹å®ä¾‹å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []

    async def _create_node_instances_with_new_context(self, 
                                                    workflow_context:WorkflowExecutionContext, 
                                                    workflow_instance_id: uuid.UUID, 
                                                    workflow_base_id: uuid.UUID,
                                                    nodes: List[Dict[str, Any]],
                                                    execute_request):
        """ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ›å»ºèŠ‚ç‚¹å®ä¾‹ï¼ˆä¿®å¤ç‰ˆï¼šçœŸæ­£çš„åˆ†é˜¶æ®µå¤„ç†ï¼Œé¿å…æ—¶åºé—®é¢˜ï¼‰"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceCreate, NodeInstanceStatus
            
            node_instance_repo = NodeInstanceRepository()
            
            logger.info(f"ğŸ—ï¸ [èŠ‚ç‚¹åˆ›å»º] å¼€å§‹ä¸ºå·¥ä½œæµ {workflow_instance_id} åˆ›å»º {len(nodes)} ä¸ªèŠ‚ç‚¹å®ä¾‹")
            
            task_creation_summary = {
                'start_nodes': 0,
                'processor_nodes': 0, 
                'end_nodes': 0,
                'start_completed': 0,
                'tasks_deferred': 0
            }
            
            # å­˜å‚¨åˆ›å»ºçš„èŠ‚ç‚¹ä¿¡æ¯
            created_nodes_info = []
            start_nodes_to_complete = []
            
            # ====== ç¬¬ä¸€é˜¶æ®µï¼šä»…åˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹å®ä¾‹ ======
            logger.info(f"ğŸ“‹ [ç¬¬ä¸€é˜¶æ®µ] åˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹å®ä¾‹")
            instance_id_set = set()  # ç”¨äºè·Ÿè¸ªå·²åˆ›å»ºçš„èŠ‚ç‚¹å®ä¾‹ID
            
            for i, node in enumerate(nodes, 1):
                logger.info(f"ğŸ“‹ [èŠ‚ç‚¹åˆ›å»º {i}/{len(nodes)}] å¤„ç†èŠ‚ç‚¹: {node['name']} (ç±»å‹: {node['type']})")
                logger.info(f"   - node_id: {node['node_id']}")
                logger.info(f"   - node_base_id: {node['node_base_id']}")
                # await asyncio.sleep(1) 
                
                # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
                task_description = node.get('task_description') or ''  # ç¡®ä¿ä¸ä¼ å…¥None
                node_instance_data = NodeInstanceCreate(
                    workflow_instance_id=workflow_instance_id,
                    node_id=node['node_id'],
                    node_base_id=node['node_base_id'],
                    node_instance_name=f"{node['name']}_instance",
                    task_description=task_description,
                    status=NodeInstanceStatus.PENDING,
                    input_data={},
                    output_data={},
                    error_message=None,
                    retry_count=0
                )
                
                node_instance = await node_instance_repo.create_node_instance(node_instance_data)
                if not node_instance:
                    logger.error(f"âŒ [èŠ‚ç‚¹åˆ›å»º] åˆ›å»ºèŠ‚ç‚¹å®ä¾‹å¤±è´¥: {node['name']}")
                    continue
                
                node_instance_id = node_instance['node_instance_id']
                if node_instance_id in instance_id_set:
                    raise ValueError(f"åˆ›å»ºèŠ‚ç‚¹å®ä¾‹å¤±è´¥: èŠ‚ç‚¹ {node['name']} å·²å­˜åœ¨äºå®ä¾‹ {workflow_instance_id}")
                instance_id_set.add(node_instance_id)
                logger.info(f"âœ… [èŠ‚ç‚¹åˆ›å»º] èŠ‚ç‚¹å®ä¾‹åˆ›å»ºæˆåŠŸ: {node['name']} (å®ä¾‹ID: {node_instance_id})")
                # logger.debug(f"ğŸ” [å˜é‡æ£€æŸ¥] èŠ‚ç‚¹ {node['name']} å˜é‡çŠ¶æ€:")
                # logger.debug(f"   - nodeå­—å…¸å†…å­˜åœ°å€: {id(node)}")
                # logger.debug(f"   - node_instance_id: {node_instance_id}")
                # logger.debug(f"   - node_instanceå­—å…¸å†…å­˜åœ°å€: {id(node_instance)}")
                
                # æ”¶é›†èŠ‚ç‚¹ä¿¡æ¯ç”¨äºç¬¬äºŒé˜¶æ®µå¤„ç†
                # ä½¿ç”¨æ·±æ‹·è´ç¡®ä¿nodeå­—å…¸çš„ç‹¬ç«‹æ€§ï¼Œé˜²æ­¢å˜é‡å¼•ç”¨é—®é¢˜
                node_info = {
                    'node': {
                        'node_id': node['node_id'],
                        'node_base_id': node['node_base_id'], 
                        'name': node['name'],
                        'type': node['type'],
                        'task_description': node.get('task_description')
                    },  # åˆ›å»ºæ–°çš„å­—å…¸å¯¹è±¡è€Œä¸æ˜¯å¼•ç”¨åŸå¯¹è±¡
                    'node_instance_id': node_instance_id  # æ˜ç¡®ä½¿ç”¨å½“å‰å¾ªç¯ä¸­çš„node_instance_id
                }
                created_nodes_info.append(node_info)
                
                # è®°å½•èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
                if node['type'] == NodeType.START.value:
                    task_creation_summary['start_nodes'] += 1
                    start_nodes_to_complete.append(node_info)
                elif node['type'] == NodeType.PROCESSOR.value:
                    task_creation_summary['processor_nodes'] += 1
                    task_creation_summary['tasks_deferred'] += 1
                elif node['type'] == NodeType.END.value:
                    task_creation_summary['end_nodes'] += 1
            
            logger.info(f"âœ… [ç¬¬ä¸€é˜¶æ®µ] æ‰€æœ‰ {len(created_nodes_info)} ä¸ªèŠ‚ç‚¹å®ä¾‹åˆ›å»ºå®Œæˆ")
            
            # ====== ç¬¬äºŒé˜¶æ®µï¼šæŸ¥è¯¢ä¾èµ–å…³ç³»å¹¶æ³¨å†Œ ======
            logger.info(f"ğŸ“‹ [ç¬¬äºŒé˜¶æ®µ] æŸ¥è¯¢ä¾èµ–å…³ç³»å¹¶æ³¨å†Œåˆ°ä¸Šä¸‹æ–‡")
            
            for i, node_info in enumerate(created_nodes_info, 1):
                node = node_info['node']
                node_instance_id = node_info['node_instance_id']
                
                logger.info(f"ğŸ”— [ä¾èµ–åˆ†æ {i}/{len(created_nodes_info)}] åˆ†æèŠ‚ç‚¹ {node['name']} çš„ä¸Šæ¸¸ä¾èµ–...")
                logger.debug(f"ğŸ” [ç¬¬äºŒé˜¶æ®µå˜é‡æ£€æŸ¥] èŠ‚ç‚¹ {node['name']} å˜é‡çŠ¶æ€:")
                logger.debug(f"   - node_infoå†…å­˜åœ°å€: {id(node_info)}")
                logger.debug(f"   - nodeå­—å…¸å†…å­˜åœ°å€: {id(node)}")
                logger.debug(f"   - ä»node_infoè·å–çš„node_instance_id: {node_instance_id}")
                
                try:
                    # ç°åœ¨æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹éƒ½å·²åˆ›å»ºï¼Œå¯ä»¥å®‰å…¨æŸ¥è¯¢ä¸Šæ¸¸ä¾èµ–
                    upstream_node_instance_ids = await self._get_upstream_node_instances(
                        node['node_id'], workflow_instance_id
                    )
                    logger.info(f"ğŸ”— [ä¾èµ–åˆ†æ] èŠ‚ç‚¹ {node['name']} æœ‰ {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸ä¾èµ–: {upstream_node_instance_ids}")
                except Exception as e:
                    logger.error(f"âŒ [ä¾èµ–åˆ†æ] æŸ¥è¯¢èŠ‚ç‚¹ {node['name']} ä¸Šæ¸¸ä¾èµ–å¤±è´¥: {e}")
                    import traceback
                    logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    upstream_node_instance_ids = []
                
                # åœ¨æ–°ä¸Šä¸‹æ–‡ä¸­æ³¨å†Œä¾èµ–
                logger.info(f"ğŸ“ [ä¾èµ–æ³¨å†Œ] å‡†å¤‡æ³¨å†ŒèŠ‚ç‚¹ {node['name']} çš„ä¾èµ–å…³ç³»")
                logger.info(f"   - å½“å‰èŠ‚ç‚¹å®ä¾‹ID: {node_instance_id}")
                logger.info(f"   - å½“å‰èŠ‚ç‚¹ID: {node['node_id']}")
                logger.info(f"   - ä¸Šæ¸¸å®ä¾‹IDåˆ—è¡¨: {upstream_node_instance_ids}")
                
                await workflow_context.register_node_dependencies(
                    node_instance_id,
                    node['node_id'],  # ä½¿ç”¨node_idè€Œä¸æ˜¯node_base_id
                    upstream_node_instance_ids  # ä½¿ç”¨node_instance_idåˆ—è¡¨
                )
                logger.info(f"ğŸ“ [ä¾èµ–æ³¨å†Œ] èŠ‚ç‚¹ {node['name']} ä¾èµ–å…³ç³»æ³¨å†Œå®Œæˆ: {len(upstream_node_instance_ids)} ä¸ªä¸Šæ¸¸ä¾èµ–")
                
                # æ›´æ–°èŠ‚ç‚¹ä¿¡æ¯
                node_info['upstream_count'] = len(upstream_node_instance_ids)
                
            logger.info(f"âœ… [ç¬¬äºŒé˜¶æ®µ] æ‰€æœ‰ {len(created_nodes_info)} ä¸ªèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»æ³¨å†Œå®Œæˆ")
            
            # ====== ç¬¬ä¸‰é˜¶æ®µï¼šæ ‡è®°STARTèŠ‚ç‚¹å®Œæˆï¼Œè§¦å‘å·¥ä½œæµæ‰§è¡Œ ======
            logger.info(f"ğŸ“‹ [ç¬¬ä¸‰é˜¶æ®µ] æ ‡è®°STARTèŠ‚ç‚¹å®Œæˆå¹¶è§¦å‘æ‰§è¡Œ")
            logger.info(f"   - å·²åˆ›å»º {len(created_nodes_info)} ä¸ªèŠ‚ç‚¹å®ä¾‹")
            logger.info(f"   - å·²æ³¨å†Œ {len(created_nodes_info)} ä¸ªèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»")
            logger.info(f"   - æ‰¾åˆ° {len(start_nodes_to_complete)} ä¸ªSTARTèŠ‚ç‚¹å¾…å®Œæˆ")
            
            for start_node_info in start_nodes_to_complete:
                node = start_node_info['node']
                node_instance_id = start_node_info['node_instance_id']
                
                logger.info(f"ğŸš€ [STARTèŠ‚ç‚¹] {node['name']} - æ ‡è®°ä¸ºå®Œæˆï¼Œä¼ é€’åˆå§‹ä¸Šä¸‹æ–‡")
                
                # STARTèŠ‚ç‚¹ä¼ é€’åˆå§‹ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«ä»»åŠ¡æè¿°å’Œä¸Šä¸‹æ–‡æ•°æ®
                initial_context = {
                    'workflow_start': True,
                    'start_time': datetime.utcnow().isoformat(),
                    'start_node': node['name'],
                    'workflow_instance_id': str(workflow_instance_id),
                    'task_description': node.get('task_description', ''),  # æ·»åŠ ä»»åŠ¡æè¿°
                    'context_data': execute_request.context_data if hasattr(execute_request, 'context_data') and execute_request.context_data else {}  # æ·»åŠ ä¸Šä¸‹æ–‡æ•°æ®
                }
                
                # ç»Ÿä¸€ä½¿ç”¨workflow_contextè€Œä¸æ˜¯self.context_manager
                await workflow_context.mark_node_completed(
                    node['node_id'], 
                    node_instance_id, 
                    initial_context
                )
                
                task_creation_summary['start_completed'] += 1
                logger.info(f"âœ… [STARTèŠ‚ç‚¹] {node['name']} å·²æ ‡è®°ä¸ºå®Œæˆï¼Œä¼ é€’åˆå§‹ä¸Šä¸‹æ–‡")
                
            # ====== æ€»ç»“æŠ¥å‘Š ======
            logger.info(f"ğŸ¯ [èŠ‚ç‚¹åˆ›å»ºæ€»ç»“] å·¥ä½œæµ {workflow_instance_id}:")
            logger.info(f"   ğŸ“Š èŠ‚ç‚¹åˆ†å¸ƒ: START={task_creation_summary['start_nodes']}, PROCESSOR={task_creation_summary['processor_nodes']}, END={task_creation_summary['end_nodes']}")
            logger.info(f"   âš¡ èŠ‚ç‚¹å¤„ç†: STARTå®Œæˆ={task_creation_summary['start_completed']}, ä»»åŠ¡å»¶è¿Ÿåˆ›å»º={task_creation_summary['tasks_deferred']}")
            logger.info(f"   ğŸ”— ä¾èµ–å…³ç³»: æ‰€æœ‰ {len(created_nodes_info)} ä¸ªèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»å·²æ³¨å†Œå®Œæˆ")
            logger.trace(f"âœ… ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡åˆ›å»ºäº† {len(nodes)} ä¸ªèŠ‚ç‚¹å®ä¾‹")
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡åˆ›å»ºèŠ‚ç‚¹å®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def _create_tasks_for_node_new_context(self, node: Dict[str, Any], 
                                               node_instance_id: uuid.UUID,
                                               workflow_instance_id: uuid.UUID):
        """ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡å®ä¾‹ï¼ˆæ–°æ¶æ„ï¼‰"""
        try:
            logger.info(f"ğŸ” [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] å¼€å§‹ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡:")
            logger.info(f"   - èŠ‚ç‚¹åç§°: {node.get('name', 'Unknown')}")
            logger.info(f"   - èŠ‚ç‚¹ç±»å‹: {node.get('type', 'Unknown')}")
            logger.info(f"   - èŠ‚ç‚¹ID: {node.get('node_id')}")
            logger.info(f"   - èŠ‚ç‚¹å®ä¾‹ID: {node_instance_id}")
            
            # è·å–èŠ‚ç‚¹çš„å¤„ç†å™¨ï¼ˆä¿®å¤ï¼šä½¿ç”¨node_idï¼‰  
            processors = await self._get_node_processors(node['node_id'])
            
            logger.info(f"ğŸ” [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] å¤„ç†å™¨æŸ¥è¯¢ç»“æœ: {len(processors)} ä¸ªå¤„ç†å™¨")
            
            if not processors:
                logger.warning(f"âš ï¸ [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] èŠ‚ç‚¹ {node.get('name')} æ²¡æœ‰ç»‘å®šå¤„ç†å™¨ï¼Œè·³è¿‡ä»»åŠ¡åˆ›å»º")
                return
            
            created_task_count = 0
            for i, processor in enumerate(processors, 1):
                logger.info(f"ğŸ” [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] å¤„ç†å¤„ç†å™¨ {i}/{len(processors)}:")
                logger.info(f"   - å¤„ç†å™¨åç§°: {processor.get('processor_name', 'Unknown')}")
                logger.info(f"   - å¤„ç†å™¨ID: {processor.get('processor_id')}")
                
                # æ ¹æ®å¤„ç†å™¨ç±»å‹ç¡®å®šä»»åŠ¡ç±»å‹å’Œåˆ†é…
                processor_type = processor.get('processor_type', processor.get('type', 'HUMAN'))
                task_type = self._determine_task_type(processor_type)
                assigned_user_id = processor.get('user_id')
                assigned_agent_id = processor.get('agent_id')
                
                logger.info(f"   - å¤„ç†å™¨ç±»å‹: {processor_type} -> ä»»åŠ¡ç±»å‹: {task_type.value}")
                logger.info(f"   - åˆ†é…ç”¨æˆ·ID: {assigned_user_id}")
                logger.info(f"   - åˆ†é…AgentID: {assigned_agent_id}")
                
                # ğŸ”§ ä¿®å¤ï¼šè·å–çœŸå®çš„ä¸Šä¸‹æ–‡å’Œè¾“å…¥æ•°æ®
                logger.info(f"ğŸ” [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] è·å–èŠ‚ç‚¹ä¸Šä¸‹æ–‡æ•°æ®...")
                logger.info(f"   - å·¥ä½œæµå®ä¾‹ID: {workflow_instance_id}")
                logger.info(f"   - èŠ‚ç‚¹å®ä¾‹ID: {node_instance_id}")
                
                context_data = await self.context_manager.get_task_context_data(workflow_instance_id, node_instance_id)
                logger.info(f"   - ä¸Šä¸‹æ–‡æ•°æ®é”®: {list(context_data.keys()) if context_data else 'ç©º'}")
                
                if context_data:
                    # è¯¦ç»†è®°å½•ä¸Šä¸‹æ–‡å†…å®¹
                    upstream_results = context_data.get('immediate_upstream_results', {})
                    logger.info(f"   - ä¸Šæ¸¸èŠ‚ç‚¹ç»“æœæ•°é‡: {len(upstream_results)}")
                    for upstream_name, upstream_data in upstream_results.items():
                        output_data = upstream_data.get('output_data', '')
                        logger.info(f"     * {upstream_name}: {len(str(output_data))} å­—ç¬¦è¾“å‡º")
                        if output_data:
                            logger.info(f"       é¢„è§ˆ: {str(output_data)[:100]}...")
                    
                    global_data = context_data.get('workflow_global', {}).get('global_data', {})
                    logger.info(f"   - å…¨å±€æ•°æ®: {len(global_data)} ä¸ªé”®")
                
                # å°†ä¸Šä¸‹æ–‡æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼ˆä¸æ—§æ–¹æ³•ä¿æŒä¸€è‡´ï¼‰
                context_text = json.dumps(context_data, ensure_ascii=False, indent=2, default=_json_serializer) if context_data else ""
                input_text = json.dumps(node.get('input_data', {}), ensure_ascii=False, indent=2, default=_json_serializer)
                
                logger.info(f"   - ä¸Šä¸‹æ–‡æ–‡æœ¬é•¿åº¦: {len(context_text)} å­—ç¬¦")
                logger.info(f"   - è¾“å…¥æ•°æ®æ–‡æœ¬é•¿åº¦: {len(input_text)} å­—ç¬¦")
                if context_text:
                    logger.info(f"   - ä¸Šä¸‹æ–‡é¢„è§ˆ: {context_text}...")

                
                # åˆ›å»ºä»»åŠ¡å®ä¾‹
                task_title = node['name']
                task_description = node.get('task_description') or node.get('description') or f"æ‰§è¡ŒèŠ‚ç‚¹ {node['name']} çš„ä»»åŠ¡"
                
                task_data = TaskInstanceCreate(
                    workflow_instance_id=workflow_instance_id,
                    node_instance_id=node_instance_id,
                    processor_id=processor.get('processor_id'),
                    task_type=task_type,
                    task_title=task_title,
                    task_description=task_description,
                    assigned_user_id=assigned_user_id,
                    assigned_agent_id=assigned_agent_id,
                    estimated_duration=processor.get('estimated_duration', 30),
                    input_data=input_text,  
                    context_data=context_text, 
                    status=TaskInstanceStatus.PENDING,
                    priority='MEDIUM'
                )
                
                logger.info(f"ğŸ” [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] å‡†å¤‡åˆ›å»ºä»»åŠ¡å®ä¾‹...")
                logger.info(f"   - ä»»åŠ¡æ ‡é¢˜: {task_title}")
                logger.info(f"   - ä»»åŠ¡æè¿°: {task_description[:100]}...")
                
                try:
                    # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•åç§°
                    task_instance = await self.task_instance_repo.create_task(task_data)
                    if task_instance:
                        created_task_count += 1
                        task_id = task_instance.get('task_instance_id')
                        logger.info(f"âœ… [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] ä»»åŠ¡åˆ›å»ºæˆåŠŸ:")
                        logger.info(f"   - ä»»åŠ¡å®ä¾‹ID: {task_id}")
                        logger.info(f"   - ä»»åŠ¡çŠ¶æ€: {task_instance.get('status')}")
                    else:
                        logger.error(f"âŒ [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] ä»»åŠ¡åˆ›å»ºå¤±è´¥: create_task_instanceè¿”å›ç©ºç»“æœ")
                except Exception as task_creation_error:
                    logger.error(f"âŒ [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] ä»»åŠ¡åˆ›å»ºå¼‚å¸¸: {task_creation_error}")
                    import traceback
                    logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                
            logger.info(f"ğŸ‰ [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] èŠ‚ç‚¹ {node.get('name')} ä»»åŠ¡åˆ›å»ºå®Œæˆï¼Œå…±åˆ›å»º {created_task_count} ä¸ªä»»åŠ¡")
                
        except Exception as e:
            logger.error(f"âŒ [æ–°æ¶æ„-ä»»åŠ¡åˆ›å»º] ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡å®ä¾‹å¤±è´¥: {e}")
            import traceback
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def _start_workflow_execution_with_new_context(self, 
                                                       workflow_context,
                                                       workflow_instance_id: uuid.UUID,
                                                       workflow_base_id: uuid.UUID):
        """ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡å¯åŠ¨å·¥ä½œæµæ‰§è¡Œ"""
        try:
            # è·å–å‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹ï¼ˆSTARTèŠ‚ç‚¹ï¼‰
            ready_nodes = await workflow_context.get_ready_nodes()
            
            logger.trace(f"æ‰¾åˆ° {len(ready_nodes)} ä¸ªå‡†å¤‡æ‰§è¡Œçš„èŠ‚ç‚¹")
            
            for node_instance_id in ready_nodes:
                try:
                    # æ‰§è¡ŒèŠ‚ç‚¹
                    await self._execute_node_with_new_context(workflow_context, node_instance_id)
                    logger.trace(f"å¯åŠ¨èŠ‚ç‚¹æ‰§è¡Œ: {node_instance_id}")
                    
                except Exception as e:
                    logger.error(f"å¯åŠ¨èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥ {node_instance_id}: {e}")
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡å¯åŠ¨å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def _execute_node_with_new_context(self, workflow_context, node_instance_id: uuid.UUID):
        """ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡æ‰§è¡ŒèŠ‚ç‚¹"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šé€šè¿‡context managerè·å–èŠ‚ç‚¹ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç›´æ¥ä»context
            from ..services.workflow_execution_context import get_context_manager
            context_manager = get_context_manager()
            
            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ
            if not context_manager.is_node_ready_to_execute(node_instance_id):
                logger.warning(f"èŠ‚ç‚¹ {node_instance_id} å°šæœªå‡†å¤‡å¥½æ‰§è¡Œ")
                return
            
            # è·å–èŠ‚ç‚¹ä¿¡æ¯
            dep_info = context_manager.get_node_dependency_info(node_instance_id)
            if not dep_info:
                logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹ {node_instance_id} çš„ä¾èµ–ä¿¡æ¯")
                return
            
            node_id = dep_info['node_id']
            workflow_instance_id = dep_info['workflow_instance_id']
            
            # æ ‡è®°èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
            await context_manager.mark_node_executing(workflow_instance_id, node_id, node_instance_id)
            
            # ğŸ”§ åŠ¨æ€åˆ›å»ºä»»åŠ¡å®ä¾‹ï¼šåªæœ‰åœ¨èŠ‚ç‚¹å‡†å¤‡æ‰§è¡Œæ—¶æ‰åˆ›å»ºä»»åŠ¡
            tasks = await self.task_instance_repo.get_tasks_by_node_instance(node_instance_id)
            
            if not tasks:
                # æ£€æŸ¥èŠ‚ç‚¹ç±»å‹ - START/ENDèŠ‚ç‚¹ä¸éœ€è¦ä»»åŠ¡ï¼Œç›´æ¥è‡ªåŠ¨å®Œæˆ
                node_instance_data = await self._get_node_instance_data(node_instance_id)
                if node_instance_data:
                    node_type = node_instance_data.get('node_type', 'PROCESSOR')
                    node_name = node_instance_data.get('node_name', 'Unknown')
                    node_id = node_instance_data.get('node_id', 'Unknown')
                    
                    # ğŸ”§ æ·»åŠ è¯¦ç»†çš„èŠ‚ç‚¹ç±»å‹è°ƒè¯•ä¿¡æ¯
                    logger.info(f"ğŸ” [èŠ‚ç‚¹ç±»å‹æ£€æŸ¥] èŠ‚ç‚¹å®ä¾‹: {node_instance_id}")
                    logger.info(f"    - èŠ‚ç‚¹ID: {node_id}")
                    logger.info(f"    - èŠ‚ç‚¹åç§°: {node_name}")
                    logger.info(f"    - èŠ‚ç‚¹ç±»å‹: {node_type}")
                    logger.info(f"    - åŸå§‹æ•°æ®: {node_instance_data}")
                    
                    if node_type.upper() in ['START', 'END']:
                        logger.info(f"ğŸš€ [{node_type}èŠ‚ç‚¹] æ— éœ€åˆ›å»ºä»»åŠ¡ï¼Œç›´æ¥è‡ªåŠ¨æ‰§è¡Œ: {node_instance_id}")
                        # START/ENDèŠ‚ç‚¹ç›´æ¥æ ‡è®°ä¸ºå®Œæˆï¼Œä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡
                        await self._auto_complete_system_node_with_context(workflow_context, node_instance_id, node_type.upper())
                        return
                    else:
                        # ğŸ”§ åªä¸ºPROCESSORç­‰éœ€è¦ä»»åŠ¡çš„èŠ‚ç‚¹åˆ›å»ºä»»åŠ¡
                        logger.info(f"ğŸ”¨ [åŠ¨æ€ä»»åŠ¡åˆ›å»º] ä¸º{node_type}èŠ‚ç‚¹åˆ›å»ºä»»åŠ¡: {node_instance_id}")
                        task_created = await self._create_task_for_node(node_instance_id, workflow_instance_id)
                        if task_created:
                            # é‡æ–°æŸ¥è¯¢ä»»åŠ¡
                            tasks = await self.task_instance_repo.get_tasks_by_node_instance(node_instance_id)
                            logger.info(f"âœ… [åŠ¨æ€ä»»åŠ¡åˆ›å»º] æˆåŠŸåˆ›å»ºå¹¶åˆ†é…ä»»åŠ¡")
                        else:
                            logger.error(f"âŒ [åŠ¨æ€ä»»åŠ¡åˆ›å»º] ä»»åŠ¡åˆ›å»ºå¤±è´¥")
                            return
            
            # ç°åœ¨æ‰€æœ‰åˆ°è¿™é‡Œçš„èŠ‚ç‚¹éƒ½åº”è¯¥æœ‰ä»»åŠ¡äº†ï¼ˆSTART/ENDå·²æå‰è¿”å›ï¼‰
            if not tasks:
                logger.error(f"âŒ PROCESSORèŠ‚ç‚¹æ²¡æœ‰ä»»åŠ¡ä¸”åˆ›å»ºå¤±è´¥: {node_instance_id}")
                return
            
            # æœ‰ä»»åŠ¡çš„èŠ‚ç‚¹ï¼Œå¯åŠ¨ä»»åŠ¡æ‰§è¡Œ
            for task in tasks:
                await self._execute_task(task)
            
            logger.trace(f"èŠ‚ç‚¹ {node_id} çš„ {len(tasks)} ä¸ªä»»åŠ¡å·²å¯åŠ¨")
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨æ–°ä¸Šä¸‹æ–‡æ‰§è¡ŒèŠ‚ç‚¹ {node_instance_id} å¤±è´¥: {e}")
            # æ ‡è®°èŠ‚ç‚¹å¤±è´¥
            if 'dep_info' in locals() and dep_info:
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨context manageræ ‡è®°èŠ‚ç‚¹å¤±è´¥
                await context_manager.mark_node_failed(
                    workflow_instance_id,
                    dep_info['node_id'],
                    node_instance_id,
                    {'error': str(e)}
                )
            raise
    
    async def _log_workflow_execution_summary_new(self, workflow_context, workflow_instance_id: uuid.UUID):
        """ç”Ÿæˆæ–°æ¶æ„çš„æ‰§è¡Œæ‘˜è¦"""
        try:
            status = await workflow_context.get_workflow_status()
            
            logger.trace(f"\nğŸ“ˆ ã€å·¥ä½œæµæ‰§è¡Œæ‘˜è¦ - æ–°æ¶æ„ã€‘")
            logger.trace(f"  å®ä¾‹ ID: {workflow_instance_id}")
            logger.trace(f"  çŠ¶æ€: {status['status']}")
            logger.trace(f"  æ€»èŠ‚ç‚¹æ•°: {status['total_nodes']}")
            logger.trace(f"  å·²å®Œæˆ: {status['completed_nodes']}")
            logger.trace(f"  æ‰§è¡Œä¸­: {status['executing_nodes']}")
            logger.trace(f"  å¾…æ‰§è¡Œ: {status['pending_nodes']}")
            logger.trace(f"  å¤±è´¥: {status['failed_nodes']}")
            logger.trace(f"  æ¶æ„ç±»å‹: WorkflowInstanceContext")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–°æ¶æ„æ‰§è¡Œæ‘˜è¦å¤±è´¥: {e}")
    
    async def get_execution_stats(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œå¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'is_running': self.is_running,
            'architecture': 'new_context_management',
            'features': {
                'instance_isolation': True,
                'thread_safe': True,
                'auto_cleanup': True,
                'resource_management': True
            }
        }
        
        if hasattr(self, 'resource_cleanup_manager') and self.resource_cleanup_manager:
            cleanup_stats = self.resource_cleanup_manager.get_cleanup_stats()
            stats['resource_cleanup'] = cleanup_stats
        
        return stats

    async def _create_tasks_for_pending_node(self, node_instance: Dict[str, Any]):
        """ä¸ºpendingçŠ¶æ€çš„èŠ‚ç‚¹åˆ›å»ºä»»åŠ¡"""
        try:
            node_type = node_instance.get('node_type', '').lower()
            node_instance_id = node_instance['node_instance_id']
            node_id = node_instance.get('node_id')
            
            # å¯¹äºSTART, ENDç­‰èŠ‚ç‚¹ï¼Œä¸éœ€è¦åˆ›å»ºä»»åŠ¡å®ä¾‹
            if node_type in ['start', 'end']:
                logger.trace(f"   èŠ‚ç‚¹ç±»å‹ {node_type} ä¸éœ€è¦ä»»åŠ¡å®ä¾‹ï¼Œè·³è¿‡")
                return
            
            # å¯¹äºprocessorç±»å‹èŠ‚ç‚¹ï¼Œéœ€è¦æŸ¥è¯¢å¤„ç†å™¨ç±»å‹æ¥ç¡®å®šä»»åŠ¡ç±»å‹
            task_type = None
            if node_type == 'processor':
                # æŸ¥è¯¢èŠ‚ç‚¹å…³è”çš„å¤„ç†å™¨ä¿¡æ¯
                processor_query = """
                    SELECT p.type as processor_type, p.user_id, p.agent_id
                    FROM node_processor np
                    LEFT JOIN processor p ON np.processor_id = p.processor_id
                    WHERE np.node_id = %s
                    LIMIT 1
                """
                processor_info = await self.task_instance_repo.db.fetch_one(processor_query, node_id)
                
                if processor_info:
                    processor_type = processor_info['processor_type'].lower() if processor_info['processor_type'] else None
                    if processor_type == 'human':
                        task_type = TaskInstanceType.HUMAN
                    elif processor_type == 'agent':
                        task_type = TaskInstanceType.AGENT
                    elif processor_type == 'mixed':
                        task_type = TaskInstanceType.MIXED
                    else:
                        logger.warning(f"æœªçŸ¥çš„å¤„ç†å™¨ç±»å‹: {processor_type}")
                        return
                else:
                    logger.warning(f"èŠ‚ç‚¹ {node_id} æ²¡æœ‰å…³è”çš„å¤„ç†å™¨ï¼Œæ— æ³•åˆ›å»ºä»»åŠ¡")
                    return
            elif node_type == 'human':
                task_type = TaskInstanceType.HUMAN
            elif node_type == 'agent':
                task_type = TaskInstanceType.AGENT
            elif node_type == 'mixed':
                task_type = TaskInstanceType.MIXED
            else:
                logger.trace(f"   èŠ‚ç‚¹ç±»å‹ {node_type} ä¸éœ€è¦ä»»åŠ¡å®ä¾‹ï¼Œè·³è¿‡")
                return
                
            if not task_type:
                logger.error(f"æ— æ³•ç¡®å®šèŠ‚ç‚¹ {node_id} çš„ä»»åŠ¡ç±»å‹")
                return
            
            # åˆ›å»ºä»»åŠ¡å®ä¾‹ - éœ€è¦è·å–æ›´å¤šå¿…è¦çš„ä¿¡æ¯
            workflow_instance_id = node_instance['workflow_instance_id']
            
            # è·å–å¤„ç†å™¨ä¿¡æ¯ç”¨äºä»»åŠ¡åˆ†é…
            processor_info = None
            if task_type == TaskInstanceType.HUMAN:
                processor_query = """
                    SELECT p.processor_id, p.user_id
                    FROM node_processor np
                    LEFT JOIN processor p ON np.processor_id = p.processor_id
                    WHERE np.node_id = %s AND p.type = 'human'
                    LIMIT 1
                """
                processor_info = await self.task_instance_repo.db.fetch_one(processor_query, node_id)
            elif task_type == TaskInstanceType.AGENT:
                processor_query = """
                    SELECT p.processor_id, p.agent_id
                    FROM node_processor np
                    LEFT JOIN processor p ON np.processor_id = p.processor_id
                    WHERE np.node_id = %s AND p.type = 'agent'
                    LIMIT 1
                """
                processor_info = await self.task_instance_repo.db.fetch_one(processor_query, node_id)
            
            if not processor_info:
                logger.error(f"æ‰¾ä¸åˆ°èŠ‚ç‚¹ {node_id} çš„å¤„ç†å™¨ä¿¡æ¯")
                return
                
            # æ„é€ ç¬¦åˆTaskInstanceCreateæ¨¡å‹çš„æ•°æ®
            task_data = TaskInstanceCreate(
                node_instance_id=node_instance_id,
                workflow_instance_id=workflow_instance_id,
                processor_id=processor_info['processor_id'],
                task_type=task_type,
                task_title=f"Task for {node_instance.get('node_instance_name', 'Unknown')}",
                task_description=node_instance.get('task_description', f"Auto-generated task for node {node_instance_id}"),
                input_data=str(node_instance.get('input_data', {})),  # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                assigned_user_id=processor_info.get('user_id') if task_type == TaskInstanceType.HUMAN else None,
                assigned_agent_id=processor_info.get('agent_id') if task_type == TaskInstanceType.AGENT else None,
                estimated_duration=30  # é»˜è®¤30åˆ†é’Ÿ
            )
            
            task_instance = await self.task_instance_repo.create_task(task_data)
            if task_instance:
                # task_instance is a dict, get the ID from it
                task_instance_id = task_instance.get('task_instance_id') if isinstance(task_instance, dict) else task_instance.task_instance_id
                logger.trace(f"âœ… ä¸ºèŠ‚ç‚¹ {node_instance_id} åˆ›å»ºäº† {task_type} ç±»å‹çš„ä»»åŠ¡: {task_instance_id}")
                
                if task_type == TaskInstanceType.HUMAN and processor_info.get('user_id'):
                    logger.info(f"ğŸ¯ äººå·¥ä»»åŠ¡å·²åˆ†é…ç»™ç”¨æˆ· {processor_info['user_id']}: {task_data.task_title}")
            else:
                logger.error(f"âŒ ä»»åŠ¡åˆ›å»ºå¤±è´¥")
            
        except Exception as e:
            logger.error(f"ä¸ºpendingèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    async def _cancel_running_tasks(self, instance_id: uuid.UUID):
        """å–æ¶ˆæ­£åœ¨è¿è¡Œçš„å¼‚æ­¥ä»»åŠ¡"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å®ä¾‹
            running_tasks_query = """
                SELECT ti.*, ni.workflow_instance_id
                FROM task_instance ti
                JOIN node_instance ni ON ti.node_instance_id = ni.node_instance_id
                WHERE ni.workflow_instance_id = $1
                AND ti.status IN ('running', 'RUNNING', 'assigned', 'ASSIGNED')
            """
            
            running_tasks = await self.task_instance_repo.db.fetch_all(
                running_tasks_query, 
                instance_id
            )
            
            logger.trace(f"æ‰¾åˆ° {len(running_tasks)} ä¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡éœ€è¦å–æ¶ˆ")
            
            for task in running_tasks:
                try:
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå–æ¶ˆ
                    task_id = task['task_instance_id']
                    update_query = """
                        UPDATE task_instance 
                        SET status = 'cancelled', 
                            error_message = 'å·¥ä½œæµè¢«å–æ¶ˆ',
                            updated_at = CURRENT_TIMESTAMP
                        WHERE task_instance_id = $1
                    """
                    
                    await self.task_instance_repo.db.execute(update_query, task_id)
                    logger.trace(f"å·²å–æ¶ˆä»»åŠ¡: {task_id}")
                    
                except Exception as e:
                    logger.error(f"å–æ¶ˆä»»åŠ¡ {task.get('task_instance_id')} å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"å–æ¶ˆè¿è¡Œä»»åŠ¡å¤±è´¥: {e}")

    async def _cancel_instance_context_tasks(self, context):
        """å–æ¶ˆå®ä¾‹ä¸Šä¸‹æ–‡ä¸­çš„ä»»åŠ¡"""
        try:
            # æ ‡è®°æ‰€æœ‰æœªå®Œæˆçš„èŠ‚ç‚¹ä¸ºå–æ¶ˆçŠ¶æ€
            for node_id, node_info in context.node_dependencies.items():
                if node_id not in context.completed_nodes:
                    # æ·»åŠ åˆ°å®ŒæˆèŠ‚ç‚¹é›†åˆä¸­ï¼Œé˜²æ­¢åç»­æ‰§è¡Œ
                    context.completed_nodes.add(node_id)
                    logger.trace(f"æ ‡è®°èŠ‚ç‚¹ {node_id} ä¸ºå·²å–æ¶ˆ")
            
            # æ¸…ç†ä¸Šä¸‹æ–‡çŠ¶æ€
            context.current_executing_nodes.clear()
            
        except Exception as e:
            logger.error(f"å–æ¶ˆå®ä¾‹ä¸Šä¸‹æ–‡ä»»åŠ¡å¤±è´¥: {e}")

    async def _notify_services_workflow_cancelled(self, instance_id: uuid.UUID):
        """é€šçŸ¥ç›¸å…³æœåŠ¡å·¥ä½œæµå·²å–æ¶ˆ"""
        try:
            # é€šçŸ¥Agentä»»åŠ¡æœåŠ¡ï¼ˆå¦‚æœæœ‰ç›¸å…³æ–¹æ³•ï¼‰
            try:
                from .agent_task_service import agent_task_service
                # æ£€æŸ¥æ˜¯å¦æœ‰å–æ¶ˆæ–¹æ³•ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡
                if hasattr(agent_task_service, 'cancel_workflow_tasks'):
                    await agent_task_service.cancel_workflow_tasks(instance_id)
                else:
                    logger.trace("Agentä»»åŠ¡æœåŠ¡æ²¡æœ‰cancel_workflow_tasksæ–¹æ³•ï¼Œè·³è¿‡é€šçŸ¥")
            except Exception as e:
                logger.warning(f"é€šçŸ¥Agentä»»åŠ¡æœåŠ¡å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ: {e}")
            
            # æ¸…ç†æ‰§è¡Œé˜Ÿåˆ—ä¸­çš„ç›¸å…³ä»»åŠ¡
            await self._remove_workflow_from_queue(instance_id)
            
            logger.trace(f"å·²é€šçŸ¥ç›¸å…³æœåŠ¡å·¥ä½œæµ {instance_id} å–æ¶ˆ")
            
        except Exception as e:
            logger.error(f"é€šçŸ¥æœåŠ¡å·¥ä½œæµå–æ¶ˆå¤±è´¥: {e}")

    async def _remove_workflow_from_queue(self, instance_id: uuid.UUID):
        """ä»æ‰§è¡Œé˜Ÿåˆ—ä¸­ç§»é™¤å·¥ä½œæµç›¸å…³ä»»åŠ¡"""
        try:
            # åˆ›å»ºæ–°çš„é˜Ÿåˆ—æ¥å­˜å‚¨ä¸éœ€è¦å–æ¶ˆçš„ä»»åŠ¡
            temp_queue = asyncio.Queue()
            cancelled_count = 0
            
            # å¤„ç†ç°æœ‰é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰é¡¹ç›®
            while not self.execution_queue.empty():
                try:
                    item = self.execution_queue.get_nowait()
                    
                    # æ£€æŸ¥æ˜¯å¦å±äºè¦å–æ¶ˆçš„å·¥ä½œæµ
                    if item.get('workflow_instance_id') == instance_id:
                        cancelled_count += 1
                        logger.trace(f"ä»é˜Ÿåˆ—ä¸­ç§»é™¤ä»»åŠ¡: {item.get('task_instance_id')}")
                    else:
                        # ä¿ç•™å…¶ä»–å·¥ä½œæµçš„ä»»åŠ¡
                        await temp_queue.put(item)
                        
                except asyncio.QueueEmpty:
                    break
            
            # å°†ä¿ç•™çš„ä»»åŠ¡æ”¾å›åŸé˜Ÿåˆ—
            while not temp_queue.empty():
                try:
                    item = temp_queue.get_nowait()
                    await self.execution_queue.put(item)
                except asyncio.QueueEmpty:
                    break
            
            if cancelled_count > 0:
                logger.trace(f"ä»æ‰§è¡Œé˜Ÿåˆ—ä¸­ç§»é™¤äº† {cancelled_count} ä¸ªå¾…æ‰§è¡Œä»»åŠ¡")
                
        except Exception as e:
            logger.error(f"ä»æ‰§è¡Œé˜Ÿåˆ—ç§»é™¤ä»»åŠ¡å¤±è´¥: {e}")

    # =============================================================================
    # äººå·¥ä»»åŠ¡ç®¡ç†æ–¹æ³• (å·²æ•´åˆåˆ°ç»Ÿä¸€æœåŠ¡)
    # =============================================================================
    
    async def get_user_tasks(self, user_id: uuid.UUID, 
                           status: Optional[TaskInstanceStatus] = None,
                           limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·çš„ä»»åŠ¡åˆ—è¡¨"""
        try:
            logger.info(f"ğŸ” [ä»»åŠ¡æŸ¥è¯¢] å¼€å§‹æŸ¥è¯¢ç”¨æˆ·ä»»åŠ¡:")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            logger.info(f"   - çŠ¶æ€è¿‡æ»¤: {status.value if status else 'å…¨éƒ¨'}")
            logger.info(f"   - é™åˆ¶æ•°é‡: {limit}")
            
            tasks = await self.task_instance_repo.get_human_tasks_for_user(user_id, status, limit)
            
            logger.info(f"ğŸ“Š [ä»»åŠ¡æŸ¥è¯¢] æŸ¥è¯¢ç»“æœ: æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
            
            # æ·»åŠ ä»»åŠ¡ä¼˜å…ˆçº§å’Œæˆªæ­¢æ—¶é—´ç­‰é™„åŠ ä¿¡æ¯
            # for i, task in enumerate(tasks, 1):
            #     logger.info(f"   ä»»åŠ¡{i}: {task.get('task_title')} | çŠ¶æ€: {task.get('status')} | ID: {task.get('task_instance_id')}")
            #     task = await self._enrich_task_info(task)
            
            if len(tasks) == 0:
                logger.warning(f"âš ï¸ [ä»»åŠ¡æŸ¥è¯¢] ç”¨æˆ· {user_id} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä»»åŠ¡")
            
            return tasks
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def get_task_details(self, task_id: uuid.UUID, user_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡è¯¦æƒ…"""
        try:
            logger.info(f"ğŸ” [ä»»åŠ¡è¯¦æƒ…] å¼€å§‹æŸ¥è¯¢ä»»åŠ¡è¯¦æƒ…:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            
            # è·å–ä»»åŠ¡åŸºç¡€ä¿¡æ¯
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                logger.warning(f"âš ï¸ [ä»»åŠ¡è¯¦æƒ…] ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return None
            
            # éªŒè¯ä»»åŠ¡åˆ†é…ç»™è¯¥ç”¨æˆ· - ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…UUIDç±»å‹é—®é¢˜
            task_assigned_user_id = task.get('assigned_user_id')
            task_assigned_user_id_str = str(task_assigned_user_id) if task_assigned_user_id else None
            user_id_str = str(user_id) if user_id else None
            
            if task_assigned_user_id_str != user_id_str:
                logger.warning(f"âš ï¸ [ä»»åŠ¡è¯¦æƒ…] ç”¨æˆ· {user_id} æ— æƒé™è®¿é—®ä»»åŠ¡ {task_id}")
                logger.warning(f"   - ä»»åŠ¡åˆ†é…ç”¨æˆ·ID: {task_assigned_user_id_str}")
                logger.warning(f"   - è¯·æ±‚ç”¨æˆ·ID: {user_id_str}")
                return None
            
            # è·å–èŠ‚ç‚¹ä¿¡æ¯
            node_info = await self._get_node_info(task.get('node_instance_id'))
            if node_info:
                task['node_info'] = node_info
            
            # è·å–å¤„ç†å™¨ä¿¡æ¯
            processor_info = await self._get_processor_info(task.get('processor_id'))
            if processor_info:
                task['processor_info'] = processor_info
            
            # è·å–ä¸Šæ¸¸ä¸Šä¸‹æ–‡
            upstream_context = await self._get_upstream_context(task)
            task['upstream_context'] = upstream_context
            
            # ğŸ” æ·»åŠ è°ƒè¯•æ—¥å¿—
            logger.info(f"ğŸ” [ä»»åŠ¡è¯¦æƒ…è°ƒè¯•] ä¸Šæ¸¸ä¸Šä¸‹æ–‡æ•°æ®:")
            logger.info(f"   - ä¸Šæ¸¸ä¸Šä¸‹æ–‡é”®: {list(upstream_context.keys()) if upstream_context else 'æ— æ•°æ®'}")

            # ğŸ†• æå–æ‰€æœ‰ä¸Šæ¸¸ä»»åŠ¡æäº¤çš„é™„ä»¶åˆ°ä»»åŠ¡é¡¶çº§å­—æ®µ
            task_attachments = []
            if upstream_context:
                immediate_results = upstream_context.get('immediate_upstream_results', {})
                logger.info(f"   - immediate_upstream_resultsé”®: {list(immediate_results.keys())}")
                for node_name, node_data in immediate_results.items():
                    logger.info(f"   - èŠ‚ç‚¹ {node_name} çš„output_data: {node_data.get('output_data', {})}")
                    # æå–ä¸Šæ¸¸ä»»åŠ¡é™„ä»¶
                    upstream_task_attachments = node_data.get('task_attachments', [])
                    if upstream_task_attachments:
                        task_attachments.extend(upstream_task_attachments)
                        logger.info(f"   - èŠ‚ç‚¹ {node_name} è´¡çŒ® {len(upstream_task_attachments)} ä¸ªä»»åŠ¡é™„ä»¶")

            # å°†åˆå¹¶çš„é™„ä»¶æ·»åŠ åˆ°ä»»åŠ¡æ•°æ®ä¸­
            task['task_attachments'] = task_attachments
            logger.info(f"ğŸ“ [é™„ä»¶åˆå¹¶] å…±æ”¶é›†åˆ° {len(task_attachments)} ä¸ªä¸Šæ¸¸ä»»åŠ¡é™„ä»¶")

            # ä¸°å¯Œä»»åŠ¡ä¿¡æ¯
            task = await self._enrich_task_info(task)
            
            # ğŸ” æœ€ç»ˆä»»åŠ¡æ•°æ®ç»“æ„è°ƒè¯•
            logger.info(f"ğŸ” [ä»»åŠ¡è¯¦æƒ…è°ƒè¯•] æœ€ç»ˆä»»åŠ¡æ•°æ®ç»“æ„:")
            logger.info(f"   - ä»»åŠ¡åŸºç¡€å­—æ®µ: {list(task.keys())}")
            logger.info(f"   - upstream_contextæ˜¯å¦å­˜åœ¨: {'upstream_context' in task}")
            logger.info(f"   - context_dataæ˜¯å¦å­˜åœ¨: {'context_data' in task}")
            
            logger.info(f"âœ… [ä»»åŠ¡è¯¦æƒ…] ä»»åŠ¡è¯¦æƒ…æŸ¥è¯¢æˆåŠŸ")
            return task
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
            raise
    
    async def start_human_task(self, task_id: uuid.UUID, user_id: uuid.UUID) -> Dict[str, Any]:
        """å¼€å§‹äººå·¥ä»»åŠ¡"""
        try:
            logger.info(f"ğŸš€ [ä»»åŠ¡å¼€å§‹] ç”¨æˆ·å¼€å§‹ä»»åŠ¡:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            logger.info(f"ğŸ” [ä»»åŠ¡å¼€å§‹] æ­£åœ¨æŸ¥è¯¢ä»»åŠ¡ä¿¡æ¯...")
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                logger.error(f"âŒ [ä»»åŠ¡å¼€å§‹] ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
            
            logger.info(f"ğŸ“‹ [ä»»åŠ¡å¼€å§‹] æ‰¾åˆ°ä»»åŠ¡ä¿¡æ¯:")
            logger.info(f"   - ä»»åŠ¡æ ‡é¢˜: {task.get('task_title')}")
            logger.info(f"   - å½“å‰çŠ¶æ€: {task.get('status')}")
            logger.info(f"   - åˆ†é…ç”¨æˆ·ID: {task.get('assigned_user_id')}")
            logger.info(f"   - ä»»åŠ¡ç±»å‹: {task.get('task_type')}")
            
            # éªŒè¯æƒé™
            logger.info(f"ğŸ” [ä»»åŠ¡å¼€å§‹] éªŒè¯ç”¨æˆ·æƒé™...")
            assigned_user_id = task.get('assigned_user_id')
            logger.info(f"   - è¯·æ±‚ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")
            logger.info(f"   - åˆ†é…ç”¨æˆ·ID: {assigned_user_id} (ç±»å‹: {type(assigned_user_id)})")
            
            # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            user_id_str = str(user_id)
            assigned_user_id_str = str(assigned_user_id) if assigned_user_id else None
            logger.info(f"   - å­—ç¬¦ä¸²æ¯”è¾ƒ: '{user_id_str}' vs '{assigned_user_id_str}'")
            
            if assigned_user_id_str != user_id_str:
                logger.error(f"âŒ [ä»»åŠ¡å¼€å§‹] æƒé™éªŒè¯å¤±è´¥:")
                logger.error(f"   - è¯·æ±‚ç”¨æˆ·ID(str): {user_id_str}")
                logger.error(f"   - åˆ†é…ç”¨æˆ·ID(str): {assigned_user_id_str}")
                return {"success": False, "message": "æ‚¨æ— æƒé™æ“ä½œæ­¤ä»»åŠ¡"}
            logger.info(f"âœ… [ä»»åŠ¡å¼€å§‹] æƒé™éªŒè¯é€šè¿‡")
            
            # éªŒè¯çŠ¶æ€
            logger.info(f"ğŸ“Š [ä»»åŠ¡å¼€å§‹] éªŒè¯ä»»åŠ¡çŠ¶æ€...")
            current_status = task.get('status')
            # æ”¯æŒå¤§å°å†™çŠ¶æ€åŒ¹é…
            allowed_statuses = ['PENDING', 'ASSIGNED', 'pending', 'assigned']
            if current_status not in allowed_statuses:
                logger.error(f"âŒ [ä»»åŠ¡å¼€å§‹] çŠ¶æ€éªŒè¯å¤±è´¥:")
                logger.error(f"   - å½“å‰çŠ¶æ€: {current_status}")
                logger.error(f"   - å…è®¸çš„çŠ¶æ€: {allowed_statuses}")
                return {"success": False, "message": f"ä»»åŠ¡çŠ¶æ€ä¸å…è®¸å¼€å§‹: {current_status}"}
            logger.info(f"âœ… [ä»»åŠ¡å¼€å§‹] çŠ¶æ€éªŒè¯é€šè¿‡")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            logger.info(f"ğŸ’¾ [ä»»åŠ¡å¼€å§‹] å‡†å¤‡æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º IN_PROGRESS...")
            update_data = TaskInstanceUpdate(
                status=TaskInstanceStatus.IN_PROGRESS,
                started_at=now_utc()
            )
            
            logger.info(f"ğŸ“ [ä»»åŠ¡å¼€å§‹] è°ƒç”¨æ•°æ®åº“æ›´æ–°...")
            result = await self.task_instance_repo.update_task(task_id, update_data)
            logger.info(f"ğŸ’¾ [ä»»åŠ¡å¼€å§‹] æ•°æ®åº“æ›´æ–°ç»“æœ: {result}")
            
            if result:
                logger.info(f"âœ… [ä»»åŠ¡å¼€å§‹] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º IN_PROGRESS")
                
                # éªŒè¯æ›´æ–°ç»“æœ
                logger.info(f"ğŸ” [ä»»åŠ¡å¼€å§‹] éªŒè¯æ›´æ–°ç»“æœ...")
                updated_task = await self.task_instance_repo.get_task_by_id(task_id)
                if updated_task:
                    logger.info(f"ğŸ“Š [ä»»åŠ¡å¼€å§‹] æ›´æ–°åçš„ä»»åŠ¡çŠ¶æ€: {updated_task.get('status')}")
                    logger.info(f"ğŸ“Š [ä»»åŠ¡å¼€å§‹] æ›´æ–°åçš„å¼€å§‹æ—¶é—´: {updated_task.get('started_at')}")
                else:
                    logger.error(f"âŒ [ä»»åŠ¡å¼€å§‹] æ— æ³•è·å–æ›´æ–°åçš„ä»»åŠ¡ä¿¡æ¯")
                
                success_result = {
                    "success": True,
                    "message": "ä»»åŠ¡å·²å¼€å§‹",
                    "task_id": str(task_id),
                    "status": "IN_PROGRESS",
                    "started_at": now_utc().isoformat()
                }
                logger.info(f"ğŸ‰ [ä»»åŠ¡å¼€å§‹] è¿”å›æˆåŠŸç»“æœ: {success_result}")
                return success_result
            else:
                logger.error(f"âŒ [ä»»åŠ¡å¼€å§‹] æ•°æ®åº“æ›´æ–°å¤±è´¥")
                return {"success": False, "message": "å¯åŠ¨ä»»åŠ¡å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"ğŸ’¥ [ä»»åŠ¡å¼€å§‹] å¼‚å¸¸å‘ç”Ÿ: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"ğŸ“„ [ä»»åŠ¡å¼€å§‹] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def submit_human_task_result(self, task_id: uuid.UUID, user_id: uuid.UUID,
                                     result_data: Dict[str, Any], result_summary: Optional[str] = None,
                                     selected_next_nodes: Optional[List[uuid.UUID]] = None) -> Dict[str, Any]:
        """æäº¤äººå·¥ä»»åŠ¡ç»“æœ"""
        try:
            # logger.info(f"ğŸ“ [ä»»åŠ¡æäº¤] ç”¨æˆ·æäº¤ä»»åŠ¡ç»“æœ:")
            # logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            # logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            # logger.info(f"   - ç»“æœæ•°æ®: {result_data}")
            # logger.info(f"   - ç»“æœé”®æ•°é‡: {len(result_data.keys()) if isinstance(result_data, dict) else 'N/A'}")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            logger.info(f"ğŸ” [ä»»åŠ¡æäº¤] æ­£åœ¨æŸ¥è¯¢ä»»åŠ¡ä¿¡æ¯...")
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                logger.error(f"âŒ [ä»»åŠ¡æäº¤] ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
            
            logger.info(f"ğŸ“‹ [ä»»åŠ¡æäº¤] æ‰¾åˆ°ä»»åŠ¡ä¿¡æ¯:")
            logger.info(f"   - ä»»åŠ¡æ ‡é¢˜: {task.get('task_title')}")
            logger.info(f"   - å½“å‰çŠ¶æ€: {task.get('status')}")
            logger.info(f"   - åˆ†é…ç”¨æˆ·ID: {task.get('assigned_user_id')}")
            logger.info(f"   - ä»»åŠ¡ç±»å‹: {task.get('task_type')}")
            
            # éªŒè¯æƒé™
            # logger.info(f"ğŸ” [ä»»åŠ¡æäº¤] éªŒè¯ç”¨æˆ·æƒé™...")
            assigned_user_id = task.get('assigned_user_id')
            # logger.info(f"   - è¯·æ±‚ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")
            # logger.info(f"   - åˆ†é…ç”¨æˆ·ID: {assigned_user_id} (ç±»å‹: {type(assigned_user_id)})")
            
            # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            user_id_str = str(user_id)
            assigned_user_id_str = str(assigned_user_id) if assigned_user_id else None
            logger.info(f"   - å­—ç¬¦ä¸²æ¯”è¾ƒ: '{user_id_str}' vs '{assigned_user_id_str}'")
            
            if assigned_user_id_str != user_id_str:
                logger.error(f"âŒ [ä»»åŠ¡æäº¤] æƒé™éªŒè¯å¤±è´¥:")
                logger.error(f"   - è¯·æ±‚ç”¨æˆ·ID(str): {user_id_str}")
                logger.error(f"   - åˆ†é…ç”¨æˆ·ID(str): {assigned_user_id_str}")
                return {"success": False, "message": "æ‚¨æ— æƒé™æ“ä½œæ­¤ä»»åŠ¡"}
            logger.info(f"âœ… [ä»»åŠ¡æäº¤] æƒé™éªŒè¯é€šè¿‡")
            
            # éªŒè¯çŠ¶æ€
            logger.info(f"ğŸ“Š [ä»»åŠ¡æäº¤] éªŒè¯ä»»åŠ¡çŠ¶æ€...")
            current_status = task.get('status')
            # æ”¯æŒå¤§å°å†™çŠ¶æ€åŒ¹é…
            allowed_statuses = ['IN_PROGRESS', 'ASSIGNED', 'in_progress', 'assigned']
            if current_status not in allowed_statuses:
                logger.error(f"âŒ [ä»»åŠ¡æäº¤] çŠ¶æ€éªŒè¯å¤±è´¥:")
                logger.error(f"   - å½“å‰çŠ¶æ€: {current_status}")
                logger.error(f"   - å…è®¸çš„çŠ¶æ€: {allowed_statuses}")
                return {"success": False, "message": f"ä»»åŠ¡çŠ¶æ€ä¸å…è®¸æäº¤: {current_status}"}
            logger.info(f"âœ… [ä»»åŠ¡æäº¤] çŠ¶æ€éªŒè¯é€šè¿‡")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€å’Œç»“æœ
            logger.info(f"ğŸ’¾ [ä»»åŠ¡æäº¤] å‡†å¤‡æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º COMPLETED...")
            
            # å°†å­—å…¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ä»¥ç¬¦åˆæ¨¡å‹è¦æ±‚
            import json
            output_data_str = json.dumps(result_data, ensure_ascii=False) if result_data else "{}"
            logger.info(f"   - è¾“å‡ºæ•°æ®å­—ç¬¦ä¸²é•¿åº¦: {len(output_data_str)}")
            
            update_data = TaskInstanceUpdate(
                status=TaskInstanceStatus.COMPLETED,
                output_data=output_data_str,
                result_summary=result_summary,
                completed_at=now_utc()
            )
            
            result = await self.task_instance_repo.update_task(task_id, update_data)
            if result:
                logger.info(f"âœ… [ä»»åŠ¡æäº¤] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º COMPLETED")

                # è·å–æ›´æ–°åçš„ä»»åŠ¡
                updated_task = await self.task_instance_repo.get_task_by_id(task_id)

                # ğŸ†• å¤„ç†ç”¨æˆ·æ¡ä»¶è¾¹é€‰æ‹©
                if selected_next_nodes:
                    logger.info(f"ğŸ”€ [æ¡ä»¶è¾¹] å¤„ç†ç”¨æˆ·é€‰æ‹©çš„ä¸‹æ¸¸èŠ‚ç‚¹: {selected_next_nodes}")
                    # å°†ç”¨æˆ·é€‰æ‹©ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ä¸­
                    workflow_instance_id = updated_task.get('workflow_instance_id')
                    node_instance_id = updated_task.get('node_instance_id')

                    if workflow_instance_id and node_instance_id:
                        try:
                            context = await self.context_manager.get_context(workflow_instance_id)
                            if context:
                                # è·å–ä¸»è·¯å¾„ID
                                main_path_id = f"main_{workflow_instance_id}"
                                if main_path_id in context.execution_context['execution_paths']:
                                    path = context.execution_context['execution_paths'][main_path_id]
                                    path.user_selections[node_instance_id] = selected_next_nodes
                                    logger.info(f"âœ… [æ¡ä»¶è¾¹] ç”¨æˆ·é€‰æ‹©å·²ä¿å­˜åˆ°æ‰§è¡Œè·¯å¾„")
                                else:
                                    logger.warning(f"âš ï¸ [æ¡ä»¶è¾¹] ä¸»æ‰§è¡Œè·¯å¾„ä¸å­˜åœ¨: {main_path_id}")
                            else:
                                logger.warning(f"âš ï¸ [æ¡ä»¶è¾¹] å·¥ä½œæµä¸Šä¸‹æ–‡ä¸å­˜åœ¨: {workflow_instance_id}")
                        except Exception as e:
                            logger.error(f"âŒ [æ¡ä»¶è¾¹] ä¿å­˜ç”¨æˆ·é€‰æ‹©å¤±è´¥: {e}")

                # ç»Ÿä¸€å¤„ç†ä»»åŠ¡å®Œæˆ - é¿å…é‡å¤è°ƒç”¨ mark_node_completed
                await self._handle_task_completion_unified(task, updated_task, result_data, "human")
                
                return {
                    "success": True,
                    "message": "ä»»åŠ¡ç»“æœå·²æäº¤",
                    "task_id": str(task_id),
                    "status": "COMPLETED"
                }
            else:
                logger.error(f"âŒ [ä»»åŠ¡æäº¤] æ•°æ®åº“æ›´æ–°å¤±è´¥")
                return {"success": False, "message": "æäº¤ä»»åŠ¡ç»“æœå¤±è´¥"}
                
        except Exception as e:
            logger.error(f"æäº¤äººå·¥ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
            raise
    
    async def pause_human_task(self, task_id: uuid.UUID, user_id: uuid.UUID,
                             pause_reason: Optional[str] = None) -> Dict[str, Any]:
        """æš‚åœäººå·¥ä»»åŠ¡"""
        try:
            logger.info(f"â¸ï¸ [ä»»åŠ¡æš‚åœ] ç”¨æˆ·æš‚åœä»»åŠ¡:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            logger.info(f"   - æš‚åœåŸå› : {pause_reason}")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
            
            # éªŒè¯æƒé™
            assigned_user_id = task.get('assigned_user_id')
            user_id_str = str(user_id)
            assigned_user_id_str = str(assigned_user_id) if assigned_user_id else None
            
            if assigned_user_id_str != user_id_str:
                return {"success": False, "message": "æ‚¨æ— æƒé™æ“ä½œæ­¤ä»»åŠ¡"}
            
            # éªŒè¯çŠ¶æ€ - æ”¯æŒå¤§å°å†™åŒ¹é…
            current_status = task.get('status')
            if current_status not in ['IN_PROGRESS', 'in_progress']:
                return {"success": False, "message": f"åªæœ‰è¿›è¡Œä¸­çš„ä»»åŠ¡å¯ä»¥æš‚åœï¼Œå½“å‰çŠ¶æ€: {current_status}"}
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            update_data = TaskInstanceUpdate(
                status=TaskInstanceStatus.PAUSED,
                paused_reason=pause_reason,
                paused_at=now_utc()
            )
            
            result = await self.task_instance_repo.update_task(task_id, update_data)
            if result:
                logger.info(f"âœ… [ä»»åŠ¡æš‚åœ] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º PAUSED")
                return {
                    "success": True,
                    "message": "ä»»åŠ¡å·²æš‚åœ",
                    "task_id": str(task_id),
                    "status": "PAUSED"
                }
            else:
                return {"success": False, "message": "æš‚åœä»»åŠ¡å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"æš‚åœäººå·¥ä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def cancel_human_task(self, task_id: uuid.UUID, user_id: uuid.UUID,
                              cancel_reason: Optional[str] = None) -> Dict[str, Any]:
        """å–æ¶ˆäººå·¥ä»»åŠ¡"""
        try:
            logger.info(f"ğŸš« [ä»»åŠ¡å–æ¶ˆ] ç”¨æˆ·å–æ¶ˆä»»åŠ¡:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            logger.info(f"   - å–æ¶ˆåŸå› : {cancel_reason}")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
            
            # éªŒè¯æƒé™
            assigned_user_id = task.get('assigned_user_id')
            user_id_str = str(user_id)
            assigned_user_id_str = str(assigned_user_id) if assigned_user_id else None
            
            if assigned_user_id_str != user_id_str:
                return {"success": False, "message": "æ‚¨æ— æƒé™æ“ä½œæ­¤ä»»åŠ¡"}
            
            # éªŒè¯çŠ¶æ€ - æ”¯æŒå¤§å°å†™åŒ¹é…
            current_status = task.get('status')
            if current_status in ['COMPLETED', 'CANCELLED', 'FAILED', 'completed', 'cancelled', 'failed']:
                return {"success": False, "message": f"ä»»åŠ¡å·²ç»“æŸï¼Œæ— æ³•å–æ¶ˆï¼Œå½“å‰çŠ¶æ€: {current_status}"}
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            update_data = TaskInstanceUpdate(
                status=TaskInstanceStatus.CANCELLED,
                error_message=cancel_reason or "ç”¨æˆ·å–æ¶ˆ",
                cancelled_at=now_utc()
            )
            
            result = await self.task_instance_repo.update_task(task_id, update_data)
            if result:
                logger.info(f"âœ… [ä»»åŠ¡å–æ¶ˆ] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º CANCELLED")

                # ğŸ”§ Linuså¼ä¿®å¤ï¼šå‘ä¸Šä¼ æ’­å–æ¶ˆçŠ¶æ€åˆ°èŠ‚ç‚¹å’Œå·¥ä½œæµ
                await self._propagate_task_cancellation(task_id, task, cancel_reason)

                return {
                    "success": True,
                    "message": "ä»»åŠ¡å·²å–æ¶ˆ",
                    "task_id": str(task_id),
                    "status": "CANCELLED"
                }
            else:
                return {"success": False, "message": "å–æ¶ˆä»»åŠ¡å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"å–æ¶ˆäººå·¥ä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    # é€šç”¨ä»»åŠ¡æ“ä½œæ–¹æ³•ï¼ˆä¸ºAPIå±‚æä¾›å…¼å®¹æ¥å£ï¼‰
    async def pause_task(self, task_id: uuid.UUID, user_id: uuid.UUID, reason: Optional[str] = None) -> Dict[str, Any]:
        """æš‚åœä»»åŠ¡ï¼ˆé€šç”¨æ¥å£ï¼‰"""
        return await self.pause_human_task(task_id, user_id, reason)
    
    async def cancel_task(self, task_id: uuid.UUID, user_id: uuid.UUID, reason: Optional[str] = None) -> Dict[str, Any]:
        """å–æ¶ˆä»»åŠ¡ï¼ˆé€šç”¨æ¥å£ï¼‰"""
        return await self.cancel_human_task(task_id, user_id, reason)
    
    async def request_help(self, task_id: uuid.UUID, user_id: uuid.UUID, help_message: str) -> Dict[str, Any]:
        """è¯·æ±‚å¸®åŠ©"""
        try:
            logger.info(f"ğŸ†˜ [è¯·æ±‚å¸®åŠ©] ç”¨æˆ·è¯·æ±‚å¸®åŠ©:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            logger.info(f"   - å¸®åŠ©ä¿¡æ¯: {help_message}")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
            
            # éªŒè¯æƒé™
            assigned_user_id = task.get('assigned_user_id')
            user_id_str = str(user_id)
            assigned_user_id_str = str(assigned_user_id) if assigned_user_id else None
            
            if assigned_user_id_str != user_id_str:
                return {"success": False, "message": "æ‚¨æ— æƒé™æ“ä½œæ­¤ä»»åŠ¡"}
            
            # è®°å½•å¸®åŠ©è¯·æ±‚åˆ°ä»»åŠ¡å®ä¾‹çš„context_dataä¸­
            current_context = task.get('context_data', {})
            if 'help_requests' not in current_context:
                current_context['help_requests'] = []
            
            help_request = {
                'timestamp': now_utc().isoformat(),
                'message': help_message,
                'status': 'pending'
            }
            current_context['help_requests'].append(help_request)
            
            # æ›´æ–°ä»»åŠ¡
            update_data = TaskInstanceUpdate(context_data=current_context)
            result = await self.task_instance_repo.update_task(task_id, update_data)
            
            if result:
                logger.info(f"âœ… [è¯·æ±‚å¸®åŠ©] å¸®åŠ©è¯·æ±‚å·²è®°å½•")
                return {
                    "success": True,
                    "message": "å¸®åŠ©è¯·æ±‚å·²æäº¤",
                    "task_id": str(task_id),
                    "help_request": help_request
                }
            else:
                return {"success": False, "message": "æäº¤å¸®åŠ©è¯·æ±‚å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"è¯·æ±‚å¸®åŠ©å¤±è´¥: {e}")
            raise
    
    async def reject_task(self, task_id: uuid.UUID, user_id: uuid.UUID, reason: str) -> Dict[str, Any]:
        """æ‹’ç»ä»»åŠ¡"""
        try:
            logger.info(f"ğŸš« [æ‹’ç»ä»»åŠ¡] ç”¨æˆ·æ‹’ç»ä»»åŠ¡:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            logger.info(f"   - æ‹’ç»åŸå› : {reason}")
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task = await self.task_instance_repo.get_task_by_id(task_id)
            if not task:
                return {"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
            
            # éªŒè¯æƒé™
            assigned_user_id = task.get('assigned_user_id')
            user_id_str = str(user_id)
            assigned_user_id_str = str(assigned_user_id) if assigned_user_id else None
            
            if assigned_user_id_str != user_id_str:
                return {"success": False, "message": "æ‚¨æ— æƒé™æ“ä½œæ­¤ä»»åŠ¡"}
            
            # éªŒè¯çŠ¶æ€ - æ”¯æŒå¤§å°å†™åŒ¹é…
            current_status = task.get('status')
            if current_status not in ['PENDING', 'ASSIGNED', 'pending', 'assigned']:
                return {"success": False, "message": f"ä»»åŠ¡çŠ¶æ€ä¸å…è®¸æ‹’ç»: {current_status}"}
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæ‹’ç»
            update_data = TaskInstanceUpdate(
                status=TaskInstanceStatus.CANCELLED,
                error_message=f"ç”¨æˆ·æ‹’ç»: {reason}",
                cancelled_at=now_utc()
            )
            
            result = await self.task_instance_repo.update_task(task_id, update_data)
            if result:
                logger.info(f"âœ… [æ‹’ç»ä»»åŠ¡] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º CANCELLED")
                
                # TODO: è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°åˆ†é…ä»»åŠ¡çš„é€»è¾‘
                
                return {
                    "success": True,
                    "message": "ä»»åŠ¡å·²æ‹’ç»",
                    "task_id": str(task_id),
                    "status": "CANCELLED"
                }
            else:
                return {"success": False, "message": "æ‹’ç»ä»»åŠ¡å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"æ‹’ç»ä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def get_task_history(self, user_id: uuid.UUID, 
                             days: int = 30, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·ä»»åŠ¡å†å²"""
        try:
            logger.info(f"ğŸ“œ [ä»»åŠ¡å†å²] æŸ¥è¯¢ç”¨æˆ·ä»»åŠ¡å†å²:")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            logger.info(f"   - å¤©æ•°: {days}")
            logger.info(f"   - é™åˆ¶: {limit}")
            
            tasks = await self.task_instance_repo.get_user_task_history(user_id, days, limit)
            
            # ä¸°å¯Œä»»åŠ¡ä¿¡æ¯
            for task in tasks:
                task = await self._enrich_task_info(task)
            
            logger.info(f"âœ… [ä»»åŠ¡å†å²] æ‰¾åˆ° {len(tasks)} æ¡å†å²è®°å½•")
            return tasks
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡å†å²å¤±è´¥: {e}")
            raise
    
    async def get_task_statistics(self, user_id: uuid.UUID) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            logger.info(f"ğŸ“Š [ä»»åŠ¡ç»Ÿè®¡] æŸ¥è¯¢ç”¨æˆ·ä»»åŠ¡ç»Ÿè®¡:")
            logger.info(f"   - ç”¨æˆ·ID: {user_id}")
            
            stats = await self.task_instance_repo.get_user_task_statistics(user_id)
            
            logger.info(f"âœ… [ä»»åŠ¡ç»Ÿè®¡] ç»Ÿè®¡ä¿¡æ¯æŸ¥è¯¢æˆåŠŸ")
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
            raise
    
    # =============================================================================
    # è¾…åŠ©æ–¹æ³• (å·²æ•´åˆåˆ°ç»Ÿä¸€æœåŠ¡)
    # =============================================================================
    
    async def _get_node_info(self, node_instance_id: Optional[uuid.UUID]) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹ä¿¡æ¯"""
        if not node_instance_id:
            return None
            
        try:
            query = """
            SELECT ni.*, n.name as node_name, n.type as node_type, n.task_description as description
            FROM node_instance ni
            JOIN node n ON ni.node_id = n.node_id
            WHERE ni.node_instance_id = %s
            """
            return await self.task_instance_repo.db.fetch_one(query, node_instance_id)
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _get_processor_info(self, processor_id: Optional[uuid.UUID]) -> Optional[Dict[str, Any]]:
        """è·å–å¤„ç†å™¨ä¿¡æ¯"""
        if not processor_id:
            return None
            
        try:
            query = """
            SELECT p.*, u.username, a.agent_name
            FROM processor p
            LEFT JOIN "user" u ON p.user_id = u.user_id
            LEFT JOIN agent a ON p.agent_id = a.agent_id
            WHERE p.processor_id = $1
            """
            return await self.processor_repo.db.fetch_one(query, processor_id)
        except Exception as e:
            logger.error(f"è·å–å¤„ç†å™¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _get_upstream_context(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–ä¸Šæ¸¸ä¸Šä¸‹æ–‡"""
        try:
            workflow_instance_id = task.get('workflow_instance_id')
            node_instance_id = task.get('node_instance_id')
            
            if not workflow_instance_id or not node_instance_id:
                return {}
            
            # ä½¿ç”¨ç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–ä¸Šæ¸¸ä¸Šä¸‹æ–‡
            return await self.context_manager.get_node_upstream_context(
                workflow_instance_id, node_instance_id
            )
        except Exception as e:
            logger.error(f"è·å–ä¸Šæ¸¸ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {}
    
    async def _enrich_task_info(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸°å¯Œä»»åŠ¡ä¿¡æ¯"""
        try:
            # æ·»åŠ æ‰§è¡Œæ—¶é—´ç­‰è®¡ç®—å­—æ®µ
            if task.get('started_at') and not task.get('completed_at'):
                execution_time = (now_utc() - task['started_at']).total_seconds() / 60
                task['execution_time_minutes'] = round(execution_time, 2)
            
            # æ·»åŠ ä»»åŠ¡ä¼˜å…ˆçº§
            task['priority'] = self._calculate_task_priority(task)
            
            # æ·»åŠ æˆªæ­¢æ—¶é—´
            task['due_date'] = self._calculate_due_date(task)
            
            return task
        except Exception as e:
            logger.error(f"ä¸°å¯Œä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
            return task
    
    def _calculate_task_priority(self, task: Dict[str, Any]) -> str:
        """è®¡ç®—ä»»åŠ¡ä¼˜å…ˆçº§"""
        try:
            # åŸºäºä»»åŠ¡ç±»å‹å’Œåˆ›å»ºæ—¶é—´è®¡ç®—ä¼˜å…ˆçº§
            created_at = task.get('created_at')
            if not created_at:
                return "normal"
            
            age_hours = (now_utc() - created_at).total_seconds() / 3600
            
            if age_hours > 24:
                return "high"
            elif age_hours > 8:
                return "medium"
            else:
                return "normal"
        except Exception:
            return "normal"
    
    def _calculate_due_date(self, task: Dict[str, Any]) -> Optional[str]:
        """è®¡ç®—æˆªæ­¢æ—¶é—´"""
        try:
            created_at = task.get('created_at')
            estimated_duration = task.get('estimated_duration', 60)  # é»˜è®¤60åˆ†é’Ÿ
            
            if created_at:
                due_date = created_at + timedelta(minutes=estimated_duration)
                return due_date.isoformat()
            return None
        except Exception:
            return None
    
    async def _handle_task_completion_unified(self, task: Dict[str, Any], 
                                            updated_task: Dict[str, Any],
                                            output_data: str,
                                            task_type: str):
        """ç»Ÿä¸€å¤„ç†ä»»åŠ¡å®Œæˆ - ä¿®å¤å¹¶å‘ç«æ€æ¡ä»¶"""
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åˆ†å¸ƒå¼é”é˜²æ­¢å¹¶å‘ç«æ€æ¡ä»¶
        lock_key = f"task_completion_{task['workflow_instance_id']}"
        
        try:
            logger.info(f"ğŸ”„ [ç»Ÿä¸€ä»»åŠ¡å®Œæˆ-å¹¶å‘ä¿®å¤] å¤„ç†{task_type}ä»»åŠ¡å®Œæˆ: {task['task_instance_id']}")
            logger.info(f"   è·å–å·¥ä½œæµçº§åˆ«é”: {lock_key}")
            
            # ğŸ”§ ä½¿ç”¨å¼‚æ­¥é”ç¡®ä¿åŒä¸€å·¥ä½œæµçš„ä»»åŠ¡å®Œæˆå¤„ç†æ˜¯ä¸²è¡Œçš„
            if not hasattr(self, '_completion_locks'):
                self._completion_locks = {}
            
            if lock_key not in self._completion_locks:
                self._completion_locks[lock_key] = asyncio.Lock()
            
            async with self._completion_locks[lock_key]:
                logger.debug(f"   ğŸ”’ å·²è·å–å·¥ä½œæµé”ï¼Œå¼€å§‹åŸå­æ“ä½œ")
                
                # è·å–èŠ‚ç‚¹ä¿¡æ¯  
                node_query = """
                SELECT n.node_id 
                FROM node_instance ni
                JOIN node n ON ni.node_id = n.node_id
                WHERE ni.node_instance_id = %s
                """
                node_info = await self.task_instance_repo.db.fetch_one(node_query, task['node_instance_id'])
                
                if not node_info:
                    logger.error(f"âŒ æ— æ³•æ‰¾åˆ°èŠ‚ç‚¹ä¿¡æ¯: {task['node_instance_id']}")
                    return
                
                # ğŸ”§ çŠ¶æ€ä¸€è‡´æ€§æ£€æŸ¥ï¼šç¡®ä¿ä»»åŠ¡å’ŒèŠ‚ç‚¹çŠ¶æ€åŒæ­¥
                fresh_task = await self.task_instance_repo.get_task_by_id(task['task_instance_id'])
                if fresh_task and fresh_task.get('status') != 'completed':
                    logger.warning(f"âš ï¸  ä»»åŠ¡çŠ¶æ€ä¸ä¸€è‡´ï¼Œé‡æ–°æ£€æŸ¥: {fresh_task.get('status')}")
                    return
                
                # ğŸ”§ æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å·²ç»è¢«æ ‡è®°ä¸ºå®Œæˆï¼ˆé˜²æ­¢é‡å¤å¤„ç†ï¼‰
                from .workflow_execution_context import get_context_manager
                context_manager = get_context_manager()
                workflow_context = await context_manager.get_context(task['workflow_instance_id'])
                
                if workflow_context:
                    node_status = workflow_context.get_node_state(task['node_instance_id'])
                    if node_status == 'COMPLETED':
                        logger.warning(f"âš ï¸  èŠ‚ç‚¹ {task['node_instance_id']} å·²ç»å®Œæˆï¼Œè·³è¿‡é‡å¤å¤„ç†")
                        return
                
                # æ„é€ è¾“å‡ºæ•°æ®
                completion_output = {
                    "message": f"{task_type}ä»»åŠ¡å®Œæˆ",
                    "task_type": task_type,
                    "output_data": output_data,
                    "completed_at": updated_task.get('completed_at').isoformat() if updated_task.get('completed_at') else None,
                    "task_id": str(task['task_instance_id'])
                }
                
                # ğŸ”§ åŸå­æ“ä½œï¼šæ ‡è®°èŠ‚ç‚¹å®Œæˆå¹¶è§¦å‘ä¸‹æ¸¸æ£€æŸ¥
                logger.debug(f"   ğŸ¯ å¼€å§‹èŠ‚ç‚¹å®Œæˆæ ‡è®°å’Œä¸‹æ¸¸è§¦å‘")
                await self.context_manager.mark_node_completed(
                    workflow_instance_id=task['workflow_instance_id'],
                    node_id=node_info['node_id'],
                    node_instance_id=task['node_instance_id'],
                    output_data=completion_output
                )
                
                # ğŸ”§ é¢å¤–çš„ä¸‹æ¸¸æ£€æŸ¥ç¡®ä¿ï¼šå¼ºåˆ¶æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ä¸‹æ¸¸èŠ‚ç‚¹
                logger.debug(f"   ğŸ” æ‰§è¡Œé¢å¤–çš„ä¸‹æ¸¸èŠ‚ç‚¹æ£€æŸ¥")
                await self._check_downstream_nodes_for_task_creation(task['workflow_instance_id'])
                
                logger.info(f"âœ… [ç»Ÿä¸€ä»»åŠ¡å®Œæˆ-å¹¶å‘ä¿®å¤] {task_type}ä»»åŠ¡å®Œæˆå¤„ç†æˆåŠŸ")
                
        except Exception as e:
            logger.error(f"ğŸ’¥ [ç»Ÿä¸€ä»»åŠ¡å®Œæˆ] å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    async def _get_node_instance_data(self, node_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹å®ä¾‹æ•°æ®ï¼ˆåŒ…å«èŠ‚ç‚¹ç±»å‹ï¼‰"""
        try:
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            # ğŸ”§ ä½¿ç”¨å¸¦è¯¦ç»†ä¿¡æ¯çš„æŸ¥è¯¢æ–¹æ³•è·å–node_type
            return await node_repo.get_instance_with_details(node_instance_id)
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹å®ä¾‹æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _get_processor_info_for_node(self, node_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹çš„å¤„ç†å™¨ä¿¡æ¯"""
        try:
            from ..repositories.node.node_repository import NodeRepository
            node_repo = NodeRepository()
            
            # æŸ¥è¯¢èŠ‚ç‚¹åŠå…¶å…³è”çš„å¤„ç†å™¨ä¿¡æ¯
            query = """
                SELECT 
                    n.*,
                    np.processor_id,
                    p.type as processor_type,
                    p.name as processor_name,
                    COALESCE(u.username, a.agent_name) as processor_display_name
                FROM node n
                LEFT JOIN node_processor np ON n.node_id = np.node_id
                LEFT JOIN processor p ON np.processor_id = p.processor_id
                LEFT JOIN user u ON p.user_id = u.user_id AND p.type = 'human'
                LEFT JOIN agent a ON p.agent_id = a.agent_id AND p.type = 'agent'
                WHERE n.node_id = $1 AND n.is_deleted = FALSE
            """
            
            result = await node_repo.db.fetch_one(query, node_id)
            if result:
                return {
                    'processor_id': result.get('processor_id'),
                    'processor_type': result.get('processor_type', 'HUMAN'),
                    'processor_name': result.get('processor_name'),
                    'processor_display_name': result.get('processor_display_name'),
                    'node_name': result.get('name')
                }
            else:
                # å¦‚æœæ²¡æœ‰æ˜¾å¼å¤„ç†å™¨ï¼Œé»˜è®¤è¿”å›HUMANç±»å‹
                logger.warning(f"èŠ‚ç‚¹ {node_id} æ²¡æœ‰åˆ†é…å¤„ç†å™¨ï¼Œä½¿ç”¨é»˜è®¤HUMANç±»å‹")
                return {
                    'processor_id': None,
                    'processor_type': 'HUMAN',
                    'processor_name': None,
                    'processor_display_name': None,
                    'node_name': 'Unknown'
                }
                
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹å¤„ç†å™¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _auto_complete_system_node_with_context(self, workflow_context, node_instance_id: uuid.UUID, node_type: str):
        """ä½¿ç”¨æŒ‡å®šä¸Šä¸‹æ–‡è‡ªåŠ¨å®Œæˆç³»ç»ŸèŠ‚ç‚¹ï¼ˆSTART/ENDèŠ‚ç‚¹ï¼‰"""
        try:
            # è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            node_instance_data = await self._get_node_instance_data(node_instance_id)
            if not node_instance_data:
                logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯: {node_instance_id}")
                return
            
            workflow_instance_id = node_instance_data['workflow_instance_id']
            node_id = node_instance_data['node_id']
            
            # ç”Ÿæˆç³»ç»ŸèŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®
            from datetime import datetime
            output_data = {
                'message': f'{node_type} node completed automatically',
                'node_type': node_type,
                'completion_time': str(datetime.now()),
                'auto_completed': True
            }
            
            # ğŸ”§ ç‰¹æ®Šå¤„ç†STARTèŠ‚ç‚¹ï¼šæ·»åŠ ä»»åŠ¡æè¿°å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
            if node_type.upper() == 'START':
                # è·å–èŠ‚ç‚¹çš„ä»»åŠ¡æè¿°
                node_data = await self.node_repo.get_node_by_id(node_id)
                if node_data:
                    task_description = node_data.get('task_description', '')
                    output_data.update({
                        'message': 'STARTèŠ‚ç‚¹è‡ªåŠ¨å®Œæˆ',
                        'task_description': task_description,
                        'completed_at': datetime.utcnow().isoformat()
                    })
                
                # æ·»åŠ å·¥ä½œæµä¸Šä¸‹æ–‡ä¿¡æ¯
                try:
                    global_data = workflow_context.execution_context.get('global_data', {})
                    workflow_context_data = global_data.get('workflow_context_data', {})
                    
                    if workflow_context_data:
                        output_data['workflow_context'] = {
                            'subdivision_context': workflow_context_data.get('subdivision_context'),
                            'subdivision_id': workflow_context_data.get('subdivision_id'),
                            'execution_type': workflow_context_data.get('execution_type'),
                            'source': 'task_subdivision_workflow'
                        }
                        logger.info(f"ğŸ“‹ STARTèŠ‚ç‚¹æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯: {workflow_context_data}")
                except Exception as ctx_error:
                    logger.error(f"âš ï¸ STARTèŠ‚ç‚¹æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯å¤±è´¥: {ctx_error}")
            
            # ä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡æ ‡è®°èŠ‚ç‚¹å®Œæˆ
            await workflow_context.mark_node_completed(node_id, node_instance_id, output_data)
            
            logger.info(f"âœ… [{node_type}èŠ‚ç‚¹] è‡ªåŠ¨å®Œæˆ: {node_instance_id}")
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨å®Œæˆç³»ç»ŸèŠ‚ç‚¹å¤±è´¥: {e}")
            raise

    async def _auto_complete_system_node(self, node_instance_id: uuid.UUID, node_type: str):
        """ğŸ”§ è‡ªåŠ¨å®Œæˆç³»ç»ŸèŠ‚ç‚¹ï¼ˆSTART/ENDèŠ‚ç‚¹ï¼‰"""
        try:
            # è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            node_instance_data = await self._get_node_instance_data(node_instance_id)
            if not node_instance_data:
                logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯: {node_instance_id}")
                return
            
            workflow_instance_id = node_instance_data['workflow_instance_id']
            node_id = node_instance_data['node_id']
            
            # ç”Ÿæˆç³»ç»ŸèŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®
            from datetime import datetime
            output_data = {
                'message': f'{node_type} node completed automatically',
                'node_type': node_type,
                'completion_time': str(datetime.now()),
                'auto_completed': True
            }
            
            from ..services.workflow_execution_context import get_context_manager
            context_manager = get_context_manager()
            
            # ç›´æ¥æ ‡è®°èŠ‚ç‚¹å®Œæˆ
            await context_manager.mark_node_completed(
                workflow_instance_id, node_id, node_instance_id, output_data
            )
            
            logger.info(f"âœ… [{node_type}èŠ‚ç‚¹] è‡ªåŠ¨å®Œæˆ: {node_instance_id}")
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨å®Œæˆç³»ç»ŸèŠ‚ç‚¹å¤±è´¥: {e}")
            raise
    
    async def _create_task_for_node(self, node_instance_id: uuid.UUID, workflow_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """ä¸ºèŠ‚ç‚¹åŠ¨æ€åˆ›å»ºä»»åŠ¡å®ä¾‹"""
        try:
            # è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯
            node_instance_data = await self._get_node_instance_data(node_instance_id)
            if not node_instance_data:
                logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹å®ä¾‹ä¿¡æ¯: {node_instance_id}")
                return None
            
            node_id = node_instance_data['node_id']
            
            # è·å–èŠ‚ç‚¹å®šä¹‰ä¿¡æ¯ï¼ˆåŒ…å«å¤„ç†å™¨ä¿¡æ¯ï¼‰
            from ..repositories.node.node_repository import NodeRepository
            node_repo = NodeRepository()
            node_data = await node_repo.get_node_by_id(node_id)
            if not node_data:
                logger.error(f"æ— æ³•è·å–èŠ‚ç‚¹å®šä¹‰ä¿¡æ¯: {node_id}")
                return None
            
            # è·å–å¤„ç†å™¨ä¿¡æ¯
            processor_info = await self._get_processor_info_for_node(node_id)
            if not processor_info:
                logger.error(f"èŠ‚ç‚¹ {node_id} æ²¡æœ‰åˆ†é…å¤„ç†å™¨ï¼Œæ— æ³•åˆ›å»ºä»»åŠ¡")
                return None
            
            # è·å–å·¥ä½œæµå®ä¾‹ä¿¡æ¯ï¼ˆç”¨äºåˆ†é…ç”¨æˆ·ï¼‰
            workflow_instance = await self.workflow_instance_repo.get_instance_by_id(workflow_instance_id)
            executor_id = workflow_instance.get('executor_id') if workflow_instance else None
            
            from ..models.instance import TaskInstanceStatus, TaskInstanceType
            import uuid
            from ..utils.helpers import now_utc
            
            # ç¡®å®šä»»åŠ¡ç±»å‹å’Œåˆ†é…å¯¹è±¡ - ğŸ”§ ä¿®å¤ None å€¼å¤„ç†
            processor_type_raw = processor_info.get('processor_type')
            processor_type = (processor_type_raw or 'HUMAN').upper()
            
            logger.debug(f"[åŠ¨æ€ä»»åŠ¡] å¤„ç†å™¨ç±»å‹: {processor_type_raw} -> {processor_type}")
            
            if processor_type == 'HUMAN':
                task_type = TaskInstanceType.HUMAN
                assigned_user_id = executor_id  # åˆ†é…ç»™å·¥ä½œæµæ‰§è¡Œè€…
                assigned_agent_id = None
            elif processor_type == 'AGENT':
                task_type = TaskInstanceType.AGENT
                assigned_user_id = None
                assigned_agent_id = processor_info.get('processor_id')
            else:
                task_type = TaskInstanceType.HUMAN
                assigned_user_id = executor_id
                assigned_agent_id = None
            
            # ğŸ”§ ä½¿ç”¨TaskInstanceCreateæ¨¡å‹è€Œä¸æ˜¯dict
            from ..models.instance import TaskInstanceCreate
            
            task_data = TaskInstanceCreate(
                node_instance_id=node_instance_id,
                workflow_instance_id=workflow_instance_id,
                processor_id=processor_info['processor_id'],
                task_title=f"{node_data['name']} - åŠ¨æ€ä»»åŠ¡",
                task_description=node_data.get('task_description') or f"{node_data['name']}èŠ‚ç‚¹çš„æ‰§è¡Œä»»åŠ¡",
                task_type=task_type,
                assigned_user_id=assigned_user_id,
                assigned_agent_id=assigned_agent_id
            )
            
            logger.debug(f"[åŠ¨æ€ä»»åŠ¡] åˆ›å»ºTaskInstanceCreateå¯¹è±¡: {task_data.task_title}")
            
            # åˆ›å»ºä»»åŠ¡å®ä¾‹
            result = await self.task_instance_repo.create_task(task_data)
            if result:
                logger.info(f"âœ… [åŠ¨æ€ä»»åŠ¡] æˆåŠŸåˆ›å»ºä»»åŠ¡: {task_data.task_title}")
                
                # ğŸ”§ Critical Fix: åˆ›å»ºä»»åŠ¡åç«‹å³ä¼ é€’èŠ‚ç‚¹å®ä¾‹é™„ä»¶
                try:
                    from ..services.file_association_service import FileAssociationService, AttachmentType
                    file_service = FileAssociationService()
                    
                    task_instance_id = result.get('task_instance_id')
                    if task_instance_id:
                        # è·å–èŠ‚ç‚¹å®ä¾‹çš„æ‰€æœ‰é™„ä»¶
                        node_files = await file_service.get_node_instance_files(node_instance_id)
                        
                        # å°†æ¯ä¸ªé™„ä»¶å…³è”åˆ°ä»»åŠ¡å®ä¾‹
                        for file_info in node_files:
                            file_id = file_info['file_id']
                            attachment_type_str = file_info.get('attachment_type', 'input')
                            
                            # è½¬æ¢å­—ç¬¦ä¸²ä¸ºAttachmentTypeæšä¸¾
                            try:
                                attachment_type = AttachmentType(attachment_type_str.upper())
                            except ValueError:
                                attachment_type = AttachmentType.INPUT
                            
                            await file_service.associate_task_instance_file(
                                task_instance_id=uuid.UUID(task_instance_id),
                                file_id=uuid.UUID(file_id),
                                uploaded_by=uuid.UUID(assigned_user_id) if assigned_user_id else uuid.UUID('00000000-0000-0000-0000-000000000000'),
                                attachment_type=attachment_type
                            )
                        
                        logger.info(f"ğŸ“ [é™„ä»¶ä¼ é€’] ä»»åŠ¡ {task_instance_id} ç»§æ‰¿äº† {len(node_files)} ä¸ªèŠ‚ç‚¹é™„ä»¶")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ä»»åŠ¡é™„ä»¶ä¼ é€’å¤±è´¥: {e}")
                
                if task_type == TaskInstanceType.HUMAN:
                    logger.info(f"ğŸ¯ [åŠ¨æ€ä»»åŠ¡] äººå·¥ä»»åŠ¡å·²åˆ†é…ç»™ç”¨æˆ·: {assigned_user_id}")
                return result
            else:
                logger.error(f"âŒ [åŠ¨æ€ä»»åŠ¡] ä»»åŠ¡åˆ›å»ºå¤±è´¥")
                return None
                
        except Exception as e:
            logger.error(f"ä¸ºèŠ‚ç‚¹åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
            return None

    async def _propagate_task_cancellation(self, task_id: uuid.UUID, task_data: Dict[str, Any], cancel_reason: Optional[str] = None):
        """
        ç®€æ´çš„çŠ¶æ€å‘ä¸Šä¼ æ’­ï¼šä»»åŠ¡å–æ¶ˆ -> èŠ‚ç‚¹å–æ¶ˆ -> å·¥ä½œæµæ£€æŸ¥
        Linuså¼è®¾è®¡ï¼šæ²¡æœ‰ç‰¹æ®Šæƒ…å†µï¼Œå°±æ˜¯ç®€å•çš„çŠ¶æ€æ›´æ–°é“¾
        """
        try:
            logger.info(f"ğŸ”„ [çŠ¶æ€ä¼ æ’­] å¼€å§‹ä¼ æ’­ä»»åŠ¡å–æ¶ˆçŠ¶æ€: {task_id}")

            # 1. è·å–èŠ‚ç‚¹å®ä¾‹ID
            node_instance_id = task_data.get('node_instance_id')
            workflow_instance_id = task_data.get('workflow_instance_id')

            if not node_instance_id or not workflow_instance_id:
                logger.warning(f"âš ï¸ [çŠ¶æ€ä¼ æ’­] ç¼ºå°‘å¿…è¦ä¿¡æ¯ï¼Œè·³è¿‡ä¼ æ’­")
                logger.warning(f"   - node_instance_id: {node_instance_id}")
                logger.warning(f"   - workflow_instance_id: {workflow_instance_id}")
                return

            # 2. æ ‡è®°èŠ‚ç‚¹å®ä¾‹ä¸ºå–æ¶ˆçŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            from ..models.instance import NodeInstanceUpdate, NodeInstanceStatus

            node_repo = NodeInstanceRepository()
            node_update = NodeInstanceUpdate(
                status=NodeInstanceStatus.CANCELLED,
                error_message=cancel_reason or "ä»»åŠ¡è¢«å–æ¶ˆ",
                completed_at=now_utc()
            )

            node_result = await node_repo.update_node_instance(node_instance_id, node_update)
            if node_result:
                logger.info(f"âœ… [çŠ¶æ€ä¼ æ’­] èŠ‚ç‚¹å®ä¾‹å·²æ ‡è®°ä¸ºå–æ¶ˆ: {node_instance_id}")

                # 3. é€šçŸ¥æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
                try:
                    await self.context_manager.mark_node_failed(
                        workflow_instance_id,
                        task_data.get('node_id'),  # éœ€è¦node_idï¼Œä¸æ˜¯node_instance_id
                        node_instance_id,
                        {"message": cancel_reason or "ä»»åŠ¡è¢«å–æ¶ˆ", "type": "user_cancelled"}
                    )
                    logger.info(f"âœ… [çŠ¶æ€ä¼ æ’­] ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²æ›´æ–°èŠ‚ç‚¹çŠ¶æ€")
                except Exception as ctx_error:
                    logger.warning(f"âš ï¸ [çŠ¶æ€ä¼ æ’­] æ›´æ–°ä¸Šä¸‹æ–‡å¤±è´¥: {ctx_error}")

                # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆæ•´ä¸ªå·¥ä½œæµ
                await self._check_and_update_workflow_status(workflow_instance_id)

            else:
                logger.error(f"âŒ [çŠ¶æ€ä¼ æ’­] æ›´æ–°èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {node_instance_id}")

        except Exception as e:
            logger.error(f"âŒ [çŠ¶æ€ä¼ æ’­] ä¼ æ’­ä»»åŠ¡å–æ¶ˆçŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"   - å †æ ˆ: {traceback.format_exc()}")

    async def _check_and_update_workflow_status(self, workflow_instance_id: uuid.UUID):
        """
        ğŸ”§ Linuså¼ä¿®å¤ï¼šç®€åŒ–å·¥ä½œæµçŠ¶æ€æ£€æŸ¥é€»è¾‘

        æ¶ˆé™¤ç‰¹æ®Šæƒ…å†µï¼šåªæœ‰ä¸€ä¸ªç®€å•çš„è§„åˆ™
        - æ²¡æœ‰è¿è¡Œä¸­èŠ‚ç‚¹ = å·¥ä½œæµç»“æŸ
        - æ ¹æ®å®Œæˆ/å¤±è´¥/å–æ¶ˆèŠ‚ç‚¹å†³å®šæœ€ç»ˆçŠ¶æ€
        """
        try:
            logger.info(f"ğŸ” [å·¥ä½œæµçŠ¶æ€æ£€æŸ¥] æ£€æŸ¥å·¥ä½œæµ: {workflow_instance_id}")

            # è·å–æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹çŠ¶æ€
            from ..repositories.instance.node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()

            nodes = await node_repo.get_instances_by_workflow_instance(workflow_instance_id)
            if not nodes:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°èŠ‚ç‚¹å®ä¾‹ï¼Œè·³è¿‡çŠ¶æ€æ£€æŸ¥")
                return

            # ç»Ÿè®¡èŠ‚ç‚¹çŠ¶æ€ - ç®€å•åˆ†ç±»
            total_nodes = len(nodes)
            completed_nodes = 0
            failed_nodes = 0
            cancelled_nodes = 0
            running_nodes = 0

            for node in nodes:
                status = node.get('status', '').lower()
                if status == 'completed':
                    completed_nodes += 1
                elif status == 'failed':
                    failed_nodes += 1
                elif status == 'cancelled':
                    cancelled_nodes += 1
                elif status in ['running', 'pending', 'waiting']:
                    running_nodes += 1

            logger.info(f"ğŸ“Š èŠ‚ç‚¹çŠ¶æ€: æ€»={total_nodes}, å®Œæˆ={completed_nodes}, å¤±è´¥={failed_nodes}, å–æ¶ˆ={cancelled_nodes}, è¿è¡Œä¸­={running_nodes}")

            # ğŸ”§ LinusåŸåˆ™ï¼šç®€å•çš„åˆ¤æ–­é€»è¾‘ï¼Œæ— ç‰¹æ®Šæƒ…å†µ
            if running_nodes == 0:
                # æ²¡æœ‰è¿è¡Œä¸­çš„èŠ‚ç‚¹ï¼Œå·¥ä½œæµç»“æŸ
                from ..repositories.instance.workflow_instance_repository import WorkflowInstanceRepository
                from ..models.instance import WorkflowInstanceUpdate, WorkflowInstanceStatus

                workflow_repo = WorkflowInstanceRepository()

                # å†³å®šæœ€ç»ˆçŠ¶æ€ï¼šä¼˜å…ˆçº§ å–æ¶ˆ > å¤±è´¥ > å®Œæˆ
                if cancelled_nodes > 0:
                    final_status = WorkflowInstanceStatus.CANCELLED
                    status_name = "å·²å–æ¶ˆ"
                elif failed_nodes > 0:
                    final_status = WorkflowInstanceStatus.FAILED
                    status_name = "å·²å¤±è´¥"
                else:
                    final_status = WorkflowInstanceStatus.COMPLETED
                    status_name = "å·²å®Œæˆ"

                from ..utils.helpers import now_utc
                workflow_update = WorkflowInstanceUpdate(
                    status=final_status,
                    completed_at=now_utc()
                )

                result = await workflow_repo.update_instance(workflow_instance_id, workflow_update)
                if result:
                    logger.info(f"âœ… å·¥ä½œæµçŠ¶æ€å·²æ›´æ–°: {status_name}")
                else:
                    logger.error(f"âŒ æ›´æ–°å·¥ä½œæµçŠ¶æ€å¤±è´¥")
            else:
                logger.info(f"â„¹ï¸ å·¥ä½œæµä»åœ¨è¿è¡Œä¸­ ({running_nodes} ä¸ªèŠ‚ç‚¹)")

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"   å †æ ˆ: {traceback.format_exc()}")


# å…¨å±€æ‰§è¡Œå¼•æ“å®ä¾‹
execution_engine = ExecutionEngine()