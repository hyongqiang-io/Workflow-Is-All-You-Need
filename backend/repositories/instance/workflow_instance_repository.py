"""
å·¥ä½œæµå®ä¾‹æ•°æ®è®¿é—®å±‚
Workflow Instance Repository
"""

import uuid
import json
from typing import Optional, Dict, Any, List
from datetime import datetime
from loguru import logger


# ä½¿ç”¨helpersä¸­çš„é€šç”¨JSONåºåˆ—åŒ–å™¨

from ..base import BaseRepository
from ...models.instance import (
    WorkflowInstance, WorkflowInstanceCreate, WorkflowInstanceUpdate, 
    WorkflowInstanceStatus, ExecutionStatistics
)
from ...utils.helpers import now_utc, safe_json_dumps, safe_json_serializer
from ...utils.database import db_manager


class WorkflowInstanceRepository(BaseRepository[WorkflowInstance]):
    """å·¥ä½œæµå®ä¾‹æ•°æ®è®¿é—®å±‚"""
    
    def __init__(self):
        super().__init__("workflow_instance")
    
    async def create_instance(self, instance_data: WorkflowInstanceCreate) -> Optional[Dict[str, Any]]:
        """åˆ›å»ºå·¥ä½œæµå®ä¾‹"""
        logger.info(f"ğŸš€ å¼€å§‹åˆ›å»ºå·¥ä½œæµå®ä¾‹: {instance_data.workflow_instance_name}")
        logger.info(f"   - å·¥ä½œæµBase ID: {instance_data.workflow_base_id}")
        logger.info(f"   - æ‰§è¡Œè€…ID: {instance_data.executor_id}")
        logger.info(f"   - è¾“å…¥æ•°æ®: {len(instance_data.input_data or {})} ä¸ªå­—æ®µ")
        
        try:
            # è·å–å½“å‰ç‰ˆæœ¬çš„å·¥ä½œæµ
            logger.info(f"ğŸ” æŸ¥è¯¢å·¥ä½œæµä¿¡æ¯: {instance_data.workflow_base_id}")
            workflow_query = """
                SELECT workflow_id, workflow_base_id, name 
                FROM workflow 
                WHERE workflow_base_id = $1 AND is_current_version = TRUE AND is_deleted = FALSE
            """
            workflow = await self.db.fetch_one(workflow_query, instance_data.workflow_base_id)
            if not workflow:
                logger.error(f"âŒ å·¥ä½œæµä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤: {instance_data.workflow_base_id}")
                raise ValueError("å·¥ä½œæµä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤")
            
            logger.info(f"âœ… æ‰¾åˆ°å·¥ä½œæµ: {workflow['name']} (ID: {workflow['workflow_id']})")
            
            # å‡†å¤‡å®ä¾‹æ•°æ®
            workflow_instance_id = uuid.uuid4()
            data = {
                "workflow_instance_id": workflow_instance_id,  # Primary key
                "workflow_base_id": instance_data.workflow_base_id,
                "workflow_id": workflow['workflow_id'],
                "executor_id": instance_data.executor_id,
                "created_by": instance_data.executor_id,  # åˆ›å»ºè€…è®¾ä¸ºæ‰§è¡Œè€…
                "workflow_instance_name": instance_data.workflow_instance_name,
                "input_data": safe_json_dumps(instance_data.input_data or {}),
                "context_data": safe_json_dumps(instance_data.context_data or {}),
                "status": WorkflowInstanceStatus.PENDING.value,
                "created_at": now_utc(),
                "updated_at": now_utc(),
                "is_deleted": False
            }
            
            logger.info(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: å·¥ä½œæµå®ä¾‹ {workflow_instance_id}")
            logger.info(f"   - å®ä¾‹åç§°: {instance_data.workflow_instance_name}")
            logger.info(f"   - åˆå§‹çŠ¶æ€: {WorkflowInstanceStatus.PENDING.value}")
            logger.info(f"   - å…³è”å·¥ä½œæµ: {workflow['name']}")
            
            result = await self.create(data)
            if result:
                logger.info(f"âœ… å·¥ä½œæµå®ä¾‹åˆ›å»ºæˆåŠŸ!")
                logger.info(f"   - å®ä¾‹ID: {result['workflow_instance_id']}")
                logger.info(f"   - å®ä¾‹åç§°: {instance_data.workflow_instance_name}")
                logger.info(f"   - çŠ¶æ€: {result.get('status', 'unknown')}")
                logger.info(f"   - åˆ›å»ºæ—¶é—´: {result.get('created_at', 'unknown')}")
                
                # è§£æJSONå­—æ®µ
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
            else:
                logger.error(f"âŒ å·¥ä½œæµå®ä¾‹åˆ›å»ºå¤±è´¥: æ•°æ®åº“è¿”å›ç©ºç»“æœ")
            
            return result
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            logger.error(f"   - å®ä¾‹åç§°: {instance_data.workflow_instance_name}")
            logger.error(f"   - å·¥ä½œæµBase ID: {instance_data.workflow_base_id}")
            import traceback
            logger.error(f"   - é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def get_instance_by_id(self, instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–å·¥ä½œæµå®ä¾‹"""
        try:
            query = """
                SELECT wi.*, w.name as workflow_name, u.username as executor_name
                FROM workflow_instance wi
                LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                LEFT JOIN "user" u ON u.user_id = wi.executor_id
                WHERE wi.workflow_instance_id = $1 AND wi.is_deleted = FALSE
            """
            result = await self.db.fetch_one(query, instance_id)
            if result:
                # è§£æJSONå­—æ®µ
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
            
            return result
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def update_instance(self, instance_id: uuid.UUID, 
                             update_data: WorkflowInstanceUpdate) -> Optional[Dict[str, Any]]:
        """æ›´æ–°å·¥ä½œæµå®ä¾‹"""
        try:
            # å‡†å¤‡æ›´æ–°æ•°æ®
            data = {"updated_at": now_utc()}
            
            if update_data.workflow_instance_name is not None:
                data["workflow_instance_name"] = update_data.workflow_instance_name
            if update_data.status is not None:
                data["status"] = update_data.status.value
            if update_data.input_data is not None:
                data["input_data"] = safe_json_dumps(update_data.input_data)
            if update_data.context_data is not None:
                data["context_data"] = safe_json_dumps(update_data.context_data)
            if update_data.output_data is not None:
                data["output_data"] = safe_json_dumps(update_data.output_data)
            if update_data.error_message is not None:
                data["error_message"] = update_data.error_message
            if update_data.current_node_id is not None:
                data["current_node_id"] = update_data.current_node_id
            
            # æ–°å¢ç»“æ„åŒ–è¾“å‡ºå­—æ®µæ”¯æŒ
            if hasattr(update_data, 'execution_summary') and update_data.execution_summary is not None:
                data["execution_summary"] = safe_json_dumps(update_data.execution_summary)
            if hasattr(update_data, 'quality_metrics') and update_data.quality_metrics is not None:
                data["quality_metrics"] = safe_json_dumps(update_data.quality_metrics)
            if hasattr(update_data, 'data_lineage') and update_data.data_lineage is not None:
                data["data_lineage"] = safe_json_dumps(update_data.data_lineage)
            if hasattr(update_data, 'output_summary') and update_data.output_summary is not None:
                # å°†Pydanticæ¨¡å‹è½¬æ¢ä¸ºå­—å…¸å†åºåˆ—åŒ–
                output_summary_dict = update_data.output_summary.dict() if hasattr(update_data.output_summary, 'dict') else update_data.output_summary
                data["output_summary"] = safe_json_dumps(output_summary_dict)
            
            # æ ¹æ®çŠ¶æ€æ›´æ–°æ—¶é—´æˆ³
            if update_data.status == WorkflowInstanceStatus.RUNNING:
                data["started_at"] = now_utc()
            elif update_data.status in [WorkflowInstanceStatus.COMPLETED, 
                                       WorkflowInstanceStatus.FAILED, 
                                       WorkflowInstanceStatus.CANCELLED]:
                data["completed_at"] = now_utc()
            
            if not data or len(data) == 1:  # åªæœ‰updated_at
                return await self.get_instance_by_id(instance_id)
            
            logger.info(f"ğŸ’¾ æ›´æ–°å·¥ä½œæµå®ä¾‹æ•°æ®åº“è®°å½•: {instance_id}")
            result = await self.update(instance_id, data, "workflow_instance_id")
            if result:
                logger.info(f"âœ… å·¥ä½œæµå®ä¾‹çŠ¶æ€æ›´æ–°æˆåŠŸ!")
                logger.info(f"   - å®ä¾‹ID: {instance_id}")
                logger.info(f"   - æ–°çŠ¶æ€: {update_data.status}")
                if update_data.status == WorkflowInstanceStatus.RUNNING:
                    logger.info(f"   - ğŸƒ å·¥ä½œæµå¼€å§‹æ‰§è¡Œ")
                elif update_data.status == WorkflowInstanceStatus.COMPLETED:
                    logger.info(f"   - ğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                elif update_data.status == WorkflowInstanceStatus.FAILED:
                    logger.info(f"   - âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                    if update_data.error_message:
                        logger.error(f"   - é”™è¯¯ä¿¡æ¯: {update_data.error_message}")
                elif update_data.status == WorkflowInstanceStatus.CANCELLED:
                    logger.info(f"   - â¹ï¸ å·¥ä½œæµè¢«å–æ¶ˆ")
                return await self.get_instance_by_id(instance_id)
            
            return None
        except Exception as e:
            logger.error(f"æ›´æ–°å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def get_instances_by_executor(self, executor_id: uuid.UUID, 
                                      status: Optional[WorkflowInstanceStatus] = None,
                                      limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–æ‰§è¡Œè€…çš„å·¥ä½œæµå®ä¾‹åˆ—è¡¨"""
        try:
            if status:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.executor_id
                    WHERE wi.executor_id = $1 AND wi.status = $2 AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $3
                """
                results = await self.db.fetch_all(query, executor_id, status.value, limit)
            else:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.executor_id
                    WHERE wi.executor_id = $1 AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $2
                """
                results = await self.db.fetch_all(query, executor_id, limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡Œè€…å®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def get_instances_by_workflow(self, workflow_base_id: uuid.UUID, 
                                       limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å·¥ä½œæµçš„æ‰€æœ‰å®ä¾‹"""
        try:
            query = """
                SELECT wi.*, w.name as workflow_name, u.username as executor_name
                FROM workflow_instance wi
                LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                LEFT JOIN "user" u ON u.user_id = wi.executor_id
                WHERE wi.workflow_base_id = $1 AND wi.is_deleted = FALSE
                ORDER BY wi.created_at DESC
                LIMIT $2
            """
            results = await self.db.fetch_all(query, workflow_base_id, limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµå®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def get_running_instances(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è¿è¡Œä¸­çš„å®ä¾‹"""
        try:
            query = """
                SELECT wi.*, w.name as workflow_name, u.username as executor_name
                FROM workflow_instance wi
                LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                LEFT JOIN "user" u ON u.user_id = wi.executor_id
                WHERE wi.status = $1 AND wi.is_deleted = FALSE
                ORDER BY wi.started_at ASC
                LIMIT $2
            """
            results = await self.db.fetch_all(query, WorkflowInstanceStatus.RUNNING.value, limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"è·å–è¿è¡Œä¸­å®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def delete_instance(self, instance_id: uuid.UUID, soft_delete: bool = True) -> bool:
        """åˆ é™¤å·¥ä½œæµå®ä¾‹"""
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹åˆ é™¤å·¥ä½œæµå®ä¾‹: {instance_id}")
            logger.info(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            
            # é¦–å…ˆæ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨
            logger.info(f"ğŸ” æ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨")
            existing_instance = await self.get_instance_by_id(instance_id)
            if not existing_instance:
                logger.warning(f"âš ï¸ è¦åˆ é™¤çš„å®ä¾‹ä¸å­˜åœ¨: {instance_id}")
                return False
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ°å¾…åˆ é™¤å®ä¾‹:")
            logger.info(f"   - å®ä¾‹åç§°: {existing_instance.get('workflow_instance_name', 'æœªå‘½å')}")
            logger.info(f"   - å½“å‰çŠ¶æ€: {existing_instance.get('status')}")
            logger.info(f"   - is_deleted: {existing_instance.get('is_deleted', False)}")
            
            if existing_instance.get('is_deleted', False):
                logger.warning(f"âš ï¸ å®ä¾‹å·²è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œè·³è¿‡æ“ä½œ")
                return True
            
            if soft_delete:
                logger.info(f"ğŸ¯ æ‰§è¡Œè½¯åˆ é™¤æ“ä½œ")
                logger.info(f"   - è°ƒç”¨ self.update({instance_id}, {{'is_deleted': True}}, 'workflow_instance_id')")
                
                try:
                    result = await self.update(instance_id, {
                        "is_deleted": True,
                        "updated_at": now_utc()
                    }, "workflow_instance_id")
                    
                    logger.info(f"   - update()æ–¹æ³•è¿”å›ç»“æœ: {result}")
                    success = result is not None
                    
                    if success:
                        logger.info(f"âœ… è½¯åˆ é™¤æˆåŠŸ")
                        # éªŒè¯åˆ é™¤ç»“æœ
                        verification = await self.get_instance_by_id(instance_id)
                        if verification:
                            logger.info(f"   - éªŒè¯: å®ä¾‹ä»å¯æŸ¥è¯¢åˆ° (è½¯åˆ é™¤)")
                            logger.info(f"   - éªŒè¯: is_deleted = {verification.get('is_deleted')}")
                        else:
                            logger.info(f"   - éªŒè¯: å®ä¾‹å·²ä¸å¯æŸ¥è¯¢ (è½¯åˆ é™¤ç”Ÿæ•ˆ)")
                    else:
                        logger.error(f"âŒ è½¯åˆ é™¤å¤±è´¥: update()è¿”å›None")
                        
                except Exception as update_error:
                    logger.error(f"âŒ æ‰§è¡Œè½¯åˆ é™¤æ—¶å‘ç”Ÿå¼‚å¸¸:")
                    logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(update_error).__name__}")
                    logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(update_error)}")
                    import traceback
                    logger.error(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    raise update_error
                    
            else:
                logger.info(f"ğŸ¯ æ‰§è¡Œç¡¬åˆ é™¤æ“ä½œ")
                query = "DELETE FROM workflow_instance WHERE workflow_instance_id = $1"
                logger.info(f"   - SQLæŸ¥è¯¢: {query}")
                logger.info(f"   - å‚æ•°: {instance_id}")
                
                try:
                    result = await self.db.execute(query, instance_id)
                    logger.info(f"   - æ•°æ®åº“æ‰§è¡Œç»“æœ: {result}")
                    success = "1" in result
                    
                    if success:
                        logger.info(f"âœ… ç¡¬åˆ é™¤æˆåŠŸ")
                    else:
                        logger.error(f"âŒ ç¡¬åˆ é™¤å¤±è´¥: æ‰§è¡Œç»“æœä¸åŒ…å«'1'")
                        
                except Exception as delete_error:
                    logger.error(f"âŒ æ‰§è¡Œç¡¬åˆ é™¤æ—¶å‘ç”Ÿå¼‚å¸¸:")
                    logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(delete_error).__name__}")
                    logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(delete_error)}")
                    import traceback
                    logger.error(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    raise delete_error
            
            if success:
                action = "è½¯åˆ é™¤" if soft_delete else "ç¡¬åˆ é™¤"
                logger.info(f"âœ… {action}å·¥ä½œæµå®ä¾‹æˆåŠŸ: {instance_id}")
            else:
                action = "è½¯åˆ é™¤" if soft_delete else "ç¡¬åˆ é™¤"
                logger.error(f"âŒ {action}å·¥ä½œæµå®ä¾‹å¤±è´¥: {instance_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å·¥ä½œæµå®ä¾‹æ€»ä½“å¼‚å¸¸:")
            logger.error(f"   - å®ä¾‹ID: {instance_id}")
            logger.error(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            import traceback
            logger.error(f"   - å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def delete_instance_cascade(self, instance_id: uuid.UUID, soft_delete: bool = True) -> Dict[str, Any]:
        """çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹åŠå…¶ç›¸å…³æ•°æ®"""
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹: {instance_id} (è½¯åˆ é™¤: {soft_delete})")
            
            # ç»Ÿè®¡åˆ é™¤çš„æ•°æ®é‡
            deletion_stats = {
                'workflow_instance_id': str(instance_id),
                'deleted_tasks': 0,
                'deleted_nodes': 0,
                'deleted_workflow': False,
                'soft_delete': soft_delete
            }
            
            # 1. é¦–å…ˆåˆ é™¤æ‰€æœ‰ä»»åŠ¡å®ä¾‹
            logger.info(f"ğŸ“‹ æ­¥éª¤1: åˆ é™¤ç›¸å…³ä»»åŠ¡å®ä¾‹")
            from .task_instance_repository import TaskInstanceRepository
            task_repo = TaskInstanceRepository()
            deleted_tasks = await task_repo.delete_tasks_by_workflow_instance(instance_id, soft_delete)
            deletion_stats['deleted_tasks'] = deleted_tasks
            
            # 2. ç„¶ååˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
            logger.info(f"ğŸ“‹ æ­¥éª¤2: åˆ é™¤ç›¸å…³èŠ‚ç‚¹å®ä¾‹")
            from .node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            deleted_nodes = await node_repo.delete_nodes_by_workflow_instance(instance_id, soft_delete)
            deletion_stats['deleted_nodes'] = deleted_nodes
            
            # 3. æœ€ååˆ é™¤å·¥ä½œæµå®ä¾‹æœ¬èº«
            logger.info(f"ğŸ“‹ æ­¥éª¤3: åˆ é™¤å·¥ä½œæµå®ä¾‹")
            workflow_deleted = await self.delete_instance(instance_id, soft_delete)
            deletion_stats['deleted_workflow'] = workflow_deleted
            
            if workflow_deleted:
                logger.info(f"âœ… çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹æˆåŠŸ:")
                logger.info(f"   - å·¥ä½œæµå®ä¾‹: {instance_id}")
                logger.info(f"   - åˆ é™¤çš„ä»»åŠ¡: {deleted_tasks} ä¸ª")
                logger.info(f"   - åˆ é™¤çš„èŠ‚ç‚¹å®ä¾‹: {deleted_nodes} ä¸ª")
                logger.info(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            else:
                logger.error(f"âŒ çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥: {instance_id}")
            
            return deletion_stats
            
        except Exception as e:
            logger.error(f"çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def get_execution_statistics(self, instance_id: uuid.UUID) -> Optional[ExecutionStatistics]:
        """è·å–å®ä¾‹æ‰§è¡Œç»Ÿè®¡"""
        try:
            # è·å–èŠ‚ç‚¹ç»Ÿè®¡
            node_stats_query = """
                SELECT 
                    COUNT(*) as total_nodes,
                    COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_nodes,
                    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_nodes,
                    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_nodes
                FROM node_instance 
                WHERE workflow_instance_id = $1 AND is_deleted = FALSE
            """
            node_stats = await self.db.fetch_one(node_stats_query, instance_id)
            
            # è·å–ä»»åŠ¡ç»Ÿè®¡
            task_stats_query = """
                SELECT 
                    COUNT(*) as total_tasks,
                    COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_tasks,
                    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_tasks,
                    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_tasks,
                    COUNT(CASE WHEN task_type = 'human' THEN 1 END) as human_tasks,
                    COUNT(CASE WHEN task_type = 'agent' THEN 1 END) as agent_tasks,
                    COUNT(CASE WHEN task_type = 'mixed' THEN 1 END) as mixed_tasks,
                    AVG(actual_duration) as average_task_duration
                FROM task_instance 
                WHERE workflow_instance_id = $1 AND is_deleted = FALSE
            """
            task_stats = await self.db.fetch_one(task_stats_query, instance_id)
            
            # è·å–æ€»æ‰§è¡Œæ—¶é—´
            instance = await self.get_instance_by_id(instance_id)
            if not instance:
                return None
            
            total_execution_time = None
            if instance.get('started_at') and instance.get('completed_at'):
                start_time = datetime.fromisoformat(instance['started_at'].replace('Z', '+00:00'))
                end_time = datetime.fromisoformat(instance['completed_at'].replace('Z', '+00:00'))
                total_execution_time = int((end_time - start_time).total_seconds() / 60)
            
            return ExecutionStatistics(
                workflow_instance_id=instance_id,
                total_nodes=node_stats['total_nodes'] or 0,
                completed_nodes=node_stats['completed_nodes'] or 0,
                failed_nodes=node_stats['failed_nodes'] or 0,
                pending_nodes=node_stats['pending_nodes'] or 0,
                total_tasks=task_stats['total_tasks'] or 0,
                completed_tasks=task_stats['completed_tasks'] or 0,
                failed_tasks=task_stats['failed_tasks'] or 0,
                pending_tasks=task_stats['pending_tasks'] or 0,
                human_tasks=task_stats['human_tasks'] or 0,
                agent_tasks=task_stats['agent_tasks'] or 0,
                mixed_tasks=task_stats['mixed_tasks'] or 0,
                average_task_duration=float(task_stats['average_task_duration']) if task_stats['average_task_duration'] else None,
                total_execution_time=total_execution_time
            )
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡Œç»Ÿè®¡å¤±è´¥: {e}")
            raise
    
    async def search_instances(self, keyword: str, executor_id: Optional[uuid.UUID] = None, 
                              limit: int = 50) -> List[Dict[str, Any]]:
        """æœç´¢å·¥ä½œæµå®ä¾‹"""
        try:
            if executor_id:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.executor_id
                    WHERE (wi.workflow_instance_name ILIKE $1 OR w.name ILIKE $1) 
                          AND wi.executor_id = $2 AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $3
                """
                results = await self.db.fetch_all(query, f"%{keyword}%", executor_id, limit)
            else:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.executor_id
                    WHERE (wi.workflow_instance_name ILIKE $1 OR w.name ILIKE $1) 
                          AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $2
                """
                results = await self.db.fetch_all(query, f"%{keyword}%", limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"æœç´¢å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    # ================== å·¥ä½œæµæŒä¹…åŒ–åŠŸèƒ½ ==================
    
    async def save_workflow_context_snapshot(self, 
                                           workflow_instance_id: uuid.UUID,
                                           context_data: Dict[str, Any],
                                           node_states: Dict[str, Any] = None,
                                           snapshot_type: str = 'auto',
                                           description: str = None,
                                           created_by: uuid.UUID = None) -> Optional[uuid.UUID]:
        """ä¿å­˜å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§"""
        try:
            logger.info(f"ğŸ’¾ [æŒä¹…åŒ–] ä¿å­˜å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§: {workflow_instance_id}")
            logger.info(f"   - å¿«ç…§ç±»å‹: {snapshot_type}")
            logger.info(f"   - æè¿°: {description or 'è‡ªåŠ¨å¿«ç…§'}")
            
            snapshot_id = uuid.uuid4()
            
            # è·å–å½“å‰å·¥ä½œæµçŠ¶æ€ç”¨äºå¿«ç…§
            workflow_instance = await self.get_instance_by_id(workflow_instance_id)
            if not workflow_instance:
                logger.error(f"âŒ [æŒä¹…åŒ–] å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {workflow_instance_id}")
                return None
            
            current_status = workflow_instance.get('status', 'unknown')
            
            data = {
                'snapshot_id': snapshot_id,
                'workflow_instance_id': workflow_instance_id,
                'snapshot_type': snapshot_type,
                'context_data': safe_json_dumps(context_data or {}),
                'node_states': safe_json_dumps(node_states or {}),
                'execution_state': current_status,
                'created_at': now_utc(),
                'created_by': created_by,
                'description': description,
                'is_deleted': False
            }
            
            # MySQLå…¼å®¹çš„æ’å…¥è¯­å¥
            insert_query = """
                INSERT INTO workflow_context_snapshot 
                (snapshot_id, workflow_instance_id, snapshot_type, context_data, 
                 node_states, execution_state, created_at, created_by, description, is_deleted)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            await self.db.execute(insert_query, 
                                snapshot_id, workflow_instance_id, snapshot_type,
                                data['context_data'], data['node_states'], 
                                current_status, data['created_at'], created_by, 
                                description, False)
            
            logger.info(f"âœ… [æŒä¹…åŒ–] å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§å·²ä¿å­˜: {snapshot_id}")
            return snapshot_id
            
        except Exception as e:
            logger.error(f"âŒ [æŒä¹…åŒ–] ä¿å­˜å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§å¤±è´¥: {e}")
            import traceback
            logger.error(f"   - é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None

    async def get_latest_context_snapshot(self, workflow_instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """è·å–æœ€æ–°çš„ä¸Šä¸‹æ–‡å¿«ç…§"""
        try:
            logger.debug(f"ğŸ“¸ [æŒä¹…åŒ–] è·å–æœ€æ–°ä¸Šä¸‹æ–‡å¿«ç…§: {workflow_instance_id}")
            
            query = """
                SELECT snapshot_id, workflow_instance_id, snapshot_type, 
                       context_data, node_states, execution_state, 
                       created_at, created_by, description
                FROM workflow_context_snapshot
                WHERE workflow_instance_id = %s AND is_deleted = FALSE
                ORDER BY created_at DESC
                LIMIT 1
            """
            
            result = await self.db.fetch_one(query, workflow_instance_id)
            
            if result:
                logger.debug(f"âœ… [æŒä¹…åŒ–] æ‰¾åˆ°æœ€æ–°å¿«ç…§: {result['snapshot_id']}")
                return dict(result)
            else:
                logger.debug(f"ğŸ“¸ [æŒä¹…åŒ–] æ²¡æœ‰æ‰¾åˆ°å¿«ç…§: {workflow_instance_id}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ [æŒä¹…åŒ–] è·å–æœ€æ–°ä¸Šä¸‹æ–‡å¿«ç…§å¤±è´¥: {e}")
            return None
    
    async def load_workflow_context_snapshot(self, 
                                           workflow_instance_id: uuid.UUID,
                                           snapshot_id: uuid.UUID = None) -> Optional[Dict[str, Any]]:
        """åŠ è½½å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§ï¼ˆé»˜è®¤åŠ è½½æœ€æ–°çš„ï¼‰"""
        try:
            if snapshot_id:
                # åŠ è½½æŒ‡å®šå¿«ç…§
                query = """
                    SELECT * FROM workflow_context_snapshot 
                    WHERE snapshot_id = %s AND workflow_instance_id = %s AND is_deleted = FALSE
                """
                result = await self.db.fetch_one(query, snapshot_id, workflow_instance_id)
            else:
                # åŠ è½½æœ€æ–°å¿«ç…§
                query = """
                    SELECT * FROM workflow_context_snapshot 
                    WHERE workflow_instance_id = %s AND is_deleted = FALSE
                    ORDER BY created_at DESC
                    LIMIT 1
                """
                result = await self.db.fetch_one(query, workflow_instance_id)
            
            if result:
                # è§£æJSONå­—æ®µ
                snapshot_data = dict(result)
                snapshot_data['context_data'] = json.loads(snapshot_data.get('context_data', '{}'))
                snapshot_data['node_states'] = json.loads(snapshot_data.get('node_states', '{}'))
                
                logger.info(f"âœ… [æŒä¹…åŒ–] æˆåŠŸåŠ è½½å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§")
                logger.info(f"   - å¿«ç…§ID: {snapshot_data.get('snapshot_id')}")
                logger.info(f"   - åˆ›å»ºæ—¶é—´: {snapshot_data.get('created_at')}")
                logger.info(f"   - å¿«ç…§ç±»å‹: {snapshot_data.get('snapshot_type')}")
                
                return snapshot_data
            else:
                logger.warning(f"âš ï¸ [æŒä¹…åŒ–] æœªæ‰¾åˆ°å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§: {workflow_instance_id}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ [æŒä¹…åŒ–] åŠ è½½å·¥ä½œæµä¸Šä¸‹æ–‡å¿«ç…§å¤±è´¥: {e}")
            return None
    
    async def log_workflow_event(self, 
                               workflow_instance_id: uuid.UUID,
                               event_type: str,
                               event_data: Dict[str, Any] = None,
                               node_instance_id: uuid.UUID = None,
                               task_instance_id: uuid.UUID = None,
                               user_id: uuid.UUID = None) -> Optional[uuid.UUID]:
        """è®°å½•å·¥ä½œæµäº‹ä»¶åˆ°äº‹ä»¶æ—¥å¿—"""
        try:
            event_id = uuid.uuid4()
            
            # è·å–ä¸‹ä¸€ä¸ªåºåˆ—å·ï¼ˆç®€åŒ–å®ç°ï¼‰
            sequence_number = int(datetime.now().timestamp() * 1000000)  # å¾®ç§’çº§æ—¶é—´æˆ³ä½œä¸ºåºåˆ—å·
            
            data = {
                'event_id': event_id,
                'workflow_instance_id': workflow_instance_id,
                'event_type': event_type,
                'event_data': safe_json_dumps(event_data or {}),
                'node_instance_id': node_instance_id,
                'task_instance_id': task_instance_id,
                'user_id': user_id,
                'timestamp': now_utc(),
                'sequence_number': sequence_number,
                'is_deleted': False
            }
            
            # MySQLå…¼å®¹çš„æ’å…¥è¯­å¥
            insert_query = """
                INSERT INTO workflow_instance_event_log 
                (event_id, workflow_instance_id, event_type, event_data, 
                 node_instance_id, task_instance_id, user_id, timestamp, 
                 sequence_number, is_deleted)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            await self.db.execute(insert_query,
                                event_id, workflow_instance_id, event_type,
                                data['event_data'], node_instance_id, 
                                task_instance_id, user_id, data['timestamp'],
                                sequence_number, False)
            
            logger.debug(f"ğŸ“ [äº‹ä»¶æ—¥å¿—] è®°å½•å·¥ä½œæµäº‹ä»¶: {event_type}")
            logger.debug(f"   - å·¥ä½œæµ: {workflow_instance_id}")
            logger.debug(f"   - äº‹ä»¶ID: {event_id}")
            
            return event_id
            
        except Exception as e:
            logger.error(f"âŒ [äº‹ä»¶æ—¥å¿—] è®°å½•å·¥ä½œæµäº‹ä»¶å¤±è´¥: {e}")
            return None
    
    async def get_workflow_event_history(self, 
                                       workflow_instance_id: uuid.UUID,
                                       event_types: List[str] = None,
                                       limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–å·¥ä½œæµäº‹ä»¶å†å²"""
        try:
            if event_types:
                placeholders = ', '.join(['%s'] * len(event_types))
                query = f"""
                    SELECT * FROM workflow_instance_event_log 
                    WHERE workflow_instance_id = %s 
                    AND event_type IN ({placeholders})
                    AND is_deleted = FALSE
                    ORDER BY sequence_number DESC
                    LIMIT %s
                """
                params = [workflow_instance_id] + event_types + [limit]
            else:
                query = """
                    SELECT * FROM workflow_instance_event_log 
                    WHERE workflow_instance_id = %s AND is_deleted = FALSE
                    ORDER BY sequence_number DESC
                    LIMIT %s
                """
                params = [workflow_instance_id, limit]
            
            results = await self.db.fetch_all(query, *params)
            
            # è§£æJSONå­—æ®µ
            formatted_events = []
            for result in results:
                event = dict(result)
                event['event_data'] = json.loads(event.get('event_data', '{}'))
                formatted_events.append(event)
            
            logger.debug(f"ğŸ“‹ [äº‹ä»¶å†å²] è·å–åˆ° {len(formatted_events)} æ¡äº‹ä»¶è®°å½•")
            return formatted_events
            
        except Exception as e:
            logger.error(f"âŒ [äº‹ä»¶å†å²] è·å–å·¥ä½œæµäº‹ä»¶å†å²å¤±è´¥: {e}")
            return []
    
    async def update_workflow_persistence_fields(self, 
                                                workflow_instance_id: uuid.UUID,
                                                execution_context: Dict[str, Any] = None,
                                                node_dependencies: Dict[str, Any] = None,
                                                completed_nodes: List[str] = None,
                                                execution_trace: List[Dict[str, Any]] = None,
                                                instance_metadata: Dict[str, Any] = None) -> bool:
        """æ›´æ–°å·¥ä½œæµå®ä¾‹çš„æŒä¹…åŒ–å­—æ®µ"""
        try:
            update_fields = []
            params = []
            
            if execution_context is not None:
                update_fields.append('execution_context = %s')
                params.append(safe_json_dumps(execution_context))
            
            if node_dependencies is not None:
                update_fields.append('node_dependencies = %s')
                params.append(safe_json_dumps(node_dependencies))
            
            if completed_nodes is not None:
                update_fields.append('completed_nodes = %s')
                params.append(safe_json_dumps(completed_nodes))
            
            if execution_trace is not None:
                update_fields.append('execution_trace = %s')
                params.append(safe_json_dumps(execution_trace))
            
            if instance_metadata is not None:
                update_fields.append('instance_metadata = %s')
                params.append(safe_json_dumps(instance_metadata))
            
            if not update_fields:
                return True  # æ²¡æœ‰å­—æ®µéœ€è¦æ›´æ–°
            
            # æ·»åŠ æ›´æ–°æ—¶é—´
            update_fields.append('updated_at = %s')
            params.append(now_utc())
            
            # æ·»åŠ WHEREæ¡ä»¶å‚æ•°
            params.append(workflow_instance_id)
            
            query = f"""
                UPDATE workflow_instance 
                SET {', '.join(update_fields)}
                WHERE workflow_instance_id = %s
            """
            
            result = await self.db.execute(query, *params)
            
            logger.debug(f"ğŸ“Š [æŒä¹…åŒ–] æ›´æ–°å·¥ä½œæµæŒä¹…åŒ–å­—æ®µæˆåŠŸ: {workflow_instance_id}")
            logger.debug(f"   - æ›´æ–°å­—æ®µ: {len(update_fields) - 1} ä¸ª")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ [æŒä¹…åŒ–] æ›´æ–°å·¥ä½œæµæŒä¹…åŒ–å­—æ®µå¤±è´¥: {e}")
            return False
    
    async def auto_save_workflow_context(self, 
                                       workflow_instance_id: uuid.UUID,
                                       context_data: Dict[str, Any],
                                       save_threshold: int = 10) -> bool:
        """è‡ªåŠ¨ä¿å­˜å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°å¿«ç…§ï¼ˆç®€åŒ–é€»è¾‘ï¼šæ¯éš”ä¸€å®šæ“ä½œæ•°ä¿å­˜ä¸€æ¬¡ï¼‰
            should_save = True  # ç®€åŒ–å®ç°ï¼Œæ¯æ¬¡è°ƒç”¨éƒ½ä¿å­˜
            
            if should_save:
                snapshot_id = await self.save_workflow_context_snapshot(
                    workflow_instance_id=workflow_instance_id,
                    context_data=context_data,
                    snapshot_type='auto',
                    description=f'è‡ªåŠ¨ä¿å­˜ - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'
                )
                
                if snapshot_id:
                    # è®°å½•è‡ªåŠ¨ä¿å­˜äº‹ä»¶
                    await self.log_workflow_event(
                        workflow_instance_id=workflow_instance_id,
                        event_type='context_auto_saved',
                        event_data={'snapshot_id': str(snapshot_id)}
                    )
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ [è‡ªåŠ¨ä¿å­˜] è‡ªåŠ¨ä¿å­˜å·¥ä½œæµä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return False
    
    async def cleanup_old_snapshots(self, 
                                  workflow_instance_id: uuid.UUID,
                                  keep_count: int = 50) -> int:
        """æ¸…ç†æ—§çš„å·¥ä½œæµå¿«ç…§ï¼Œä¿ç•™æœ€æ–°çš„Nä¸ª"""
        try:
            # æŸ¥è¯¢è¦åˆ é™¤çš„å¿«ç…§
            query = """
                SELECT snapshot_id FROM workflow_context_snapshot 
                WHERE workflow_instance_id = %s AND is_deleted = FALSE
                ORDER BY created_at DESC
                LIMIT 18446744073709551615 OFFSET %s
            """
            
            old_snapshots = await self.db.fetch_all(query, workflow_instance_id, keep_count)
            
            if old_snapshots:
                # æ‰¹é‡æ ‡è®°ä¸ºåˆ é™¤
                snapshot_ids = [row['snapshot_id'] for row in old_snapshots]
                placeholders = ', '.join(['%s'] * len(snapshot_ids))
                
                delete_query = f"""
                    UPDATE workflow_context_snapshot 
                    SET is_deleted = TRUE, updated_at = %s
                    WHERE snapshot_id IN ({placeholders})
                """
                
                await self.db.execute(delete_query, now_utc(), *snapshot_ids)
                
                logger.info(f"ğŸ§¹ [æ¸…ç†] æ¸…ç†äº† {len(snapshot_ids)} ä¸ªæ—§å¿«ç…§")
                return len(snapshot_ids)
            
            return 0
            
        except Exception as e:
            logger.error(f"âŒ [æ¸…ç†] æ¸…ç†æ—§å¿«ç…§å¤±è´¥: {e}")
            return 0