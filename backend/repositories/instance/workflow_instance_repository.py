"""
å·¥ä½œæµå®ä¾‹æ•°æ®è®¿é—®å±‚
Workflow Instance Repository
"""

import uuid
import json
from typing import Optional, Dict, Any, List
from datetime import datetime
from loguru import logger


# ä½¿ç”¨helpersä¸­çš„é€šç”¨JSONåºåˆ—åŒ–å™¨

from ..base import BaseRepository
from ...models.instance import (
    WorkflowInstance, WorkflowInstanceCreate, WorkflowInstanceUpdate, 
    WorkflowInstanceStatus, ExecutionStatistics
)
from ...utils.helpers import now_utc, safe_json_dumps, safe_json_serializer


class WorkflowInstanceRepository(BaseRepository[WorkflowInstance]):
    """å·¥ä½œæµå®ä¾‹æ•°æ®è®¿é—®å±‚"""
    
    def __init__(self):
        super().__init__("workflow_instance")
    
    async def create_instance(self, instance_data: WorkflowInstanceCreate) -> Optional[Dict[str, Any]]:
        """åˆ›å»ºå·¥ä½œæµå®ä¾‹"""
        logger.info(f"ğŸš€ å¼€å§‹åˆ›å»ºå·¥ä½œæµå®ä¾‹: {instance_data.instance_name}")
        logger.info(f"   - å·¥ä½œæµBase ID: {instance_data.workflow_base_id}")
        logger.info(f"   - æ‰§è¡Œè€…ID: {instance_data.executor_id}")
        logger.info(f"   - è¾“å…¥æ•°æ®: {len(instance_data.input_data or {})} ä¸ªå­—æ®µ")
        
        try:
            # è·å–å½“å‰ç‰ˆæœ¬çš„å·¥ä½œæµ
            logger.info(f"ğŸ” æŸ¥è¯¢å·¥ä½œæµä¿¡æ¯: {instance_data.workflow_base_id}")
            workflow_query = """
                SELECT workflow_id, workflow_base_id, name 
                FROM workflow 
                WHERE workflow_base_id = $1 AND is_current_version = TRUE AND is_deleted = FALSE
            """
            workflow = await self.db.fetch_one(workflow_query, instance_data.workflow_base_id)
            if not workflow:
                logger.error(f"âŒ å·¥ä½œæµä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤: {instance_data.workflow_base_id}")
                raise ValueError("å·¥ä½œæµä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤")
            
            logger.info(f"âœ… æ‰¾åˆ°å·¥ä½œæµ: {workflow['name']} (ID: {workflow['workflow_id']})")
            
            # å‡†å¤‡å®ä¾‹æ•°æ®
            workflow_instance_id = uuid.uuid4()
            data = {
                "workflow_instance_id": workflow_instance_id,  # Primary key
                "workflow_base_id": instance_data.workflow_base_id,
                "workflow_id": workflow['workflow_id'],
                "trigger_user_id": instance_data.executor_id,  # Map executor_id to trigger_user_id for database
                "workflow_instance_name": instance_data.instance_name,
                "input_data": safe_json_dumps(instance_data.input_data or {}),
                "context_data": safe_json_dumps(instance_data.context_data or {}),
                "status": WorkflowInstanceStatus.PENDING.value,
                "created_at": now_utc(),
                "updated_at": now_utc(),
                "is_deleted": False
            }
            
            logger.info(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: å·¥ä½œæµå®ä¾‹ {workflow_instance_id}")
            logger.info(f"   - å®ä¾‹åç§°: {instance_data.instance_name}")
            logger.info(f"   - åˆå§‹çŠ¶æ€: {WorkflowInstanceStatus.PENDING.value}")
            logger.info(f"   - å…³è”å·¥ä½œæµ: {workflow['name']}")
            
            result = await self.create(data)
            if result:
                logger.info(f"âœ… å·¥ä½œæµå®ä¾‹åˆ›å»ºæˆåŠŸ!")
                logger.info(f"   - å®ä¾‹ID: {result['workflow_instance_id']}")
                logger.info(f"   - å®ä¾‹åç§°: {instance_data.instance_name}")
                logger.info(f"   - çŠ¶æ€: {result.get('status', 'unknown')}")
                logger.info(f"   - åˆ›å»ºæ—¶é—´: {result.get('created_at', 'unknown')}")
                
                # è§£æJSONå­—æ®µ
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
            else:
                logger.error(f"âŒ å·¥ä½œæµå®ä¾‹åˆ›å»ºå¤±è´¥: æ•°æ®åº“è¿”å›ç©ºç»“æœ")
            
            return result
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            logger.error(f"   - å®ä¾‹åç§°: {instance_data.instance_name}")
            logger.error(f"   - å·¥ä½œæµBase ID: {instance_data.workflow_base_id}")
            import traceback
            logger.error(f"   - é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def get_instance_by_id(self, instance_id: uuid.UUID) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–å·¥ä½œæµå®ä¾‹"""
        try:
            query = """
                SELECT wi.*, w.name as workflow_name, u.username as executor_name
                FROM workflow_instance wi
                LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                WHERE wi.workflow_instance_id = $1 AND wi.is_deleted = FALSE
            """
            result = await self.db.fetch_one(query, instance_id)
            if result:
                # è§£æJSONå­—æ®µ
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
            
            return result
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def update_instance(self, instance_id: uuid.UUID, 
                             update_data: WorkflowInstanceUpdate) -> Optional[Dict[str, Any]]:
        """æ›´æ–°å·¥ä½œæµå®ä¾‹"""
        try:
            # å‡†å¤‡æ›´æ–°æ•°æ®
            data = {"updated_at": now_utc()}
            
            if update_data.instance_name is not None:
                data["workflow_instance_name"] = update_data.instance_name
            if update_data.status is not None:
                data["status"] = update_data.status.value
            if update_data.input_data is not None:
                data["input_data"] = safe_json_dumps(update_data.input_data)
            if update_data.context_data is not None:
                data["context_data"] = safe_json_dumps(update_data.context_data)
            if update_data.output_data is not None:
                data["output_data"] = safe_json_dumps(update_data.output_data)
            if update_data.error_message is not None:
                data["error_message"] = update_data.error_message
            if update_data.current_node_id is not None:
                data["current_node_id"] = update_data.current_node_id
            
            # æ–°å¢ç»“æ„åŒ–è¾“å‡ºå­—æ®µæ”¯æŒ
            if hasattr(update_data, 'execution_summary') and update_data.execution_summary is not None:
                data["execution_summary"] = safe_json_dumps(update_data.execution_summary)
            if hasattr(update_data, 'quality_metrics') and update_data.quality_metrics is not None:
                data["quality_metrics"] = safe_json_dumps(update_data.quality_metrics)
            if hasattr(update_data, 'data_lineage') and update_data.data_lineage is not None:
                data["data_lineage"] = safe_json_dumps(update_data.data_lineage)
            if hasattr(update_data, 'output_summary') and update_data.output_summary is not None:
                # å°†Pydanticæ¨¡å‹è½¬æ¢ä¸ºå­—å…¸å†åºåˆ—åŒ–
                output_summary_dict = update_data.output_summary.dict() if hasattr(update_data.output_summary, 'dict') else update_data.output_summary
                data["output_summary"] = safe_json_dumps(output_summary_dict)
            
            # æ ¹æ®çŠ¶æ€æ›´æ–°æ—¶é—´æˆ³
            if update_data.status == WorkflowInstanceStatus.RUNNING:
                data["started_at"] = now_utc()
            elif update_data.status in [WorkflowInstanceStatus.COMPLETED, 
                                       WorkflowInstanceStatus.FAILED, 
                                       WorkflowInstanceStatus.CANCELLED]:
                data["completed_at"] = now_utc()
            
            if not data or len(data) == 1:  # åªæœ‰updated_at
                return await self.get_instance_by_id(instance_id)
            
            logger.info(f"ğŸ’¾ æ›´æ–°å·¥ä½œæµå®ä¾‹æ•°æ®åº“è®°å½•: {instance_id}")
            result = await self.update(instance_id, data, "workflow_instance_id")
            if result:
                logger.info(f"âœ… å·¥ä½œæµå®ä¾‹çŠ¶æ€æ›´æ–°æˆåŠŸ!")
                logger.info(f"   - å®ä¾‹ID: {instance_id}")
                logger.info(f"   - æ–°çŠ¶æ€: {update_data.status}")
                if update_data.status == WorkflowInstanceStatus.RUNNING:
                    logger.info(f"   - ğŸƒ å·¥ä½œæµå¼€å§‹æ‰§è¡Œ")
                elif update_data.status == WorkflowInstanceStatus.COMPLETED:
                    logger.info(f"   - ğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                elif update_data.status == WorkflowInstanceStatus.FAILED:
                    logger.info(f"   - âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                    if update_data.error_message:
                        logger.error(f"   - é”™è¯¯ä¿¡æ¯: {update_data.error_message}")
                elif update_data.status == WorkflowInstanceStatus.CANCELLED:
                    logger.info(f"   - â¹ï¸ å·¥ä½œæµè¢«å–æ¶ˆ")
                return await self.get_instance_by_id(instance_id)
            
            return None
        except Exception as e:
            logger.error(f"æ›´æ–°å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def get_instances_by_executor(self, executor_id: uuid.UUID, 
                                      status: Optional[WorkflowInstanceStatus] = None,
                                      limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–æ‰§è¡Œè€…çš„å·¥ä½œæµå®ä¾‹åˆ—è¡¨"""
        try:
            if status:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                    WHERE wi.trigger_user_id = $1 AND wi.status = $2 AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $3
                """
                results = await self.db.fetch_all(query, executor_id, status.value, limit)
            else:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                    WHERE wi.trigger_user_id = $1 AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $2
                """
                results = await self.db.fetch_all(query, executor_id, limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡Œè€…å®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def get_instances_by_workflow(self, workflow_base_id: uuid.UUID, 
                                       limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å·¥ä½œæµçš„æ‰€æœ‰å®ä¾‹"""
        try:
            query = """
                SELECT wi.*, w.name as workflow_name, u.username as executor_name
                FROM workflow_instance wi
                LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                WHERE wi.workflow_base_id = $1 AND wi.is_deleted = FALSE
                ORDER BY wi.created_at DESC
                LIMIT $2
            """
            results = await self.db.fetch_all(query, workflow_base_id, limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"è·å–å·¥ä½œæµå®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def get_running_instances(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è¿è¡Œä¸­çš„å®ä¾‹"""
        try:
            query = """
                SELECT wi.*, w.name as workflow_name, u.username as executor_name
                FROM workflow_instance wi
                LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                WHERE wi.status = $1 AND wi.is_deleted = FALSE
                ORDER BY wi.started_at ASC
                LIMIT $2
            """
            results = await self.db.fetch_all(query, WorkflowInstanceStatus.RUNNING.value, limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"è·å–è¿è¡Œä¸­å®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    async def delete_instance(self, instance_id: uuid.UUID, soft_delete: bool = True) -> bool:
        """åˆ é™¤å·¥ä½œæµå®ä¾‹"""
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹åˆ é™¤å·¥ä½œæµå®ä¾‹: {instance_id}")
            logger.info(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            
            # é¦–å…ˆæ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨
            logger.info(f"ğŸ” æ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨")
            existing_instance = await self.get_instance_by_id(instance_id)
            if not existing_instance:
                logger.warning(f"âš ï¸ è¦åˆ é™¤çš„å®ä¾‹ä¸å­˜åœ¨: {instance_id}")
                return False
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ°å¾…åˆ é™¤å®ä¾‹:")
            logger.info(f"   - å®ä¾‹åç§°: {existing_instance.get('instance_name', 'æœªå‘½å')}")
            logger.info(f"   - å½“å‰çŠ¶æ€: {existing_instance.get('status')}")
            logger.info(f"   - is_deleted: {existing_instance.get('is_deleted', False)}")
            
            if existing_instance.get('is_deleted', False):
                logger.warning(f"âš ï¸ å®ä¾‹å·²è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œè·³è¿‡æ“ä½œ")
                return True
            
            if soft_delete:
                logger.info(f"ğŸ¯ æ‰§è¡Œè½¯åˆ é™¤æ“ä½œ")
                logger.info(f"   - è°ƒç”¨ self.update({instance_id}, {{'is_deleted': True}}, 'workflow_instance_id')")
                
                try:
                    result = await self.update(instance_id, {
                        "is_deleted": True,
                        "updated_at": now_utc()
                    }, "workflow_instance_id")
                    
                    logger.info(f"   - update()æ–¹æ³•è¿”å›ç»“æœ: {result}")
                    success = result is not None
                    
                    if success:
                        logger.info(f"âœ… è½¯åˆ é™¤æˆåŠŸ")
                        # éªŒè¯åˆ é™¤ç»“æœ
                        verification = await self.get_instance_by_id(instance_id)
                        if verification:
                            logger.info(f"   - éªŒè¯: å®ä¾‹ä»å¯æŸ¥è¯¢åˆ° (è½¯åˆ é™¤)")
                            logger.info(f"   - éªŒè¯: is_deleted = {verification.get('is_deleted')}")
                        else:
                            logger.info(f"   - éªŒè¯: å®ä¾‹å·²ä¸å¯æŸ¥è¯¢ (è½¯åˆ é™¤ç”Ÿæ•ˆ)")
                    else:
                        logger.error(f"âŒ è½¯åˆ é™¤å¤±è´¥: update()è¿”å›None")
                        
                except Exception as update_error:
                    logger.error(f"âŒ æ‰§è¡Œè½¯åˆ é™¤æ—¶å‘ç”Ÿå¼‚å¸¸:")
                    logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(update_error).__name__}")
                    logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(update_error)}")
                    import traceback
                    logger.error(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    raise update_error
                    
            else:
                logger.info(f"ğŸ¯ æ‰§è¡Œç¡¬åˆ é™¤æ“ä½œ")
                query = "DELETE FROM workflow_instance WHERE workflow_instance_id = $1"
                logger.info(f"   - SQLæŸ¥è¯¢: {query}")
                logger.info(f"   - å‚æ•°: {instance_id}")
                
                try:
                    result = await self.db.execute(query, instance_id)
                    logger.info(f"   - æ•°æ®åº“æ‰§è¡Œç»“æœ: {result}")
                    success = "1" in result
                    
                    if success:
                        logger.info(f"âœ… ç¡¬åˆ é™¤æˆåŠŸ")
                    else:
                        logger.error(f"âŒ ç¡¬åˆ é™¤å¤±è´¥: æ‰§è¡Œç»“æœä¸åŒ…å«'1'")
                        
                except Exception as delete_error:
                    logger.error(f"âŒ æ‰§è¡Œç¡¬åˆ é™¤æ—¶å‘ç”Ÿå¼‚å¸¸:")
                    logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(delete_error).__name__}")
                    logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(delete_error)}")
                    import traceback
                    logger.error(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    raise delete_error
            
            if success:
                action = "è½¯åˆ é™¤" if soft_delete else "ç¡¬åˆ é™¤"
                logger.info(f"âœ… {action}å·¥ä½œæµå®ä¾‹æˆåŠŸ: {instance_id}")
            else:
                action = "è½¯åˆ é™¤" if soft_delete else "ç¡¬åˆ é™¤"
                logger.error(f"âŒ {action}å·¥ä½œæµå®ä¾‹å¤±è´¥: {instance_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å·¥ä½œæµå®ä¾‹æ€»ä½“å¼‚å¸¸:")
            logger.error(f"   - å®ä¾‹ID: {instance_id}")
            logger.error(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            logger.error(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"   - å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            import traceback
            logger.error(f"   - å®Œæ•´å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            raise
    
    async def delete_instance_cascade(self, instance_id: uuid.UUID, soft_delete: bool = True) -> Dict[str, Any]:
        """çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹åŠå…¶ç›¸å…³æ•°æ®"""
        try:
            logger.info(f"ğŸ—‘ï¸ å¼€å§‹çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹: {instance_id} (è½¯åˆ é™¤: {soft_delete})")
            
            # ç»Ÿè®¡åˆ é™¤çš„æ•°æ®é‡
            deletion_stats = {
                'workflow_instance_id': str(instance_id),
                'deleted_tasks': 0,
                'deleted_nodes': 0,
                'deleted_workflow': False,
                'soft_delete': soft_delete
            }
            
            # 1. é¦–å…ˆåˆ é™¤æ‰€æœ‰ä»»åŠ¡å®ä¾‹
            logger.info(f"ğŸ“‹ æ­¥éª¤1: åˆ é™¤ç›¸å…³ä»»åŠ¡å®ä¾‹")
            from .task_instance_repository import TaskInstanceRepository
            task_repo = TaskInstanceRepository()
            deleted_tasks = await task_repo.delete_tasks_by_workflow_instance(instance_id, soft_delete)
            deletion_stats['deleted_tasks'] = deleted_tasks
            
            # 2. ç„¶ååˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
            logger.info(f"ğŸ“‹ æ­¥éª¤2: åˆ é™¤ç›¸å…³èŠ‚ç‚¹å®ä¾‹")
            from .node_instance_repository import NodeInstanceRepository
            node_repo = NodeInstanceRepository()
            deleted_nodes = await node_repo.delete_nodes_by_workflow_instance(instance_id, soft_delete)
            deletion_stats['deleted_nodes'] = deleted_nodes
            
            # 3. æœ€ååˆ é™¤å·¥ä½œæµå®ä¾‹æœ¬èº«
            logger.info(f"ğŸ“‹ æ­¥éª¤3: åˆ é™¤å·¥ä½œæµå®ä¾‹")
            workflow_deleted = await self.delete_instance(instance_id, soft_delete)
            deletion_stats['deleted_workflow'] = workflow_deleted
            
            if workflow_deleted:
                logger.info(f"âœ… çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹æˆåŠŸ:")
                logger.info(f"   - å·¥ä½œæµå®ä¾‹: {instance_id}")
                logger.info(f"   - åˆ é™¤çš„ä»»åŠ¡: {deleted_tasks} ä¸ª")
                logger.info(f"   - åˆ é™¤çš„èŠ‚ç‚¹å®ä¾‹: {deleted_nodes} ä¸ª")
                logger.info(f"   - åˆ é™¤æ–¹å¼: {'è½¯åˆ é™¤' if soft_delete else 'ç¡¬åˆ é™¤'}")
            else:
                logger.error(f"âŒ çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥: {instance_id}")
            
            return deletion_stats
            
        except Exception as e:
            logger.error(f"çº§è”åˆ é™¤å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise
    
    async def get_execution_statistics(self, instance_id: uuid.UUID) -> Optional[ExecutionStatistics]:
        """è·å–å®ä¾‹æ‰§è¡Œç»Ÿè®¡"""
        try:
            # è·å–èŠ‚ç‚¹ç»Ÿè®¡
            node_stats_query = """
                SELECT 
                    COUNT(*) as total_nodes,
                    COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_nodes,
                    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_nodes,
                    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_nodes
                FROM node_instance 
                WHERE workflow_instance_id = $1 AND is_deleted = FALSE
            """
            node_stats = await self.db.fetch_one(node_stats_query, instance_id)
            
            # è·å–ä»»åŠ¡ç»Ÿè®¡
            task_stats_query = """
                SELECT 
                    COUNT(*) as total_tasks,
                    COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_tasks,
                    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_tasks,
                    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_tasks,
                    COUNT(CASE WHEN task_type = 'human' THEN 1 END) as human_tasks,
                    COUNT(CASE WHEN task_type = 'agent' THEN 1 END) as agent_tasks,
                    COUNT(CASE WHEN task_type = 'mixed' THEN 1 END) as mixed_tasks,
                    AVG(actual_duration) as average_task_duration
                FROM task_instance 
                WHERE workflow_instance_id = $1 AND is_deleted = FALSE
            """
            task_stats = await self.db.fetch_one(task_stats_query, instance_id)
            
            # è·å–æ€»æ‰§è¡Œæ—¶é—´
            instance = await self.get_instance_by_id(instance_id)
            if not instance:
                return None
            
            total_execution_time = None
            if instance.get('started_at') and instance.get('completed_at'):
                start_time = datetime.fromisoformat(instance['started_at'].replace('Z', '+00:00'))
                end_time = datetime.fromisoformat(instance['completed_at'].replace('Z', '+00:00'))
                total_execution_time = int((end_time - start_time).total_seconds() / 60)
            
            return ExecutionStatistics(
                workflow_instance_id=instance_id,
                total_nodes=node_stats['total_nodes'] or 0,
                completed_nodes=node_stats['completed_nodes'] or 0,
                failed_nodes=node_stats['failed_nodes'] or 0,
                pending_nodes=node_stats['pending_nodes'] or 0,
                total_tasks=task_stats['total_tasks'] or 0,
                completed_tasks=task_stats['completed_tasks'] or 0,
                failed_tasks=task_stats['failed_tasks'] or 0,
                pending_tasks=task_stats['pending_tasks'] or 0,
                human_tasks=task_stats['human_tasks'] or 0,
                agent_tasks=task_stats['agent_tasks'] or 0,
                mixed_tasks=task_stats['mixed_tasks'] or 0,
                average_task_duration=float(task_stats['average_task_duration']) if task_stats['average_task_duration'] else None,
                total_execution_time=total_execution_time
            )
        except Exception as e:
            logger.error(f"è·å–æ‰§è¡Œç»Ÿè®¡å¤±è´¥: {e}")
            raise
    
    async def search_instances(self, keyword: str, executor_id: Optional[uuid.UUID] = None, 
                              limit: int = 50) -> List[Dict[str, Any]]:
        """æœç´¢å·¥ä½œæµå®ä¾‹"""
        try:
            if executor_id:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                    WHERE (wi.workflow_instance_name ILIKE $1 OR w.name ILIKE $1) 
                          AND wi.trigger_user_id = $2 AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $3
                """
                results = await self.db.fetch_all(query, f"%{keyword}%", executor_id, limit)
            else:
                query = """
                    SELECT wi.*, w.name as workflow_name, u.username as executor_name
                    FROM workflow_instance wi
                    LEFT JOIN workflow w ON w.workflow_id = wi.workflow_id
                    LEFT JOIN "user" u ON u.user_id = wi.trigger_user_id
                    WHERE (wi.workflow_instance_name ILIKE $1 OR w.name ILIKE $1) 
                          AND wi.is_deleted = FALSE
                    ORDER BY wi.created_at DESC
                    LIMIT $2
                """
                results = await self.db.fetch_all(query, f"%{keyword}%", limit)
            
            # è§£æJSONå­—æ®µ
            formatted_results = []
            for result in results:
                result = dict(result)
                result['input_data'] = json.loads(result.get('input_data', '{}'))
                result['context_data'] = json.loads(result.get('context_data', '{}'))
                if result.get('output_data'):
                    result['output_data'] = json.loads(result['output_data'])
                
                # è§£ææ–°å¢çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ
                if result.get('execution_summary'):
                    result['execution_summary'] = json.loads(result['execution_summary'])
                if result.get('quality_metrics'):
                    result['quality_metrics'] = json.loads(result['quality_metrics'])
                if result.get('data_lineage'):
                    result['data_lineage'] = json.loads(result['data_lineage'])
                if result.get('output_summary'):
                    result['output_summary'] = json.loads(result['output_summary'])
                    
                formatted_results.append(result)
            
            return formatted_results
        except Exception as e:
            logger.error(f"æœç´¢å·¥ä½œæµå®ä¾‹å¤±è´¥: {e}")
            raise